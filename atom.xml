<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Coding and Talking</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://moqimoqidea.github.io/"/>
  <updated>2018-11-26T12:58:29.732Z</updated>
  <id>http://moqimoqidea.github.io/</id>
  
  <author>
    <name>moqi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark 调优解析</title>
    <link href="http://moqimoqidea.github.io/2017/09/07/Spark-Tuning-Analysis/"/>
    <id>http://moqimoqidea.github.io/2017/09/07/Spark-Tuning-Analysis/</id>
    <published>2017-09-07T09:20:48.000Z</published>
    <updated>2018-11-26T12:58:29.732Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要解析部分 Spark 调优的技术点。</p><a id="more"></a> <h1 id="调优基本原则"><a href="#调优基本原则" class="headerlink" title="调优基本原则"></a>调优基本原则</h1><h2 id="基本概念和原则"><a href="#基本概念和原则" class="headerlink" title="基本概念和原则"></a>基本概念和原则</h2><p>首先，要搞清楚Spark的几个基本概念和原则，否则系统的性能调优无从谈起：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/01-Spark-Architecture.png?raw=true" alt="01-Spark-Architecture"></p><p>每一台host上面可以并行N个worker，每一个worker下面可以并行M个executor，task们会被分配到executor上面去执行。Stage指的是一组并行运行的task，stage内部是不能出现shuffle的，因为shuffle的就像篱笆一样阻止了并行task的运行，遇到shuffle就意味着到了stage的边界。<br>CPU的core数量，每个executor可以占用一个或多个core，可以通过观察CPU的使用率变化来了解计算资源的使用情况，例如，很常见的一种浪费是一个executor占用了多个core，但是总的CPU使用率却不高（因为一个executor并不总能充分利用多核的能力），这个时候可以考虑让一个executor占用更少的core，同时worker下面增加更多的executor，或者一台host上面增加更多的worker来增加并行执行的executor的数量，从而增加CPU利用率。但是增加executor的时候需要考虑好内存消耗，因为一台机器的内存分配给越多的executor，每个executor的内存就越小，以致出现过多的数据spill over甚至out of memory的情况。</p><p>partition和parallelism，partition指的就是数据分片的数量，每一次task只能处理一个partition的数据，这个值太小了会导致每片数据量太大，导致内存压力，或者诸多executor的计算能力无法利用充分；但是如果太大了则会导致分片太多，执行效率降低。在执行action类型操作的时候（比如各种reduce操作），partition的数量会选择parent RDD中最大的那一个。而parallelism则指的是在RDD进行reduce类操作的时候，默认返回数据的paritition数量（而在进行map类操作的时候，partition数量通常取自parent RDD中较大的一个，而且也不会涉及shuffle，因此这个parallelism的参数没有影响）。所以说，这两个概念密切相关，都是涉及到数据分片的，作用方式其实是统一的。通过spark.default.parallelism可以设置默认的分片数量，而很多RDD的操作都可以指定一个partition参数来显式控制具体的分片数量。<br>看这样几个例子：<br>（1）实践中跑的Spark job，有的特别慢，查看CPU利用率很低，可以尝试减少每个executor占用CPU core的数量，增加并行的executor数量，同时配合增加分片，整体上增加了CPU的利用率，加快数据处理速度。<br>（2）发现某job很容易发生内存溢出，我们就增大分片数量，从而减少了每片数据的规模，同时还减少并行的executor数量，这样相同的内存资源分配给数量更少的executor，相当于增加了每个task的内存分配，这样运行速度可能慢了些，但是总比OOM强。<br>（3）数据量特别少，有大量的小文件生成，就减少文件分片，没必要创建那么多task，这种情况，如果只是最原始的input比较小，一般都能被注意到；但是，如果是在运算过程中，比如应用某个reduceBy或者某个filter以后，数据大量减少，这种低效情况就很少被留意到。<br>最后再补充一点，随着参数和配置的变化，性能的瓶颈是变化的，在分析问题的时候不要忘记。例如在每台机器上部署的executor数量增加的时候，性能一开始是增加的，同时也观察到CPU的平均使用率在增加；但是随着单台机器上的executor越来越多，性能下降了，因为随着executor的数量增加，被分配到每个executor的内存数量减小，在内存里直接操作的越来越少，spill over到磁盘上的数据越来越多，自然性能就变差了。<br>​    下面给这样一个直观的例子，当前总的cpu利用率并不高：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/02-CPU-1.png?raw=true" alt="02-CPU-1"></p><p>但是经过根据上述原则的的调整之后，可以显著发现cpu总利用率增加了：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/03-CPU-2.png?raw=true" alt="03-CPU-2"></p><p>其次，涉及性能调优我们经常要改配置，在Spark里面有三种常见的配置方式，虽然有些参数的配置是可以互相替代，但是作为最佳实践，还是需要遵循不同的情形下使用不同的配置：<br>1.设置环境变量，这种方式主要用于和环境、硬件相关的配置；<br>2.命令行参数，这种方式主要用于不同次的运行会发生变化的参数，用双横线开头；<br>3.代码里面（比如Scala）显式设置（SparkConf对象），这种配置通常是application级别的配置，一般不改变。<br>举一个配置的具体例子。slave、worker和executor之间的比例调整。我们经常需要调整并行的executor的数量，那么简单说有两种方式：<br>1.每个worker内始终跑一个executor，但是调整单台slave上并行的worker的数量。比如，SPARK_WORKER_INSTANCES可以设置每个slave的worker的数量，但是在改变这个参数的时候，比如改成2，一定要相应设置SPARK_WORKER_CORES的值，让每个worker使用原有一半的core，这样才能让两个worker一同工作；<br>2.每台slave内始终只部署一个worker，但是worker内部署多个executor。我们是在YARN框架下采用这个调整来实现executor数量改变的，一种典型办法是，一个host只跑一个worker，然后配置spark.executor.cores为host上CPU core的N分之一，同时也设置spark.executor.memory为host上分配给Spark计算内存的N分之一，这样这个host上就能够启动N个executor。<br>有的配置在不同的MR框架/工具下是不一样的，比如YARN下有的参数的默认取值就不同，这点需要注意。<br>明确这些基础的事情以后，再来一项一项看性能调优的要点。</p><h2 id="性能监控方式"><a href="#性能监控方式" class="headerlink" title="性能监控方式"></a>性能监控方式</h2><h3 id="Spark-Web-UI"><a href="#Spark-Web-UI" class="headerlink" title="Spark Web UI"></a>Spark Web UI</h3><p>Spark提供了一些基本的Web监控页面，对于日常监控十分有用。<br>通过<a href="http://master:4040（默认端口是4040，可以通过spark.ui.port修改）我们可以获得运行中的程序信息：（1）stages和tasks调度情况；（2）RDD大小及内存使用；（3）系统环境信息；（4）正在执行的executor信息。" target="_blank" rel="noopener">http://master:4040（默认端口是4040，可以通过spark.ui.port修改）我们可以获得运行中的程序信息：（1）stages和tasks调度情况；（2）RDD大小及内存使用；（3）系统环境信息；（4）正在执行的executor信息。</a><br>如果想当Spark应用退出后，仍可以获得历史Spark应用的stages和tasks执行信息，便于分析程序不明原因挂掉的情况。可以开启History Server。配置方法如下：<br>（1）$SPARK_HOME/conf/spark-env.sh<br>export SPARK_HISTORY_OPTS=”-Dspark.history.retainedApplications=50<br>Dspark.history.fs.logDirectory=hdfs://master01:9000/directory”<br>说明：spark.history.retainedApplications仅显示最近50个应用spark.history.fs.logDirectory：Spark History Server页面只展示该路径下的信息。<br>（2）$SPARK_HOME/conf/spark-defaults.conf<br>spark.eventLog.enabled true<br>spark.eventLog.dir hdfs://hadoop000:8020/directory #应用在运行过程中所有的信息均记录在该属性指定的路径下<br>spark.eventLog.compress true<br>（3）HistoryServer启动<br>$SPARK_HOME/bin/start-histrory-server.sh<br>（4）HistoryServer停止<br>$SPARK_HOME/bin/stop-histrory-server.sh<br>同时Executor的logs也是查看的一个出处：<br>Standalone模式：$SPARK_HOME/logs<br>YARN模式：在yarn-site.xml文件中配置了YARN日志的存放位置：yarn.nodemanager.log-dirs，或使用命令获取yarn logs -applicationId。<br>同时通过配置ganglia，可以分析集群的使用状况和资源瓶颈，但是默认情况下ganglia是未被打包的，需要在mvn编译时添加-Pspark-ganglia-lgpl，并修改配置文件$SPARK_HOME/conf/metrics.properties。</p><h3 id="其他监控工具"><a href="#其他监控工具" class="headerlink" title="其他监控工具"></a>其他监控工具</h3><p>Nmon（<a href="http://www.ibm.com/developerworks/aix/library/au-analyze_aix/）" target="_blank" rel="noopener">http://www.ibm.com/developerworks/aix/library/au-analyze_aix/）</a><br>Nmon 输入：c：CPU n：网络 m：内存 d：磁盘</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/04-Nmon.png?raw=true" alt="04-Nmon"></p><p>Jmeter（<a href="http://jmeter" target="_blank" rel="noopener">http://jmeter</a>. apache.org/）<br>通常使用Jmeter做系统性能参数的实时展示，JMeter的安装非常简单，从官方网站上下载，解压之后即可使用。运行命令在%JMETER_HOME%/bin下，对于 Windows 用户，直接使用jmeter.bat。<br>启动jmeter：创建测试计划，设置线程组设置循环次数。<br>添加监听器：jp@gc - PerfMon Metrics Collector。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/05-Jmeter.png?raw=true" alt="05-Jmeter"></p><p>设置监听器：监听主机端口及监听内容，例如CPU。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/06-Jmeter.png?raw=true" alt="06-Jmeter"></p><p>启动监听：可以实时获得节点的CPU状态信息，从下图可看出CPU已出现瓶颈。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/07-Jmeter-Graph.png?raw=true" alt="07-Jmeter-Graph"></p><p>Jprofiler（<a href="http://www.ej-technologies.com/products/jprofiler/overview.html）" target="_blank" rel="noopener">http://www.ej-technologies.com/products/jprofiler/overview.html）</a><br>JProfiler是一个全功能的Java剖析工具（profiler），专用于分析J2SE和J2EE应用程式。它把CPU、线程和内存的剖析组合在一个强大的应用中。JProfiler的GUI可以更方便地找到性能瓶颈、抓住内存泄漏（memory leaks），并解决多线程的问题。例如分析哪个对象占用的内存比较多；哪个方法占用较大的CPU资源等；我们通常使用Jprofiler来监控Spark应用在local模式下运行时的性能瓶颈和内存泄漏情况。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/08-Jprofile.png?raw=true" alt="08-Jprofile"></p><h2 id="调优要点"><a href="#调优要点" class="headerlink" title="调优要点"></a>调优要点</h2><h3 id="内存调优"><a href="#内存调优" class="headerlink" title="内存调优"></a>内存调优</h3><p>Memory Tuning，Java对象会占用原始数据2~5倍甚至更多的空间。最好的检测对象内存消耗的办法就是创建RDD，然后放到cache里面去，然后在UI上面看storage的变化。使用-XX:+UseCompressedOops选项可以压缩指针（8字节变成4字节）。在调用collect等API的时候也要小心—大块数据往内存拷贝的时候心里要清楚。内存要留一些给操作系统，比如20%，这里面也包括了OS的buffercache，如果预留得太少了，会见到这样的错误：<br>“Required executor memory (235520+23552 MB) is above the max threshold (241664MB) of this cluster! Please increase the value of ‘yarn.scheduler.maximum-allocation-mb’.<br>或者干脆就没有这样的错误，但是依然有因为内存不足导致的问题，有的会有警告，比如这个：<br>“16/01/13 23:54:48 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory<br>有的时候连这样的日志都见不到，而是见到一些不清楚原因的executor丢失信息：<br>“Exception in thread “main” org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 17.0 failed 4 times, most recent failure: Lost task 12.3 in stage 17.0 (TID 1257, ip-10-184-192-56.ec2.internal): ExecutorLostFailure (executor 79 lost)<br>Reduce Task的内存使用。在某些情况下reduce task特别消耗内存，比如当shuffle出现的时候，比如sortByKey、groupByKey、reduceByKey和join等，要在内存里面建立一个巨大的hash table。其中一个解决办法是增大level of parallelism，这样每个task的输入规模就相应减小。另外，注意shuffle的内存上限设置，有时候有足够的内存，但是shuffle内存不够的话，性能也是上不去的。我们在有大量数据join等操作的时候，shuffle的内存上限经常配置到executor的50%。<br>注意原始input的大小，有很多操作始终都是需要某类全集数据在内存里面完成的，那么并非拼命增加parallelism和partition的值就可以把内存占用减得非常小的。我们遇到过某些性能低下甚至OOM的问题，是改变这两个参数所难以缓解的。但是可以通过增加每台机器的内存，或者增加机器的数量都可以直接或间接增加内存总量来解决。<br>另外，有一些RDD的API，比如cache，persist，都会把数据强制放到内存里面，如果并不明确这样做带来的好处，就不要用它们。<br>内存优化有三个方面的考虑：对象所占用的内存，访问对象的消耗以及垃圾回收所占用的开销。</p><ol><li>对象所占内存，优化数据结构<br> Spark 默认使用Java序列化对象，虽然Java对象的访问速度更快，但其占用的空间通常比其内部的属性数据大2-5倍。为了减少内存的使用，减少Java序列化后的额外开销，下面列举一些Spark官网提供的方法。<br> （1）使用对象数组以及原始类型（primitive type）数组以替代Java或者Scala集合类（collection class)。fastutil 库为原始数据类型提供了非常方便的集合类，且兼容Java标准类库。<br> （2）尽可能地避免采用含有指针的嵌套数据结构来保存小对象。<br> （3）考虑采用数字ID或者枚举类型以便替代String类型的主键。<br> （4）如果内存少于32GB，设置JVM参数-XX:+UseCom-pressedOops以便将8字节指针修改成4字节。与此同时，在Java 7或者更高版本，设置JVM参数-XX:+UseC—–ompressedStrings以便采用8比特来编码每一个ASCII字符。</li><li>内存回收<br> （1）获取内存统计信息：优化内存前需要了解集群的内存回收频率、内存回收耗费时间等信息，可以在spark-env.sh中设置SPARK_JAVA_OPTS=“-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps $ SPARK_JAVA_OPTS”来获取每一次内存回收的信息。<br> （2）优化缓存大小：默认情况Spark采用运行内存（spark.executor.memory）的60%来进行RDD缓存。这表明在任务执行期间，有40%的内存可以用来进行对象创建。如果任务运行速度变慢且JVM频繁进行内存回收，或者内存空间不足，那么降低缓存大小设置可以减少内存消耗，可以降低spark.storage.memoryFraction的大小。</li><li>频繁GC或者OOM<br> 针对这种情况，首先要确定现象是发生在Driver端还是在Executor端，然后在分别处理。<br> Driver端：通常由于计算过大的结果集被回收到Driver端导致，需要调大Driver端的内存解决，或者进一步减少结果集的数量。<br> Executor端：<br> （1）以外部数据作为输入的Stage：这类Stage中出现GC通常是因为在Map侧进行map-side-combine时，由于group过多引起的。解决方法可以增加partition的数量（即task的数量）来减少每个task要处理的数据，来减少GC的可能性。<br> （2）以shuffle作为输入的Stage：这类Stage中出现GC的通常原因也是和shuffle有关，常见原因是某一个或多个group的数据过多，也就是所谓的数据倾斜，最简单的办法就是增加shuffle的task数量，比如在SparkSQL中设置SET spark.sql.shuffle.partitions=400，如果调大shuffle的task无法解决问题，说明你的数据倾斜很严重，某一个group的数据远远大于其他的group，需要你在业务逻辑上进行调整，预先针对较大的group做单独处理。</li></ol><h3 id="集群并行调优"><a href="#集群并行调优" class="headerlink" title="集群并行调优"></a>集群并行调优</h3><p>在Spark集群环境下，只有足够高的并行度才能使系统资源得到充分的利用，可以通过修改spark-env.sh来调整Executor的数量和使用资源，Standalone和YARN方式资源的调度管理是不同的。<br>在Standalone模式下:</p><ol><li>每个节点使用的最大内存数：SPARK_WORKER_INSTANCES*SPARK_WORKER_MEMORY；</li><li>每个节点的最大并发task数：SPARK_WORKER_INSTANCES*SPARK_WORKER_CORES。</li></ol><p>在YARN模式下：</p><ol><li>集群task并行度：SPARK_ EXECUTOR_INSTANCES* SPARK_EXECUTOR_CORES；</li><li>集群内存总量：(executor个数) * (SPARK_EXECUTOR_MEMORY+ spark.yarn.executor.memoryOverhead)<br> +(SPARK_DRIVER_MEMORY+spark.yarn.driver.memoryOverhead)。</li></ol><p>重点强调：Spark对Executor和Driver额外添加堆内存大小，Executor端：由spark.yarn.executor.memoryOverhead设置，默认值executorMemory <em> 0.07与384的最大值。Driver端：由spark.yarn.driver.memoryOverhead设置，默认值driverMemory </em> 0.07与384的最大值。<br>通过调整上述参数，可以提高集群并行度，让系统同时执行的任务更多，那么对于相同的任务，并行度高了，可以减少轮询次数。举例说明：如果一个stage有100task，并行度为50，那么执行完这次任务，需要轮询两次才能完成，如果并行度为100，那么一次就可以了。<br>但是在资源相同的情况，并行度高了，相应的Executor内存就会减少，所以需要根据实际实况协调内存和core。此外，Spark能够非常有效的支持短时间任务（例如：200ms），因为会对所有的任务复用JVM，这样能减小任务启动的消耗，Standalone模式下，core可以允许1-2倍于物理core的数量进行超配。<br>Level of Parallelism。指定它以后，在进行reduce类型操作的时候，默认partition的数量就被指定了。这个参数在实际工程中通常是必不可少的，一般都要根据input和每个executor内存的大小来确定。设置level of parallelism或者属性spark.default.parallelism来改变并行级别，通常来说，每一个CPU核可以分配2~3个task。<br>CPU core的访问模式是共享还是独占。即CPU核是被同一host上的executor共享还是瓜分并独占。比如，一台机器上共有32个CPU core的资源，同时部署了两个executor，总内存是50G，那么一种方式是配置spark.executor.cores为16，spark.executor.memory为20G，这样由于内存的限制，这台机器上会部署两个executor，每个都使用20G内存，并且各使用“独占”的16个CPU core资源；而在内存资源不变的前提下，也可以让这两个executor“共享”这32个core。根据测试，独占模式的性能要略好与共享模式。<br>GC调优。打印GC信息：-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps。要记得默认60%的executor内存可以被用来作为RDD的缓存，因此只有40%的内存可以被用来作为对象创建的空间，这一点可以通过设置spark.storage.memoryFraction改变。如果有很多小对象创建，但是这些对象在不完全GC的过程中就可以回收，那么增大Eden区会有一定帮助。如果有任务从HDFS拷贝数据，内存消耗有一个简单的估算公式——比如HDFS的block size是64MB，工作区内有4个task拷贝数据，而解压缩一个block要增大3倍大小，那么估算内存消耗就是：4<em>3</em>64MB。另外，还有一种情况：GC默认情况下有一个限制，默认是GC时间不能超过2%的CPU时间，但是如果大量对象创建（在Spark里很容易出现，代码模式就是一个RDD转下一个RDD），就会导致大量的GC时间，从而出现“OutOfMemoryError: GC overhead limit exceeded”，对于这个，可以通过设置-XX:-UseGCOverheadLimit关掉它。</p><h3 id="序列化和传输"><a href="#序列化和传输" class="headerlink" title="序列化和传输"></a>序列化和传输</h3><p>Data Serialization，默认使用的是Java Serialization，这个程序员最熟悉，但是性能、空间表现都比较差。还有一个选项是Kryo Serialization，更快，压缩率也更高，但是并非支持任意类的序列化。在Spark UI上能够看到序列化占用总时间开销的比例，如果这个比例高的话可以考虑优化内存使用和序列化。<br>Broadcasting Large Variables。在task使用静态大对象的时候，可以把它broadcast出去。Spark会打印序列化后的大小，通常来说如果它超过20KB就值得这么做。有一种常见情形是，一个大表join一个小表，把小表broadcast后，大表的数据就不需要在各个node之间疯跑，安安静静地呆在本地等小表broadcast过来就好了。<br>Data Locality。数据和代码要放到一起才能处理，通常代码总比数据要小一些，因此把代码送到各处会更快。Data Locality是数据和处理的代码在物理空间上接近的程度：PROCESS_LOCAL（同一个JVM）、NODE_LOCAL（同一个node，比如数据在HDFS上，但是和代码在同一个node）、NO_PREF、RACK_LOCAL（不在同一个server，但在同一个机架）、ANY。当然优先级从高到低，但是如果在空闲的executor上面没有未处理数据了，那么就有两个选择：<br>（1）要么等如今繁忙的CPU闲下来处理尽可能“本地”的数据，<br>（2）要么就不等直接启动task去处理相对远程的数据。<br>默认当这种情况发生Spark会等一会儿（spark.locality），即策略（1），如果繁忙的CPU停不下来，就会执行策略（2）。<br>代码里对大对象的引用。在task里面引用大对象的时候要小心，因为它会随着task序列化到每个节点上去，引发性能问题。只要序列化的过程不抛出异常，引用对象序列化的问题事实上很少被人重视。如果，这个大对象确实是需要的，那么就不如干脆把它变成RDD好了。绝大多数时候，对于大对象的序列化行为，是不知不觉发生的，或者说是预期之外的，比如在我们的项目中有这样一段代码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd.map(r =&gt; &#123;</span><br><span class="line">  println(<span class="type">BackfillTypeIndex</span>)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>其实呢，它等价于这样：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd.map(r =&gt; &#123;</span><br><span class="line">  println(<span class="keyword">this</span>.<span class="type">BackfillTypeIndex</span>)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>不要小看了这个this，有时候它的序列化是非常大的开销。<br>对于这样的问题，一种最直接的解决方法就是：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dereferencedVariable = <span class="keyword">this</span>.<span class="type">BackfillTypeIndex</span></span><br><span class="line">rdd.map(r =&gt; println(dereferencedVariable)) <span class="comment">// "this" is not serialized</span></span><br></pre></td></tr></table></figure><p>相关地，注解@transient用来标识某变量不要被序列化，这对于将大对象从序列化的陷阱中排除掉是很有用的。另外，注意class之间的继承层级关系，有时候一个小的case class可能来自一棵大树。</p><h3 id="文件读写"><a href="#文件读写" class="headerlink" title="文件读写"></a>文件读写</h3><p>文件存储和读取的优化。比如对于一些case而言，如果只需要某几列，使用rcfile和parquet这样的格式会大大减少文件读取成本。再有就是存储文件到S3上或者HDFS上，可以根据情况选择更合适的格式，比如压缩率更高的格式。另外，特别是对于shuffle特别多的情况，考虑留下一定量的额外内存给操作系统作为操作系统的buffer cache，比如总共50G的内存，JVM最多分配到40G多一点。<br>文件分片。比如在S3上面就支持文件以分片形式存放，后缀是partXX。使用coalesce方法来设置分成多少片，这个调整成并行级别或者其整数倍可以提高读写性能。但是太高太低都不好，太低了没法充分利用S3并行读写的能力，太高了则是小文件太多，预处理、合并、连接建立等等都是时间开销啊，读写还容易超过throttle。</p><h3 id="任务调优要点"><a href="#任务调优要点" class="headerlink" title="任务调优要点"></a>任务调优要点</h3><p>Spark的Speculation。通过设置spark.speculation等几个相关选项，可以让Spark在发现某些task执行特别慢的时候，可以在不等待完成的情况下被重新执行，最后相同的task只要有一个执行完了，那么最快执行完的那个结果就会被采纳。<br>减少Shuffle。其实Spark的计算往往很快，但是大量开销都花在网络和IO上面，而shuffle就是一个典型。举个例子，如果(k, v1) join (k, v2) =&gt; (k, v3)，那么，这种情况其实Spark是优化得非常好的，因为需要join的都在一个node的一个partition里面，join很快完成，结果也是在同一个node（这一系列操作可以被放在同一个stage里面）。但是如果数据结构被设计为(obj1) join (obj2) =&gt; (obj3)，而其中的join条件为obj1.column1 == obj2.column1，这个时候往往就被迫shuffle了，因为不再有同一个key使得数据在同一个node上的强保证。在一定要shuffle的情况下，尽可能减少shuffle前的数据规模。</p><p>Repartition。运算过程中数据量时大时小，选择合适的partition数量关系重大，如果太多partition就导致有很多小任务和空任务产生；如果太少则导致运算资源没法充分利用，必要时候可以使用repartition来调整，不过它也不是没有代价的，其中一个最主要代价就是shuffle。再有一个常见问题是数据大小差异太大，这种情况主要是数据的partition的key其实取值并不均匀造成的（默认使用HashPartitioner），需要改进这一点，比如重写hash算法。测试的时候想知道partition的数量可以调用rdd.partitions().size()获知。<br>Task时间分布。关注Spark UI，在Stage的详情页面上，可以看得到shuffle写的总开销，GC时间，当前方法栈，还有task的时间花费。如果你发现task的时间花费分布太散，就是说有的花费时间很长，有的很短，这就说明计算分布不均，需要重新审视数据分片、key的hash、task内部的计算逻辑等等，瓶颈出现在耗时长的task上面。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/09-Task.png?raw=true" alt="09-Task"></p><p>重用资源。有的资源申请开销巨大，而且往往相当有限，比如建立连接，可以考虑在partition建立的时候就创建好（比如使用mapPartition方法），这样对于每个partition内的每个元素的操作，就只要重用这个连接就好了，不需要重新建立连接。<br>同时Spark的任务数量是由stage中的起始的所有RDD的partition之和数量决定，所以需要了解每个RDD的partition的计算方法。以Spark应用从HDFS读取数据为例，HadoopRDD的partition切分方法完全继承于MapReduce中的FileInputFormat，具体的partition数量由HDFS的块大小、mapred.min.split.size的大小、文件的压缩方式等多个因素决定，详情需要参见FileInputFormat的代码。</p><h3 id="开启推测机制"><a href="#开启推测机制" class="headerlink" title="开启推测机制"></a>开启推测机制</h3><p>推测机制后，如果集群中，某一台机器的几个task特别慢，推测机制会将任务分配到其他机器执行，最后Spark会选取最快的作为最终结果。<br>在spark-default.conf 中添加：spark.speculation true<br>推测机制与以下几个参数有关：</p><ol><li>spark.speculation.interval 100：检测周期，单位毫秒；</li><li>spark.speculation.quantile 0.75：完成task的百分比时启动推测；</li><li>spark.speculation.multiplier 1.5：比其他的慢多少倍时启动推测。</li></ol><h1 id="数据倾斜优化"><a href="#数据倾斜优化" class="headerlink" title="数据倾斜优化"></a>数据倾斜优化</h1><h2 id="什么是数据倾斜"><a href="#什么是数据倾斜" class="headerlink" title="什么是数据倾斜"></a>什么是数据倾斜</h2><p>对Spark/Hadoop这样的大数据系统来讲，数据量大并不可怕，可怕的是数据倾斜。<br>何谓数据倾斜？数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。<br>如果数据倾斜没有解决，完全没有可能进行性能调优，其他所有的调优手段都是一个笑话。数据倾斜是最能体现一个spark大数据工程师水平的性能调优问题。<br>数据倾斜如果能够解决的话，代表对spark运行机制了如指掌。<br>数据倾斜俩大直接致命后果。<br>1 数据倾斜直接会导致一种情况：Out Of Memory。<br>2 运行速度慢,特别慢，非常慢，极端的慢，不可接受的慢。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/10-BigData.png?raw=true" alt="10-BigData"></p><p>我们以100亿条数据为列子。<br>个别Task(80亿条数据的那个Task)处理过度大量数据。导致拖慢了整个Job的执行时间。这可能导致该Task所在的机器OOM,或者运行速度非常慢。<br>数据倾斜是如何造成的<br>在Shuffle阶段。同样Key的数据条数太多了。导致了某个key(上图中的80亿条)所在的Task数据量太大了。远远超过其他Task所处理的数据量。<br>而这样的场景太常见了。二八定律可以证实这种场景。<br>搞定数据倾斜需要：<br>1 搞定shuffle<br>2 搞定业务场景<br>3 搞定 cpu core的使用情况<br>4 搞定OOM的根本原因等。<br>所以搞定了数据倾斜需要对至少以上的原理了如指掌。所以搞定数据倾斜是关键中的关键。<br>一个经验结论是：一般情况下，OOM的原因都是数据倾斜。某个task任务数据量太大，GC的压力就很大。这比不了Kafka,因为kafka的内存是不经过JVM的。是基于Linux内核的Page.<br>数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。<br>因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。<br>下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/11-Hello.png?raw=true" alt="11-Hello"></p><p>由于同一个Stage内的所有Task执行相同的计算，在排除不同计算节点计算能力差异的前提下，不同Task之间耗时的差异主要由该Task所处理的数据量决定。</p><h2 id="如何定位数据倾斜"><a href="#如何定位数据倾斜" class="headerlink" title="如何定位数据倾斜"></a>如何定位数据倾斜</h2><p>数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。<br>某个task执行特别慢的情况<br>首先要看的，就是数据倾斜发生在第几个stage中。<br>可以通过Spark Web UI来查看当前运行到了第几个stage,看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。<br>比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/12-Task-Time.png?raw=true" alt="12-Task-Time"></p><p>知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，这里介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。<br>这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。<br>stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。<br>stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> lines = sc.textFile(<span class="string">"hdfs://..."</span>)</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> pairs = words.map((_, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line">wordCounts.collect().foreach(println(_))</span><br></pre></td></tr></table></figure><p>通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由reduceByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。<br>某个task莫名其妙内存溢出的情况<br>这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。<br>但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。<br>查看导致数据倾斜的key的数据分布情况<br>知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。<br>此时根据你执行操作的情况不同，可以有很多种查看key分布的方式：<br>如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。<br>如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。<br>举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sampledPairs = pairs.sample(<span class="literal">false</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">val</span> sampledWordCounts = sampledPairs.countByKey()</span><br><span class="line">sampledWordCounts.foreach(println(_))</span><br></pre></td></tr></table></figure><h2 id="如何缓解-消除数据倾斜"><a href="#如何缓解-消除数据倾斜" class="headerlink" title="如何缓解/消除数据倾斜"></a>如何缓解/消除数据倾斜</h2><h3 id="尽量避免数据源的数据倾斜"><a href="#尽量避免数据源的数据倾斜" class="headerlink" title="尽量避免数据源的数据倾斜"></a>尽量避免数据源的数据倾斜</h3><p>比如数据源是Kafka：<br>以Spark Stream通过DirectStream方式读取Kafka数据为例。由于Kafka的每一个Partition对应Spark的一个Task（Partition），所以Kafka内相关Topic的各Partition之间数据是否平衡，直接决定Spark处理该数据时是否会产生数据倾斜。<br>Kafka某一Topic内消息在不同Partition之间的分布，主要由Producer端所使用的Partition实现类决定。如果使用随机Partitioner，则每条消息会随机发送到一个Partition中，从而从概率上来讲，各Partition间的数据会达到平衡。此时源Stage（直接读取Kafka数据的Stage）不会产生数据倾斜。<br>但很多时候，业务场景可能会要求将具备同一特征的数据顺序消费，此时就需要将具有相同特征的数据放于同一个Partition中。一个典型的场景是，需要将同一个用户相关的PV信息置于同一个Partition中。此时，如果产生了数据倾斜，则需要通过其它方式处理。<br>比如数据源是Hive：<br>导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。<br>方案实现思路：此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。<br>方案实现原理：这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。<br>方案优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。<br>方案缺点：治标不治本，Hive ETL中还是会发生数据倾斜。<br>方案实践经验：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。<br>项目实践经验：在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。</p><h3 id="调整并行度分散同一个Task的不同Key"><a href="#调整并行度分散同一个Task的不同Key" class="headerlink" title="调整并行度分散同一个Task的不同Key"></a>调整并行度分散同一个Task的不同Key</h3><p>方案适用场景：如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。<br>方案实现思路：在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。<br>方案实现原理：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。<br>方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。<br>方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。<br>方案实践经验：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用最简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p><p>原理<br>Spark在做Shuffle时，默认使用HashPartitioner（非Hash Shuffle）对数据进行分区。如果并行度设置的不合适，可能造成大量不相同的Key对应的数据被分配到了同一个Task上，造成该Task所处理的数据远大于其它Task，从而造成数据倾斜。<br>如果调整Shuffle时的并行度，使得原本被分配到同一Task的不同Key发配到不同Task上处理，则可降低原Task所需处理的数据量，从而缓解数据倾斜问题造成的短板效应。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/13-Shuffle.png?raw=true" alt="13-Shuffle"></p><p>案例<br>现有一张测试数据集，内有100万条数据，每条数据有一个唯一的id值。现通过一些处理，使得id为90万之下的所有数据对12取模后余数为8（即在Shuffle并行度为12时该数据集全部被HashPartition分配到第8个Task），其它数据集id不变，从而使得id大于90万的数据在Shuffle时可被均匀分配到所有Task中，而id小于90万的数据全部分配到同一个Task中。处理过程如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> sourceRdd = sc.textFile(<span class="string">"hdfs://master01:9000/source_index"</span>)</span><br><span class="line">sourceRdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = hdfs:<span class="comment">//master01:9000/source_index MapPartitionsRDD[1] at textFile at &lt;console&gt;:24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">brower</span>(<span class="params">id:<span class="type">Int</span>, time:<span class="type">Long</span>, uid:<span class="type">String</span>, keyword:<span class="type">String</span>, url_rank:<span class="type">Int</span>, click_num:<span class="type">Int</span>, click_url:<span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span></span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">brower</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">ds</span> </span>= sourceRdd.map(_.split(<span class="string">"\t"</span>)).map(attr =&gt; brower(attr(<span class="number">0</span>).toInt, attr(<span class="number">1</span>).toLong, attr(<span class="number">2</span>), attr(<span class="number">3</span>), attr(<span class="number">4</span>).toInt, attr(<span class="number">5</span>).toInt, attr(<span class="number">6</span>))).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[brower] = [id: int, time: bigint ... <span class="number">5</span> more fields]</span><br><span class="line"></span><br><span class="line">scala&gt; ds.createOrReplaceTempView(<span class="string">"sourceTable"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> newSource = spark.sql(<span class="string">"SELECT CASE WHEN id &lt; 900000 THEN (8  + (CAST (RAND() * 50000 AS bigint)) * 12 ) ELSE id END, time, uid, keyword, url_rank, click_num, click_url  FROM sourceTable"</span>)</span><br><span class="line">newSource: org.apache.spark.sql.<span class="type">DataFrame</span> = [<span class="type">CASE</span> <span class="type">WHEN</span> (id &lt; <span class="number">900000</span>) <span class="type">THEN</span> (<span class="type">CAST</span>(<span class="number">8</span> <span class="type">AS</span> <span class="type">BIGINT</span>) + (<span class="type">CAST</span>((rand(<span class="number">-5486683549522524104</span>) * <span class="type">CAST</span>(<span class="number">50000</span> <span class="type">AS</span> <span class="type">DOUBLE</span>)) <span class="type">AS</span> <span class="type">BIGINT</span>) * <span class="type">CAST</span>(<span class="number">12</span> <span class="type">AS</span> <span class="type">BIGINT</span>))) <span class="type">ELSE</span> <span class="type">CAST</span>(id <span class="type">AS</span> <span class="type">BIGINT</span>) <span class="type">END</span>: bigint, time: bigint ... <span class="number">5</span> more fields]</span><br><span class="line"></span><br><span class="line">scala&gt; newSource.rdd.map(_.mkString(<span class="string">"\t"</span>)).saveAsTextFile(<span class="string">"hdfs://master01:9000/test_data"</span>)</span><br></pre></td></tr></table></figure><p>通过上述处理，一份可能造成后续数据倾斜的测试数据即已准备好。接下来，使用Spark读取该测试数据，并通过groupByKey(12)对id分组处理，且Shuffle并行度为12。代码如下</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> sourceRdd = sc.textFile(<span class="string">"hdfs://master01:9000/test_data/p*"</span>)</span><br><span class="line">sourceRdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = hdfs:<span class="comment">//master01:9000/test_data/p* MapPartitionsRDD[1] at textFile at &lt;console&gt;:24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> kvRdd = sourceRdd.map(x =&gt;&#123; <span class="keyword">val</span> parm=x.split(<span class="string">"\t"</span>);(parm(<span class="number">0</span>).trim().toInt,parm(<span class="number">1</span>).trim()) &#125;)</span><br><span class="line">kvRdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; kvRdd.groupByKey(<span class="number">12</span>).count</span><br><span class="line">res0: <span class="type">Long</span> = <span class="number">150000</span>                                                             </span><br><span class="line"></span><br><span class="line">scala&gt; :quit</span><br></pre></td></tr></table></figure><p>本次实验所使用集群节点数为3，每个节点可被Yarn使用的CPU核数为3，内存为2GB。在Spark-shell中进行提交<br>GroupBy Stage的Task状态如下图所示，Task 8处理的记录数为90万，远大于（9倍于）其它11个Task处理的10万记录。而Task 8所耗费的时间为1秒，远高于其它11个Task的平均时间。整个Stage的时间也为1秒，该时间主要由最慢的Task 8决定。数据之间处理的比例最大为105倍。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/14-Data-Not-Banlance.png?raw=true" alt="14-Data-Not-Banlance"></p><p>在这种情况下，可以通过调整Shuffle并行度，使得原来被分配到同一个Task（即该例中的Task 8）的不同Key分配到不同Task，从而降低Task 8所需处理的数据量，缓解数据倾斜。<br>通过groupByKey(17)将Shuffle并行度调整为17，重新提交到Spark。新的Job的GroupBy Stage所有Task状态如下图所示。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> sourceRdd = sc.textFile(<span class="string">"hdfs://master01:9000/test_data/p*"</span>)</span><br><span class="line">sourceRdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = hdfs:<span class="comment">//master01:9000/test_data/p* MapPartitionsRDD[1] at textFile at &lt;console&gt;:24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> kvRdd = sourceRdd.map(x =&gt;&#123; <span class="keyword">val</span> parm=x.split(<span class="string">"\t"</span>);(parm(<span class="number">0</span>).trim().toInt,parm(<span class="number">1</span>).trim()) &#125;)</span><br><span class="line">kvRdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; kvRdd.groupByKey(<span class="number">17</span>).count</span><br><span class="line">res0: <span class="type">Long</span> = <span class="number">150000</span>                                                             </span><br><span class="line"></span><br><span class="line">scala&gt; :quit</span><br></pre></td></tr></table></figure><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/15-Data-Banlance.png?raw=true" alt="15-Data-Banlance"></p><p>从上图可知，相比以上次一计算，目前每一个计算的数据都比较平均，数据之间的最大比例基本为1:1，总体时间降到了0.8秒。<br>在这种场景下，调整并行度，并不意味着一定要增加并行度，也可能是减小并行度。如果通过groupByKey(7)将Shuffle并行度调整为7，重新提交到Spark。新Job的GroupBy Stage的所有Task状态如下图所示。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/16-Data-Banlance.png?raw=true" alt="16-Data-Banlance"></p><p>从上图可见，处理记录数都比较平均。<br>总结<br>适用场景<br>大量不同的Key被分配到了相同的Task造成该Task数据量过大。<br>解决方案<br>调整并行度。一般是增大并行度，但有时如本例减小并行度也可达到效果。<br>优势<br>实现简单，可在需要Shuffle的操作算子上直接设置并行度或者使用spark.default.parallelism设置。如果是Spark SQL，还可通过SET spark.sql.shuffle.partitions=[num_tasks]设置并行度。可用最小的代价解决问题。一般如果出现数据倾斜，都可以通过这种方法先试验几次，如果问题未解决，再尝试其它方法。<br>劣势<br>适用场景少，只能将分配到同一Task的不同Key分散开，但对于同一Key倾斜严重的情况该方法并不适用。并且该方法一般只能缓解数据倾斜，没有彻底消除问题。从实践经验来看，其效果一般。</p><h3 id="自定义-Partitioner"><a href="#自定义-Partitioner" class="headerlink" title="自定义 Partitioner"></a>自定义 Partitioner</h3><p>原理<br>使用自定义的Partitioner（默认为HashPartitioner），将原本被分配到同一个Task的不同Key分配到不同Task。<br>案例<br>以上述数据集为例，继续将并发度设置为12，但是在groupByKey算子上，使用自定义的Partitioner（实现如下）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomerPartitioner</span>(<span class="params">numParts:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">Partitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//覆盖分区数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = numParts</span><br><span class="line"></span><br><span class="line">  <span class="comment">//覆盖分区号获取函数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> id: <span class="type">Int</span> = key.toString.toInt</span><br><span class="line">    <span class="keyword">if</span> (id &lt;= <span class="number">900000</span>)</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> java.util.<span class="type">Random</span>().nextInt(<span class="number">100</span>) % <span class="number">12</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      <span class="keyword">return</span> id % <span class="number">12</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行如下代码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomerPartitioner</span>(<span class="params">numParts:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">  <span class="comment">//覆盖分区数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = numParts</span><br><span class="line">  <span class="comment">//覆盖分区号获取函数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> id: <span class="type">Int</span> = key.toString.toInt</span><br><span class="line">    <span class="keyword">if</span> (id &lt;= <span class="number">900000</span>)</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> java.util.<span class="type">Random</span>().nextInt(<span class="number">100</span>) % <span class="number">12</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      <span class="keyword">return</span> id % <span class="number">12</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">CustomerPartitioner</span></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">sourceRdd</span> </span>= sc.textFile(<span class="string">"hdfs://master01:9000/test_data/p*"</span>)</span><br><span class="line">sourceRdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = hdfs:<span class="comment">//master01:9000/test_data/p* MapPartitionsRDD[1] at textFile at &lt;console&gt;:24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> kvRdd = sourceRdd.map(x =&gt;&#123; <span class="keyword">val</span> parm=x.split(<span class="string">"\t"</span>);(parm(<span class="number">0</span>).trim().toInt,parm(<span class="number">1</span>).trim()) &#125;)</span><br><span class="line">kvRdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; kvRdd.groupByKey(<span class="keyword">new</span> <span class="type">CustomerPartitioner</span>(<span class="number">12</span>)).count</span><br><span class="line">res0: <span class="type">Long</span> = <span class="number">565312</span>                                                             </span><br><span class="line"></span><br><span class="line">scala&gt; :quit</span><br></pre></td></tr></table></figure><p>由下图可见，使用自定义Partition后，各Task所处理的数据集大小相当。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/17-Customize-Partition.png?raw=true" alt="17-Customize-Partition"></p><p>总结<br>适用场景<br>大量不同的Key被分配到了相同的Task造成该Task数据量过大。<br>解决方案<br>使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。<br>优势<br>不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。<br>劣势<br>适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。</p><h3 id="将Reduce-side-Join转变为Map-side-Join"><a href="#将Reduce-side-Join转变为Map-side-Join" class="headerlink" title="将Reduce side Join转变为Map side Join"></a>将Reduce side Join转变为Map side Join</h3><p>方案适用场景：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。<br>方案实现思路：不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。<br>方案实现原理：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/18-Shuffle.png?raw=true" alt="18-Shuffle"></p><p>方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。<br>方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。<br>通过Spark的Broadcast机制，将Reduce侧Join转化为Map侧Join，避免Shuffle从而完全消除Shuffle带来的数据倾斜。<br>案例<br>准备数据：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> sourceRdd = sc.textFile(<span class="string">"hdfs://master01:9000/source_index/p*"</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> kvRdd = sourceRdd.map(x =&gt;&#123; <span class="keyword">val</span> parm=x.split(<span class="string">"\t"</span>);(parm(<span class="number">0</span>).trim().toInt, x) &#125;)</span><br><span class="line">scala&gt; <span class="keyword">val</span> kvRdd2 = kvRdd.map(x=&gt;&#123;<span class="keyword">if</span>(x._1 &lt; <span class="number">900001</span>) (<span class="number">900001</span>,x._2) <span class="keyword">else</span> x&#125;)</span><br><span class="line">scala&gt; kvRdd2.map(x=&gt;x._1 +<span class="string">","</span>+x._2).saveAsTextFile(<span class="string">"hdfs://master01:9000/big_data/"</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> joinRdd2 = kvRdd.filter(_._1 &gt; <span class="number">900000</span>)</span><br><span class="line">scala&gt; joinRdd2.map(x=&gt;x._1 +<span class="string">","</span>+x._2).saveAsTextFile(<span class="string">"hdfs://master01:9000/small_data/"</span>)</span><br></pre></td></tr></table></figure><p>测试案例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> sourceRdd = sc.textFile(<span class="string">"hdfs://master01:9000/big_data/p*"</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> sourceRdd2 = sc.textFile(<span class="string">"hdfs://master01:9000/small_data/p*"</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> joinRdd = sourceRdd.map(x =&gt;&#123; <span class="keyword">val</span> parm=x.split(<span class="string">","</span>);(parm(<span class="number">0</span>).trim().toInt, parm(<span class="number">1</span>).trim) &#125;)</span><br><span class="line">scala&gt; <span class="keyword">val</span> joinRdd2 = sourceRdd2.map(x =&gt;&#123; <span class="keyword">val</span> parm=x.split(<span class="string">","</span>);(parm(<span class="number">0</span>).trim().toInt, parm(<span class="number">1</span>).trim) &#125;)</span><br><span class="line"></span><br><span class="line">scala&gt; joinRdd.join(joinRdd2).count</span><br></pre></td></tr></table></figure><p>直接通过将joinRdd（大数据集）和joinRdd2（小数据集）进行join计算，如下：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/19-Join.png?raw=true" alt="19-Join"></p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/20-Time-Cost.png?raw=true" alt="20-Time-Cost"></p><p>通过广播变量修正后：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> sourceRdd = sc.textFile(<span class="string">"hdfs://master01:9000/big_data/p*"</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> sourceRdd2 = sc.textFile(<span class="string">"hdfs://master01:9000/small_data/p*"</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> joinRdd = sourceRdd.map(x =&gt;&#123; <span class="keyword">val</span> parm=x.split(<span class="string">","</span>);(parm(<span class="number">0</span>).trim().toInt, parm(<span class="number">1</span>).trim) &#125;)</span><br><span class="line">scala&gt; <span class="keyword">val</span> joinRdd2 = sourceRdd2.map(x =&gt;&#123; <span class="keyword">val</span> parm=x.split(<span class="string">","</span>);(parm(<span class="number">0</span>).trim().toInt, parm(<span class="number">1</span>).trim) &#125;)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> broadcastVar = sc.broadcast(joinRdd2.collectAsMap)</span><br><span class="line">scala&gt; joinRdd.map(x =&gt; (x._1,(x._2,broadcastVar.value.getOrElse(x._1,<span class="string">""</span>)))).count</span><br></pre></td></tr></table></figure><p>通过广播变量 + Map完成了相同的工作（没有发生shuffle）：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/21-No-Shuffle.png?raw=true" alt="21-No-Shuffle"></p><p>总结<br>适用场景<br>参与Join的一边数据集足够小，可被加载进Driver并通过Broadcast方法广播到各个Executor中。<br>优势<br>避免了Shuffle，彻底消除了数据倾斜产生的条件，可极大提升性能。<br>劣势<br>要求参与Join的一侧数据集足够小，并且主要适用于Join的场景，不适合聚合的场景，适用条件有限。</p><h3 id="两阶段聚合（局部聚合-全局聚合）"><a href="#两阶段聚合（局部聚合-全局聚合）" class="headerlink" title="两阶段聚合（局部聚合+全局聚合）"></a>两阶段聚合（局部聚合+全局聚合）</h3><p>方案适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。<br>方案实现思路：这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。<br>方案实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。<br>方案优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。<br>方案缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/22-Add-Random-Prefix.png?raw=true" alt="22-Add-Random-Prefix"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 第一步，给RDD中的每个key都打上一个随机前缀。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt; randomPrefixRdd = rdd.mapToPair(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Long</span>&gt;, <span class="type">String</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; tuple)</span><br><span class="line">    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">      <span class="type">Random</span> random = <span class="keyword">new</span> <span class="type">Random</span>();</span><br><span class="line">      int prefix = random.nextInt(<span class="number">10</span>);</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line"><span class="comment">// 第二步，对打上随机前缀的key进行局部聚合。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt; localAggrRdd = randomPrefixRdd.reduceByKey(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    public <span class="type">Long</span> call(<span class="type">Long</span> v1, <span class="type">Long</span> v2) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> v1 + v2;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line"><span class="comment">// 第三步，去除RDD中每个key的随机前缀。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>,<span class="type">Long</span>&gt;, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    public <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Long</span>&gt; tuple)</span><br><span class="line">    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">      long originalKey = <span class="type">Long</span>.valueOf(tuple._1.split(<span class="string">"_"</span>)[<span class="number">1</span>]);</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt;(originalKey, tuple._2);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line"><span class="comment">// 第四步，对去除了随机前缀的RDD进行全局聚合。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    public <span class="type">Long</span> call(<span class="type">Long</span> v1, <span class="type">Long</span> v2) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> v1 + v2;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br></pre></td></tr></table></figure><h3 id="为数据倾斜的key增加随机前-后缀"><a href="#为数据倾斜的key增加随机前-后缀" class="headerlink" title="为数据倾斜的key增加随机前/后缀"></a>为数据倾斜的key增加随机前/后缀</h3><p>原理<br>为数据量特别大的Key增加随机前/后缀，使得原来Key相同的数据变为Key不相同的数据，从而使倾斜的数据集分散到不同的Task中，彻底解决数据倾斜问题。Join另一侧的数据中，与倾斜Key对应的部分数据，与随机前缀集作笛卡尔乘积，从而保证无论数据倾斜侧倾斜Key如何加前缀，都能与之正常Join。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/23-Add-Random-Prefix.png?raw=true" alt="23-Add-Random-Prefix"></p><p>案例<br>通过如下SQL，将id为9亿到9.08亿共800万条数据的id转为9500048或者9500096，其它数据的id除以100取整。从而该数据集中，id为9500048和9500096的数据各400万，其它id对应的数据记录数均为100条。这些数据存于名为test的表中。<br>对于另外一张小表test_new，取出50万条数据，并将id（递增且唯一）除以100取整，使得所有id都对应100条数据。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">NSERT OVERWRITE TABLE test</span><br><span class="line">  <span class="keyword">SELECT</span> <span class="keyword">CAST</span>(<span class="keyword">CASE</span> <span class="keyword">WHEN</span> <span class="keyword">id</span> &lt; <span class="number">908000000</span> <span class="keyword">THEN</span> (<span class="number">9500000</span> + (<span class="keyword">CAST</span> (<span class="keyword">RAND</span>() * <span class="number">2</span> <span class="keyword">AS</span> <span class="built_in">INT</span>) + <span class="number">1</span>) * <span class="number">48</span> )</span><br><span class="line">  <span class="keyword">ELSE</span> <span class="keyword">CAST</span>(<span class="keyword">id</span>/<span class="number">100</span> <span class="keyword">AS</span> <span class="built_in">INT</span>) <span class="keyword">END</span> <span class="keyword">AS</span> <span class="keyword">STRING</span>),</span><br><span class="line"><span class="keyword">name</span></span><br><span class="line"><span class="keyword">FROM</span> student_external</span><br><span class="line">  <span class="keyword">WHERE</span> <span class="keyword">id</span> <span class="keyword">BETWEEN</span> <span class="number">900000000</span> <span class="keyword">AND</span> <span class="number">1050000000</span>;</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> test_new</span><br><span class="line">  <span class="keyword">SELECT</span> <span class="keyword">CAST</span>(<span class="keyword">CAST</span>(<span class="keyword">id</span>/<span class="number">100</span> <span class="keyword">AS</span> <span class="built_in">INT</span>) <span class="keyword">AS</span> <span class="keyword">STRING</span>),</span><br><span class="line"><span class="keyword">name</span></span><br><span class="line"><span class="keyword">FROM</span> student_delta_external</span><br><span class="line">  <span class="keyword">WHERE</span> <span class="keyword">id</span> <span class="keyword">BETWEEN</span> <span class="number">950000000</span> <span class="keyword">AND</span> <span class="number">950500000</span>;</span><br></pre></td></tr></table></figure><p>通过如下代码，读取test表对应的文件夹内的数据并转换为JavaPairRDD存于leftRDD中，同样读取test表对应的数据存于rightRDD中。通过RDD的join算子对leftRDD与rightRDD进行Join，并指定并行度为48。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">SparkDataSkew</span></span>&#123;</span><br><span class="line">  public static void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line">    <span class="type">SparkConf</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>();</span><br><span class="line">    sparkConf.setAppName(<span class="string">"DemoSparkDataFrameWithSkewedBigTableDirect"</span>);</span><br><span class="line">    sparkConf.set(<span class="string">"spark.default.parallelism"</span>, parallelism + <span class="string">""</span>);</span><br><span class="line">    <span class="type">JavaSparkContext</span> javaSparkContext = <span class="keyword">new</span> <span class="type">JavaSparkContext</span>(sparkConf);</span><br><span class="line">    <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; leftRDD = javaSparkContext.textFile(<span class="string">"hdfs://hadoop1:8020/apps/hive/warehouse/default/test/"</span>)</span><br><span class="line">      .mapToPair((<span class="type">String</span> row) -&gt; &#123;</span><br><span class="line">        <span class="type">String</span>[] str = row.split(<span class="string">","</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(str[<span class="number">0</span>], str[<span class="number">1</span>]);</span><br><span class="line">      &#125;);</span><br><span class="line">    <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; rightRDD = javaSparkContext.textFile(<span class="string">"hdfs://hadoop1:8020/apps/hive/warehouse/default/test_new/"</span>)</span><br><span class="line">      .mapToPair((<span class="type">String</span> row) -&gt; &#123;</span><br><span class="line">        <span class="type">String</span>[] str = row.split(<span class="string">","</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(str[<span class="number">0</span>], str[<span class="number">1</span>]);</span><br><span class="line">      &#125;);</span><br><span class="line">    leftRDD.join(rightRDD, parallelism)</span><br><span class="line">      .mapToPair((<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;&gt; tuple) -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(tuple._1(), tuple._2()._2()))</span><br><span class="line">      .foreachPartition((<span class="type">Iterator</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;&gt; iterator) -&gt; &#123;</span><br><span class="line">        <span class="type">AtomicInteger</span> atomicInteger = <span class="keyword">new</span> <span class="type">AtomicInteger</span>();</span><br><span class="line">        iterator.forEachRemaining((<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; tuple) -&gt; atomicInteger.incrementAndGet());</span><br><span class="line">      &#125;);</span><br><span class="line">    javaSparkContext.stop();</span><br><span class="line">    javaSparkContext.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从下图可看出，整个Join耗时1分54秒，其中Join Stage耗时1.7分钟。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/24-Time-Cost.png?raw=true" alt="24-Time-Cost"></p><p>通过分析Join Stage的所有Task可知，在其它Task所处理记录数为192.71万的同时Task 32的处理的记录数为992.72万，故它耗时为1.7分钟，远高于其它Task的约10秒。这与上文准备数据集时，将id为9500048为9500096对应的数据量设置非常大，其它id对应的数据集非常均匀相符合。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/25-Single-BigData.png?raw=true" alt="25-Single-BigData"></p><p>现通过如下操作，实现倾斜Key的分散处理<br>将leftRDD中倾斜的key（即9500048与9500096）对应的数据单独过滤出来，且加上1到24的随机前缀，并将前缀与原数据用逗号分隔（以方便之后去掉前缀）形成单独的leftSkewRDD<br>将rightRDD中倾斜key对应的数据抽取出来，并通过flatMap操作将该数据集中每条数据均转换为24条数据（每条分别加上1到24的随机前缀），形成单独的rightSkewRDD<br>将leftSkewRDD与rightSkewRDD进行Join，并将并行度设置为48，且在Join过程中将随机前缀去掉，得到倾斜数据集的Join结果skewedJoinRDD<br>将leftRDD中不包含倾斜Key的数据抽取出来作为单独的leftUnSkewRDD<br>对leftUnSkewRDD与原始的rightRDD进行Join，并行度也设置为48，得到Join结果unskewedJoinRDD<br>通过union算子将skewedJoinRDD与unskewedJoinRDD进行合并，从而得到完整的Join结果集<br>具体实现代码如下</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">SparkDataSkew</span></span>&#123;</span><br><span class="line">  public static void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line">    int parallelism = <span class="number">48</span>;</span><br><span class="line">    <span class="type">SparkConf</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>();</span><br><span class="line">    sparkConf.setAppName(<span class="string">"SolveDataSkewWithRandomPrefix"</span>);</span><br><span class="line">    sparkConf.set(<span class="string">"spark.default.parallelism"</span>, parallelism + <span class="string">""</span>);</span><br><span class="line">    <span class="type">JavaSparkContext</span> javaSparkContext = <span class="keyword">new</span> <span class="type">JavaSparkContext</span>(sparkConf);</span><br><span class="line">    <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; leftRDD = javaSparkContext.textFile(<span class="string">"hdfs://hadoop1:8020/apps/hive/warehouse/default/test/"</span>)</span><br><span class="line">      .mapToPair((<span class="type">String</span> row) -&gt; &#123;</span><br><span class="line">        <span class="type">String</span>[] str = row.split(<span class="string">","</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(str[<span class="number">0</span>], str[<span class="number">1</span>]);</span><br><span class="line">      &#125;);</span><br><span class="line">    <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; rightRDD = javaSparkContext.textFile(<span class="string">"hdfs://hadoop1:8020/apps/hive/warehouse/default/test_new/"</span>)</span><br><span class="line">      .mapToPair((<span class="type">String</span> row) -&gt; &#123;</span><br><span class="line">        <span class="type">String</span>[] str = row.split(<span class="string">","</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(str[<span class="number">0</span>], str[<span class="number">1</span>]);</span><br><span class="line">      &#125;);</span><br><span class="line">    <span class="type">String</span>[] skewedKeyArray = <span class="keyword">new</span> <span class="type">String</span>[]&#123;<span class="string">"9500048"</span>, <span class="string">"9500096"</span>&#125;;</span><br><span class="line">    <span class="type">Set</span>&lt;<span class="type">String</span>&gt; skewedKeySet = <span class="keyword">new</span> <span class="type">HashSet</span>&lt;<span class="type">String</span>&gt;();</span><br><span class="line">    <span class="type">List</span>&lt;<span class="type">String</span>&gt; addList = <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;<span class="type">String</span>&gt;();</span><br><span class="line">    <span class="keyword">for</span>(int i = <span class="number">1</span>; i &lt;=<span class="number">24</span>; i++) &#123;</span><br><span class="line">      addList.add(i + <span class="string">""</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">String</span> key : skewedKeyArray) &#123;</span><br><span class="line">      skewedKeySet.add(key);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Broadcast</span>&lt;<span class="type">Set</span>&lt;<span class="type">String</span>&gt;&gt; skewedKeys = javaSparkContext.broadcast(skewedKeySet);</span><br><span class="line">    <span class="type">Broadcast</span>&lt;<span class="type">List</span>&lt;<span class="type">String</span>&gt;&gt; addListKeys = javaSparkContext.broadcast(addList);</span><br><span class="line">    <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; leftSkewRDD = leftRDD</span><br><span class="line">      .filter((<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; tuple) -&gt; skewedKeys.value().contains(tuple._1()))</span><br><span class="line">      .mapToPair((<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; tuple) -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;((<span class="keyword">new</span> <span class="type">Random</span>().nextInt(<span class="number">24</span>) + <span class="number">1</span>) + <span class="string">","</span> + tuple._1(), tuple._2()));</span><br><span class="line">    <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; rightSkewRDD = rightRDD.filter((<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; tuple) -&gt; skewedKeys.value().contains(tuple._1()))</span><br><span class="line">      .flatMapToPair((<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; tuple) -&gt; addListKeys.value().stream()</span><br><span class="line">        .map((<span class="type">String</span> i) -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;( i + <span class="string">","</span> + tuple._1(), tuple._2()))</span><br><span class="line">        .collect(<span class="type">Collectors</span>.toList())</span><br><span class="line">        .iterator()</span><br><span class="line">      );</span><br><span class="line">    <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; skewedJoinRDD = leftSkewRDD</span><br><span class="line">      .join(rightSkewRDD, parallelism)</span><br><span class="line">      .mapToPair((<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;&gt; tuple) -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(tuple._1().split(<span class="string">","</span>)[<span class="number">1</span>], tuple._2()._2()));</span><br><span class="line">    <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; leftUnSkewRDD = leftRDD.filter((<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; tuple) -&gt; !skewedKeys.value().contains(tuple._1()));</span><br><span class="line">    <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; unskewedJoinRDD = leftUnSkewRDD.join(rightRDD, parallelism).mapToPair((<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;&gt; tuple) -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(tuple._1(), tuple._2()._2()));</span><br><span class="line">    skewedJoinRDD.union(unskewedJoinRDD).foreachPartition((<span class="type">Iterator</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;&gt; iterator) -&gt; &#123;</span><br><span class="line">      <span class="type">AtomicInteger</span> atomicInteger = <span class="keyword">new</span> <span class="type">AtomicInteger</span>();</span><br><span class="line">      iterator.forEachRemaining((<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; tuple) -&gt; atomicInteger.incrementAndGet());</span><br><span class="line">    &#125;);</span><br><span class="line">    javaSparkContext.stop();</span><br><span class="line">    javaSparkContext.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从下图可看出，整个Join耗时58秒，其中Join Stage耗时33秒。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/26-Time-Cost.png?raw=true" alt="26-Time-Cost"></p><p>通过分析Join Stage的所有Task可知<br>由于Join分倾斜数据集Join和非倾斜数据集Join，而各Join的并行度均为48，故总的并行度为96<br>由于提交任务时，设置的Executor个数为4，每个Executor的core数为12，故可用Core数为48，所以前48个Task同时启动（其Launch时间相同），后48个Task的启动时间各不相同（等待前面的Task结束才开始）<br>由于倾斜Key被加上随机前缀，原本相同的Key变为不同的Key，被分散到不同的Task处理，故在所有Task中，未发现所处理数据集明显高于其它Task的情况</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/27-Task-In-Stage.png?raw=true" alt="27-Task-In-Stage"></p><p>实际上，由于倾斜Key与非倾斜Key的操作完全独立，可并行进行。而本实验受限于可用总核数为48，可同时运行的总Task数为48，故而该方案只是将总耗时减少一半（效率提升一倍）。如果资源充足，可并发执行Task数增多，该方案的优势将更为明显。在实际项目中，该方案往往可提升数倍至10倍的效率。<br>总结<br>适用场景<br>两张表都比较大，无法使用Map则Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。<br>解决方案<br>将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join并去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。<br>优势<br>相对于Map则Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。<br>劣势<br>如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。</p><h3 id="使用随机前缀和扩容RDD进行join"><a href="#使用随机前缀和扩容RDD进行join" class="headerlink" title="使用随机前缀和扩容RDD进行join"></a>使用随机前缀和扩容RDD进行join</h3><p>方案适用场景：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。<br>方案实现思路：<br>该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。<br>然后将该RDD的每条数据都打上一个n以内的随机前缀。<br>同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。<br>最后将两个处理后的RDD进行join即可。<br>方案实现原理：将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。<br>方案优点：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。<br>方案缺点：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。<br>方案实践经验：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt; expandedRDD = rdd1.flatMapToPair(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">PairFlatMapFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Row</span>&gt;, <span class="type">String</span>, <span class="type">Row</span>&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    public <span class="type">Iterable</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; tuple)</span><br><span class="line">    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">      <span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; list = <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;();</span><br><span class="line">      <span class="keyword">for</span>(int i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">        list.add(<span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;(<span class="number">0</span> + <span class="string">"_"</span> + tuple._1, tuple._2));</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> list;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line"><span class="comment">// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; mappedRDD = rdd2.mapToPair(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">String</span>, <span class="type">String</span>&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple)</span><br><span class="line">    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">      <span class="type">Random</span> random = <span class="keyword">new</span> <span class="type">Random</span>();</span><br><span class="line">      int prefix = random.nextInt(<span class="number">100</span>);</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line"><span class="comment">// 将两个处理后的RDD进行join即可。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD);</span><br></pre></td></tr></table></figure><h3 id="大表随机添加N种随机前缀，小表扩大N倍"><a href="#大表随机添加N种随机前缀，小表扩大N倍" class="headerlink" title="大表随机添加N种随机前缀，小表扩大N倍"></a>大表随机添加N种随机前缀，小表扩大N倍</h3><p>原理<br>如果出现数据倾斜的Key比较多，上一种方法将这些大量的倾斜Key分拆出来，意义不大。此时更适合直接对存在数据倾斜的数据集全部加上随机前缀，然后对另外一个不存在严重数据倾斜的数据集整体与随机前缀集作笛卡尔乘积（即将数据量扩大N倍）。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/28-BigData.png?raw=true" alt="28-BigData"></p><p>案例<br>这里给出示例代码，读者可参考上文中分拆出少数倾斜Key添加随机前缀的方法，自行测试。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">SparkDataSkew</span> </span>&#123;</span><br><span class="line">  public static void main(<span class="type">String</span>[] args) &#123;</span><br><span class="line">    <span class="type">SparkConf</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>();</span><br><span class="line">    sparkConf.setAppName(<span class="string">"ResolveDataSkewWithNAndRandom"</span>);</span><br><span class="line">    sparkConf.set(<span class="string">"spark.default.parallelism"</span>, parallelism + <span class="string">""</span>);</span><br><span class="line">    <span class="type">JavaSparkContext</span> javaSparkContext = <span class="keyword">new</span> <span class="type">JavaSparkContext</span>(sparkConf);</span><br><span class="line">    <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; leftRDD = javaSparkContext.textFile(<span class="string">"hdfs://hadoop1:8020/apps/hive/warehouse/default/test/"</span>)</span><br><span class="line">      .mapToPair((<span class="type">String</span> row) -&gt; &#123;</span><br><span class="line">        <span class="type">String</span>[] str = row.split(<span class="string">","</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(str[<span class="number">0</span>], str[<span class="number">1</span>]);</span><br><span class="line">      &#125;);</span><br><span class="line">    <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; rightRDD = javaSparkContext.textFile(<span class="string">"hdfs://hadoop1:8020/apps/hive/warehouse/default/test_new/"</span>)</span><br><span class="line">      .mapToPair((<span class="type">String</span> row) -&gt; &#123;</span><br><span class="line">        <span class="type">String</span>[] str = row.split(<span class="string">","</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(str[<span class="number">0</span>], str[<span class="number">1</span>]);</span><br><span class="line">      &#125;);</span><br><span class="line">    <span class="type">List</span>&lt;<span class="type">String</span>&gt; addList = <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;<span class="type">String</span>&gt;();</span><br><span class="line">    <span class="keyword">for</span>(int i = <span class="number">1</span>; i &lt;=<span class="number">48</span>; i++) &#123;</span><br><span class="line">      addList.add(i + <span class="string">""</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Broadcast</span>&lt;<span class="type">List</span>&lt;<span class="type">String</span>&gt;&gt; addListKeys = javaSparkContext.broadcast(addList);</span><br><span class="line">    <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; leftRandomRDD = leftRDD.mapToPair((<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; tuple) -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(<span class="keyword">new</span> <span class="type">Random</span>().nextInt(<span class="number">48</span>) + <span class="string">","</span> + tuple._1(), tuple._2()));</span><br><span class="line">    <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; rightNewRDD = rightRDD</span><br><span class="line">      .flatMapToPair((<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; tuple) -&gt; addListKeys.value().stream()</span><br><span class="line">        .map((<span class="type">String</span> i) -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;( i + <span class="string">","</span> + tuple._1(), tuple._2()))</span><br><span class="line">        .collect(<span class="type">Collectors</span>.toList())</span><br><span class="line">        .iterator()</span><br><span class="line">      );</span><br><span class="line">    <span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; joinRDD = leftRandomRDD</span><br><span class="line">      .join(rightNewRDD, parallelism)</span><br><span class="line">      .mapToPair((<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;&gt; tuple) -&gt; <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(tuple._1().split(<span class="string">","</span>)[<span class="number">1</span>], tuple._2()._2()));</span><br><span class="line">    joinRDD.foreachPartition((<span class="type">Iterator</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;&gt; iterator) -&gt; &#123;</span><br><span class="line">      <span class="type">AtomicInteger</span> atomicInteger = <span class="keyword">new</span> <span class="type">AtomicInteger</span>();</span><br><span class="line">      iterator.forEachRemaining((<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; tuple) -&gt; atomicInteger.incrementAndGet());</span><br><span class="line">    &#125;);</span><br><span class="line">    javaSparkContext.stop();</span><br><span class="line">    javaSparkContext.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>总结<br>适用场景<br>一个数据集存在的倾斜Key比较多，另外一个数据集数据分布比较均匀。<br>优势<br>对大部分场景都适用，效果不错。<br>劣势<br>需要将一个数据集整体扩大N倍，会增加资源消耗。<br>总结<br>对于数据倾斜，并无一个统一的一劳永逸的方法。更多的时候，是结合数据特点（数据集大小，倾斜Key的多少等）综合使用上文所述的多种方法。</p><h3 id="采样倾斜key并分拆join操作"><a href="#采样倾斜key并分拆join操作" class="headerlink" title="采样倾斜key并分拆join操作"></a>采样倾斜key并分拆join操作</h3><p>方案适用场景：两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。<br>方案实现思路：<br>对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。<br>然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。<br>接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。<br>再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。<br>而另外两个普通的RDD就照常join即可。<br>最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。<br>方案实现原理：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。<br>方案优点：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。<br>方案缺点：如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/29-Expand.png?raw=true" alt="29-Expand"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; sampledRDD = rdd1.sample(<span class="literal">false</span>, <span class="number">0.1</span>);</span><br><span class="line"><span class="comment">// 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。</span></span><br><span class="line"><span class="comment">// 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。</span></span><br><span class="line"><span class="comment">// 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; mappedSampledRDD = sampledRDD.mapToPair(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    public <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple)</span><br><span class="line">    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt;(tuple._1, <span class="number">1</span>L);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; countedSampledRDD = mappedSampledRDD.reduceByKey(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Function2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    public <span class="type">Long</span> call(<span class="type">Long</span> v1, <span class="type">Long</span> v2) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> v1 + v2;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; reversedSampledRDD = countedSampledRDD.mapToPair(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Long</span>&gt;, <span class="type">Long</span>, <span class="type">Long</span>&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    public <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt; tuple)</span><br><span class="line">    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Long</span>&gt;(tuple._2, tuple._1);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line"><span class="keyword">final</span> <span class="type">Long</span> skewedUserid = reversedSampledRDD.sortByKey(<span class="literal">false</span>).take(<span class="number">1</span>).get(<span class="number">0</span>)._2;</span><br><span class="line"><span class="comment">// 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; skewedRDD = rdd1.filter(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Function</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">Boolean</span>&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    public <span class="type">Boolean</span> call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line"><span class="comment">// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; commonRDD = rdd1.filter(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Function</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">Boolean</span>&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    public <span class="type">Boolean</span> call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> !tuple._1.equals(skewedUserid);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line"><span class="comment">// rdd2，就是那个所有key的分布相对较为均匀的rdd。</span></span><br><span class="line"><span class="comment">// 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。</span></span><br><span class="line"><span class="comment">// 对扩容的每条数据，都打上0～100的前缀。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt; skewedRdd2 = rdd2.filter(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Function</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Row</span>&gt;, <span class="type">Boolean</span>&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    public <span class="type">Boolean</span> call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;).flatMapToPair(<span class="keyword">new</span> <span class="type">PairFlatMapFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">Row</span>&gt;, <span class="type">String</span>, <span class="type">Row</span>&gt;() &#123;</span><br><span class="line">  <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  public <span class="type">Iterable</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; call(</span><br><span class="line">    <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Row</span>&gt; tuple) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">    <span class="type">Random</span> random = <span class="keyword">new</span> <span class="type">Random</span>();</span><br><span class="line">    <span class="type">List</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; list = <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;();</span><br><span class="line">    <span class="keyword">for</span>(int i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">      list.add(<span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;(i + <span class="string">"_"</span> + tuple._1, tuple._2));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> list;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。</span></span><br><span class="line"><span class="comment">// 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRDD1 = skewedRDD.mapToPair(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Long</span>,<span class="type">String</span>&gt;, <span class="type">String</span>, <span class="type">String</span>&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    public <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt; call(<span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; tuple)</span><br><span class="line">    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">      <span class="type">Random</span> random = <span class="keyword">new</span> <span class="type">Random</span>();</span><br><span class="line">      int prefix = random.nextInt(<span class="number">100</span>);</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">String</span>&gt;(prefix + <span class="string">"_"</span> + tuple._1, tuple._2);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line">  .join(skewedUserid2infoRDD)</span><br><span class="line">  .mapToPair(<span class="keyword">new</span> <span class="type">PairFunction</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">String</span>,<span class="type">Tuple2</span>&lt;<span class="type">String</span>,<span class="type">Row</span>&gt;&gt;, <span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    public <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; call(</span><br><span class="line">      <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; tuple)</span><br><span class="line">    <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">      long key = <span class="type">Long</span>.valueOf(tuple._1.split(<span class="string">"_"</span>)[<span class="number">1</span>]);</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Tuple2</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt;(key, tuple._2);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRDD2 = commonRDD.join(rdd2);</span><br><span class="line"><span class="comment">// 将倾斜key join后的结果与普通key join后的结果，uinon起来。</span></span><br><span class="line"><span class="comment">// 就是最终的join结果。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">Tuple2</span>&lt;<span class="type">String</span>, <span class="type">Row</span>&gt;&gt; joinedRDD = joinedRDD1.union(joinedRDD2);</span><br></pre></td></tr></table></figure><h3 id="过滤少数导致倾斜的key"><a href="#过滤少数导致倾斜的key" class="headerlink" title="过滤少数导致倾斜的key"></a>过滤少数导致倾斜的key</h3><p>方案适用场景：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。<br>方案实现思路：如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。<br>方案实现原理：将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。<br>方案优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。<br>方案缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。<br>方案实践经验：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p><h1 id="Shuffle-调优"><a href="#Shuffle-调优" class="headerlink" title="Shuffle 调优"></a>Shuffle 调优</h1><h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。</p><h2 id="ShuffleManager-发展概述"><a href="#ShuffleManager-发展概述" class="headerlink" title="ShuffleManager 发展概述"></a>ShuffleManager 发展概述</h2><p>在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。<br>在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。<br>因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。<br>下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。</p><h2 id="HashShuffleManager-运行原理"><a href="#HashShuffleManager-运行原理" class="headerlink" title="HashShuffleManager 运行原理"></a>HashShuffleManager 运行原理</h2><p>未经优化的HashShuffleManager<br>下图说明了未经优化的HashShuffleManager的原理。这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。<br>我们先从shuffle write开始说起。shuffle write阶段，主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。<br>那么每个执行shuffle write的task，要为下一个stage创建多少个磁盘文件呢？很简单，下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。<br>接着我们来说说shuffle read。shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。<br>shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/30-Task-And-Buffer.png?raw=true" alt="30-Task-And-Buffer"></p><p>优化后的HashShuffleManager<br>下图说明了优化后的HashShuffleManager的原理。这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。<br>开启consolidate机制之后，在shuffle write过程中，task就不是为下游stage的每个task创建一个磁盘文件了。此时会出现shuffleFileGroup的概念，每个shuffleFileGroup会对应一批磁盘文件，磁盘文件的数量与下游stage的task数量是相同的。一个Executor上有多少个CPU core，就可以并行执行多少个task。而第一批并行执行的每个task都会创建一个shuffleFileGroup，并将数据写入对应的磁盘文件内。<br>当Executor的CPU core执行完一批task，接着执行下一批task时，下一批task就会复用之前已有的shuffleFileGroup，包括其中的磁盘文件。也就是说，此时task会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate机制允许不同的task复用同一批磁盘文件，这样就可以有效将多个task的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升shuffle write的性能。<br>假设第二个stage有100个task，第一个stage有50个task，总共还是有10个Executor，每个Executor执行5个task。那么原本使用未经优化的HashShuffleManager时，每个Executor会产生500个磁盘文件，所有Executor会产生5000个磁盘文件的。但是此时经过优化之后，每个Executor创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。也就是说，每个Executor此时只会创建100个磁盘文件，所有Executor只会创建1000个磁盘文件。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/31-Task-And-Buffer.png?raw=true" alt="31-Task-And-Buffer"></p><h2 id="SortShuffleManager-运行原理"><a href="#SortShuffleManager-运行原理" class="headerlink" title="SortShuffleManager 运行原理"></a>SortShuffleManager 运行原理</h2><p>SortShuffleManager的运行机制主要分成两种，一种是普通运行机制，另一种是bypass运行机制。当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。<br>普通运行机制<br>下图说明了普通的SortShuffleManager的原理。在该模式下，数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存；如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。<br>在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。<br>一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。<br>SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于每个task最终只有一个磁盘文件，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/32-Sort-Shuffle-Manager.png?raw=true" alt="32-Sort-Shuffle-Manager"></p><p>bypass运行机制<br>下图说明了bypass SortShuffleManager的原理。bypass运行机制的触发条件如下：<br>shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。<br>不是聚合类的shuffle算子（比如reduceByKey）。<br>此时task会为每个下游task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。<br>该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。<br>而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/33-Sort-Shuffle-Manager.png?raw=true" alt="33-Sort-Shuffle-Manager"></p><h2 id="shuffle相关参数调优"><a href="#shuffle相关参数调优" class="headerlink" title="shuffle相关参数调优"></a>shuffle相关参数调优</h2><p>以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。<br>spark.shuffle.file.buffer<br>默认值：32k<br>参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。<br>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。<br>spark.reducer.maxSizeInFlight<br>默认值：48m<br>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。<br>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。<br>spark.shuffle.io.maxRetries<br>默认值：3<br>参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。<br>调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。<br>spark.shuffle.io.retryWait<br>默认值：5s<br>参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。<br>调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。<br>spark.shuffle.memoryFraction<br>默认值：0.2<br>参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。<br>调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。<br>spark.shuffle.manager<br>默认值：sort<br>参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。<br>调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。<br>spark.shuffle.sort.bypassMergeThreshold<br>默认值：200<br>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。<br>调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。<br>spark.shuffle.consolidateFiles<br>默认值：false<br>参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。<br>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</p><h2 id="程序开发调优"><a href="#程序开发调优" class="headerlink" title="程序开发调优"></a>程序开发调优</h2><p>Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。</p><h3 id="避免创建重复的-RDD"><a href="#避免创建重复的-RDD" class="headerlink" title="避免创建重复的 RDD"></a>避免创建重复的 RDD</h3><p>通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。<br>我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。<br>一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。<br>一个简单的例子</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。</span></span><br><span class="line"><span class="comment">//也就是说，需要对一份数据执行两次算子操作。</span></span><br><span class="line"><span class="comment">//错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。</span></span><br><span class="line"><span class="comment">//这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，</span></span><br><span class="line"><span class="comment">//然后分别对每个RDD都执行了一个算子操作。</span></span><br><span class="line"><span class="comment">//这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；</span></span><br><span class="line"><span class="comment">//第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd2.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">//正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。</span></span><br><span class="line"><span class="comment">//这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，</span></span><br><span class="line"><span class="comment">//然后对这一个RDD执行了多次算子操作。</span></span><br><span class="line"><span class="comment">//但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，</span></span><br><span class="line"><span class="comment">//还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。</span></span><br><span class="line"><span class="comment">//要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，</span></span><br><span class="line"><span class="comment">//才能保证一个RDD被多次使用时只被计算一次。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><h3 id="尽可能复用同一个-RDD"><a href="#尽可能复用同一个-RDD" class="headerlink" title="尽可能复用同一个 RDD"></a>尽可能复用同一个 RDD</h3><p>除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。<br>一个简单的例子</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 错误的做法。</span></span><br><span class="line"><span class="comment">// 有一个&lt;long , String&gt;格式的RDD，即rdd1。</span></span><br><span class="line"><span class="comment">// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，</span></span><br><span class="line"><span class="comment">//而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;long , <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line"><span class="type">JavaRDD</span>&lt;string&gt; rdd2 = rdd1.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分别对rdd1和rdd2执行了不同的算子操作。</span></span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd2.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"><span class="comment">// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，</span></span><br><span class="line"><span class="comment">//rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。</span></span><br><span class="line"><span class="comment">// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。</span></span><br><span class="line"><span class="comment">// 其实在这种情况下完全可以复用同一个RDD。</span></span><br><span class="line"><span class="comment">// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。</span></span><br><span class="line"><span class="comment">// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。</span></span><br><span class="line"></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;long , <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd1.map(tuple._2...)</span><br><span class="line"><span class="comment">// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。</span></span><br><span class="line"><span class="comment">// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。</span></span><br><span class="line"><span class="comment">// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，</span></span><br><span class="line"><span class="comment">//才能保证一个RDD被多次使用时只被计算一次。</span></span><br></pre></td></tr></table></figure><h3 id="对多次使用的-RDD-进行持久化"><a href="#对多次使用的-RDD-进行持久化" class="headerlink" title="对多次使用的 RDD 进行持久化"></a>对多次使用的 RDD 进行持久化</h3><p>当你在Spark代码中多次对一个RDD做了算子操作后，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。<br>Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。<br>因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不从源头处重新计算一遍这个RDD，再执行算子操作。<br>对多次使用的RDD进行持久化的代码示例。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"><span class="comment">// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span></span><br><span class="line"><span class="comment">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。</span></span><br><span class="line"><span class="comment">// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).cache()</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span></span><br><span class="line"><span class="comment">// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，</span></span><br><span class="line"><span class="comment">//内存不充足时持久化到磁盘文件中。</span></span><br><span class="line"><span class="comment">// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition</span></span><br><span class="line"><span class="comment">//都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span></span><br><span class="line"><span class="comment">// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，</span></span><br><span class="line"><span class="comment">//从而发生频繁GC。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">  .persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><p>对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。<br>Spark的持久化级别</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/34-Spark-Persistence-Level.png?raw=true" alt="34-Spark-Persistence-Level"></p><p>MEMORY_ONLY 使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。<br>MEMORY_AND_DISK 使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。<br>MEMORY_ONLY_SER 基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。<br>MEMORY_AND_DISK_SER 基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。<br>DISK_ONLY 使用未序列化的Java对象格式，将数据全部写入磁盘文件中。<br>MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等. 对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。<br>如何选择一种最合适的持久化策略<br>1、默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。<br>2、如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。<br>3、如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。<br>4、通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</p><h3 id="尽量避免使用-Shuffle-类算子"><a href="#尽量避免使用-Shuffle-类算子" class="headerlink" title="尽量避免使用 Shuffle 类算子"></a>尽量避免使用 Shuffle 类算子</h3><p>如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。<br>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。<br>因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。<br>Broadcast与map进行join代码示例</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传统的join操作会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line"><span class="comment">// Broadcast+map的join操作，不会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span></span><br><span class="line"><span class="keyword">val</span> rdd2Data = rdd2.collect()</span><br><span class="line"><span class="keyword">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class="line"><span class="comment">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span></span><br><span class="line"><span class="comment">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，</span></span><br><span class="line"><span class="comment">//那么就判定可以进行join。</span></span><br><span class="line"><span class="comment">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，</span></span><br><span class="line"><span class="comment">//拼接在一起（String或Tuple）。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class="line"><span class="comment">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span></span><br><span class="line"><span class="comment">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span></span><br></pre></td></tr></table></figure><h3 id="使用-map-side-预聚合的-shuffle-操作"><a href="#使用-map-side-预聚合的-shuffle-操作" class="headerlink" title="使用 map-side 预聚合的 shuffle 操作"></a>使用 map-side 预聚合的 shuffle 操作</h3><p>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。<br>所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。<br>比如下图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/35-Map-Side.png?raw=true" alt="35-Map-Side"></p><h3 id="使用高性能的算子"><a href="#使用高性能的算子" class="headerlink" title="使用高性能的算子"></a>使用高性能的算子</h3><p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。<br>使用reduceByKey/aggregateByKey替代groupByKey<br>详情见“原则五：使用map-side预聚合的shuffle操作”。<br>使用mapPartitions替代普通map<br>mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！<br>使用foreachPartitions替代foreach<br>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。<br>使用filter之后进行coalesce操作<br>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。<br>使用repartitionAndSortWithinPartitions替代repartition与sort类操作<br>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p><h3 id="广播大变量"><a href="#广播大变量" class="headerlink" title="广播大变量"></a>广播大变量</h3><p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。<br>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。<br>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。<br>广播大变量的代码示例</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 以下代码在算子函数中，使用了外部的变量。</span></span><br><span class="line"><span class="comment">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line">rdd1.map(list1...)</span><br><span class="line"><span class="comment">// 以下代码将list1封装成了Broadcast类型的广播变量。</span></span><br><span class="line"><span class="comment">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span></span><br><span class="line"><span class="comment">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span></span><br><span class="line"><span class="comment">// 每个Executor内存中，就只会驻留一份广播变量副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line"><span class="keyword">val</span> list1Broadcast = sc.broadcast(list1)</span><br><span class="line">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure><h3 id="使用Kryo优化序列化性能"><a href="#使用Kryo优化序列化性能" class="headerlink" title="使用Kryo优化序列化性能"></a>使用Kryo优化序列化性能</h3><p>在Spark中，主要有三个地方涉及到了序列化：<br>1、在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。<br>2、将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。<br>3、使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。<br>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。<br>以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建SparkConf对象。</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line"><span class="comment">// 设置序列化器为KryoSerializer。</span></span><br><span class="line">conf.set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line"><span class="comment">// 注册要序列化的自定义类型。</span></span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br></pre></td></tr></table></figure><h3 id="优化数据结构"><a href="#优化数据结构" class="headerlink" title="优化数据结构"></a>优化数据结构</h3><p>Java中，有三种类型比较耗费内存：<br>1、对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。<br>2、字符串，每个字符串内部都有一个字符数组以及长度等额外信息。<br>3、集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。<br>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p><h1 id="运行资源调优"><a href="#运行资源调优" class="headerlink" title="运行资源调优"></a>运行资源调优</h1><h2 id="调优概述-1"><a href="#调优概述-1" class="headerlink" title="调优概述"></a>调优概述</h2><p>在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。</p><h3 id="Spark-作业基本原理"><a href="#Spark-作业基本原理" class="headerlink" title="Spark 作业基本原理"></a>Spark 作业基本原理</h3><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/36-Spark-Job.png?raw=true" alt="36-Spark-Job"></p><p>详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团、大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。<br>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。<br>Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。<br>当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。<br>因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。<br>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。<br>以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。</p><h3 id="资源参数调优"><a href="#资源参数调优" class="headerlink" title="资源参数调优"></a>资源参数调优</h3><p>了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。<br>num-executors<br>参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。<br>参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。<br>executor-memory<br>参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。<br>参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。<br>executor-cores<br>参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。<br>参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors <em> executor-cores不要超过队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。<br>driver-memory<br>参数说明：该参数用于设置Driver进程的内存。<br>参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。<br>spark.default.parallelism<br>参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。<br>参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors </em> executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。<br>spark.storage.memoryFraction<br>参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。<br>参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。<br>spark.shuffle.memoryFraction<br>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。<br>参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。<br>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考给出的原理以及调优建议，合理地设置上述参数。<br>资源参数参考示例<br>以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --master yarn-cluster \</span><br><span class="line">  --num-executors <span class="number">100</span> \</span><br><span class="line">  --executor-memory <span class="number">6</span>G \</span><br><span class="line">  --executor-cores <span class="number">4</span> \</span><br><span class="line">  --driver-memory <span class="number">1</span>G \</span><br><span class="line">  --conf spark.<span class="keyword">default</span>.parallelism=<span class="number">1000</span> \</span><br><span class="line">  --conf spark.storage.memoryFraction=<span class="number">0.5</span> \</span><br><span class="line">  --conf spark.shuffle.memoryFraction=<span class="number">0.3</span> \</span><br></pre></td></tr></table></figure><h1 id="GC-调优"><a href="#GC-调优" class="headerlink" title="GC 调优"></a>GC 调优</h1><p>Spark立足内存计算，常常需要在内存中存放大量数据，因此也更依赖JVM的垃圾回收机制。与此同时，它也兼容批处理和流式处理，对于程序吞吐量和延迟都有较高要求，因此GC参数的调优在Spark应用实践中显得尤为重要。<br>按照经验来说，当我们配置垃圾收集器时，主要有两种策略——Parallel GC和CMS GC。前者注重更高的吞吐量，而后者则注重更低的延迟。两者似乎是鱼和熊掌，不能兼得。在实际应用中，我们只能根据应用对性能瓶颈的侧重性，来选取合适的垃圾收集器。例如，当我们运行需要有实时响应的场景的应用时，我们一般选用CMS GC，而运行一些离线分析程序时，则选用Parallel GC。那么对于Spark这种既支持流式计算，又支持传统的批处理运算的计算框架来说，是否存在一组通用的配置选项呢？<br>通常CMS GC是企业比较常用的GC配置方案，并在长期实践中取得了比较好的效果。例如对于进程中若存在大量寿命较长的对象，Parallel GC经常带来较大的性能下降。因此，即使是批处理的程序也能从CMS GC中获益。不过，在从1.6开始的HOTSPOT JVM中，我们发现了一个新的GC设置项：Garbage-First GC(G1 GC)，Oracle将其定位为CMS GC的长期演进。</p><h2 id="JVM-虚拟机"><a href="#JVM-虚拟机" class="headerlink" title="JVM 虚拟机"></a>JVM 虚拟机</h2><p>每个Java开发者都知道Java字节码是执行在JRE(Java Runtime Environment Java运行时环境）上的。JRE中最重要的部分是Java虚拟机（JVM），JVM负责分析和执行Java字节码。Java开发人员并不需要去关心JVM是如何运行的。在没有深入理解JVM的情况下，许多开发者已经开发出了非常多的优秀的应用以及Java类库。不过，如果你了解JVM的话，你会更加了解Java的，并且你会轻松解决那些看似简单但是无从下手的问题。</p><h3 id="虚拟机"><a href="#虚拟机" class="headerlink" title="虚拟机"></a>虚拟机</h3><p>JRE是由Java API和JVM组成的。JVM的主要作用是通过Class Loader来加载Java程序，并且按照Java API来执行加载的程序。<br>虚拟机是通过软件的方式来模拟实现的机器（比如说计算机），它可以像物理机一样运行程序。设计虚拟机的初衷是让Java能够通过它来实现WORA(Write Once Run Anywher 一次编译，到处运行），尽管这个目标现在已经被大多数人忽略了。因此，JVM可以在不修改Java代码的情况下，在所有的硬件环境上运行Java字节码。<br>Java虚拟机的特点如下：<br>1)    基于栈的虚拟机：Intel x86和ARM这两种最常见的计算机体系的机构都是基于寄存器的。不同的是，JVM是基于栈的。<br>2)    符号引用：除了基本类型以外的数据（类和接口）都是通过符号来引用，而不是通过显式地使用内存地址来引用。<br>3)    垃圾回收机制：类的实例都是通过用户代码进行创建，并且自动被垃圾回收机制进行回收。<br>4)    通过对基本类型的清晰定义来保证平台独立性：传统的编程语言，例如C/C++，int类型的大小取决于不同的平台。JVM通过对基本类型的清晰定义来保证它的兼容性以及平台独立性。<br>5)    网络字节码顺序：Java class文件用网络字节码顺序来进行存储:为了保证和小端的Intel x86架构以及大端的RISC系列的架构保持无关性，JVM使用用于网络传输的网络字节顺序，也就是大端。<br>虽然是Sun公司开发了Java，但是所有的开发商都可以开发并且提供遵循Java虚拟机规范的JVM。正是由于这个原因，使得Oracle HotSpot和IBM JVM等不同的JVM能够并存。Google的Android系统里的Dalvik VM也是一种JVM，虽然它并不遵循Java虚拟机规范。和基于栈的Java虚拟机不同，Dalvik VM是基于寄存器的架构，因此它的Java字节码也被转化成基于寄存器的指令集。</p><h3 id="Java-字节码"><a href="#Java-字节码" class="headerlink" title="Java 字节码"></a>Java 字节码</h3><p>为了保证WORA，JVM使用Java字节码这种介于Java和机器语言之间的中间语言。字节码是部署Java代码的最小单位。<br>在解释Java字节码之前，我们先通过实例来简单了解它。这个案例是一个在开发环境出现的真实案例的总结。<br>​     现象<br>一个一直运行正常的应用突然无法运行了。在类库被更新之后，返回下面的错误。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread <span class="string">"main"</span> java.lang.NoSuchMethodError: com.moqi.user.UserAdmin.addUser(Ljava/lang/String;)V  </span><br><span class="line">at com.moqi.service.UserService.add(UserService.java:<span class="number">14</span>)</span><br><span class="line">at com.moqi.service.UserService.main(UserService.java:<span class="number">19</span>)</span><br></pre></td></tr></table></figure><p>应用的代码如下，而且它没有被改动过。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// UserService.java  </span></span><br><span class="line">…  </span><br><span class="line">public void add(<span class="type">String</span> userName) &#123;  </span><br><span class="line">    admin.addUser(userName);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>更新后的类库的源代码和原始的代码如下。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// UserAdmin.java - Updated library source code  </span></span><br><span class="line">…  </span><br><span class="line"><span class="function"><span class="keyword">public</span> User <span class="title">addUser</span><span class="params">(String userName)</span> </span>&#123;  </span><br><span class="line">    User user = <span class="keyword">new</span> User(userName);  </span><br><span class="line">    User prevUser = userMap.put(userName, user);  </span><br><span class="line">    <span class="keyword">return</span> prevUser;  </span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"><span class="comment">// UserAdmin.java - Original library source code  </span></span><br><span class="line">…  </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addUser</span><span class="params">(String userName)</span> </span>&#123;  </span><br><span class="line">    User user = <span class="keyword">new</span> User(userName);  </span><br><span class="line">    userMap.put(userName, user);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>简而言之，之前没有返回值的addUser()被改修改成返回一个User类的实例的方法。不过，应用的代码没有做任何修改，因为它没有使用addUser()的返回值。<br>咋一看，com.moqi.user.UserAdmin.addUser()方法似乎仍然存在，如果存在的话，那么怎么还会出现NoSuchMethodError的错误呢？<br>原因<br>上面问题的原因是在于应用的代码没有用新的类库来进行编译。换句话来说，应用代码似乎是调了正确的方法，只是没有使用它的返回值而已。不管怎样，编译后的class文件表明了这个方法是有返回值的。你可以从下面的错误信息里看到答案。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError: com.aiguigu.user.UserAdmin.addUser(Ljava/lang/String;)V</span><br></pre></td></tr></table></figure><p>NoSuchMethodError出现的原因是“com.moqi.user.UserAdmin.addUser(Ljava/lang/String;)V”方法找不到。注意一下”Ljava/lang/String；”和最后面的“V”。在Java字节码的表达式里，”L<classname>;”表示的是类的实例。这里表示addUser()方法有一个java/lang/String的对象作为参数。在这个类库里，参数没有被改变，所以它是正常的。最后面的“V”表示这个方法的返回值。在Java字节码的表达式里，”V”表示没有返回子（Void）。综上所述，上面的错误信息是表示有一个java.lang.String类型的参数，并且没有返回值的com.moqi.user.UserAdmin.addUser方法没有找到。<br>因为应用是用之前的类库编译的，所以返回值为空的方法被调用了。但是在修改后的类库里，返回值为空的方法不存在，并且添加了一个返回值为“Lcom/moqi/user/User”的方法。因此，就出现了NoSuchMethodError。<br>这个错误出现的原因是因为开发者没有用新的类库来重新编译应用。不过，出现这种问题的大部分责任在于类库的提供者。这个public的方法本来没有返回值的，但是后来却被修改成返回User类的实例。很明显，方法的签名被修改了，这也表明了这个类库的后向兼容性被破坏了。因此，这个类库的提供者应该告知使用者这个方法已经被改变了。<br>我们再回到Java字节码上来。Java字节码是JVM很重要的部分。JVM是模拟执行Java字节码的一个模拟器。Java编译器不会直接把高级语言（例如C/C++）编写的代码直接转换成机器语言（CPU指令）；它会把开发者可以理解的Java语言转换成JVM能够理解的Java字节码。因为Java字节码本身是平台无关的，所以它可以在任何安装了JVM（确切地说，是相匹配的JRE）的硬件上执行，即使是在CPU和OS都不相同的平台上（在Windows PC上开发和编译的字节码可以不做任何修改就直接运行在Linux机器上）。编译后的代码的大小和源代码大小基本一致，这样就可以很容易地通过网络来传输和执行编译后的代码。<br>Java class文件是一种人很难去理解的二进文件。为了便于理解它，JVM提供者提供了javap，反汇编器。使用javap产生的结果是Java汇编语言。在上面的例子中，下面的Java汇编代码是通过javap-c对UserServiceadd()方法进行反汇编得到的。</classname></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(java.lang.String)</span></span>;  </span><br><span class="line">  Code:  </span><br><span class="line">   <span class="number">0</span>:   aload_0  </span><br><span class="line">   1:   getfield        #15; //Field admin:Lcom/moqi/user/UserAdmin;  </span><br><span class="line">   <span class="number">4</span>:   aload_1  </span><br><span class="line">   5:   invokevirtual   #23; //Method com/moqi/user/UserAdmin.addUser:(Ljava/lang/String;)V  </span><br><span class="line">   <span class="number">8</span>:   <span class="keyword">return</span></span><br></pre></td></tr></table></figure><p>invokeinterface:调用一个接口方法在这段Java汇编代码中，addUser()方法是在第四行的“5:invokevitual#23″进行调用的。这表示对应索引为23的方法会被调用。索引为23的方法的名称已经被javap给注解在旁边了。invokevirtual是Java字节码里调用方法的最基本的操作码。在Java字节码里，有四种操作码可以用来调用一个方法，分别是：invokeinterface，invokespecial，invokestatic以及invokevirtual。操作码的作用分别如下：<br>1)    invokespecial: 调用一个初始化方法，私有方法或者父类的方法<br>2)    invokestatic:调用静态方法<br>3)    invokevirtual:调用实例方法<br>Java字节码的指令集由操作码和操作数组成。类似invokevirtual这样的操作数需要2个字节的操作数。<br>用更新的类库来编译上面的应用代码，然后反编译它，将会得到下面的结果。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(java.lang.String)</span></span>;  </span><br><span class="line">  Code:  </span><br><span class="line">   <span class="number">0</span>:   aload_0  </span><br><span class="line">   1:   getfield        #15; //Field admin:Lcom/moqi/user/UserAdmin;  </span><br><span class="line">   <span class="number">4</span>:   aload_1  </span><br><span class="line">   5:   invokevirtual   #23; //Method com/moqi/user/UserAdmin.addUser:(Ljava/lang/String;)Lcom/moqi/user/User;  </span><br><span class="line">   <span class="number">8</span>:   pop  </span><br><span class="line">   <span class="number">9</span>:   <span class="keyword">return</span></span><br></pre></td></tr></table></figure><p>你会发现，对应索引为23的方法被替换成了一个返回值为”Lcom/moqi/user/User”的方法。<br>在上面的反汇编代码里，代码前面的数字代码什么呢？<br>它表示的是字节数。大概这就是为什么运行在JVM上面的代码成为Java“字节”码的原因。简而言之，Java字节码指令的操作码，例如aload_0，getfield和invokevirtual等，都是用一个字节的数字来表示的（aload_0=0x2a,getfield=0xb4,invokevirtual=0xb6)。由此可知Java字节码指令的操作码最多有256个。<br>aload_0和aload_1这样的指令不需要任何操作数。因此，aload_0指令的下一个字节是下一个指令的操作码。不过，getfield和invokevirtual指令需要2字节的操作数。因此，getfiled的下一条指令是跳过两个字节，写在第四个字节的位置上的。十六进制编译器里查看字节码的结果如下所示。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2</span>a b4 <span class="number">00</span> <span class="number">0f</span> <span class="number">2</span>b b6 <span class="number">00</span> <span class="number">17</span> <span class="number">57</span> b1</span><br></pre></td></tr></table></figure><p>表一：Java字节码中的类型表达式在Java字节码里，类的实例用字母“L;”表示，void 用字母“V”表示。通过这种方式，其他的类型也有对应的表达式。下面的表格对此作了总结。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/37-Java-ByteCode.png?raw=true" alt="37-Java-ByteCode"></p><p>下面的表格给出了字节码表达式的几个实例。<br>表二：Java字节码表达式范例</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/38-Java-ByteCode-Example.png?raw=true" alt="38-Java-ByteCode-Example"></p><h3 id="Class-文件格式"><a href="#Class-文件格式" class="headerlink" title="Class 文件格式"></a>Class 文件格式</h3><p>在讲解Java class文件格式之前，我们先看看一个在Java Web应用中经常出现的问题。<br>当我们编写完Jsp代码，并且在Tomcat运行时，Jsp代码没有正常运行，而是出现了下面的错误。<br>现象<br>当我们编写完Jsp代码，并且在Tomcat运行时，Jsp代码没有正常运行，而是出现了下面的错误。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Servlet.service() for servlet jsp threw exception org.apache.jasper.JasperException: Unable to compile class for JSP Generated servlet error:  </span><br><span class="line"><span class="function">The code of method <span class="title">_jspService</span><span class="params">(HttpServletRequest, HttpServletResponse)</span> is exceeding the 65535 bytes limit"</span></span><br></pre></td></tr></table></figure><p>原因在不同的Web服务器上，上面的错误信息可能会有点不同，不过有有一点肯定是相同的，它出现的原因是65535字节的限制。这个65535字节的限制是JVM规范里的限制，它规定了一个方法的大小不能超过65535字节。<br>下面我会更加详细地讲解这个65535字节限制的意义以及它出现的原因。<br>Java字节码里的分支和跳转指令分别是”goto”和”jsr”。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">goto [branchbyte1] [branchbyte2]  </span><br><span class="line">jsr [branchbyte1] [branchbyte2]</span><br></pre></td></tr></table></figure><p>这两个指令都接收一个2字节的有符号的分支跳转偏移量做为操作数，因此偏移量最大只能达到65535。不过，为了支持更多的跳转，Java字节码提供了”goto_w”和”jsr_w”这两个可以接收4字节分支偏移的指令。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">goto_w [branchbyte1] [branchbyte2] [branchbyte3] [branchbyte4]  </span><br><span class="line">jsr_w [branchbyte1] [branchbyte2] [branchbyte3] [branchbyte4]</span><br></pre></td></tr></table></figure><p>有了这两个指令，索引超过65535的分支也是可用的。因此，Java方法的65535字节的限制就可以解除了。不过，由于Java class文件的更多的其他的限制，使得Java方法还是不能超过65535字节。<br>为了展示其他的限制，我会简单讲解一下class 文件的格式。<br>Java class文件的大致结构如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ClassFile &#123;  </span><br><span class="line">    u4 magic;  </span><br><span class="line">    u2 minor_version;  </span><br><span class="line">    u2 major_version;  </span><br><span class="line">    u2 constant_pool_count;  </span><br><span class="line">    cp_info constant_pool[constant_pool_count-<span class="number">1</span>];  </span><br><span class="line">    u2 access_flags;  </span><br><span class="line">    u2 this_class;  </span><br><span class="line">    u2 super_class;  </span><br><span class="line">u2 interfaces_count;  </span><br><span class="line">u2 interfaces[interfaces_count];  </span><br><span class="line">u2 fields_count;  </span><br><span class="line">field_info fields[fields_count];  </span><br><span class="line">u2 methods_count;  </span><br><span class="line">method_info methods[methods_count];  </span><br><span class="line">u2 attributes_count;  </span><br><span class="line">attribute_info attributes[attributes_count];&#125;</span><br></pre></td></tr></table></figure><p>之前反汇编的UserService.class文件反汇编的结果的前16个字节在十六进制编辑器中如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ca fe ba be <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">32</span> <span class="number">00</span> <span class="number">28</span> <span class="number">07</span> <span class="number">00</span> <span class="number">02</span> <span class="number">01</span> <span class="number">00</span> <span class="number">1</span>b</span><br></pre></td></tr></table></figure><p>通过这些数值，我们可以来看看class文件的格式。<br>1)    magic：class文件最开始的四个字节是魔数。它的值是用来标识Java class文件的。从上面的内容里可以看出，魔数 的值是0xCAFEBABE。简而言之，只有一个文件的起始4字节是0xCAFEBABE的时候，它才会被当作Java class文件来处理。<br>2)    minor_version,major_version:接下来的四个字节表示的是class文件的版本。UserService.class文件里的是0x00000032，所以这个class文件的版本是50.0。JDK 1.6编译的class文件的版本是50.0，JDK 1.5编译出来的class文件的版本是49.0。JVM必须对低版本的class文件保持后向兼容性，也就是低版本的class文件可以运行在高版本的JVM上。不过，反过来就不行了，当一个高版本的class文件运行在低版本的JVM上时，会出现java.lang.UnsupportedClassVersionError的错误。<br>3)    constant_pool_count,constant_pool[]:在版本号之后，存放的是类的常量池。这里保存的信息将会放入运行时常量池(Runtime Constant Pool)中去，这个后面会讲解的。在加载一个class文件的时候，JVM会把常量池里的信息存放在方法区的运行时常量区里。UserService.class文件里的constant_pool_count的值是0x0028，这表示常量池里有39(40-1)个常量。<br>4)    access_flags:这是表示一个类的描述符的标志；换句话说，它表示一个类是public,final还是abstract以及是不是接口的标志。<br>5)    fields_count,fields[]:当前类的成员变量的数量以及成员变量的信息。成员变量的信息包含变量名，类型，修饰符以及变量在constant_pool里的索引。<br>6)    methods_count,methods[]:当前类的方法数量以及方法的信息。方法的信息包含方法名，参数的数量和类型，返回值的类型，修饰符，以及方法在constant_pool里的索引，方法的可执行代码以及异常信息。<br>7)    attributes_count,attributes[]:attribution_info结构包含不同种类的属性。field_info和method_info里都包含了attribute_info结构。<br>javap简要地给出了class文件的一个可读形式。当你用”java -verbose”命令来分析UserService.class时，会输出如下的内容：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">Compiled from <span class="string">"UserService.java"</span> </span><br><span class="line">   </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">com</span>.<span class="title">moqi</span>.<span class="title">service</span>.<span class="title">UserService</span> <span class="keyword">extends</span> <span class="title">java</span>.<span class="title">lang</span>.<span class="title">Object</span>  </span></span><br><span class="line">  SourceFile: "UserService.java" </span><br><span class="line">  minor version: <span class="number">0</span> </span><br><span class="line">  major version: <span class="number">50</span> </span><br><span class="line">  Constant pool:const #1 = class        #2;     //  com/moqi/service/UserService  </span><br><span class="line">  const #2 = Asciz        com/moqi/service/UserService;  </span><br><span class="line">  const #3 = class        #4;     //  java/lang/Object  </span><br><span class="line">const #4 = Asciz        java/lang/Object;  </span><br><span class="line">const #5 = Asciz        admin;  </span><br><span class="line">const #6 = Asciz        Lcom/moqi/user/UserAdmin;;// … omitted - constant pool continued …  </span><br><span class="line">   </span><br><span class="line">&#123;  </span><br><span class="line"><span class="comment">// … omitted - method information …  </span></span><br><span class="line">   </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(java.lang.String)</span></span>;  </span><br><span class="line">  Code:  </span><br><span class="line">   Stack=<span class="number">2</span>, Locals=<span class="number">2</span>, Args_size=<span class="number">2</span> </span><br><span class="line">   <span class="number">0</span>:   aload_0  </span><br><span class="line">   1:   getfield        #15; //Field admin:Lcom/moqi/user/UserAdmin;  </span><br><span class="line">   <span class="number">4</span>:   aload_1  </span><br><span class="line">   5:   invokevirtual   #23; //Method com/moqi/user/UserAdmin.addUser:(Ljava/lang/String;)Lcom/nhn/user/User;  </span><br><span class="line">   <span class="number">8</span>:   pop  </span><br><span class="line">   <span class="number">9</span>:   <span class="keyword">return</span>  LineNumberTable:  </span><br><span class="line">   line <span class="number">14</span>: <span class="number">0</span> </span><br><span class="line">   line <span class="number">15</span>: <span class="number">9</span>  LocalVariableTable:  </span><br><span class="line">   Start  Length  Slot  Name   Signature  </span><br><span class="line">   <span class="number">0</span>      <span class="number">10</span>      <span class="number">0</span>    <span class="keyword">this</span>       Lcom/moqi/service/UserService;  </span><br><span class="line">   <span class="number">0</span>      <span class="number">10</span>      <span class="number">1</span>    userName       Ljava/lang/String; <span class="comment">// … Omitted - Other method information …  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>javap输出的内容太长，我这里只是提出了整个输出的一部分。整个的输出展示了constant_pool里的不同信息，以及方法的内容。<br>关于方法的65565字节大小的限制是和method_info struct相关的。method_info结构包含Code,LineNumberTable,以及LocalViriable attribute几个属性，这个在“javap -verbose”的输出里可以看到。Code属性里的LineNumberTable，LocalVariableTable以及exception_table的长度都是用一个固定的2字节来表示的。因此，方法的大小是不能超过LineNumberTable，LocalVariableTable以及exception_table的长度的，它们都是65535字节。<br>许多人都在抱怨方法的大小限制，而且在JVM规范里还说名了”这个长度以后有可能会是可扩展的“。不过，到现在为止，还没有为这个限制做出任何动作。从JVM规范里的把class文件里的内容直接拷贝到方法区这个特点来看，要想在保持后向兼容性的同时来扩展方法区的大小是非常困难的。<br>如果因为Java编译器的错误而导致class文件的错误，会怎么样呢？或者，因为网络传输的错误导致拷贝的class文件的损坏呢？<br>为了预防这种场景，Java的类装载器通过一个严格而且慎密的过程来校验class文件。在JVM规范里详细地讲解了这方面的内容。<br>注意<br>我们怎样能够判断JVM正确地执行了class文件校验的所有过程呢？我们怎么来判断不同提供商的不同JVM实现是符合JVM规范的呢？为了能够验证以上两点，Oracle提供了一个测试工具TCK(Technology Compatibility Kit)。这个TCK工具通过执行成千上万的测试用例来验证一个JVM是否符合规范，这些测试里面包含了各种非法的class文件。只有通过了TCK的测试的JVM才能称作JVM。<br>和TCK相似，有一个组织JCP(Java Community Process;<a href="http://jcp.org)负责Java规范以及新的Java技术规范。对于JCP而言，如果要完成一项Java规范请求" target="_blank" rel="noopener">http://jcp.org)负责Java规范以及新的Java技术规范。对于JCP而言，如果要完成一项Java规范请求</a>(Java Specification Request, JSR)的话，需要具备规范文档，可参考的实现以及通过TCK测试。任何人如果想使用一项申请JSR的新技术的话，他要么使用RI提供许可的实现，要么自己实现一个并且保证通过TCK的测试。</p><h3 id="JVM-结构"><a href="#JVM-结构" class="headerlink" title="JVM 结构"></a>JVM 结构</h3><p>Java编写的代码会按照下图的流程来执行：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/39-JVM.png?raw=true" alt="39-JVM"></p><p>类装载器装载负责装载编译后的字节码，并加载到运行时数据区（Runtime Data Area），然后执行引擎执行会执行这些字节码。<br>类加载器（Class Loader）<br>Java提供了动态的装载特性；它会在运行时的第一次引用到一个class的时候对它进行装载和链接，而不是在编译期进行。JVM的类装载器负责动态装载。Java类装载器有如下几个特点：<br>•    层级结构：Java里的类装载器被组织成了有父子关系的层级结构。Bootstrap类装载器是所有装载器的父亲。<br>•    代理模式：基于层级结构，类的装载可以在装载器之间进行代理。当装载器装载一个类时，首先会检查它是否在父装载器中进行装载了。如果上层的装载器已经装载了这个类，这个类会被直接使用。反之，类装载器会请求装载这个类。<br>•    可见性限制：一个子装载器可以查找父装载器中的类，但是一个父装载器不能查找子装载器里的类。<br>•    不允许卸载：类装载器可以装载一个类但是不可以卸载它，不过可以删除当前的类装载器，然后创建一个新的类装载器。<br>每个类装载器都有一个自己的命名空间用来保存已装载的类。当一个类装载器装载一个类时，它会通过保存在命名空间里的类全局限定名(Fully Qualified Class Name)进行搜索来检测这个类是否已经被加载了。如果两个类的全局限定名是一样的，但是如果命名空间不一样的话，那么它们还是不同的类。不同的命名空间表示class被不同的类装载器装载。<br>下图展示了类装载器的代理模型。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/40-Class-Loader.png?raw=true" alt="40-Class-Loader"></p><p>当一个类装载器（class loader）被请求装载类时，它首先按照顺序在上层装载器、父装载器以及自身的装载器的缓存里检查这个类是否已经存在。简单来说，就是在缓存里查看这个类是否已经被自己装载过了，如果没有的话，继续查找父类的缓存，直到在bootstrap类装载器里也没有找到的话，它就会自己在文件系统里去查找并且加载这个类。<br>•    启动类加载器（Bootstrap class loader）:这个类装载器是在JVM启动的时候创建的。它负责装载Java API，包含Object对象。和其他的类装载器不同的地方在于这个装载器是通过native code来实现的，而不是用Java代码。<br>•    扩展类加载器（Extension class loader）:它装载除了基本的Java API以外的扩展类。它也负责装载其他的安全扩展功能。<br>•    系统类加载器（System class loader）:如果说bootstrap class loader和extension class loader负责加载的是JVM的组件，那么system class loader负责加载的是应用程序类。它负责加载用户在$CLASSPATH里指定的类。<br>•    用户自定义类加载器（User-defined class loader）:这是应用程序开发者用直接用代码实现的类装载器。<br>类似于web应用服务(WAS)之类的框架会用这种结构来对Web应用和企业级应用进行分离。换句话来说，类装载器的代理模型可以用来保证不同应用之间的相互独立。WAS类装载器使用这种层级结构，不同的WAS供应商的装载器结构有稍许区别。<br>如果类装载器查找到一个没有装载的类，它会按照下图的流程来装载和链接这个类：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/41-Class-Loader-Stage.png?raw=true" alt="41-Class-Loader-Stage"></p><p>每个阶段的描述如下：<br>•    Loading: 类的信息从文件中获取并且载入到JVM的内存里。<br>•    Verifying:检查读入的结构是否符合Java语言规范以及JVM规范的描述。这是类装载中最复杂的过程，并且花费的时间也是最长的。并且JVM TCK工具的大部分场景的用例也用来测试在装载错误的类的时候是否会出现错误。<br>•    Preparing:分配一个结构用来存储类信息，这个结构中包含了类中定义的成员变量，方法和接口的信息。<br>•    Resolving:把这个类的常量池中的所有的符号引用改变成直接引用。<br>•    Initializing:把类中的变量初始化成合适的值。执行静态初始化程序，把静态变量初始化成指定的值。<br>JVM规范定义了上面的几个任务，不过它允许具体执行的时候能够有些灵活的变动。<br>运行时数据区(Runtime Data Areas)</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/42-Runtime-Data-Areas.png?raw=true" alt="42-Runtime-Data-Areas"></p><p>运行时数据区是在JVM运行的时候操作所分配的内存区。运行时内存区可以划分为6个区域。在这6个区域中，一个PC Register,JVM stack 以及Native Method Statck都是按照线程创建的，Heap,Method Area以及Runtime Constant Pool都是被所有线程公用的。<br>•    PC寄存器(PC register):每个线程启动的时候，都会创建一个PC(Program Counter ,程序计数器)寄存器。PC寄存器里保存有当前正在执行的JVM指令的地址。<br>•    JVM 堆栈(JVM stack)：每个线程启动的时候，都会创建一个JVM堆栈。它是用来保存栈帧的。JVM只会在JVM堆栈上对栈帧进行push和pop的操作。如果出现了异常，堆栈跟踪信息的每一行都代表一个栈帧立的信息，这些信息它是通过类似于printStackTrace()这样的方法来展示的。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/43-JVM-Stack.png?raw=true" alt="43-JVM-Stack"></p><p>•    栈帧(stack frame)：每当一个方法在JVM上执行的时候，都会创建一个栈帧，并且会添加到当前线程的JVM堆栈上。当这个方法执行结束的时候，这个栈帧就会被移除。每个栈帧里都包含有当前正在执行的方法所属类的本地变量数组，操作数栈，以及运行时常量池的引用。本地变量数组的和操作数栈的大小都是在编译时确定的。因此，一个方法的栈帧的大小也是固定不变的。<br>•    局部变量数组(Local variable array)：这个数组的索引从0开始。索引为0的变量表示这个方法所属的类的实例。从1开始，首先存放的是传给该方法的参数，在参数后面保存的是方法的局部变量。<br>•    操作数栈(Operand stack)：方法实际运行的工作空间。每个方法都在操作数栈和局部变量数组之间交换数据，并且压入或者弹出其他方法返回的结果。操作数栈所需的最大空间是在编译期确定的。因此，操作数栈的大小也可以在编译期间确定。<br>•    本地方法栈(Native method stack)：供用非Java语言实现的本地方法的堆栈。换句话说，它是用来调用通过JNI(Java Native Interface Java本地接口）调用的C/C++代码。根据具体的语言，一个C堆栈或者C++堆栈会被创建。<br>•    方法区(Method area)：方法区是所有线程共享的，它是在JVM启动的时候创建的。它保存所有被JVM加载的类和接口的运行时常量池，成员变量以及方法的信息，静态变量以及方法的字节码。JVM的提供者可以通过不同的方式来实现方法区。在Oracle 的HotSpot JVM里，方法区被称为永久区或者永久代（PermGen）。是否对方法区进行垃圾回收对JVM的实现是可选的。<br>•    运行时常量池(Runtime constant pool)：这个区域和class文件里的constant_pool是相对应的。这个区域是包含在方法区里的，不过，对于JVM的操作而言，它是一个核心的角色。因此在JVM规范里特别提到了它的重要性。除了包含每个类和接口的常量，它也包含了所有方法和变量的引用。简而言之，当一个方法或者变量被引用时，JVM通过运行时常量区来查找方法或者变量在内存里的实际地址。<br>•    堆(Heap)：用来保存实例或者对象的空间，而且它是垃圾回收的主要目标。当讨论类似于JVM性能之类的问题时，它经常会被提及。JVM提供者可以决定怎么来配置堆空间，以及不对它进行垃圾回收。<br>现在我们再会过头来看看之前反汇编的字节码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(java.lang.String)</span></span>;  </span><br><span class="line">  Code:  </span><br><span class="line">   <span class="number">0</span>:   aload_0  </span><br><span class="line">   1:   getfield        #15; //Field admin:Lcom/nhn/user/UserAdmin;  </span><br><span class="line">   <span class="number">4</span>:   aload_1  </span><br><span class="line">   5:   invokevirtual   #23; //Method com/nhn/user/UserAdmin.addUser:(Ljava/lang/String;)Lcom/nhn/user/User;  </span><br><span class="line">   <span class="number">8</span>:   pop  </span><br><span class="line">   <span class="number">9</span>:   <span class="keyword">return</span></span><br></pre></td></tr></table></figure><p>把上面的反汇编代码和我们平时所见的x86架构的汇编代码相比较，我们会发现这两者的结构有点相似，都使用了操作码；不过，有一点不同的地方是Java字节码并不会在操作数里写入寄存器的名称、内存地址或者偏移量。之前已经说过，JVM用的是栈，它不会使用寄存器。和使用寄存器的x86架构不同，它自己负责内存的管理。它用索引例如15和23来代替实际的内存地址。15和23都是当前类（这里是UserService类）的常量池里的索引。简而言之，JVM为每个类创建了一个常量池，并且这个常量池里保存了实际目标的引用。<br>每行反汇编代码的解释如下：<br>•    aload_0:把局部变量数组中索引为#0的变量添加到操作数栈上。索引#0所表示的变量是this，即是当前实例的引用。<br>•    getfield #15:把当前类的常量池里的索引为#15的变量添加到操作数栈。这里添加的是UserAdmin的admin成员变量。因为admin变量是个类的实例，因此添加的是一个引用。<br>•    aload_1:把局部变量数组里的索引为#1的变量添加到操作数栈。来自局部变量数组里的索引为1的变量是方法的一个参数。因此，在调用add()方法的时候，会把userName指向的String的引用添加到操作数栈上。<br>•    invokevirtual #23:调用当前类的常量池里的索引为#23的方法。这个时候，通过getfile和aload_1添加到操作数栈上的引用都被作为方法的参数。当方法运行完成并且返回时，它的返回值会被添加到操作数栈上。<br>•    pop:把通过invokevirtual调用的方法的返回值从操作数栈里弹出来。你可以看到，在前面的例子里，用老的类库编译的那段代码是没有返回值的。简而言之，正因为之前的代码没有返回值，所以没必要吧把返回值从操作数栈上给弹出来。<br>•    return：结束当前方法调用<br>下图可以帮助你更好地理解上面的内容。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/44-UserService.png?raw=true" alt="44-UserService"></p><p>顺便提一下，在这个方法里，局部变量数组没有被修改。所以上图只显示了操作数栈的变化。不过，大部分的情况下，局部变量数组也是会改变的。局部变量数组和操作数栈之间的数据传输是使用通过大量的load指令(aload,iload)和store指令（astore,istore)来实现的。<br>在这个图里，我们简单验证了运行时常量池和JVM栈的描述。当JVM运行的时候，每个类的实例都会在堆上进行分配，User，UserAdmin，UserService以及String等类的信息都会保存在方法区。<br>执行引擎（Execution Engine）通过类装载器装载的，被分配到JVM的运行时数据区的字节码会被执行引擎执行。执行引擎以指令为单位读取Java字节码。它就像一个CPU一样，一条一条地执行机器指令。每个字节码指令都由一个1字节的操作码和附加的操作数组成。执行引擎取得一个操作码，然后根据操作数来执行任务，完成后就继续执行下一条操作码。<br>不过Java字节码是用一种人类可以读懂的语言编写的，而不是用机器可以直接执行的语言。因此，执行引擎必须把字节码转换成可以直接被JVM执行的语言。字节码可以通过以下两种方式转换成合适的语言。<br>•    解释器：一条一条地读取，解释并且执行字节码指令。因为它一条一条地解释和执行指令，所以它可以很快地解释字节码，但是执行起来会比较慢。这是解释执行的语言的一个缺点。字节码这种“语言”基本来说是解释执行的。<br>•    即时（Just-In-Time)编译器：即时编译器被引入用来弥补解释器的缺点。执行引擎首先按照解释执行的方式来执行，然后在合适的时候，即时编译器把整段字节码编译成本地代码。然后，执行引擎就没有必要再去解释执行方法了，它可以直接通过本地代码去执行它。执行本地代码比一条一条进行解释执行的速度快很多。编译后的代码可以执行的很快，因为本地代码是保存在缓存里的。<br>不过，用JIT编译器来编译代码所花的时间要比用解释器去一条条解释执行花的时间要多。因此，如果代码只被执行一次的话，那么最好还是解释执行而不是编译后再执行。因此，内置了JIT编译器的JVM都会检查方法的执行频率，如果一个方法的执行频率超过一个特定的值的话，那么这个方法就会被编译成本地代码。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/45-JIT.png?raw=true" alt="45-JIT"></p><p>JVM规范没有定义执行引擎该如何去执行。因此，JVM的提供者通过使用不同的技术以及不同类型的JIT编译器来提高执行引擎的效率。<br>大部分的JIT编译器都是按照下图的方式来执行的：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/46-JIT.png?raw=true" alt="46-JIT"></p><p>JIT编译器把字节码转换成一个中间层表达式，一种中间层的表示方式，来进行优化，然后再把这种表示转换成本地代码。<br>Oracle Hotspot VM使用一种叫做热点编译器的JIT编译器。它之所以被称作”热点“是因为热点编译器通过分析找到最需要编译的“热点”代码，然后把热点代码编译成本地代码。如果已经被编译成本地代码的字节码不再被频繁调用了，换句话说，这个方法不再是热点了，那么Hotspot VM会把编译过的本地代码从cache里移除，并且重新按照解释的方式来执行它。Hotspot VM分为Server VM和Client VM两种，这两种VM使用不同的JIT编译器。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/47-Hotspot-VM.png?raw=true" alt="47-Hotspot-VM"></p><p>Client VM 和Server VM使用完全相同的运行时，不过如上图所示，它们所使用的JIT编译器是不同的。Server VM用的是更高级的动态优化编译器，这个编译器使用了更加复杂并且更多种类的性能优化技术。<br>IBM 在IBM JDK 6里不仅引入了JIT编译器，它同时还引入了AOT(Ahead-Of-Time)编译器。它使得多个JVM可以通过共享缓存来共享编译过的本地代码。简而言之，通过AOT编译器编译过的代码可以直接被其他JVM使用。除此之外，IBM JVM通过使用AOT编译器来提前把代码编译器成JXE（Java EXecutable)文件格式来提供一种更加快速的执行方式。<br>大部分Java程序的性能都是通过提升执行引擎的性能来达到的。正如JIT编译器一样，很多优化的技术都被引入进来使得JVM的性能一直能够得到提升。最原始的JVM和最新的JVM最大的差别之处就是在于执行引擎。<br>Hotspot编译器在1.3版本的时候就被引入到Oracle Hotspot VM里了，JIT编译技术在Anroid 2.2版本的时候被引入到Dalvik VM里。<br>引入一种中间语言，例如字节码，虚拟机执行字节码，并且通过JIT编译器来提升JVM的性能的这种技术以及广泛应用在使用中间语言的编程语言上。例如微软的.Net，CLR（Common Language Runtime 公共语言运行时），也是一种VM，它执行一种被称作CIL（Common Intermediate Language）的字节码。CLR提供了AOT编译器和JIT编译器。因此，用C#或者VB.NET编写的源代码被编译后，编译器会生成CIL并且CIL会执行在有JIT编译器的CLR上。CLR和JVM相似，它也有垃圾回收机制，并且也是基于堆栈运行。<br>Java 虚拟机规范，Java SE 第7版2011年7月28日，Oracle发布了Java SE的第7个版本，并且把JVM规也更新到了相应的版本。在1999年发布《The Java Virtual Machine Specification,Second Edition》后，Oracle花了12年来发布这个更新的版本。这个更新的版本包含了这12年来累积的众多变化以及修改，并且更加细致地对规范进行了描述。此外，它还反映了《The Java Language Specificaion,Java SE 7 Edition》里的内容。主要的变化总结如下：<br>•    来自Java SE 5.0里的泛型，支持可变参数的方法<br>•    从Java SE 6以来，字节码校验的处理技术所发生的改变<br>•    添加invokedynamic指令以及class文件对于该指令的支持<br>•    删除了关于Java语言概念的内容，并且指引读者去参考Java语言规范<br>•    删除关于Java线程和锁的描述，并且把它们移到Java语言规范里<br>最大的改变是添加了invokedynamic指令。也就是说JVM的内部指令集做了修改，使得JVM开始支持动态类型的语言，这种语言的类型不是固定的，例如脚本语言以及来自Java SE 7里的Java语言。之前没有被用到的操作码186被分配给新指令invokedynamic，而且class文件格式里也添加了新的内容来支持invokedynamic指令。<br>Java SE 7的编译器生成的class文件的版本号是51.0。Java SE 6的是50.0。class文件的格式变动比较大，因此，51.0版本的class文件不能够在Java SE 6的虚拟机上执行。<br>尽管有了这么多的变动，但是Java方法的65535字节的限制还是没有被去掉。除非class文件的格式彻底改变，否者这个限制将来也是不可能去掉的。<br>值得说明的是，Oracle Java SE 7 VM支持G1这种新的垃圾回收机制，不过，它被限制在Oracle JVM上，因此，JVM本身对于垃圾回收的实现不做任何限制。也因此，在JVM规范里没有对它进行描述。<br>switch语句里的StringJava SE 7里添加了很多新的语法和特性。不过，在Java SE 7的版本里，相对于语言本身而言，JVM没有多少的改变。那么，这些新的语言特性是怎么来实现的呢？我们通过反汇编的方式来看看switch语句里的String（把字符串作为switch()语句的比较对象）是怎么实现的？<br>例如，下面的代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SwitchTest</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SwitchTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">doSwitch</span><span class="params">(String str)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">switch</span> (str) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">"abc"</span>:        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">"123"</span>:        <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">default</span>:         <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因为这是Java SE 7的一个新特性，所以它不能在Java SE 6或者更低版本的编译器上来编译。用Java SE 7的javac来编译。下面是通过javap -c来反编译后的结果。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">C:Test&gt;javap -c SwitchTest.classCompiled from <span class="string">"SwitchTest.java"</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SwitchTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">SwitchTest</span><span class="params">()</span></span>;</span><br><span class="line">    Code:</span><br><span class="line">       <span class="number">0</span>: aload_0</span><br><span class="line">       1: invokespecial #1                  // Method java/lang/Object."&lt;init&gt;":()V</span><br><span class="line">       <span class="number">4</span>: <span class="keyword">return</span>  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">doSwitch</span><span class="params">(java.lang.String)</span></span>;</span><br><span class="line">    Code:</span><br><span class="line">       <span class="number">0</span>: aload_1</span><br><span class="line">       <span class="number">1</span>: astore_2</span><br><span class="line">       <span class="number">2</span>: iconst_m1</span><br><span class="line">       <span class="number">3</span>: istore_3</span><br><span class="line">       <span class="number">4</span>: aload_2</span><br><span class="line">       5: invokevirtual #2                  // Method java/lang/String.hashCode:()I</span><br><span class="line">       <span class="number">8</span>: lookupswitch  &#123; <span class="comment">// 2</span></span><br><span class="line">                 <span class="number">48690</span>: <span class="number">50</span></span><br><span class="line">                 <span class="number">96354</span>: <span class="number">36</span></span><br><span class="line">               <span class="keyword">default</span>: <span class="number">61</span></span><br><span class="line">          &#125;</span><br><span class="line">      <span class="number">36</span>: aload_2</span><br><span class="line">      37: ldc           #3                  // String abc</span><br><span class="line">      39: invokevirtual #4                  // Method java/lang/String.equals:(Ljava/lang/Object;)Z</span><br><span class="line">      <span class="number">42</span>: ifeq          <span class="number">61</span></span><br><span class="line">      <span class="number">45</span>: iconst_0</span><br><span class="line">      <span class="number">46</span>: istore_3</span><br><span class="line">      <span class="number">47</span>: goto          <span class="number">61</span></span><br><span class="line">      <span class="number">50</span>: aload_2</span><br><span class="line">      51: ldc           #5                  // String 123</span><br><span class="line">      53: invokevirtual #4                  // Method java/lang/String.equals:(Ljava/lang/Object;)Z</span><br><span class="line">      <span class="number">56</span>: ifeq          <span class="number">61</span></span><br><span class="line">      <span class="number">59</span>: iconst_1</span><br><span class="line">      <span class="number">60</span>: istore_3</span><br><span class="line">      <span class="number">61</span>: iload_3</span><br><span class="line">      <span class="number">62</span>: lookupswitch  &#123; <span class="comment">// 2</span></span><br><span class="line">                     <span class="number">0</span>: <span class="number">88</span></span><br><span class="line">                     <span class="number">1</span>: <span class="number">90</span></span><br><span class="line">               <span class="keyword">default</span>: <span class="number">92</span></span><br><span class="line">          &#125;</span><br><span class="line">      <span class="number">88</span>: iconst_1</span><br><span class="line">      <span class="number">89</span>: ireturn</span><br><span class="line">      <span class="number">90</span>: iconst_2</span><br><span class="line">      <span class="number">91</span>: ireturn</span><br><span class="line">      <span class="number">92</span>: iconst_0</span><br><span class="line">      <span class="number">93</span>: ireturn</span><br></pre></td></tr></table></figure><p>在#5和#8字节处，首先是调用了hashCode()方法，然后它作为参数调用了switch(int)。在lookupswitch的指令里，根据hashCode的结果进行不同的分支跳转。字符串“abc”的hashCode是96354，它会跳转到#36处。字符串”123“的hashCode是48690，它会跳转到#50处。生成的字节码的长度比Java源码长多了。首先，你可以看到字节码里用lookupswitch指令来实现switch()语句。不过，这里使用了两个lookupswitch指令，而不是一个。如果反编译的是针对Int的switch()语句的话，字节码里只会使用一个lookupswitch指令。也就是说，针对string的switch语句被分成用两个语句来实现。留心标号为#5，#39和#53的指令，来看看switch()语句是如何处理字符串的。<br>在第#36，#37，#39，以及#42字节的地方，你可以看见str参数被equals()方法来和字符串“abc”进行比较。如果比较的结果是相等的话，‘0’会被放入到局部变量数组的索引为#3的位置，然后跳抓转到第#61字节。<br>在第#50，#51，#53，以及#56字节的地方，你可以看见str参数被equals()方法来和字符串“123”进行比较。如果比较的结果是相等的话，10’会被放入到局部变量数组的索引为#3的位置，然后跳转到第#61字节。<br>在第#61和#62字节的地方，局部变量数组里索引为#3的值，这里是’0’，‘1’或者其他的值，被lookupswitch用来进行搜索并进行相应的分支跳转。<br>换句话来说，在Java代码里的用来作为switch()的参数的字符串str变量是通过hashCode()和equals()方法来进行比较，然后根据比较的结果，来执行swtich()语句。<br>在这个结果里，编译后的字节码和之前版本的JVM规范没有不兼容的地方。Java SE 7的这个用字符串作为switch参数的特性是通过Java编译器来处理的，而不是通过JVM来支持的。通过这种方式还可以把其他的Java SE 7的新特性也通过Java编译器来实现。</p><h2 id="GC-算法原理"><a href="#GC-算法原理" class="headerlink" title="GC 算法原理"></a>GC 算法原理</h2><p>在传统JVM内存管理中，我们把Heap空间分为Young/Old两个分区，Young分区又包括一个Eden和两个Survivor分区，如下图所示。新产生的对象首先会被存放在Eden区，而每次minor GC发生时，JVM一方面将Eden分区内存活的对象拷贝到一个空的Survivor分区，另一方面将另一个正在被使用的Survivor分区中的存活对象也拷贝到空的Survivor分区内。在此过程中，JVM始终保持一个Survivor分区处于全空的状态。一个对象在两个Survivor之间的拷贝到一定次数后，如果还是存活的，就将其拷入Old分区。当Old分区没有足够空间时，GC会停下所有程序线程，进行Full GC，即对Old区中的对象进行整理。这个所有线程都暂停的阶段被称为Stop-The-World(STW)，也是大多数GC算法中对性能影响最大的部分。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/48-JVM-GC-1.png?raw=true" alt="48-JVM-GC-1"></p><p>而G1 GC则完全改变了这一传统思路。它将整个Heap分为若干个预先设定的小区域块（如图2），每个区域块内部不再进行新旧分区， 而是将整个区域块标记为Eden/Survivor/Old。当创建新对象时，它首先被存放到某一个可用区块（Region）中。当该区块满了，JVM就会创建新的区块存放对象。当发生minor GC时，JVM将一个或几个区块中存活的对象拷贝到一个新的区块中，并在空余的空间中选择几个全新区块作为新的Eden分区。当所有区域中都有存活对象，找不到全空区块时，才发生Full GC。而在标记存活对象时，G1使用RememberSet的概念，将每个分区外指向分区内的引用记录在该分区的RememberSet中，避免了对整个Heap的扫描，使得各个分区的GC更加独立。在这样的背景下，我们可以看出G1 GC大大提高了触发Full GC时的Heap占用率，同时也使得Minor GC的暂停时间更加可控，对于内存较大的环境非常友好。这些颠覆性的改变，将给GC性能带来怎样的变化呢？最简单的方式，我们可以将老的GC设置直接迁移为G1 GC，然后观察性能变化。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/49-G1-GC.png?raw=true" alt="49-G1-GC"></p><p>由于G1取消了对于heap空间不同新旧对象固定分区的概念，所以我们需要在GC配置选项上作相应的调整，使得应用能够合理地运行在G1 GC收集器上。一般来说，对于原运行在Parallel GC上的应用，需要去除的参数包括-Xmn, -XX:-UseAdaptiveSizePolicy, -XX:SurvivorRatio=n等；而对于原来使用CMS GC的应用，我们需要去掉-Xmn -XX:InitialSurvivorRatio -XX:SurvivorRatio -XX:InitialTenuringThreshold -XX:MaxTenuringThreshold等参数。另外在CMS中已经调优过的-XX:ParallelGCThreads -XX:ConcGCThreads参数最好也移除掉，因为对于CMS来说性能最好的不一定是对于G1性能最好的选择。我们先统一置为默认值，方便后期调优。此外，当应用开启的线程较多时，最好使用-XX:-ResizePLAB来关闭PLAB()的大小调整，以避免大量的线程通信所导致的性能下降。</p><p>关于Hotspot JVM所支持的完整的GC参数列表，可以使用参数-XX:+PrintFlagsFinal打印出来，也可以参见Oracle官方的文档中对部分参数的解释。</p><h2 id="Spark-内存管理"><a href="#Spark-内存管理" class="headerlink" title="Spark 内存管理"></a>Spark 内存管理</h2><p>Spark的核心概念是RDD，实际运行中内存消耗都与RDD密切相关。Spark允许用户将应用中重复使用的RDD数据持久化缓存起来，从而避免反复计算的开销，而RDD的持久化形态之一就是将全部或者部分数据缓存在JVM的Heap中。Spark Executor会将JVM的heap空间大致分为两个部分，一部分用来存放Spark应用中持久化到内存中的RDD数据，剩下的部分则用来作为JVM运行时的堆空间，负责RDD转化等过程中的内存消耗。我们可以通过spark.storage.memoryFraction参数调节这两块内存的比例，Spark会控制缓存RDD总大小不超过heap空间体积乘以这个参数所设置的值，而这块缓存RDD的空间中没有使用的部分也可以为JVM运行时所用。因此，分析Spark应用GC问题时应当分别分析两部分内存的使用情况。<br>而当我们观察到GC延迟影响效率时，应当先检查Spark应用本身是否有效利用有限的内存空间。RDD占用的内存空间比较少的话，程序运行的heap空间也会比较宽松，GC效率也会相应提高；而RDD如果占用大量空间的话，则会带来巨大的性能损失。下面我们从一个用户案例展开：<br>该应用是利用Spark的组件Bagel来实现的，其本质就是一个简单的迭代计算。而每次迭代计算依赖于上一次的迭代结果，因此每次迭代结果都会被主动持续化到内存空间中。当运行用户程序时，我们观察到随着迭代次数的增加，进程占用的内存空间不断快速增长，GC问题越来越突出。但是，仔细分析Bagel实现机制，我们很快发现Bagel将每次迭代产生的RDD都持久化下来了，而没有及时释放掉不再使用的RDD，从而造成了内存空间不断增长，触发了更多GC执行。经过简单的修改，我们修复了这个问题（SPARK-2661）。应用的内存空间得到了有效的控制后，迭代次数三次以后RDD大小趋于稳定，缓存空间得到有效控制（如表1所示），GC效率得以大大提高，程序总的运行时间缩短了10%~20%。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/50-Spark-Memory.png?raw=true" alt="50-Spark-Memory"></p><p>小结：当观察到GC频繁或者延时长的情况，也可能是Spark进程或者应用中内存空间没有有效利用。所以可以尝试检查是否存在RDD持久化后未得到及时释放等情况。</p><h2 id="选择垃圾收集器"><a href="#选择垃圾收集器" class="headerlink" title="选择垃圾收集器"></a>选择垃圾收集器</h2><p>在解决了应用本身的问题之后，我们就要开始针对Spark应用的GC调优了。基于修复了SPARK-2661的Spark版本，我们搭建了一个4个节点的集群，给每个Executor分配88G的Heap，在Spark的Standalone模式下来进行我们的实验。在使用默认的Parallel GC运行我们的Spark应用时，我们发现，由于Spark应用对于内存的开销比较大，而且大部分对象并不能在一个较短的生命周期中被回收，Parallel GC也常常受困于Full GC，而每次Full GC都给性能带来了较大的下降。而Parallel GC可以进行参数调优的空间也非常有限，我们只能通过调节一些基本参数来提高性能，如各年代分区大小比例、进入老年代前的拷贝次数等。而且这些调优策略只能推迟Full GC的到来，如果是长期运行的应用，Parallel GC调优的意义就非常有限了。因此，本文中不会再对Parallel GC进行调优。表2列出了Parallel GC的运行情况，其中CPU利用率较低的部分正是发生Full GC的时候。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/51-Choose-GC.png?raw=true" alt="51-Choose-GC"></p><p>Parallel GC运行情况(未调优)<br>至于CMS GC，也没有办法消除这个Spark应用中的Full GC，而且CMS的Full GC的暂停时间远远超过了Parallel GC，大大拖累了该应用的吞吐量。<br>接下来，我们就使用最基本的G1 GC配置来运行我们的应用。实验结果发现，G1 GC竟然也出现了不可忍受的Full GC（表3的CPU利用率图中，可以明显发现Job 3中出现了将近100秒的暂停），超长的暂停时间大大拖累了整个应用的运行。如表4所示，虽然总的运行时间比Parallel GC略长，不过G1 GC表现略好于CMS GC。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/52-Choose-GC.png?raw=true" alt="52-Choose-GC"></p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/53-GC-Time-Cost.png?raw=true" alt="53-GC-Time-Cost"></p><p>三种垃圾收集器对应的程序运行时间比较（88GB heap未调优）</p><h2 id="根据日志进一步调优"><a href="#根据日志进一步调优" class="headerlink" title="根据日志进一步调优"></a>根据日志进一步调优</h2><p>在让G1 GC跑起来之后，我们下一步就是需要根据GC log，来进一步进行性能调优。首先，我们要让JVM记录比较详细的GC日志. 对于Spark而言，我们需要在SPARK_JAVA_OPTS中设置参数使得Spark保留下我们需要用到的日志. 一般而言，我们需要设置这样一串参数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-XX:+PrintFlagsFinal</span><br><span class="line">-XX:+PrintReferenceGC -verbose:gc</span><br><span class="line">-XX:+PrintGCDetails</span><br><span class="line">-XX:+PrintGCTimeStamps</span><br><span class="line">-XX:+PrintAdaptiveSizePolicy</span><br><span class="line">-XX:+UnlockDiagnosticVMOptions</span><br><span class="line">-XX:+G1SummarizeConcMark</span><br></pre></td></tr></table></figure><p>有了这些参数，我们就可以在SPARK的EXECUTOR日志中（默认输出到各worker节点的$SPARK_HOME/work/$app_id/$executor_id/stdout中）读到详尽的GC日志以及生效的GC 参数了。接下来，我们就可以根据GC日志来分析问题，使程序获得更优性能。我们先来了解一下G1中一次GC的日志结构。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">251.354</span>: [G1Ergonomics (Mixed GCs) <span class="keyword">continue</span> mixed GCs,</span><br><span class="line">reason: candidate old regions available,</span><br><span class="line">candidate old regions: <span class="number">363</span> regions,</span><br><span class="line">reclaimable: <span class="number">9830652576</span> bytes (<span class="number">10.40</span> %),</span><br><span class="line">threshold: <span class="number">10.00</span> %]</span><br><span class="line">[Parallel Time: <span class="number">145.1</span> ms, GC Workers: <span class="number">23</span>]</span><br><span class="line">[<span class="function">GC Worker <span class="title">Start</span> <span class="params">(ms)</span>: Min: 251176.0, Avg: 251176.4, Max: 251176.7, Diff: 0.7]</span></span><br><span class="line"><span class="function">[Ext Root <span class="title">Scanning</span> <span class="params">(ms)</span>: Min: 0.8, Avg: 1.2, Max: 1.7, Diff: 0.9, Sum: 28.1]</span></span><br><span class="line"><span class="function">[Update <span class="title">RS</span> <span class="params">(ms)</span>: Min: 0.0, Avg: 0.3, Max: 0.6, Diff: 0.6, Sum: 5.8]</span></span><br><span class="line"><span class="function">[Processed Buffers: Min: 0, Avg: 1.6, Max: 9, Diff: 9, Sum: 37]</span></span><br><span class="line"><span class="function">[Scan <span class="title">RS</span> <span class="params">(ms)</span>: Min: 6.0, Avg: 6.2, Max: 6.3, Diff: 0.3, Sum: 143.0]</span></span><br><span class="line"><span class="function">[Object <span class="title">Copy</span> <span class="params">(ms)</span>: Min: 136.2, Avg: 136.3, Max: 136.4, Diff: 0.3, Sum: 3133.9]</span></span><br><span class="line"><span class="function">[<span class="title">Termination</span> <span class="params">(ms)</span>: Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.3]</span></span><br><span class="line"><span class="function">[GC Worker <span class="title">Other</span> <span class="params">(ms)</span>: Min: 0.0, Avg: 0.1, Max: 0.2, Diff: 0.2, Sum: 1.9]</span></span><br><span class="line"><span class="function">[GC Worker <span class="title">Total</span> <span class="params">(ms)</span>: Min: 143.7, Avg: 144.0, Max: 144.5, Diff: 0.8, Sum: 3313.0]</span></span><br><span class="line"><span class="function">[GC Worker <span class="title">End</span> <span class="params">(ms)</span>: Min: 251320.4, Avg: 251320.5, Max: 251320.6, Diff: 0.2]</span></span><br><span class="line"><span class="function">[Code Root Fixup: 0.0 ms]</span></span><br><span class="line"><span class="function">[Clear CT: 6.6 ms]</span></span><br><span class="line"><span class="function">[Other: 26.8 ms]</span></span><br><span class="line"><span class="function">[Choose CSet: 0.2 ms]</span></span><br><span class="line"><span class="function">[Ref Proc: 16.6 ms]</span></span><br><span class="line"><span class="function">[Ref Enq: 0.9 ms]</span></span><br><span class="line"><span class="function">[Free CSet: 2.0 ms]</span></span><br><span class="line"><span class="function">[Eden: 3904.0<span class="title">M</span><span class="params">(<span class="number">3904.0</span>M)</span>-&gt;0.0<span class="title">B</span><span class="params">(<span class="number">4448.0</span>M)</span> Survivors: 576.0M-&gt;32.0M Heap: 63.7<span class="title">G</span><span class="params">(<span class="number">88.0</span>G)</span>-&gt;58.3<span class="title">G</span><span class="params">(<span class="number">88.0</span>G)</span>]</span></span><br><span class="line"><span class="function">[Times: user</span>=<span class="number">3.43</span> sys=<span class="number">0.01</span>, real=<span class="number">0.18</span> secs]</span><br></pre></td></tr></table></figure><p>以G1 GC的一次mixed GC为例，从这段日志中，我们可以看到G1 GC日志的层次是非常清晰的。日志列出了这次暂停发生的时间、原因，并分级各种线程所消耗的时长以及CPU时间的均值和最值。最后，G1 GC列出了本次暂停的清理结果，以及总共消耗的时间。<br>而在我们现在的G1 GC运行日志中，我们明显发现这样一段特殊的日志：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">(to-space exhausted), <span class="number">1.0552680</span> secs]</span><br><span class="line">[Parallel Time: <span class="number">958.8</span> ms, GC Workers: <span class="number">23</span>]</span><br><span class="line">[<span class="function">GC Worker <span class="title">Start</span> <span class="params">(ms)</span>: Min: 759925.0, Avg: 759925.1, Max: 759925.3, Diff: 0.3]</span></span><br><span class="line"><span class="function">[Ext Root <span class="title">Scanning</span> <span class="params">(ms)</span>: Min: 1.1, Avg: 1.4, Max: 1.8, Diff: 0.6, Sum: 33.0]</span></span><br><span class="line"><span class="function">[SATB <span class="title">Filtering</span> <span class="params">(ms)</span>: Min: 0.0, Avg: 0.0, Max: 0.3, Diff: 0.3, Sum: 0.3]</span></span><br><span class="line"><span class="function">[Update <span class="title">RS</span> <span class="params">(ms)</span>: Min: 0.0, Avg: 1.2, Max: 2.1, Diff: 2.1, Sum: 26.9]</span></span><br><span class="line"><span class="function">[Processed Buffers: Min: 0, Avg: 2.8, Max: 11, Diff: 11, Sum: 65]</span></span><br><span class="line"><span class="function">[Scan <span class="title">RS</span> <span class="params">(ms)</span>: Min: 1.6, Avg: 2.5, Max: 3.0, Diff: 1.4, Sum: 58.0]</span></span><br><span class="line"><span class="function">[Object <span class="title">Copy</span> <span class="params">(ms)</span>: Min: 952.5, Avg: 953.0, Max: 954.3, Diff: 1.7, Sum: 21919.4]</span></span><br><span class="line"><span class="function">[<span class="title">Termination</span> <span class="params">(ms)</span>: Min: 0.0, Avg: 0.1, Max: 0.2, Diff: 0.2, Sum: 2.2]</span></span><br><span class="line"><span class="function">[GC Worker <span class="title">Other</span> <span class="params">(ms)</span>: Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.6]</span></span><br><span class="line"><span class="function">[GC Worker <span class="title">Total</span> <span class="params">(ms)</span>: Min: 958.1, Avg: 958.3, Max: 958.4, Diff: 0.3, Sum: 22040.4]</span></span><br><span class="line"><span class="function">[GC Worker <span class="title">End</span> <span class="params">(ms)</span>: Min: 760883.4, Avg: 760883.4, Max: 760883.4, Diff: 0.0]</span></span><br><span class="line"><span class="function">[Code Root Fixup: 0.0 ms]</span></span><br><span class="line"><span class="function">[Clear CT: 0.4 ms]</span></span><br><span class="line"><span class="function">[Other: 96.0 ms]</span></span><br><span class="line"><span class="function">[Choose CSet: 0.0 ms]</span></span><br><span class="line"><span class="function">[Ref Proc: 0.4 ms]</span></span><br><span class="line"><span class="function">[Ref Enq: 0.0 ms]</span></span><br><span class="line"><span class="function">[Free CSet: 0.1 ms]</span></span><br><span class="line"><span class="function">[Eden: 160.0<span class="title">M</span><span class="params">(<span class="number">3904.0</span>M)</span>-&gt;0.0<span class="title">B</span><span class="params">(<span class="number">4480.0</span>M)</span> Survivors: 576.0M-&gt;0.0B Heap: 87.7<span class="title">G</span><span class="params">(<span class="number">88.0</span>G)</span>-&gt;87.7<span class="title">G</span><span class="params">(<span class="number">88.0</span>G)</span>]</span></span><br><span class="line"><span class="function">[Times: user</span>=<span class="number">1.69</span> sys=<span class="number">0.24</span>, real=<span class="number">1.05</span> secs]</span><br><span class="line"><span class="number">760.981</span>: [G1Ergonomics (Heap Sizing) attempt heap expansion, reason: allocation request failed, allocation request: <span class="number">90128</span> bytes]</span><br><span class="line"><span class="number">760.981</span>: [G1Ergonomics (Heap Sizing) expand the heap, requested expansion amount: <span class="number">33554432</span> bytes, attempted expansion amount: <span class="number">33554432</span> bytes]</span><br><span class="line"><span class="number">760.981</span>: [G1Ergonomics (Heap Sizing) did not expand the heap, reason: heap expansion operation failed]</span><br><span class="line"><span class="number">760.981</span>: [Full GC <span class="number">87</span>G-&gt;<span class="number">36</span>G(<span class="number">88</span>G), <span class="number">67.4381220</span> secs]</span><br></pre></td></tr></table></figure><p>显然最大的性能下降是这样的Full GC导致的，我们可以在日志中看到类似To-space Exhausted或者To-space Overflow这样的输出（取决于不同版本的JVM，输出略有不同）。这是G1 GC收集器在将某个需要垃圾回收的分区进行回收时，无法找到一个能将其中存活对象拷贝过去的空闲分区。这种情况被称为Evacuation Failure，常常会引发Full GC。而且很显然，G1 GC的Full GC效率相对于Parallel GC实在是相差太远，我们想要获得比Parallel GC更好的表现，一定要尽力规避Full GC的出现。对于这种情况，我们常见的处理办法有两种：<br>将InitiatingHeapOccupancyPercent参数调低（默认值是45），可以使G1 GC收集器更早开始Mixed GC；但另一方面，会增加GC发生频率。<br>提高ConcGCThreads的值，在Mixed GC阶段投入更多的并发线程，争取提高每次暂停的效率。但是此参数会占用一定的有效工作线程资源。<br>调试这两个参数可以有效降低Full GC出现的概率。Full GC被消除之后，最终的性能获得了大幅提升。但是我们发现，仍然有一些地方GC产生了大量的暂停时间。比如，我们在日志中读到很多类似这样的片断：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">280.008</span>: [G1Ergonomics (Concurrent Cycles)</span><br><span class="line">request concurrent cycle initiation,</span><br><span class="line">reason: occupancy higher than threshold,</span><br><span class="line">occupancy: <span class="number">62344134656</span> bytes,</span><br><span class="line">allocation request: <span class="number">46137368</span> bytes,</span><br><span class="line">threshold: <span class="number">42520176225</span> bytes (<span class="number">45.00</span> %),</span><br><span class="line">source: concurrent humongous allocation]</span><br></pre></td></tr></table></figure><p>这里就是Humongous object，一些比G1的一个分区的一半更大的对象。对于这些对象，G1会专门在Heap上开出一个个Humongous Area来存放，每个分区只放一个对象。但是申请这么大的空间是比较耗时的，而且这些区域也仅当Full GC时才进行处理，所以我们要尽量减少这样的对象产生。或者提高G1HeapRegionSize的值减少HumongousArea的创建。不过在内存比较大的时，JVM默认把这个值设到了最大(32M)，此时我们只能通过分析程序本身找到这些对象并且尽量减少这样的对象产生。当然，相信随着G1 GC的发展，在后期的版本中相信这个最大值也会越来越大，毕竟G1号称是在1024～2048个Region时能够获得最佳性能。<br>接下来，我们可以分析一下单次cycle start到Mixed GC为止的时间间隔。如果这一时间过长，可以考虑进一步提升ConcGCThreads，需要注意的是，这会进一步占用一定CPU资源。<br>对于追求更短暂停时间的在线应用，如果观测到较长的Mixed GC pause，我们还要把G1RSetUpdatingPauseTimePercent调低，把G1ConcRefinementThreads调高。前文提到G1 GC通过为每个分区维护RememberSet来记录分区外对分区内的引用，G1RSetUpdatingPauseTimePercent则正是在STW阶段为G1收集器指定更新RememberSet的时间占总STW时间的期望比例，默认为10。而G1ConcRefinementThreads则是在程序运行时维护RememberSet的线程数目。通过对这两个值的对应调整，我们可以把STW阶段的RememberSet更新工作压力更多地移到Concurrent阶段。<br>另外，对于需要长时间运行的应用，我们不妨加上AlwaysPreTouch参数，这样JVM会在启动时就向OS申请所有需要使用的内存，避免动态申请，也可以提高运行时性能。但是该参数也会大大延长启动时间。<br>最终，经过几轮GC参数调试，其结果如下表5所示。较之先前的结果，我们最终还是获得了较满意的运行效率。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Tuning/54-Choose-GC.png?raw=true" alt="54-Choose-GC"></p><p>小结：综合考虑G1 GC是较为推崇的默认Spark GC机制。进一步的GC日志分析，可以收获更多的GC优化。经过上面的调优过程，我们将该应用的运行时间缩短到了4.3分钟，相比调优之前，我们获得了1.7倍左右的性能提升，而相比Parallel GC也获得了1.5倍左右的性能提升。<br>对于大量依赖于内存计算的Spark应用，GC调优显得尤为重要。在发现GC问题的时候，不要着急调试GC。而是先考虑是否存在Spark进程内存管理的效率问题，例如RDD缓存的持久化和释放。至于GC参数的调试，首先我们比较推荐使用G1 GC来运行Spark应用。相较于传统的垃圾收集器，随着G1的不断成熟，需要配置的选项会更少，能同时满足高吞吐量和低延迟的寻求。当然，GC的调优不是绝对的，不同的应用会有不同应用的特性，掌握根据GC日志进行调优的方法，才能以不变应万变。最后，也不能忘了先对程序本身的逻辑和代码编写进行考量，例如减少中间变量的创建或者复制，控制大对象的创建，将长期存活对象放在Off-heap中等等。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要解析部分 Spark 调优的技术点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="http://moqimoqidea.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 内核解析</title>
    <link href="http://moqimoqidea.github.io/2017/09/02/Spark-Kernel-Analysis/"/>
    <id>http://moqimoqidea.github.io/2017/09/02/Spark-Kernel-Analysis/</id>
    <published>2017-09-02T14:30:33.000Z</published>
    <updated>2018-11-26T09:21:11.769Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要解析部分 Spark 内核的技术点。</p><a id="more"></a> ]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要解析部分 Spark 内核的技术点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="http://moqimoqidea.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 应用解析</title>
    <link href="http://moqimoqidea.github.io/2017/08/21/Spark-Streaming-Analysis/"/>
    <id>http://moqimoqidea.github.io/2017/08/21/Spark-Streaming-Analysis/</id>
    <published>2017-08-21T07:40:12.000Z</published>
    <updated>2018-11-25T14:30:47.726Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要解析部分 Spark Streaming 的技术点。</p><a id="more"></a> <h1 id="Spark-Streaming-概述"><a href="#Spark-Streaming-概述" class="headerlink" title="Spark Streaming 概述"></a>Spark Streaming 概述</h1><h2 id="什么是-Spark-Streaming"><a href="#什么是-Spark-Streaming" class="headerlink" title="什么是 Spark Streaming"></a>什么是 Spark Streaming</h2><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/01-Spark-Streaming-LOGO.png?raw=true" alt="01-Spark-Streaming-LOGO"></p><p>Spark Streaming类似于Apache Storm，用于流式数据的处理。根据其官方文档介绍，Spark Streaming有高吞吐量和容错能力强等特点。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等。数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算。而结果也能保存在很多地方，如HDFS，数据库等。另外Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/02-Main-Application-Of-Spark-Streaming.png?raw=true" alt="02-Main-Application-Of-Spark-Streaming"></p><p>和Spark基于RDD的概念很相似，Spark Streaming使用离散化流(discretized stream)作为抽象表示，叫作DStream。DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而 DStream 是由这些 RDD 所组成的序列(因此 得名“离散化”)。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/03-Spark-Streaming-And-Spark-Engine.png?raw=true" alt="03-Spark-Streaming-And-Spark-Engine"></p><p>DStream 可以从各种输入源创建，比如 Flume、Kafka 或者 HDFS。创建出来的DStream 支持两种操作，一种是转化操作(transformation)，会生成一个新的DStream，另一种是输出操作(output operation)，可以把数据写入外部系统中。DStream 提供了许多与 RDD 所支持的操作相类似的操作支持，还增加了与时间相关的新操作，比如滑动窗口。 </p><h2 id="Spark-Streaming-特点"><a href="#Spark-Streaming-特点" class="headerlink" title="Spark Streaming 特点"></a>Spark Streaming 特点</h2><ol><li>易用</li><li>容错</li><li>整合 Spark 体系</li></ol><h2 id="Spark-Streaming-与-Storm-对比"><a href="#Spark-Streaming-与-Storm-对比" class="headerlink" title="Spark Streaming 与 Storm 对比"></a>Spark Streaming 与 Storm 对比</h2><ul><li><p>Spark Streaming：开发语言 scala, 编程模型 DStream.</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/04-Spark-DStream.png?raw=true" alt="04-Spark-DStream"></p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/06-Spark-Streaming-Architecture.png?raw=true" alt="06-Spark-Streaming-Architecture"></p></li><li><p>Strom：开发语言：Clojure, 编程模型：Spout/Bolt.</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/05-Storm-Sqout-And-Bolt.png?raw=true" alt="05-Storm-Sqout-And-Bolt"></p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/07-Storm-Architecture.png?raw=true" alt="07-Storm-Architecture"></p></li></ul><h1 id="运行-Spark-Streaming"><a href="#运行-Spark-Streaming" class="headerlink" title="运行 Spark Streaming"></a>运行 Spark Streaming</h1><h2 id="Maven-依赖"><a href="#Maven-依赖" class="headerlink" title="Maven 依赖"></a>Maven 依赖</h2><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h2 id="程序如下"><a href="#程序如下" class="headerlink" title="程序如下"></a>程序如下</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.moqi.streaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WorldCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"master01"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Split each line into words</span></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3</span></span><br><span class="line">    <span class="comment">// Count each word in each batch</span></span><br><span class="line">    <span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></span><br><span class="line">    wordCounts.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()             <span class="comment">// Start the computation</span></span><br><span class="line">    ssc.awaitTermination()  <span class="comment">// Wait for the computation to terminate</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>按照Spark Core中的方式进行打包，并将程序上传到Spark机器上。并运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --class com.moqi.streaming.WorldCount ~/wordcount-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><p>通过Netcat发送数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> TERMINAL 1:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Running Netcat</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> nc -lk 9999</span></span><br><span class="line"></span><br><span class="line">hello world</span><br></pre></td></tr></table></figure><p>如果程序运行时，log日志太多，可以将spark conf目录下的log4j文件里面的日志级别改成WARN。</p><h1 id="架构与抽象"><a href="#架构与抽象" class="headerlink" title="架构与抽象"></a>架构与抽象</h1><p>Spark Streaming使用“微批次”的架构，把流式计算当作一系列连续的小规模批处理来对待。Spark Streaming从各种输入源中读取数据，并把数据分组为小的批次。新的批次按均匀的时间间隔创建出来。在每个时间区间开始的时候，一个新的批次就创建出来，在该区间内收到的数据都会被添加到这个批次中。在时间区间结束时，批次停止增长。时间区间的大小是由批次间隔这个参数决定的。批次间隔一般设在500毫秒到几秒之间，由应用开发者配置。每个输入批次都形成一个RDD，以 Spark 作业的方式处理并生成其他的 RDD。 处理的结果可以以批处理的方式传给外部系统。高层次的架构如图：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/08-Spark-Streaming-Abstract.png?raw=true" alt="08-Spark-Streaming-Abstract"></p><p>Spark Streaming的编程抽象是离散化流，也就是DStream。它是一个 RDD 序列，每个RDD代表数据流中一个时间片内的数据。 </p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/09-DStream.png?raw=true" alt="09-DStream"></p><p>Spark Streaming在Spark的驱动器程序—工作节点的结构的执行过程如下图所示。Spark Streaming为每个输入源启动对 应的接收器。接收器以任务的形式运行在应用的执行器进程中，从输入源收集数据并保存为 RDD。它们收集到输入数据后会把数据复制到另一个执行器进程来保障容错性(默 认行为)。数据保存在执行器进程的内存中，和缓存 RDD 的方式一样。驱动器程序中的 StreamingContext 会周期性地运行 Spark 作业来处理这些数据，把数据与之前时间区间中的 RDD 进行整合。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/10-Driver-And-Worker.png?raw=true" alt="10-Driver-And-Worker"></p><h1 id="Spark-Streaming-解析"><a href="#Spark-Streaming-解析" class="headerlink" title="Spark Streaming 解析"></a>Spark Streaming 解析</h1><h2 id="初始化-StreamingContext"><a href="#初始化-StreamingContext" class="headerlink" title="初始化 StreamingContext"></a>初始化 StreamingContext</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(appName).setMaster(master)</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"><span class="comment">// 可以通过ssc.sparkContext 来访问SparkContext</span></span><br><span class="line"><span class="comment">// 或者通过已经存在的SparkContext来创建StreamingContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = ...                <span class="comment">// existing SparkContext</span></span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>初始化完Context之后：</p><ol><li>定义消息输入源来创建DStreams.。</li><li>定义DStreams的转化操作和输出操作。</li><li>通过 streamingContext.start()来启动消息采集和处理。</li><li>等待程序终止，可以通过streamingContext.awaitTermination()来设置。</li><li>通过streamingContext.stop()来手动终止处理程序。</li></ol><p>StreamingContext和SparkContext什么关系？</p><p>StreamingContext一旦启动，对DStreams的操作就不能修改了。<br>在同一时间一个JVM中只有一个StreamingContext可以启动<br>stop() 方法将同时停止SparkContext，可以传入参数stopSparkContext用于只停止StreamingContext<br>在Spark1.4版本后，如何优雅的停止SparkStreaming而不丢失数据，通过设置sparkConf.set(“spark.streaming.stopGracefullyOnShutdown”,”true”) 即可。在StreamingContext的start方法中已经注册了Hook方法。</p><h2 id="什么是-DStreams"><a href="#什么是-DStreams" class="headerlink" title="什么是 DStreams"></a>什么是 DStreams</h2><p>Discretized Stream是Spark Streaming的基础抽象，代表持续性的数据流和经过各种Spark原语操作后的结果数据流。在内部实现上，DStream是一系列连续的RDD来表示。每个RDD含有一段时间间隔内的数据，如下图：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/11-DStream.png?raw=true" alt="11-DStream"></p><p>对数据的操作也是按照RDD为单位来进行的：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/12-DStream.png?raw=true" alt="12-DStream"></p><p>计算过程由Spark engine来完成：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/13-Spark-Streaming-And-Spark-Engine.png?raw=true" alt="13-Spark-Streaming-And-Spark-Engine"></p><h2 id="DStreams-输入"><a href="#DStreams-输入" class="headerlink" title="DStreams 输入"></a>DStreams 输入</h2><p>Spark Streaming原生支持一些不同的数据源。一些“核心”数据源已经被打包到Spark Streaming 的 Maven 工件中，而其他的一些则可以通过 spark-streaming-kafka 等附加工件获取。每个接收器都以 Spark 执行器程序中一个长期运行的任务的形式运行，因此会占据分配给应用的 CPU 核心。此外，我们还需要有可用的 CPU 核心来处理数据。这意味着如果要运行多个接收器，就必须至少有和接收器数目相同的核心数，还要加上用来完成计算所需要的核心数。例如，如果我们想要在流计算应用中运行 10 个接收器，那么至少需要为应用分配 11 个 CPU 核心。所以如果在本地模式运行，不要使用local或者local[1]。</p><h3 id="基本数据源"><a href="#基本数据源" class="headerlink" title="基本数据源"></a>基本数据源</h3><h4 id="文件数据源"><a href="#文件数据源" class="headerlink" title="文件数据源"></a>文件数据源</h4><p>Socket数据流前面的例子已经看到过。<br>文件数据流：能够读取所有HDFS API兼容的文件系统文件，通过fileStream方法进行读取：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">streamingContext.fileStream[<span class="type">KeyClass</span>, <span class="type">ValueClass</span>, <span class="type">InputFormatClass</span>](dataDirectory)</span><br></pre></td></tr></table></figure><p>Spark Streaming 将会监控 dataDirectory 目录并不断处理移动进来的文件，记住目前不支持嵌套目录。</p><ol><li>文件需要有相同的数据格式。</li><li>文件进入 dataDirectory的方式需要通过移动或者重命名来实现。</li><li>一旦文件移动进目录，则不能再修改，即便修改了也不会读取新数据。</li></ol><p>如果文件比较简单，则可以使用 streamingContext.textFileStream(dataDirectory)方法来读取文件。文件流不需要接收器，不需要单独分配CPU核。<br>Hdfs读取实例（需要提前需要在HDFS上建好目录）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line">ssc: org.apache.spark.streaming.<span class="type">StreamingContext</span> = org.apache.spark.streaming.<span class="type">StreamingContext</span>@<span class="number">4027</span>edeb</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> lines = ssc.textFileStream(<span class="string">"hdfs://master01:9000/data/"</span>)</span><br><span class="line">lines: org.apache.spark.streaming.dstream.<span class="type">DStream</span>[<span class="type">String</span>] = org.apache.spark.streaming.dstream.<span class="type">MappedDStream</span>@<span class="number">61</span>d9dd15</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">words: org.apache.spark.streaming.dstream.<span class="type">DStream</span>[<span class="type">String</span>] = org.apache.spark.streaming.dstream.<span class="type">FlatMappedDStream</span>@<span class="number">1e084</span>a26</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">wordCounts: org.apache.spark.streaming.dstream.<span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = org.apache.spark.streaming.dstream.<span class="type">ShuffledDStream</span>@<span class="number">8947</span>a4b</span><br><span class="line"></span><br><span class="line">scala&gt; wordCounts.print()</span><br><span class="line"></span><br><span class="line">scala&gt; ssc.start()</span><br></pre></td></tr></table></figure><p>上传文件上去：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[bigdata<span class="meta">@master</span>01 hadoop<span class="number">-2.7</span><span class="number">.3</span>]$ ls</span><br><span class="line">bin  data  etc  include  lib  libexec  <span class="type">LICENSE</span>.txt  logs  <span class="type">NOTICE</span>.txt  <span class="type">README</span>.txt  sbin  sdata  share</span><br><span class="line">[bigdata<span class="meta">@master</span>01 hadoop<span class="number">-2.7</span><span class="number">.3</span>]$ bin/hdfs dfs -put ./<span class="type">LICENSE</span>.txt /data/</span><br><span class="line">[bigdata<span class="meta">@master</span>01 hadoop<span class="number">-2.7</span><span class="number">.3</span>]$ bin/hdfs dfs -put ./<span class="type">README</span>.txt /data/</span><br></pre></td></tr></table></figure><p>获取计算结果：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line"><span class="type">Time</span>: <span class="number">1504665716000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">-------------------------------------------</span><br><span class="line"><span class="type">Time</span>: <span class="number">1504665717000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">-------------------------------------------</span><br><span class="line"><span class="type">Time</span>: <span class="number">1504665718000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(<span class="number">227.7202</span><span class="number">-1</span>,<span class="number">2</span>)</span><br><span class="line">(created,<span class="number">2</span>)</span><br><span class="line">(offer,<span class="number">8</span>)</span><br><span class="line">(<span class="type">BUSINESS</span>,<span class="number">11</span>)</span><br><span class="line">(agree,<span class="number">10</span>)</span><br><span class="line">(hereunder,,<span class="number">1</span>)</span><br><span class="line">(“control”,<span class="number">1</span>)</span><br><span class="line">(<span class="type">Grant</span>,<span class="number">2</span>)</span><br><span class="line">(<span class="number">2.2</span>.,<span class="number">2</span>)</span><br><span class="line">(include,<span class="number">11</span>)</span><br><span class="line">...</span><br><span class="line">-------------------------------------------</span><br><span class="line"><span class="type">Time</span>: <span class="number">1504665719000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"><span class="type">Time</span>: <span class="number">1504665739000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">-------------------------------------------</span><br><span class="line"><span class="type">Time</span>: <span class="number">1504665740000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(under,<span class="number">1</span>)</span><br><span class="line">(<span class="type">Technology</span>,<span class="number">1</span>)</span><br><span class="line">(distribution,<span class="number">2</span>)</span><br><span class="line">(http:<span class="comment">//hadoop.apache.org/core/,1)</span></span><br><span class="line">(<span class="type">Unrestricted</span>,<span class="number">1</span>)</span><br><span class="line">(<span class="number">740.13</span>),<span class="number">1</span>)</span><br><span class="line">(check,<span class="number">1</span>)</span><br><span class="line">(have,<span class="number">1</span>)</span><br><span class="line">(policies,<span class="number">1</span>)</span><br><span class="line">(uses,<span class="number">1</span>)</span><br><span class="line">...</span><br><span class="line">-------------------------------------------</span><br><span class="line"><span class="type">Time</span>: <span class="number">1504665741000</span> ms</span><br><span class="line">-------------------------------------------</span><br></pre></td></tr></table></figure><h4 id="自定义数据源"><a href="#自定义数据源" class="headerlink" title="自定义数据源"></a>自定义数据源</h4><p>通过继承Receiver，并实现onStart、onStop方法来自定义数据源采集。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomReceiver</span>(<span class="params">host: <span class="type">String</span>, port: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Receiver</span>[<span class="type">String</span>](<span class="params"><span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_2</span></span>) <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>() &#123;</span><br><span class="line">    <span class="comment">// Start the thread that receives data over a connection</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">"Socket Receiver"</span>) &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123; receive() &#125;</span><br><span class="line">    &#125;.start()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">onStop</span></span>() &#123;</span><br><span class="line">    <span class="comment">// There is nothing much to do as the thread calling receive()</span></span><br><span class="line">    <span class="comment">// is designed to stop by itself if isStopped() returns false</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Create a socket connection and receive data until receiver is stopped */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>() &#123;</span><br><span class="line">    <span class="keyword">var</span> socket: <span class="type">Socket</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">var</span> userInput: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// Connect to host:port</span></span><br><span class="line">      socket = <span class="keyword">new</span> <span class="type">Socket</span>(host, port)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Until stopped or connection broken continue reading</span></span><br><span class="line">      <span class="keyword">val</span> reader = <span class="keyword">new</span> <span class="type">BufferedReader</span>(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">InputStreamReader</span>(socket.getInputStream(), <span class="type">StandardCharsets</span>.<span class="type">UTF_8</span>))</span><br><span class="line">      userInput = reader.readLine()</span><br><span class="line">      <span class="keyword">while</span>(!isStopped &amp;&amp; userInput != <span class="literal">null</span>) &#123;</span><br><span class="line">        store(userInput)</span><br><span class="line">        userInput = reader.readLine()</span><br><span class="line">      &#125;</span><br><span class="line">      reader.close()</span><br><span class="line">      socket.close()</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Restart in an attempt to connect again when server is active again</span></span><br><span class="line">      restart(<span class="string">"Trying to connect again"</span>)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: java.net.<span class="type">ConnectException</span> =&gt;</span><br><span class="line">        <span class="comment">// restart if could not connect to server</span></span><br><span class="line">        restart(<span class="string">"Error connecting to "</span> + host + <span class="string">":"</span> + port, e)</span><br><span class="line">      <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        <span class="comment">// restart if there is any other error</span></span><br><span class="line">        restart(<span class="string">"Error receiving data"</span>, t)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以通过streamingContext.receiverStream(\<instance of="" custom="" receiver\="">)，来使用自定义的数据采集源：</instance></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Assuming ssc is the StreamingContext</span></span><br><span class="line"><span class="keyword">val</span> customReceiverStream = ssc.receiverStream(<span class="keyword">new</span> <span class="type">CustomReceiver</span>(host, port))</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>模拟Spark内置的Socket链接：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.moqi.streaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.&#123;<span class="type">BufferedReader</span>, <span class="type">InputStreamReader</span>&#125;</span><br><span class="line"><span class="keyword">import</span> java.net.<span class="type">Socket</span></span><br><span class="line"><span class="keyword">import</span> java.nio.charset.<span class="type">StandardCharsets</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.receiver.<span class="type">Receiver</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomReceiver</span> (<span class="params">host: <span class="type">String</span>, port: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Receiver</span>[<span class="type">String</span>](<span class="params"><span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_2</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Start the thread that receives data over a connection</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">"Socket Receiver"</span>) &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123; receive() &#125;</span><br><span class="line">    &#125;.start()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStop</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// There is nothing much to do as the thread calling receive()</span></span><br><span class="line">    <span class="comment">// is designed to stop by itself if isStopped() returns false</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Create a socket connection and receive data until receiver is stopped */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>() &#123;</span><br><span class="line">    <span class="keyword">var</span> socket: <span class="type">Socket</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">var</span> userInput: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// Connect to host:port</span></span><br><span class="line">      socket = <span class="keyword">new</span> <span class="type">Socket</span>(host, port)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Until stopped or connection broken continue reading</span></span><br><span class="line">      <span class="keyword">val</span> reader = <span class="keyword">new</span> <span class="type">BufferedReader</span>(<span class="keyword">new</span> <span class="type">InputStreamReader</span>(socket.getInputStream(), <span class="type">StandardCharsets</span>.<span class="type">UTF_8</span>))</span><br><span class="line"></span><br><span class="line">      userInput = reader.readLine()</span><br><span class="line">      <span class="keyword">while</span>(!isStopped &amp;&amp; userInput != <span class="literal">null</span>) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 传送出来</span></span><br><span class="line">        store(userInput)</span><br><span class="line"></span><br><span class="line">        userInput = reader.readLine()</span><br><span class="line">      &#125;</span><br><span class="line">      reader.close()</span><br><span class="line">      socket.close()</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Restart in an attempt to connect again when server is active again</span></span><br><span class="line">      restart(<span class="string">"Trying to connect again"</span>)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: java.net.<span class="type">ConnectException</span> =&gt;</span><br><span class="line">        <span class="comment">// restart if could not connect to server</span></span><br><span class="line">        restart(<span class="string">"Error connecting to "</span> + host + <span class="string">":"</span> + port, e)</span><br><span class="line">      <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        <span class="comment">// restart if there is any other error</span></span><br><span class="line">        restart(<span class="string">"Error receiving data"</span>, t)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CustomReceiver</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.receiverStream(<span class="keyword">new</span> <span class="type">CustomReceiver</span>(<span class="string">"master01"</span>, <span class="number">9999</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Split each line into words</span></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3</span></span><br><span class="line">    <span class="comment">// Count each word in each batch</span></span><br><span class="line">    <span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></span><br><span class="line">    wordCounts.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()             <span class="comment">// Start the computation</span></span><br><span class="line">    ssc.awaitTermination()  <span class="comment">// Wait for the computation to terminate</span></span><br><span class="line">    <span class="comment">//ssc.stop()</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="RDD-队列"><a href="#RDD-队列" class="headerlink" title="RDD 队列"></a>RDD 队列</h4><p>测试过程中，可以通过使用streamingContext.queueStream(queueOfRDDs)来创建DStream，每一个推送到这个队列中的RDD，都会作为一个DStream处理。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.moqi.streaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">QueueRdd</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"QueueRdd"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create the queue through which RDDs can be pushed to</span></span><br><span class="line">    <span class="comment">// a QueueInputDStream</span></span><br><span class="line">    <span class="comment">//创建RDD队列</span></span><br><span class="line">    <span class="keyword">val</span> rddQueue = <span class="keyword">new</span> mutable.<span class="type">SynchronizedQueue</span>[<span class="type">RDD</span>[<span class="type">Int</span>]]()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create the QueueInputDStream and use it do some processing</span></span><br><span class="line">    <span class="comment">// 创建QueueInputDStream</span></span><br><span class="line">    <span class="keyword">val</span> inputStream = ssc.queueStream(rddQueue)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理队列中的RDD数据</span></span><br><span class="line">    <span class="keyword">val</span> mappedStream = inputStream.map(x =&gt; (x % <span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> reducedStream = mappedStream.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//打印结果</span></span><br><span class="line">    reducedStream.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动计算</span></span><br><span class="line">    ssc.start()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create and push some RDDs into</span></span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">30</span>) &#123;</span><br><span class="line">      rddQueue += ssc.sparkContext.makeRDD(<span class="number">1</span> to <span class="number">300</span>, <span class="number">10</span>)</span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">//通过程序停止StreamingContext的运行</span></span><br><span class="line">      <span class="comment">//ssc.stop()</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[bigdata<span class="meta">@master</span>01 spark<span class="number">-2.1</span><span class="number">.1</span>-bin-hadoop2<span class="number">.7</span>]$ bin/spark-submit --<span class="class"><span class="keyword">class</span> <span class="title">com</span>.<span class="title">atguigu</span>.<span class="title">streaming</span>.<span class="title">QueueRdd</span> <span class="title">~/queueRdd-jar-with-dependencies</span>.<span class="title">jar</span></span></span><br><span class="line"><span class="class">17<span class="title">/09/05</span> 23</span>:<span class="number">28</span>:<span class="number">03</span> <span class="type">WARN</span> <span class="type">NativeCodeLoader</span>: <span class="type">Unable</span> to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line">-------------------------------------------</span><br><span class="line"><span class="type">Time</span>: <span class="number">1504668485000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(<span class="number">4</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">0</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">6</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">8</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">2</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">1</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">3</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">7</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">9</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">5</span>,<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line"><span class="type">Time</span>: <span class="number">1504668486000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line"><span class="type">Time</span>: <span class="number">1504668487000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(<span class="number">4</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">0</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">6</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">8</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">2</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">1</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">3</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">7</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">9</span>,<span class="number">30</span>)</span><br><span class="line">(<span class="number">5</span>,<span class="number">30</span>)</span><br></pre></td></tr></table></figure><h3 id="高级数据源"><a href="#高级数据源" class="headerlink" title="高级数据源"></a>高级数据源</h3><p>除核心数据源外，还可以用附加数据源接收器来从一些知名数据获取系统中接收的数据，这些接收器都作为Spark Streaming的组件进行独立打包了。它们仍然是Spark的一部分，不过你需要在构建文件中添加额外的包才能使用它们。现有的接收器包括 Twitter、Apache Kafka、Amazon Kinesis、Apache Flume，以及ZeroMQ。可以通过添加与Spark版本匹配 的 Maven 工件 spark-streaming-[projectname]_2.10 来引入这些附加接收器。 </p><h4 id="Apache-Kafka"><a href="#Apache-Kafka" class="headerlink" title="Apache Kafka"></a>Apache Kafka</h4><p>在工程中需要引入 Maven 工件 spark- streaming-kafka_2.10 来使用它。包内提供的 KafkaUtils 对象可以在 StreamingContext 和 JavaStreamingContext 中以你的 Kafka 消息创建出 DStream。由于 KafkaUtils 可以订阅多个主题，因此它创建出的 DStream 由成对的主题和消息组成。要创建出一个流数据，需 要使用 StreamingContext 实例、一个由逗号隔开的 ZooKeeper 主机列表字符串、消费者组的名字(唯一名字)，以及一个从主题到针对这个主题的接收器线程数的映射表来调用 createStream() 方法。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka._...<span class="comment">// 创建一个从主题到接收器线程数的映射表</span></span><br><span class="line"><span class="keyword">val</span> topics = <span class="type">List</span>((<span class="string">"pandas"</span>, <span class="number">1</span>), (<span class="string">"logs"</span>, <span class="number">1</span>)).toMap</span><br><span class="line"><span class="keyword">val</span> topicLines = <span class="type">KafkaUtils</span>.createStream(ssc, zkQuorum, group, topics) </span><br><span class="line">topicLines.map(_._2)</span><br></pre></td></tr></table></figure><p>下面我们进行一个实例，演示SparkStreaming如何从Kafka读取消息，如果通过连接池方法把消息处理完成后再写会Kafka：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/14-Kafka-Connection-Pool.png?raw=true" alt="14-Kafka-Connection-Pool"></p><p>kafka Connection Pool程序：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.moqi.streaming</span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.pool2.impl.<span class="type">DefaultPooledObject</span></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.pool2.&#123;<span class="type">BasePooledObjectFactory</span>, <span class="type">PooledObject</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.&#123;<span class="type">KafkaProducer</span>, <span class="type">ProducerRecord</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducerProxy</span>(<span class="params">brokerList: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                            producerConfig: <span class="type">Properties</span> = new <span class="type">Properties</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                            defaultTopic: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                            producer: <span class="type">Option</span>[<span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">None</span></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">Key</span> </span>= <span class="type">String</span></span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">Val</span> </span>= <span class="type">String</span></span><br><span class="line"></span><br><span class="line">  require(brokerList == <span class="literal">null</span> || !brokerList.isEmpty, <span class="string">"Must set broker list"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> p = producer getOrElse &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> props:<span class="type">Properties</span>= <span class="keyword">new</span> <span class="type">Properties</span>();</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, brokerList);</span><br><span class="line">    props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">    props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> <span class="type">KafkaProducer</span>[<span class="type">String</span>,<span class="type">String</span>](props)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">toMessage</span></span>(value: <span class="type">Val</span>, key: <span class="type">Option</span>[<span class="type">Key</span>] = <span class="type">None</span>, topic: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span>): <span class="type">ProducerRecord</span>[<span class="type">Key</span>, <span class="type">Val</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> t = topic.getOrElse(defaultTopic.getOrElse(<span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Must provide topic or default topic"</span>)))</span><br><span class="line">    require(!t.isEmpty, <span class="string">"Topic must not be empty"</span>)</span><br><span class="line">    key <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(k) =&gt; <span class="keyword">new</span> <span class="type">ProducerRecord</span>(t, k, value)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt; <span class="keyword">new</span> <span class="type">ProducerRecord</span>(t, value)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">send</span></span>(key: <span class="type">Key</span>, value: <span class="type">Val</span>, topic: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span>) &#123;</span><br><span class="line">    p.send(toMessage(value, <span class="type">Option</span>(key), topic))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">send</span></span>(value: <span class="type">Val</span>, topic: <span class="type">Option</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    send(<span class="literal">null</span>, value, topic)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">send</span></span>(value: <span class="type">Val</span>, topic: <span class="type">String</span>) &#123;</span><br><span class="line">    send(<span class="literal">null</span>, value, <span class="type">Option</span>(topic))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">send</span></span>(value: <span class="type">Val</span>) &#123;</span><br><span class="line">    send(<span class="literal">null</span>, value, <span class="type">None</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">shutdown</span></span>(): <span class="type">Unit</span> = p.close()</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducerFactory</span>(<span class="params">brokerList: <span class="type">String</span>, config: <span class="type">Properties</span>, topic: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">newInstance</span></span>(): <span class="type">KafkaProducerProxy</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseKafkaProducerFactory</span>(<span class="params">brokerList: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                                  config: <span class="type">Properties</span> = new <span class="type">Properties</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                                  defaultTopic: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">KafkaProducerFactory</span>(<span class="params">brokerList, config, defaultTopic</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">newInstance</span></span>() = <span class="keyword">new</span> <span class="type">KafkaProducerProxy</span>(brokerList, config, defaultTopic)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PooledKafkaProducerAppFactory</span>(<span class="params">val factory: <span class="type">KafkaProducerFactory</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">BasePooledObjectFactory</span>[<span class="type">KafkaProducerProxy</span>] <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">create</span></span>(): <span class="type">KafkaProducerProxy</span> = factory.newInstance()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">wrap</span></span>(obj: <span class="type">KafkaProducerProxy</span>): <span class="type">PooledObject</span>[<span class="type">KafkaProducerProxy</span>] = <span class="keyword">new</span> <span class="type">DefaultPooledObject</span>(obj)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">destroyObject</span></span>(p: <span class="type">PooledObject</span>[<span class="type">KafkaProducerProxy</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    p.getObject.shutdown()</span><br><span class="line">    <span class="keyword">super</span>.destroyObject(p)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>KafkaStreaming main：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.moqi.streaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.pool2.impl.&#123;<span class="type">GenericObjectPool</span>, <span class="type">GenericObjectPoolConfig</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerRecord</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.<span class="type">VoidFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">createKafkaProducerPool</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(brokerList: <span class="type">String</span>, topic: <span class="type">String</span>):  <span class="type">GenericObjectPool</span>[<span class="type">KafkaProducerProxy</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> producerFactory = <span class="keyword">new</span> <span class="type">BaseKafkaProducerFactory</span>(brokerList, defaultTopic = <span class="type">Option</span>(topic))</span><br><span class="line">    <span class="keyword">val</span> pooledProducerFactory = <span class="keyword">new</span> <span class="type">PooledKafkaProducerAppFactory</span>(producerFactory)</span><br><span class="line">    <span class="keyword">val</span> poolConfig = &#123;</span><br><span class="line">      <span class="keyword">val</span> c = <span class="keyword">new</span> <span class="type">GenericObjectPoolConfig</span></span><br><span class="line">      <span class="keyword">val</span> maxNumProducers = <span class="number">10</span></span><br><span class="line">      c.setMaxTotal(maxNumProducers)</span><br><span class="line">      c.setMaxIdle(maxNumProducers)</span><br><span class="line">      c</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">GenericObjectPool</span>[<span class="type">KafkaProducerProxy</span>](pooledProducerFactory, poolConfig)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaStreaming</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[4]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建topic</span></span><br><span class="line">    <span class="keyword">val</span> brobrokers = <span class="string">"172.16.148.150:9092,172.16.148.151:9092,172.16.148.152:9092"</span></span><br><span class="line">    <span class="keyword">val</span> sourcetopic=<span class="string">"source"</span>;</span><br><span class="line">    <span class="keyword">val</span> targettopic=<span class="string">"target"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建消费者组</span></span><br><span class="line">    <span class="keyword">var</span> group=<span class="string">"con-consumer-group"</span></span><br><span class="line">    <span class="comment">//消费者配置</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParam = <span class="type">Map</span>(</span><br><span class="line">      <span class="string">"bootstrap.servers"</span> -&gt; brobrokers,<span class="comment">//用于初始化链接到集群的地址</span></span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//用于标识这个消费者属于哪个消费团体</span></span><br><span class="line">      <span class="string">"group.id"</span> -&gt; group,</span><br><span class="line">      <span class="comment">//如果没有初始化偏移量或者当前的偏移量不存在任何服务器上，可以使用这个配置属性</span></span><br><span class="line">      <span class="comment">//可以使用这个配置，latest自动重置偏移量为最新的偏移量</span></span><br><span class="line">      <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"latest"</span>,</span><br><span class="line">      <span class="comment">//如果是true，则这个消费者的偏移量会在后台自动提交</span></span><br><span class="line">      <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//ssc.sparkContext.broadcast(pool)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建DStream，返回接收到的输入数据</span></span><br><span class="line">    <span class="keyword">var</span> stream=<span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>,<span class="type">String</span>](ssc, <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,<span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>,<span class="type">String</span>](<span class="type">Array</span>(sourcetopic),kafkaParam))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//每一个stream都是一个ConsumerRecord</span></span><br><span class="line">    stream.map(s =&gt;(<span class="string">"id:"</span> + s.key(),<span class="string">"&gt;&gt;&gt;&gt;:"</span>+s.value())).foreachRDD(rdd =&gt; &#123;</span><br><span class="line">      rdd.foreachPartition(partitionOfRecords =&gt; &#123;</span><br><span class="line">        <span class="comment">// Get a producer from the shared pool</span></span><br><span class="line">        <span class="keyword">val</span> pool = createKafkaProducerPool(brobrokers, targettopic)</span><br><span class="line">        <span class="keyword">val</span> p = pool.borrowObject()</span><br><span class="line"></span><br><span class="line">        partitionOfRecords.foreach &#123;message =&gt; <span class="type">System</span>.out.println(message._2);p.send(message._2,<span class="type">Option</span>(targettopic))&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Returning the producer to the pool also shuts it down</span></span><br><span class="line">        pool.returnObject(p)</span><br><span class="line"></span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>程序部署：</p><ol><li><p>启动zookeeper和kafka：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh -deamon ./config/server.properties</span><br></pre></td></tr></table></figure></li><li><p>创建两个topic，一个为source，一个为target：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper 192.168.56.150:2181,192.168.56.151:2181,192.168.56.152:2181 --replication-factor 2 --partitions 2 --topic <span class="built_in">source</span></span><br><span class="line"></span><br><span class="line">bin/kafka-topics.sh --create --zookeeper 172.16.148.150:2181,172.16.148.151:2181,172.16.148.152:2181 --replication-factor 2 --partitions 2 --topic target</span><br></pre></td></tr></table></figure></li><li><p>启动kafka console producer 写入source topic：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list 192.168.56.150:9092, 192.168.56.151:9092, 192.168.56.152:9092 --topic <span class="built_in">source</span></span><br></pre></td></tr></table></figure></li><li><p>启动kafka console consumer 监听target topic：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server 192.168.56.150:9092, 192.168.56.151:9092, 192.168.56.152:9092 --topic <span class="built_in">source</span></span><br></pre></td></tr></table></figure></li><li><p>启动kafkaStreaming程序：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[bigdata@master01 ~]$ ./hadoop/spark-2.1.1-bin-hadoop2.7/bin/spark-submit --class com.moqi.streaming.KafkaStreaming ./kafkastreaming-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure></li><li><p>程序运行。</p></li></ol><h4 id="Spark-两种连接-Kafka-方式"><a href="#Spark-两种连接-Kafka-方式" class="headerlink" title="Spark 两种连接 Kafka 方式"></a>Spark 两种连接 Kafka 方式</h4><p>Spark对于Kafka的连接主要有两种方式，一种是DirectKafkaInputDStream，另外一种是KafkaInputDStream。DirectKafkaInputDStream 只在 driver 端接收数据，所以继承了 InputDStream，是没有 receivers 的。<br>主要通过KafkaUtils#createDirectStream以及KafkaUtils#createStream这两个 API 来创建，除了要传入的参数不同外，接收 kafka 数据的节点、拉取数据的时机也完全不同。<br>KafkaUtils#createStream【Receiver-based】<br>这种方法使用一个 Receiver 来接收数据。在该 Receiver 的实现中使用了 Kafka high-level consumer API。Receiver 从 kafka 接收的数据将被存储到 Spark executor 中，随后启动的 job 将处理这些数据。<br>在默认配置下，该方法失败后会丢失数据（保存在 executor 内存里的数据在 application 失败后就没了），若要保证数据不丢失，需要启用 WAL（即预写日志至 HDFS、S3等），这样再失败后可以从日志文件中恢复数据。<br>在该函数中，会新建一个 KafkaInputDStream对象，KafkaInputDStream继承于 ReceiverInputDStream。KafkaInputDStream实现了getReceiver方法，返回接收器的实例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getReceiver</span></span>(): <span class="type">Receiver</span>[(<span class="type">K</span>, <span class="type">V</span>)] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!useReliableReceiver) &#123;</span><br><span class="line">      <span class="comment">//&lt; 不启用 WAL</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">KafkaReceiver</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">U</span>, <span class="type">T</span>](kafkaParams, topics, storageLevel)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">//&lt; 启用 WAL</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">ReliableKafkaReceiver</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">U</span>, <span class="type">T</span>](kafkaParams, topics, storageLevel)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>根据是否启用 WAL，receiver 分为 KafkaReceiver 和 ReliableKafkaReceiver。下图描述了 KafkaReceiver 接收数据的具体流程：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/15-KafkaReceive-onStart.png?raw=true" alt="15-KafkaReceive-onStart"></p><p>需要注意的点：</p><ul><li>Kafka Topic 的 partitions 与RDD 的 partitions 没有直接关系，不能一一对应。如果增加 topic 的 partition 个数的话仅仅会增加单个 Receiver 接收数据的线程数。事实上，使用这种方法只会在一个 executor 上启用一个 Receiver，该 Receiver 包含一个线程池，线程池的线程个数与所有 topics 的 partitions 个数总和一致，每条线程接收一个 topic 的一个 partition 的数据。而并不会增加处理数据时的并行度。</li><li>对于一个 topic，可以使用多个 groupid 相同的 input DStream 来使用多个 Receivers 来增加并行度，然后 union 他们；对于多个 topics，除了可以用上个办法增加并行度外，还可以对不同的 topic 使用不同的 input DStream 然后 union 他们来增加并行度。</li><li>如果你启用了 WAL，为能将接收到的数据将以 log 的方式在指定的存储系统备份一份，需要指定输入数据的存储等级为 StorageLevel.MEMORY_AND_DISK_SER 或 StorageLevel.MEMORY_AND_DISK_SER_2.</li></ul><p>KafkaUtils#createDirectStream【WithOut Receiver】<br>自 Spark-1.3.0 起，提供了不需要 Receiver 的方法。替代了使用 receivers 来接收数据，该方法定期查询每个 topic+partition 的 lastest offset，并据此决定每个 batch 要接收的 offsets 范围。<br>KafkaUtils#createDirectStream调用中，会新建DirectKafkaInputDStream，DirectKafkaInputDStream#compute(validTime: Time)会从 kafka 拉取数据并生成 RDD，流程如下：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/16-DirectKafkaInputDStream-compute.png?raw=true" alt="16-DirectKafkaInputDStream-compute"></p><p>如上图所示，该函数主要做了以下三个事情：</p><ol><li>确定要接收的 partitions 的 offsetRange，以作为第2步创建的 RDD 的数据来源。</li><li>创建 RDD 并执行 count 操作，使 RDD 真实具有数据。</li><li>以 streamId、数据条数，offsetRanges 信息初始化 inputInfo 并添加到 JobScheduler 中。</li></ol><p>进一步看 KafkaRDD 的 getPartitions 实现：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = &#123;</span><br><span class="line">    offsetRanges.zipWithIndex.map &#123; <span class="keyword">case</span> (o, i) =&gt;</span><br><span class="line">        <span class="keyword">val</span> (host, port) = leaders(<span class="type">TopicAndPartition</span>(o.topic, o.partition))</span><br><span class="line">        <span class="keyword">new</span> <span class="type">KafkaRDDPartition</span>(i, o.topic, o.partition, o.fromOffset, o.untilOffset, host, port)</span><br><span class="line">    &#125;.toArray</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>从上面的代码可以很明显看到，KafkaRDD 的 partition 数据与 Kafka topic 的某个 partition 的 o.fromOffset 至 o.untilOffset 数据是相对应的，也就是说 KafkaRDD 的 partition 与 Kafka partition 是一一对应的<br>该方式相比使用 Receiver 的方式有以下好处：</p><ul><li>简化并行：不再需要创建多个 kafka input DStream 然后再 union 这些 input DStream。使用 directStream，Spark Streaming会创建与 Kafka partitions 相同数量的 paritions 的 RDD，RDD 的 partition与 Kafka 的 partition 一一对应，这样更易于理解及调优。</li><li>高效：在方式一中要保证数据零丢失需要启用 WAL（预写日志），这会占用更多空间。而在方式二中，可以直接从 Kafka 指定的 topic 的指定 offsets 处恢复数据，不需要使用 WAL. </li><li>恰好一次语义保证：基于Receiver方式使用了 Kafka 的 high level API 来在 Zookeeper 中存储已消费的 offsets。这在某些情况下会导致一些数据被消费两次，比如 streaming app 在处理某个 batch  内已接受到的数据的过程中挂掉，但是数据已经处理了一部分，但这种情况下无法将已处理数据的 offsets 更新到 Zookeeper 中，下次重启时，这批数据将再次被消费且处理。基于direct的方式，使用kafka的简单api，Spark Streaming自己就负责追踪消费的offset，并保存在checkpoint中。Spark自己一定是同步的，因此可以保证数据是消费一次且仅消费一次。这种方式中，只要将 output 操作和保存 offsets 操作封装成一个原子操作就能避免失败后的重复消费和处理，从而达到恰好一次的语义（Exactly-once）。</li></ul><p>通过以上分析，我们可以对这两种方式的区别做一个总结：</p><ol><li>createStream会使用 Receiver；而createDirectStream不会。</li><li>createStream使用的 Receiver 会分发到某个 executor 上去启动并接受数据；而createDirectStream直接在 driver 上接收数据。</li><li>createStream使用 Receiver 源源不断的接收数据并把数据交给 ReceiverSupervisor 处理最终存储为 blocks 作为 RDD 的输入，从 kafka 拉取数据与计算消费数据相互独立；而createDirectStream会在每个 batch 拉取数据并就地消费，到下个 batch 再次拉取消费，周而复始，从 kafka 拉取数据与计算消费数据是连续的，没有独立开。</li><li>createStream中创建的KafkaInputDStream 每个 batch 所对应的 RDD 的 partition 不与 Kafka partition 一一对应；而createDirectStream中创建的 DirectKafkaInputDStream 每个 batch 所对应的 RDD 的 partition 与 Kafka partition 一一对应。</li></ol><h4 id="Flume-ng"><a href="#Flume-ng" class="headerlink" title="Flume-ng"></a>Flume-ng</h4><p>Spark提供两个不同的接收器来使用<a href="http://flume.apache.org/" target="_blank" rel="noopener">Apache Flume</a>。 两个接收器简介如下。 </p><ul><li>推式接收器该接收器以 Avro 数据池的方式工作，由 Flume 向其中推数据。 </li><li>拉式接收器该接收器可以从自定义的中间数据池中拉数据，而其他进程可以使用 Flume 把数据推进 该中间数据池。 </li></ul><p>两种方式都需要重新配置 Flume，并在某个节点配置的端口上运行接收器(不是已有的 Spark 或者 Flume 使用的端口)。要使用其中任何一种方法，都需要在工程中引入 Maven 工件 spark-streaming-flume_2.10。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/17-Flume.png?raw=true" alt="17-Flume"></p><p>推式接收器的方法设置起来很容易，但是它不使用事务来接收数据。在这种方式中，接收 器以 Avro 数据池的方式工作，我们需要配置 Flume 来把数据发到 Avro 数据池。我们提供的 FlumeUtils 对象会把接收器配置在一个特定的工作节点的主机名及端口号上。这些设置必须和 Flume 配置相匹配。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// Flume 对 Avro 池的配置</span><br><span class="line">a1.sinks = avroSink</span><br><span class="line">a1.sinks.avroSink.type = avro</span><br><span class="line">a1.sinks.avroSink.channel = memoryChannel</span><br><span class="line">a1.sinks.avroSink.hostname = receiver-hostname</span><br><span class="line">a1.sinks.avroSink.port = port-used-for-avro-sink-not-spark-port</span><br><span class="line"></span><br><span class="line">// Scala 中的 FlumeUtils 代理</span><br><span class="line">val events = FlumeUtils.createStream(ssc, receiverHostname, receiverPort)</span><br></pre></td></tr></table></figure><p>虽然这种方式很简洁，但缺点是没有事务支持。这会增加运行接收器的工作节点发生错误 时丢失少量数据的几率。不仅如此，如果运行接收器的工作节点发生故障，系统会尝试从 另一个位置启动接收器，这时需要重新配置 Flume 才能将数据发给新的工作节点。这样配 置会比较麻烦。<br>较新的方式是拉式接收器(在Spark 1.1中引入)，它设置了一个专用的Flume数据池供 Spark Streaming读取，并让接收器主动从数据池中拉取数据。这种方式的优点在于弹性较 好，Spark Streaming通过事务从数据池中读取并复制数据。在收到事务完成的通知前，这 些数据还保留在数据池中。<br>我们需要先把自定义数据池配置为 Flume 的第三方插件。安装插件的最新方法请参考 <a href="https://flume.apache.org/FlumeUserGuide.html#installing-third-party-plugins" target="_blank" rel="noopener">Flume 文档的相关部分</a>。由于插件是用 Scala 写的，因此需要把插件本身以及 Scala 库都添加到 Flume 插件 中。Spark 1.1 中对应的 Maven 索引如例 10-37 所示。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-streaming-flume-sink_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.2.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scala-library&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.11.11&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>当你把自定义 Flume 数据池添加到一个节点上之后，就需要配置 Flume 来把数据推送到这个数据池中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a1.sinks = spark</span><br><span class="line">a1.sinks.spark.type = org.apache.spark.streaming.flume.sink.SparkSink</span><br><span class="line">a1.sinks.spark.hostname = receiver-hostname</span><br><span class="line">a1.sinks.spark.port = port-used-for-sync-not-spark-port</span><br><span class="line">a1.sinks.spark.channel = memoryChannel</span><br></pre></td></tr></table></figure><p>等到数据已经在数据池中缓存起来，就可以调用 FlumeUtils 来读取数据了 :</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 在 Scala 中使用 FlumeUtils 读取自定义数据池</span></span><br><span class="line"><span class="keyword">val</span> events = <span class="type">FlumeUtils</span>.createPollingStream(ssc, receiverHostname, receiverPort)</span><br></pre></td></tr></table></figure><h2 id="DStreams-转换"><a href="#DStreams-转换" class="headerlink" title="DStreams 转换"></a>DStreams 转换</h2><p>DStream上的原语与RDD的类似，分为Transformations（转换）和Output Operations（输出）两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种Window相关的原语。</p><table><thead><tr><th style="text-align:center"><strong>Transformation</strong></th><th style="text-align:center"><strong>Meaning</strong></th></tr></thead><tbody><tr><td style="text-align:center">map(func)</td><td style="text-align:center">将源DStream中的每个元素通过一个函数func从而得到新的DStreams。</td></tr><tr><td style="text-align:center">flatMap(func)</td><td style="text-align:center">和map类似，但是每个输入的项可以被映射为0或更多项。</td></tr><tr><td style="text-align:center">filter(func)</td><td style="text-align:center">选择源DStream中函数func判为true的记录作为新DStreams</td></tr><tr><td style="text-align:center">repartition(numPartitions)</td><td style="text-align:center">通过创建更多或者更少的partition来改变此DStream的并行级别。</td></tr><tr><td style="text-align:center">union(otherStream)</td><td style="text-align:center">联合源DStreams和其他DStreams来得到新DStream</td></tr><tr><td style="text-align:center">count()</td><td style="text-align:center">统计源DStreams中每个RDD所含元素的个数得到单元素RDD的新DStreams。</td></tr><tr><td style="text-align:center">reduce(func)</td><td style="text-align:center">通过函数func(两个参数一个输出)来整合源DStreams中每个RDD元素得到单元素RDD的DStreams。这个函数需要关联从而可以被并行计算。</td></tr><tr><td style="text-align:center">countByValue()</td><td style="text-align:center">对于DStreams中元素类型为K调用此函数，得到包含(K,Long)对的新DStream，其中Long值表明相应的K在源DStream中每个RDD出现的频率。</td></tr><tr><td style="text-align:center">reduceByKey(func,   [numTasks])</td><td style="text-align:center">对(K,V)对的DStream调用此函数，返回同样（K,V)对的新DStream，但是新DStream中的对应V为使用reduce函数整合而来。<em>Note</em>：默认情况下，这个操作使用Spark默认数量的并行任务（本地模式为2，集群模式中的数量取决于配置参数spark.default.parallelism）。你也可以传入可选的参数numTaska来设置不同数量的任务。</td></tr><tr><td style="text-align:center">join(otherStream,   [numTasks])</td><td style="text-align:center">两DStream分别为(K,V)和(K,W)对，返回(K,(V,W))对的新DStream。</td></tr><tr><td style="text-align:center">cogroup(otherStream,   [numTasks])</td><td style="text-align:center">两DStream分别为(K,V)和(K,W)对，返回(K,(Seq[V],Seq[W])对新DStreams</td></tr><tr><td style="text-align:center">transform(func)</td><td style="text-align:center">将RDD到RDD映射的函数func作用于源DStream中每个RDD上得到新DStream。这个可用于在DStream的RDD上做任意操作。</td></tr><tr><td style="text-align:center">updateStateByKey(func)</td><td style="text-align:center">得到”状态”DStream，其中每个key状态的更新是通过将给定函数用于此key的上一个状态和新值而得到。这个可用于保存每个key值的任意状态数据。</td></tr></tbody></table><p>DStream 的转化操作可以分为无状态(stateless)和有状态(stateful)两种。 </p><ul><li>在无状态转化操作中，每个批次的处理不依赖于之前批次的数据。常见的 RDD 转化操作，例如 map()、filter()、reduceByKey() 等，都是无状态转化操作。 </li><li>相对地，有状态转化操作需要使用之前批次的数据或者是中间结果来计算当前批次的数据。有状态转化操作包括基于滑动窗口的转化操作和追踪状态变化的转化操作。</li></ul><h3 id="无状态转化操作"><a href="#无状态转化操作" class="headerlink" title="无状态转化操作"></a>无状态转化操作</h3><p>无状态转化操作就是把简单的 RDD 转化操作应用到每个批次上，也就是转化 DStream 中的每一个 RDD。部分无状态转化操作列在了下表中。 注意，针对键值对的 DStream 转化操作(比如 reduceByKey())要添加import StreamingContext._ 才能在 Scala中使用。 </p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/18-DStream-NoStatus-Transformation.png?raw=true" alt="18-DStream-NoStatus-Transformation"></p><p>需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个 DStream 在内部是由许多 RDD(批次)组成，且无状态转化操作是分别应用到每个 RDD 上的。例如， reduceByKey() 会归约每个时间区间中的数据，但不会归约不同区间之间的数据。<br>举个例子，在之前的wordcount程序中，我们只会统计1秒内接收到的数据的单词个数，而不会累加。<br>无状态转化操作也能在多个 DStream 间整合数据，不过也是在各个时间区间内。例如，键 值对 DStream 拥有和 RDD 一样的与连接相关的转化操作，也就是 cogroup()、join()、 leftOuterJoin() 等。我们可以在 DStream 上使用这些操作，这样就对每个批次分别执行了对应的 RDD 操作。<br>我们还可以像在常规的 Spark 中一样使用 DStream 的 union() 操作将它和另一个 DStream 的内容合并起来，也可以使用 StreamingContext.union() 来合并多个流。 </p><h3 id="有状态转化操作"><a href="#有状态转化操作" class="headerlink" title="有状态转化操作"></a>有状态转化操作</h3><h4 id="追踪状态变化-UpdateStateByKey"><a href="#追踪状态变化-UpdateStateByKey" class="headerlink" title="追踪状态变化 UpdateStateByKey"></a>追踪状态变化 UpdateStateByKey</h4><p>UpdateStateByKey原语用于记录历史记录，有时，我们需要在 DStream 中跨批次维护状态(例如流计算中累加wordcount)。针对这种情况，updateStateByKey() 为我们提供了对一个状态变量的访问，用于键值对形式的 DStream。给定一个由(键，事件)对构成的 DStream，并传递一个指定如何根据新的事件 更新每个键对应状态的函数，它可以构建出一个新的 DStream，其内部数据为(键，状态) 对。<br>updateStateByKey() 的结果会是一个新的 DStream，其内部的 RDD 序列是由每个时间区间对应的(键，状态)对组成的。<br>updateStateByKey操作使得我们可以在用新信息进行更新时保持任意的状态。为使用这个功能，你需要做下面两步： </p><ol><li>定义状态，状态可以是一个任意的数据类型。 </li><li>定义状态更新函数，用此函数阐明如何使用之前的状态和来自输入流的新值对状态进行更新。</li></ol><p>使用updateStateByKey需要对检查点目录进行配置，会使用检查点来保存状态。<br>更新版的wordcount：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.moqi.streaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WorldCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义更新状态方法，参数values为当前批次单词频度，state为以往批次单词频度</span></span><br><span class="line">    <span class="keyword">val</span> updateFunc = (values: <span class="type">Seq</span>[<span class="type">Int</span>], state: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> currentCount = values.foldLeft(<span class="number">0</span>)(_ + _)</span><br><span class="line">      <span class="keyword">val</span> previousCount = state.getOrElse(<span class="number">0</span>)</span><br><span class="line">      <span class="type">Some</span>(currentCount + previousCount)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    ssc.checkpoint(<span class="string">"."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"master01"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Split each line into words</span></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3</span></span><br><span class="line">    <span class="comment">// Count each word in each batch</span></span><br><span class="line">    <span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用updateStateByKey来更新状态，统计从运行开始以来单词总的次数</span></span><br><span class="line">    <span class="keyword">val</span> stateDstream = pairs.updateStateByKey[<span class="type">Int</span>](updateFunc)</span><br><span class="line">    stateDstream.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//val wordCounts = pairs.reduceByKey(_ + _)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></span><br><span class="line">    <span class="comment">//wordCounts.print()</span></span><br><span class="line"></span><br><span class="line">    ssc.start()             <span class="comment">// Start the computation</span></span><br><span class="line">    ssc.awaitTermination()  <span class="comment">// Wait for the computation to terminate</span></span><br><span class="line">    <span class="comment">//ssc.stop()</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动nc –lk 9999：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[bigdata@master01 ~]$ nc -lk 9999</span><br><span class="line">ni shi shui</span><br><span class="line">ni hao ma</span><br></pre></td></tr></table></figure><p>启动统计程序：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[bigdata@master01 ~]$ ./hadoop/spark-2.1.1-bin-hadoop2.7/bin/spark-submit --class com.moqi.streaming.WorldCount ./statefulwordcount-jar-with-dependencies.jar</span><br><span class="line">17/09/06 04:06:09 WARN NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1504685175000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1504685181000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(shi,1)</span><br><span class="line">(shui,1)</span><br><span class="line">(ni,1)</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1504685187000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(shi,1)</span><br><span class="line">(ma,1)</span><br><span class="line">(hao,1)</span><br><span class="line">(shui,1)</span><br><span class="line">(ni,2)</span><br><span class="line"></span><br><span class="line">[bigdata@master01 ~]$ ls</span><br><span class="line">2df8e0c3-174d-401a-b3a7-f7776c3987db  checkpoint-1504685205000     data</span><br><span class="line">backup                                checkpoint-1504685205000.bk  debug.log</span><br><span class="line">checkpoint-1504685199000              checkpoint-1504685208000     hadoop</span><br><span class="line">checkpoint-1504685199000.bk           checkpoint-1504685208000.bk  receivedBlockMetadata</span><br><span class="line">checkpoint-1504685202000              checkpoint-1504685211000     software</span><br><span class="line">checkpoint-1504685202000.bk           checkpoint-1504685211000.bk  statefulwordcount-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><h4 id="Window-Operations"><a href="#Window-Operations" class="headerlink" title="Window Operations"></a>Window Operations</h4><p>Window Operations有点类似于Storm中的State，可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming的允许状态。<br>基于窗口的操作会在一个比 StreamingContext 的批次间隔更长的时间范围内，通过整合多个批次的结果，计算出整个窗口的结果。 </p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/19-Window-Operations.png?raw=true" alt="19-Window-Operations"></p><p>所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长，两者都必须是 StreamContext 的批次间隔的整数倍。窗口时长控制每次计算最近的多少个批次的数据，其实就是最近的 windowDuration/batchInterval 个批次。如果有一个以 10 秒为批次间隔的源 DStream，要创建一个最近 30 秒的时间窗口(即最近 3 个批次)，就应当把 windowDuration 设为 30 秒。而滑动步长的默认值与批次间隔相等，用来控制对新的 DStream 进行计算的间隔。如果源 DStream 批次间隔为 10 秒，并且我们只希望每两个批次计算一次窗口结果， 就应该把滑动步长设置为 20 秒。<br>假设，你想拓展前例从而每隔十秒对持续30秒的数据生成word count。为做到这个，我们需要在持续30秒数据的(word,1)对DStream上应用reduceByKey。使用操作reduceByKeyAndWindow.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># reduce last <span class="number">30</span> seconds of data, every <span class="number">10</span> second</span><br><span class="line">windowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x -y, <span class="number">30</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/20-Window-Operations.png?raw=true" alt="20-Window-Operations"></p><table><thead><tr><th style="text-align:center">Transformation</th><th>Meaning</th></tr></thead><tbody><tr><td style="text-align:center">window<strong>(windowLength, slideInterval)</strong></td><td>基于对源DStream窗化的批次进行计算返回一个新的DStream</td></tr><tr><td style="text-align:center">countByWindow<strong>(windowLength, slideInterval)</strong></td><td>返回一个滑动窗口计数流中的元素。</td></tr><tr><td style="text-align:center">reduceByWindow<strong>(func, windowLength, slideInterval)</strong></td><td>通过使用自定义函数整合滑动区间流元素来创建一个新的单元素流。</td></tr><tr><td style="text-align:center">reduceByKeyAndWindow<strong>(func, windowLength, slideInterval,   [numTasks])</strong></td><td>当在一个(K,V)对的DStream上调用此函数，会返回一个新(K,V)对的DStream，此处通过对滑动窗口中批次数据使用reduce函数来整合每个key的value值。<strong>Note:</strong>默认情况下，这个操作使用Spark的默认数量并行任务(本地是2)，在集群模式中依据配置属性(spark.default.parallelism)来做grouping。你可以通过设置可选参数numTasks来设置不同数量的tasks。</td></tr><tr><td style="text-align:center">reduceByKeyAndWindow<strong>(func, invFunc, windowLength, slideInterval,   [numTasks])</strong></td><td>这个函数是上述函数的更高效版本，每个窗口的reduce值都是通过用前一个窗的reduce值来递增计算。通过reduce进入到滑动窗口数据并”反向reduce”离开窗口的旧数据来实现这个操作。一个例子是随着窗口滑动对keys的“加”“减”计数。通过前边介绍可以想到，这个函数只适用于”可逆的reduce函数”，也就是这些reduce函数有相应的”反reduce”函数(以参数invFunc形式传入)。如前述函数，reduce任务的数量通过可选参数来配置。<strong>注意：</strong>为了使用这个操作，CheckPoint 必须可用。</td></tr><tr><td style="text-align:center">countByValueAndWindow<strong>(windowLength,slideInterval,   [numTasks])</strong></td><td>对(K,V)对的DStream调用，返回(K,Long)对的新DStream，其中每个key的值是其在滑动窗口中频率。如上，可配置reduce任务数量。</td></tr></tbody></table><p>reduceByWindow() 和 reduceByKeyAndWindow() 让我们可以对每个窗口更高效地进行归约操作。它们接收一个归约函数，在整个窗口上执行，比如 +。除此以外，它们还有一种特殊形式，通过只考虑新进入窗口的数据和离开窗 口的数据，让 Spark 增量计算归约结果。这种特殊形式需要提供归约函数的一个逆函数，比 如 + 对应的逆函数为 -。对于较大的窗口，提供逆函数可以大大提高执行效率。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/21-Window-Operations.png?raw=true" alt="21-Window-Operations"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ipDStream = accessLogsDStream.map(logEntry =&gt; (logEntry.getIpAddress(), <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> ipCountDStream = ipDStream.reduceByKeyAndWindow(</span><br><span class="line">  &#123;(x, y) =&gt; x + y&#125;,</span><br><span class="line">  &#123;(x, y) =&gt; x - y&#125;,</span><br><span class="line">  <span class="type">Seconds</span>(<span class="number">30</span>),</span><br><span class="line">  <span class="type">Seconds</span>(<span class="number">10</span>))</span><br><span class="line">  <span class="comment">// 加上新进入窗口的批次中的元素 // 移除离开窗口的老批次中的元素 // 窗口时长</span> <span class="comment">// 滑动步长</span></span><br></pre></td></tr></table></figure><p>countByWindow() 和 countByValueAndWindow() 作为对数据进行 计数操作的简写。countByWindow() 返回一个表示每个窗口中元素个数的 DStream，而 countByValueAndWindow() 返回的 DStream 则包含窗口中每个值的个数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ipDStream = accessLogsDStream.map&#123;entry =&gt; entry.getIpAddress()&#125; </span><br><span class="line"><span class="keyword">val</span> ipAddressRequestCount = ipDStream.countByValueAndWindow(<span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>)) </span><br><span class="line"><span class="keyword">val</span> requestCount = accessLogsDStream.countByWindow(<span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>WordCount第三版：3秒一个批次，窗口12秒，滑步6秒。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.moqi.streaming</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WorldCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义更新状态方法，参数values为当前批次单词频度，state为以往批次单词频度</span></span><br><span class="line">    <span class="keyword">val</span> updateFunc = (values: <span class="type">Seq</span>[<span class="type">Int</span>], state: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> currentCount = values.foldLeft(<span class="number">0</span>)(_ + _)</span><br><span class="line">      <span class="keyword">val</span> previousCount = state.getOrElse(<span class="number">0</span>)</span><br><span class="line">      <span class="type">Some</span>(currentCount + previousCount)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line">    ssc.checkpoint(<span class="string">"."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"master01"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Split each line into words</span></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3</span></span><br><span class="line">    <span class="comment">// Count each word in each batch</span></span><br><span class="line">    <span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> wordCounts = pairs.reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; (a + b),<span class="type">Seconds</span>(<span class="number">12</span>), <span class="type">Seconds</span>(<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></span><br><span class="line">    wordCounts.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()             <span class="comment">// Start the computation</span></span><br><span class="line">    ssc.awaitTermination()  <span class="comment">// Wait for the computation to terminate</span></span><br><span class="line">    <span class="comment">//ssc.stop()</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="重要操作"><a href="#重要操作" class="headerlink" title="重要操作"></a>重要操作</h3><h4 id="Transform-Operation"><a href="#Transform-Operation" class="headerlink" title="Transform Operation"></a>Transform Operation</h4><p>Transform原语允许DStream上执行任意的RDD-to-RDD函数。即使这些函数并没有在DStream的API中暴露出来，通过该函数可以方便的扩展Spark API。<br>该函数每一批次调度一次。<br>比如下面的例子，在进行单词统计的时候，想要过滤掉spam的信息。<br>其实也就是对DStream中的RDD应用转换。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spamInfoRDD = ssc.sparkContext.newAPIHadoopRDD(...) <span class="comment">// RDD containing spam information</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> cleanedDStream = wordCounts.transform &#123; rdd =&gt;</span><br><span class="line">  rdd.join(spamInfoRDD).filter(...) <span class="comment">// join data stream with spam information to do data cleaning</span></span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="join-操作"><a href="#join-操作" class="headerlink" title="join 操作"></a>join 操作</h4><p>连接操作（leftOuterJoin, rightOuterJoin, fullOuterJoin也可以），可以连接Stream-Stream，windows-stream to windows-stream、stream-dataset.</p><p>Stream-Stream Joins:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stream1: <span class="type">DStream</span>[<span class="type">String</span>, <span class="type">String</span>] = ...</span><br><span class="line"><span class="keyword">val</span> stream2: <span class="type">DStream</span>[<span class="type">String</span>, <span class="type">String</span>] = ...</span><br><span class="line"><span class="keyword">val</span> joinedStream = stream1.join(stream2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> windowedStream1 = stream1.window(<span class="type">Seconds</span>(<span class="number">20</span>))</span><br><span class="line"><span class="keyword">val</span> windowedStream2 = stream2.window(<span class="type">Minutes</span>(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> joinedStream = windowedStream1.join(windowedStream2)</span><br></pre></td></tr></table></figure><p>Stream-dataset joins</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataset: <span class="type">RDD</span>[<span class="type">String</span>, <span class="type">String</span>] = ...</span><br><span class="line"><span class="keyword">val</span> windowedStream = stream.window(<span class="type">Seconds</span>(<span class="number">20</span>))...</span><br><span class="line"><span class="keyword">val</span> joinedStream = windowedStream.transform &#123; rdd =&gt; rdd.join(dataset) &#125;</span><br></pre></td></tr></table></figure><h2 id="DStreams-输出"><a href="#DStreams-输出" class="headerlink" title="DStreams 输出"></a>DStreams 输出</h2><p>输出操作指定了对流数据经转化操作得到的数据所要执行的操作(例如把结果推入外部数据库或输出到屏幕上)。与 RDD 中的惰性求值类似，如果一个 DStream 及其派生出的 DStream 都没有被执行输出操作，那么这些 DStream 就都不会被求值。如果 StreamingContext 中没有设定输出操作，整个 context 就都不会启动。</p><table><thead><tr><th style="text-align:center"><strong>Output Operation</strong></th><th style="text-align:center"><strong>Meaning</strong></th></tr></thead><tbody><tr><td style="text-align:center"><strong>print</strong>()</td><td style="text-align:center">在运行流程序的驱动结点上打印DStream中每一批次数据的最开始10个元素。这用于开发和调试。在Python API中，同样的操作叫pprint()。</td></tr><tr><td style="text-align:center"><strong>saveAsTextFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td><td style="text-align:center">以text文件形式存储这个DStream的内容。每一批次的存储文件名基于参数中的prefix和suffix。”prefix-Time_IN_MS[.suffix]”.</td></tr><tr><td style="text-align:center"><strong>saveAsObjectFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td><td style="text-align:center">以Java对象序列化的方式将Stream中的数据保存为 SequenceFiles .   每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”. Python中目前不可用。</td></tr><tr><td style="text-align:center"><strong>saveAsHadoopFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td><td style="text-align:center">将Stream中的数据保存为 Hadoop files. 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”.     Python API Python中目前不可用。</td></tr><tr><td style="text-align:center"><strong>foreachRDD</strong>(<em>func</em>)</td><td style="text-align:center">这是最通用的输出操作，即将函数func用于产生于stream的每一个RDD。其中参数传入的函数func应该实现将每一个RDD中数据推送到外部系统，如将RDD存入文件或者通过网络将其写入数据库。注意：函数func在运行流应用的驱动中被执行，同时其中一般函数RDD操作从而强制其对于流RDD的运算。</td></tr></tbody></table><p>通用的输出操作 foreachRDD()，它用来对 DStream 中的 RDD 运行任意计算。这和transform() 有些类似，都可以让我们访问任意 RDD。在 foreachRDD() 中，可以重用我们在 Spark 中实现的所有行动操作。比如，常见的用例之一是把数据写到诸如 MySQL 的外部数据库中。<br>需要注意的：</p><ol><li>连接不能写在driver层面。</li><li>如果写在foreach则每个RDD都创建，得不偿失。</li><li>增加foreachPartition，在分区创建。</li><li>可以考虑使用连接池优化。</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">dstream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  <span class="comment">// error val connection = createNewConnection()  // executed at the driver 序列化错误</span></span><br><span class="line"></span><br><span class="line">  rdd.foreachPartition &#123; partitionOfRecords =&gt;</span><br><span class="line">    <span class="comment">// ConnectionPool is a static, lazily initialized pool of connections</span></span><br><span class="line">    <span class="keyword">val</span> connection = <span class="type">ConnectionPool</span>.getConnection()</span><br><span class="line">    partitionOfRecords.foreach(record =&gt; connection.send(record) <span class="comment">// executed at the worker</span></span><br><span class="line">    )</span><br><span class="line">    <span class="type">ConnectionPool</span>.returnConnection(connection)  <span class="comment">// return to the pool for future reuse</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="累加器和广播变量"><a href="#累加器和广播变量" class="headerlink" title="累加器和广播变量"></a>累加器和广播变量</h2><p>累加器(Accumulators)和广播变量(Broadcast variables)不能从Spark Streaming的检查点中恢复。如果你启用检查并也使用了累加器和广播变量，那么你必须创建累加器和广播变量的延迟单实例从而在驱动因失效重启后他们可以被重新实例化。如下例述：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordBlacklist</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> instance: <span class="type">Broadcast</span>[<span class="type">Seq</span>[<span class="type">String</span>]] = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getInstance</span></span>(sc: <span class="type">SparkContext</span>): <span class="type">Broadcast</span>[<span class="type">Seq</span>[<span class="type">String</span>]] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (instance == <span class="literal">null</span>) &#123;</span><br><span class="line">      synchronized &#123;</span><br><span class="line">        <span class="keyword">if</span> (instance == <span class="literal">null</span>) &#123;</span><br><span class="line">          <span class="keyword">val</span> wordBlacklist = <span class="type">Seq</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>)</span><br><span class="line">          instance = sc.broadcast(wordBlacklist)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    instance</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DroppedWordsCounter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> instance: <span class="type">LongAccumulator</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getInstance</span></span>(sc: <span class="type">SparkContext</span>): <span class="type">LongAccumulator</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (instance == <span class="literal">null</span>) &#123;</span><br><span class="line">      synchronized &#123;</span><br><span class="line">        <span class="keyword">if</span> (instance == <span class="literal">null</span>) &#123;</span><br><span class="line">          instance = sc.longAccumulator(<span class="string">"WordsInBlacklistCounter"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    instance</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">wordCounts.foreachRDD &#123; (rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)], time: <span class="type">Time</span>) =&gt;</span><br><span class="line">  <span class="comment">// Get or register the blacklist Broadcast</span></span><br><span class="line">  <span class="keyword">val</span> blacklist = <span class="type">WordBlacklist</span>.getInstance(rdd.sparkContext)</span><br><span class="line">  <span class="comment">// Get or register the droppedWordsCounter Accumulator</span></span><br><span class="line">  <span class="keyword">val</span> droppedWordsCounter = <span class="type">DroppedWordsCounter</span>.getInstance(rdd.sparkContext)</span><br><span class="line">  <span class="comment">// Use blacklist to drop words and use droppedWordsCounter to count them</span></span><br><span class="line">  <span class="keyword">val</span> counts = rdd.filter &#123; <span class="keyword">case</span> (word, count) =&gt;</span><br><span class="line">    <span class="keyword">if</span> (blacklist.value.contains(word)) &#123;</span><br><span class="line">      droppedWordsCounter.add(count)</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;.collect().mkString(<span class="string">"["</span>, <span class="string">", "</span>, <span class="string">"]"</span>)</span><br><span class="line">  <span class="keyword">val</span> output = <span class="string">"Counts at time "</span> + time + <span class="string">" "</span> + counts</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h2 id="DataFrame-and-SQL-Operations"><a href="#DataFrame-and-SQL-Operations" class="headerlink" title="DataFrame and SQL Operations"></a>DataFrame and SQL Operations</h2><p>你可以很容易地在流数据上使用DataFrames和SQL。你必须使用SparkContext来创建StreamingContext要用的SQLContext。此外，这一过程可以在驱动失效后重启。我们通过创建一个实例化的SQLContext单实例来实现这个工作。如下例所示。我们对前例word count进行修改从而使用DataFrames和SQL来产生word counts。每个RDD被转换为DataFrame，以临时表格配置并用SQL进行查询。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> words: <span class="type">DStream</span>[<span class="type">String</span>] = ...</span><br><span class="line"></span><br><span class="line">words.foreachRDD &#123; rdd =&gt;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Get the singleton instance of SparkSession</span></span><br><span class="line">  <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder.config(rdd.sparkContext.getConf).getOrCreate()</span><br><span class="line">  <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Convert RDD[String] to DataFrame</span></span><br><span class="line">  <span class="keyword">val</span> wordsDataFrame = rdd.toDF(<span class="string">"word"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create a temporary view</span></span><br><span class="line">  wordsDataFrame.createOrReplaceTempView(<span class="string">"words"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Do word count on DataFrame using SQL and print it</span></span><br><span class="line">  <span class="keyword">val</span> wordCountsDataFrame =</span><br><span class="line">  spark.sql(<span class="string">"select word, count(*) as total from words group by word"</span>)</span><br><span class="line">  wordCountsDataFrame.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>你也可以从不同的线程在定义于流数据的表上运行SQL查询（也就是说，异步运行StreamingContext）。仅确定你设置StreamingContext记住了足够数量的流数据以使得查询操作可以运行。否则，StreamingContext不会意识到任何异步的SQL查询操作，那么其就会在查询完成之后删除旧的数据。例如，如果你要查询最后一批次，但是你的查询会运行5分钟，那么你需要调用streamingContext.remember(Minutes(5))(in Scala, 或者其他语言的等价操作)。</p><h2 id="Caching-Persistence"><a href="#Caching-Persistence" class="headerlink" title="Caching / Persistence"></a>Caching / Persistence</h2><p>和RDDs类似，DStreams同样允许开发者将流数据保存在内存中。也就是说，在DStream上使用persist()方法将会自动把DStreams中的每个RDD保存在内存中。当DStream中的数据要被多次计算时，这个非常有用（如在同样数据上的多次操作）。对于像reduceByWindow和reduceByKeyAndWindow以及基于状态的(updateStateByKey)这种操作，保存是隐含默认的。因此，即使开发者没有调用persist()，由基于窗操作产生的DStreams会自动保存在内存中。 </p><h2 id="7-24-不间断运行"><a href="#7-24-不间断运行" class="headerlink" title="7 * 24 不间断运行"></a>7 * 24 不间断运行</h2><h3 id="CheckPoint-机制"><a href="#CheckPoint-机制" class="headerlink" title="CheckPoint 机制"></a>CheckPoint 机制</h3><p>检查点机制是我们在Spark Streaming中用来保障容错性的主要机制。与应用程序逻辑无关的错误（即系统错位，JVM崩溃等）有迅速恢复的能力.<br>它可以使Spark Streaming阶段性地把应用数据存储到诸如HDFS或Amazon S3这样的可靠存储系统中， 以供恢复时使用。具体来说，检查点机制主要为以下两个目的服务。 </p><ol><li>控制发生失败时需要重算的状态数。SparkStreaming可以通 过转化图的谱系图来重算状态，检查点机制则可以控制需要在转化图中回溯多远。 </li><li>提供驱动器程序容错。如果流计算应用中的驱动器程序崩溃了，你可以重启驱动器程序 并让驱动器程序从检查点恢复，这样Spark Streaming就可以读取之前运行的程序处理 数据的进度，并从那里继续。  </li></ol><p>为了实现这个，Spark Streaming需要为容错存储系统checkpoint足够的信息从而使得其可以从失败中恢复过来。有两种类型的数据设置检查点。<br>​    Metadata checkpointing：将定义流计算的信息存入容错的系统如HDFS。元数据包括：<br>​    配置 – 用于创建流应用的配置。<br>​    DStreams操作 – 定义流应用的DStreams操作集合。<br>​    不完整批次 – 批次的工作已进行排队但是并未完成。<br>​    Data checkpointing： 将产生的RDDs存入可靠的存储空间。对于在多批次间合并数据的状态转换，这个很有必要。在这样的转换中，RDDs的产生基于之前批次的RDDs，这样依赖链长度随着时间递增。为了避免在恢复期这种无限的时间增长（和链长度成比例），状态转换中间的RDDs周期性写入可靠地存储空间（如HDFS）从而切短依赖链。<br>​    总而言之，元数据检查点在由驱动失效中恢复是首要需要的。而数据或者RDD检查点甚至在使用了状态转换的基础函数中也是必要的。<br>出于这些原因，检查点机制对于任何生产环境中的流计算应用都至关重要。你可以通过向 ssc.checkpoint() 方法传递一个路径参数(HDFS、S3 或者本地路径均可)来配置检查点机制,同时你的应用应该能够使用检查点的数据</p><ol><li>当程序首次启动，其将创建一个新的StreamingContext，设置所有的流并调用start()。 </li><li>当程序在失效后重启，其将依据检查点目录的检查点数据重新创建一个StreamingContext。 通过使用StraemingContext.getOrCreate很容易获得这个性能。</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">ssc.checkpoint(<span class="string">"hdfs://..."</span>)  </span><br><span class="line"></span><br><span class="line"># 创建和设置一个新的<span class="type">StreamingContext</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">functionToCreateContext</span></span>():</span><br><span class="line">    sc = <span class="type">SparkContext</span>(...) # <span class="keyword">new</span> context</span><br><span class="line">    ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(...)</span><br><span class="line">    lines = ssc.socketTextStream(...) # create <span class="type">DStreams</span></span><br><span class="line">    ...</span><br><span class="line">    ssc.checkpoint(checkpointDirectory) # 设置检查点目录</span><br><span class="line">    <span class="keyword">return</span> ssc</span><br><span class="line"># 从检查点数据中获取<span class="type">StreamingContext</span>或者重新创建一个</span><br><span class="line">context = <span class="type">StreamingContext</span>.getOrCreate(checkpointDirectory, functionToCreateContext)</span><br><span class="line"></span><br><span class="line"># 在需要完成的context上做额外的配置</span><br><span class="line"># 无论其有没有启动</span><br><span class="line">context ...</span><br><span class="line"># 启动context</span><br><span class="line">context.start()</span><br><span class="line">contaxt.awaitTermination()</span><br></pre></td></tr></table></figure><p>如果检查点目录(checkpointDirectory)存在，那么context将会由检查点数据重新创建。如果目录不存在（首次运行），那么函数functionToCreateContext将会被调用来创建一个新的context并设置DStreams。<br>​    注意RDDs的检查点引起存入可靠内存的开销。在RDDs需要检查点的批次里，处理的时间会因此而延长。所以，检查点的间隔需要很仔细地设置。在小尺寸批次（1秒钟）。每一批次检查点会显著减少操作吞吐量。反之，检查点设置的过于频繁导致“血统”和任务尺寸增长，这会有很不好的影响对于需要RDD检查点设置的状态转换，默认间隔是批次间隔的乘数一般至少为10秒钟。可以通过dstream.checkpoint(checkpointInterval)。通常，检查点设置间隔是5-10个DStream的滑动间隔。</p><h3 id="WAL-预写日志"><a href="#WAL-预写日志" class="headerlink" title="WAL 预写日志"></a>WAL 预写日志</h3><p>WAL 即 write ahead log（预写日志），是在 1.2 版本中就添加的特性。作用就是，将数据通过日志的方式写到可靠的存储，比如 HDFS、s3，在 driver 或 worker failure 时可以从在可靠存储上的日志文件恢复数据。WAL 在 driver 端和 executor 端都有应用。<br>WAL在 driver 端的应用<br>用于写日志的对象 writeAheadLogOption: WriteAheadLog。在 StreamingContext 中的 JobScheduler 中的 ReceiverTracker 的 ReceivedBlockTracker 构造函数中被创建，ReceivedBlockTracker 用于管理已接收到的 blocks 信息。需要注意的是，这里只需要启用 checkpoint 就可以创建该 driver 端的 WAL 管理实例，而不需要将 spark.streaming.receiver.writeAheadLog.enable 设置为 true。<br>写什么、何时写、写什么<br>首选需要明确的是，ReceivedBlockTracker 通过 WAL 写入 log 文件的内容是3种事件（当然，会进行序列化）：</p><ul><li>case class BlockAdditionEvent(receivedBlockInfo: ReceivedBlockInfo)；即新增了一个 block 及该 block 的具体信息，包括 streamId、blockId、数据条数等</li><li>case class BatchAllocationEvent(time: Time, allocatedBlocks: AllocatedBlocks)；即为某个 batchTime 分配了哪些 blocks 作为该 batch RDD 的数据源</li><li>case class BatchCleanupEvent(times: Seq[Time])；即清理了哪些 batchTime 对应的 block</li><li>知道了写了什么内容，结合源码，也不难找出是什么时候写了这些内容。需要再次注意的是，写上面这三种事件，也不需要将 spark.streaming.receiver.writeAheadLog.enable 设置为 true。</li></ul><p>WAL 在 executor 端的应用<br>Receiver 接收到的数据会源源不断的传递给 ReceiverSupervisor，是否启用 WAL 机制（即是否将 spark.streaming.receiver.writeAheadLog.enable 设置为 true）会影响 ReceiverSupervisor 在存储 block 时的行为：</p><ul><li>不启用 WAL：你设置的StorageLevel是什么，就怎么存储。比如MEMORY_ONLY只会在内存中存一份，MEMORY_AND_DISK会在内存和磁盘上各存一份等</li><li>启用 WAL：在StorageLevel指定的存储的基础上，写一份到 WAL 中。存储一份在 WAL 上，更不容易丢数据但性能损失也比较大</li></ul><p>关于是否要启用 WAL，要视具体的业务而定：</p><ul><li>若可以接受一定的数据丢失，则不需要启用 WAL，因为对性能影响较大</li><li>若完全不能接受数据丢失，那就需要同时启用 checkpoint 和 WAL，checkpoint 保存着执行进度（比如已生成但未完成的 jobs），WAL 中保存着 blocks 及 blocks 元数据（比如保存着未完成的 jobs 对应的 blocks 信息及 block 文件）。同时，这种情况可能要在数据源和 Streaming Application 中联合来保证 exactly once 语义</li></ul><p>预写日志功能的流程是：</p><ol><li>一个SparkStreaming应用开始时（也就是driver开始时），相关的StreamingContext使用SparkContext启动接收器成为长驻运行任务。这些接收器接收并保存流数据到Spark内存中以供处理。</li><li>接收器通知driver。</li><li>接收块中的元数据（metadata）被发送到driver的StreamingContext。<br>这个元数据包括：<ol><li>定位其在executor内存中数据的块referenceid.</li><li>块数据在日志中的偏移信息（如果启用了）。<br>用户传送数据的生命周期如下图所示。</li></ol></li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/22-Application-Driver-And-Executor.png?raw=true" alt="22-Application-Driver-And-Executor"></p><p>类似Kafka这样的系统可以通过复制数据保持可靠性。</p><h3 id="背压机制"><a href="#背压机制" class="headerlink" title="背压机制"></a>背压机制</h3><p>默认情况下，Spark Streaming通过Receiver以生产者生产数据的速率接收数据，计算过程中会出现batch processing time &gt; batch interval的情况，其中batch processing time 为实际计算一个批次花费时间， batch interval为Streaming应用设置的批处理间隔。这意味着Spark Streaming的数据接收速率高于Spark从队列中移除数据的速率，也就是数据处理能力低，在设置间隔内不能完全处理当前接收速率接收的数据。如果这种情况持续过长的时间，会造成数据在内存中堆积，导致Receiver所在Executor内存溢出等问题（如果设置StorageLevel包含disk, 则内存存放不下的数据会溢写至disk, 加大延迟）。Spark 1.5以前版本，用户如果要限制Receiver的数据接收速率，可以通过设置静态配制参数“spark.streaming.receiver.maxRate”的值来实现，此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如：producer数据生产高于maxRate，当前集群处理能力也高于maxRate，这就会造成资源利用率下降等问题。为了更好的协调数据接收速率与资源处理能力，Spark Streaming 从v1.5开始引入反压机制（back-pressure）,通过动态控制数据接收速率来适配集群数据处理能力。<br>Spark Streaming Backpressure:  根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。通过属性“spark.streaming.backpressure.enabled”来控制是否启用backpressure机制，默认值false，即不启用。<br>Streaming架构如下图所示：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/23-Spark-Streaming-Driver-And-Executor.png?raw=true" alt="23-Spark-Streaming-Driver-And-Executor"></p><p>在原架构的基础上加上一个新的组件RateController,这个组件负责监听“OnBatchCompleted”事件，然后从中抽取processingDelay 及schedulingDelay信息.  Estimator依据这些信息估算出最大处理速度（rate），最后由基于Receiver的Input Stream将rate通过ReceiverTracker与ReceiverSupervisorImpl转发给BlockGenerator（继承自RateLimiter）.</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/24-StreamListenerBus.png?raw=true" alt="24-StreamListenerBus"></p><p>流量控制点<br>当Receiver开始接收数据时，会通过supervisor.pushSingle()方法将接收的数据存入currentBuffer等待BlockGenerator定时将数据取走，包装成block. 在将数据存放入currentBuffer之时，要获取许可（令牌）。如果获取到许可就可以将数据存入buffer, 否则将被阻塞，进而阻塞Receiver从数据源拉取数据。<br>其令牌投放采用令牌桶机制进行， 原理如下图所示:</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/25-Token.png?raw=true" alt="25-Token"></p><p>令牌桶机制： 大小固定的令牌桶可自行以恒定的速率源源不断地产生令牌。如果令牌不被消耗，或者被消耗的速度小于产生的速度，令牌就会不断地增多，直到把桶填满。后面再产生的令牌就会从桶中溢出。最后桶中可以保存的最大令牌数永远不会超过桶的大小。当进行某操作时需要令牌时会从令牌桶中取出相应的令牌数，如果获取到则继续操作，否则阻塞。用完之后不用放回。</p><h3 id="驱动器程序容错"><a href="#驱动器程序容错" class="headerlink" title="驱动器程序容错"></a>驱动器程序容错</h3><p>驱动器程序的容错要求我们以特殊的方式创建 StreamingContext。我们需要把检查点目录提供给 StreamingContext。与直接调用 new StreamingContext 不同，应该使用 StreamingContext.getOrCreate() 函数。<br>配置过程如下：</p><ol><li><p>启动Driver自动重启功能</p><ol><li>standalone: 提交任务时添加 –supervise 参数</li><li>yarn:设置yarn.resourcemanager.am.max-attempts 或者spark.yarn.maxAppAttempts</li><li>mesos: 提交任务时添加 –supervise 参数</li></ol></li><li><p>设置checkpoint<br>StreamingContext.setCheckpoint(hdfsDirectory)</p></li><li><p>支持从checkpoint中重启配置</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createContext</span></span>(checkpointDirectory: <span class="type">String</span>): <span class="type">StreamingContext</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span></span><br><span class="line">    ssc.checkpoint(checkpointDirectory)</span><br><span class="line">    ssc</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="type">StreamingContext</span>.getOrCreate(checkpointDirectory, createContext(checkpointDirectory))</span><br></pre></td></tr></table></figure></li></ol><h3 id="工作节点容错"><a href="#工作节点容错" class="headerlink" title="工作节点容错"></a>工作节点容错</h3><p>为了应对工作节点失败的问题，Spark Streaming使用与Spark的容错机制相同的方法。所 有从外部数据源中收到的数据都在多个工作节点上备份。所有从备份数据转化操作的过程 中创建出来的 RDD 都能容忍一个工作节点的失败，因为根据 RDD 谱系图，系统可以把丢 失的数据从幸存的输入数据备份中重算出来。对于reduceByKey等Stateful操作重做的lineage较长的，强制启动checkpoint，减少重做几率。</p><h3 id="接收器容错"><a href="#接收器容错" class="headerlink" title="接收器容错"></a>接收器容错</h3><p>运行接收器的工作节点的容错也是很重要的。如果这样的节点发生错误，Spark Streaming 会在集群中别的节点上重启失败的接收器。然而，这种情况会不会导致数据的丢失取决于 数据源的行为(数据源是否会重发数据)以及接收器的实现(接收器是否会向数据源确认 收到数据)。举个例子，使用 Flume 作为数据源时，两种接收器的主要区别在于数据丢失 时的保障。在“接收器从数据池中拉取数据”的模型中，Spark 只会在数据已经在集群中 备份时才会从数据池中移除元素。而在“向接收器推数据”的模型中，如果接收器在数据 备份之前失败，一些数据可能就会丢失。总的来说，对于任意一个接收器，你必须同时考 虑上游数据源的容错性(是否支持事务)来确保零数据丢失。<br>一般主要是通过将接收到数据后先写日志（WAL）到可靠文件系统中，后才写入实际的RDD。如果后续处理失败则成功写入WAL的数据通过WAL进行恢复，未成功写入WAL的数据通过可回溯的Source进行重放<br>总的来说，接收器提供以下保证：</p><ul><li>所有从可靠文件系统中读取的数据(比如通过StreamingContext.hadoopFiles读取的) 都是可靠的，因为底层的文件系统是有备份的。Spark Streaming会记住哪些数据存放到 了检查点中，并在应用崩溃后从检查点处继续执行。 </li><li>对于像Kafka、推式Flume、Twitter这样的不可靠数据源，Spark会把输入数据复制到其 他节点上，但是如果接收器任务崩溃，Spark 还是会丢失数据。在 Spark 1.1 以及更早的版 本中，收到的数据只被备份到执行器进程的内存中，所以一旦驱动器程序崩溃(此时所 有的执行器进程都会丢失连接)，数据也会丢失。在 Spark 1.2 中，收到的数据被记录到诸 如 HDFS 这样的可靠的文件系统中，这样即使驱动器程序重启也不会导致数据丢失。 </li></ul><p>综上所述，确保所有数据都被处理的最佳方式是使用可靠的数据源(例如 HDFS、拉式 Flume 等)。如果你还要在批处理作业中处理这些数据，使用可靠数据源是最佳方式，因为 这种方式确保了你的批处理作业和流计算作业能读取到相同的数据，因而可以得到相同的结果。<br>操作过程如下：</p><ul><li>启用checkpoint<br>ssc.setCheckpoint(checkpointDir)</li><li>启用WAL<br>sparkConf.set(“spark.streaming.receiver.writeAheadLog.enable”, “true”)</li><li>对Receiver使用可靠性存储StoreageLevel.MEMORY_AND_DISK_SER or StoreageLevel.MEMORY_AND_DISK_SER2</li></ul><h3 id="处理保证"><a href="#处理保证" class="headerlink" title="处理保证"></a>处理保证</h3><p>由于Spark Streaming工作节点的容错保障，Spark Streaming可以为所有的转化操作提供 “精确一次”执行的语义，即使一个工作节点在处理部分数据时发生失败，最终的转化结<br>果(即转化操作得到的 RDD)仍然与数据只被处理一次得到的结果一样。<br>然而，当把转化操作得到的结果使用输出操作推入外部系统中时，写结果的任务可能因故 障而执行多次，一些数据可能也就被写了多次。由于这引入了外部系统，因此我们需要专 门针对各系统的代码来处理这样的情况。我们可以使用事务操作来写入外部系统(即原子 化地将一个 RDD 分区一次写入)，或者设计幂等的更新操作(即多次运行同一个更新操作 仍生成相同的结果)。比如 Spark Streaming 的 saveAs…File 操作会在一个文件写完时自动 将其原子化地移动到最终位置上，以此确保每个输出文件只存在一份。 </p><h3 id="性能考量"><a href="#性能考量" class="headerlink" title="性能考量"></a>性能考量</h3><p>最常见的问题是Spark Streaming可以使用的最小批次间隔是多少。总的来说，500毫秒已经被证实为对许多应用而言是比较好的最小批次大小。寻找最小批次大小的最佳实践是从一个比较大的批次大小(10 秒左右)开始，不断使用更小的批次大小。如果 Streaming 用 户界面中显示的处理时间保持不变，你就可以进一步减小批次大小。如果处理时间开始增 加，你可能已经达到了应用的极限。<br>相似地，对于窗口操作，计算结果的间隔(也就是滑动步长)对于性能也有巨大的影响。 当计算代价巨大并成为系统瓶颈时，就应该考虑提高滑动步长了。<br>减少批处理所消耗时间的常见方式还有提高并行度。有以下三种方式可以提高并行度：</p><ul><li>增加接收器数目 有时如果记录太多导致单台机器来不及读入并分发的话，接收器会成为系统瓶颈。这时 你就需要通过创建多个输入 DStream(这样会创建多个接收器)来增加接收器数目，然 后使用 union 来把数据合并为一个数据源。 </li><li>将收到的数据显式地重新分区如果接收器数目无法再增加，你可以通过使用 DStream.repartition 来显式重新分区输 入流(或者合并多个流得到的数据流)来重新分配收到的数据。 </li><li>提高聚合计算的并行度 对于像 reduceByKey() 这样的操作，你可以在第二个参数中指定并行度，我们在介绍 RDD 时提到过类似的手段。 </li></ul><h1 id="Spark-Streaming-高级解析"><a href="#Spark-Streaming-高级解析" class="headerlink" title="Spark Streaming 高级解析"></a>Spark Streaming 高级解析</h1><h2 id="DStreamGraph-对象解析"><a href="#DStreamGraph-对象解析" class="headerlink" title="DStreamGraph 对象解析"></a>DStreamGraph 对象解析</h2><p>在 Spark Streaming 中，DStreamGraph 是一个非常重要的组件，主要用来：</p><ol><li>通过成员 inputStreams 持有 Spark Streaming 输入源及接收数据的方式</li><li>通过成员 outputStreams 持有 Streaming app 的 output 操作，并记录 DStream 依赖关系</li><li>生成每个 batch 对应的 jobs</li></ol><p>下面，通过分析一个简单的例子，结合源码分析来说明 DStreamGraph 是如何发挥作用的。例子如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"HdfsWordCount"</span>)</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lines = ssc.textFileStream(args(<span class="number">0</span>))</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">wordCounts.print()</span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure><p>创建 DStreamGraph 实例<br>代码val ssc = new StreamingContext(sparkConf, Seconds(2))创建了 StreamingContext 实例，StreamingContext 包含了 DStreamGraph 类型的成员graph，graph 在 StreamingContext主构造函数中被创建，如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[streaming] <span class="keyword">val</span> graph: <span class="type">DStreamGraph</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (isCheckpointPresent) &#123;</span><br><span class="line">      cp_.graph.setContext(<span class="keyword">this</span>)</span><br><span class="line">      cp_.graph.restoreCheckpointData()</span><br><span class="line">      cp_.graph</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      require(batchDur_ != <span class="literal">null</span>, <span class="string">"Batch duration for StreamingContext cannot be null"</span>)</span><br><span class="line">      <span class="keyword">val</span> newGraph = <span class="keyword">new</span> <span class="type">DStreamGraph</span>()</span><br><span class="line">      newGraph.setBatchDuration(batchDur_)</span><br><span class="line">      newGraph</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>可以看到，若当前 checkpoint 可用，会优先从 checkpoint 恢复 graph，否则新建一个。还可以从这里知道的一点是：graph 是运行在 driver 上的</p><p>DStreamGraph记录输入源及如何接收数据<br>DStreamGraph有和application 输入数据相关的成员和方法，如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> inputStreams = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">InputDStream</span>[_]]()</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addInputStream</span></span>(inputStream: <span class="type">InputDStream</span>[_]) &#123;</span><br><span class="line">    <span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">      inputStream.setGraph(<span class="keyword">this</span>)</span><br><span class="line">      inputStreams += inputStream</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>成员inputStreams为 InputDStream 类型的数组，InputDStream是所有 input streams(数据输入流) 的虚基类。该类提供了 start() 和 stop()方法供 streaming 系统来开始和停止接收数据。那些只需要在 driver 端接收数据并转成 RDD 的 input streams 可以直接继承 InputDStream，例如 FileInputDStream是 InputDStream 的子类，它监控一个 HDFS 目录并将新文件转成RDDs。而那些需要在 workers 上运行receiver 来接收数据的 Input DStream，需要继承 ReceiverInputDStream，比如 KafkaReceiver。<br>我们来看看val lines = ssc.textFileStream(args(0))调用。<br>为了更容易理解，我画出了val lines = ssc.textFileStream(args(0))的调用流程：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/26-TextFileStream.png?raw=true" alt="26-TextFileStream"></p><p>从上面的调用流程图我们可以知道：</p><ol><li>ssc.textFileStream会触发新建一个FileInputDStream。FileInputDStream继承于InputDStream，其start()方法定义了数据源及如何接收数据</li><li>在FileInputDStream构造函数中，会调用ssc.graph.addInputStream(this)，将自身添加到 DStreamGraph 的 inputStreams: ArrayBuffer[InputDStream[_]] 中，这样 DStreamGraph 就知道了这个 Streaming App 的输入源及如何接收数据。可能你会奇怪为什么inputStreams 是数组类型，举个例子，这里再来一个 val lines1 = ssc.textFileStream(args(0))，那么又将生成一个 FileInputStream 实例添加到inputStreams，所以这里需要集合类型</li><li>生成FileInputDStream调用其 map 方法，将以 FileInputDStream 本身作为 partent 来构造新的 MappedDStream。对于 DStream 的 transform 操作，都将生成一个新的 DStream，和 RDD transform 生成新的 RDD 类似<br>与MappedDStream 不同，所有继承了 InputDStream 的定义了输入源及接收数据方式的 sreams 都没有 parent，因为它们就是最初的 streams。</li></ol><p>DStream 的依赖链<br>每个 DStream 的子类都会继承 def dependencies: List[DStream[_]] = List()方法，该方法用来返回自己的依赖的父 DStream 列表。比如，没有父DStream 的 InputDStream 的 dependencies方法返回List()。<br>MappedDStream 的实现如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MappedDStream</span>[<span class="type">T</span>: <span class="type">ClassTag</span>, <span class="type">U</span>: <span class="type">ClassTag</span>] (<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    parent: <span class="type">DStream</span>[<span class="type">T</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">    mapFunc: <span class="type">T</span> =&gt; <span class="type">U</span></span></span></span><br><span class="line"><span class="class"><span class="params">  </span>) <span class="keyword">extends</span> <span class="title">DStream</span>[<span class="type">U</span>](<span class="params">parent.ssc</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dependencies</span></span>: <span class="type">List</span>[<span class="type">DStream</span>[_]] = <span class="type">List</span>(parent)</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在上例中，构造函数参数列表中的 parent 即在 ssc.textFileStream 中new 的定义了输入源及数据接收方式的最初的 FileInputDStream实例，这里的 dependencies方法将返回该FileInputDStream实例，这就构成了第一条依赖。可用如下图表示，这里特地将 input streams 用蓝色表示，以强调其与普通由 transform 产生的 DStream 的不同：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/27-FileInputDStream-To-MappedDStream.png?raw=true" alt="27-FileInputDStream-To-MappedDStream"></p><p>继续来看val words = lines.flatMap(_.split(“ “))，flatMap如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMapU</span></span>: <span class="type">ClassTag</span>: <span class="type">DStream</span>[<span class="type">U</span>] = ssc.withScope &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">FlatMappedDStream</span>(<span class="keyword">this</span>, context.sparkContext.clean(flatMapFunc))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>每一个 transform 操作都将创建一个新的 DStream，flatMap 操作也不例外，它会创建一个FlatMappedDStream，FlatMappedDStream的实现如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FlatMappedDStreamT</span></span>: <span class="type">ClassTag</span>, <span class="type">U</span>: <span class="type">ClassTag</span> <span class="keyword">extends</span> <span class="type">DStreamU</span> &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dependencies</span></span>: <span class="type">List</span>[<span class="type">DStream</span>[_]] = <span class="type">List</span>(parent)</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>与 MappedDStream 相同，FlatMappedDStream#dependencies也返回其依赖的父 DStream，及 lines，到这里，依赖链就变成了下图：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/28-FileInputDStream-To-MappedDStream.png?raw=true" alt="28-FileInputDStream-To-MappedDStream"></p><p>之后的几步操作不再这样具体分析，到生成wordCounts时，依赖图将变成下面这样：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/29-FileInputDStream-To-MappedDStream.png?raw=true" alt="29-FileInputDStream-To-MappedDStream"></p><p>在 DStream 中，与 transofrm 相对应的是 output 操作，包括 print, saveAsTextFiles, saveAsObjectFiles, saveAsHadoopFiles, foreachRDD。output 操作中，会创建ForEachDStream实例并调用register方法将自身添加到DStreamGraph.outputStreams成员中，该ForEachDStream实例也会持有是调用的哪个 output 操作。本例的代码调用如下，只需看箭头所指几行代码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Print the first ten elements of each RDD generated in this DStream. This is an output</span></span><br><span class="line"><span class="comment">   * operator, so this DStream will be registered as an output stream and there materialized.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">print</span></span>(): <span class="type">Unit</span> = ssc.withScope &#123;</span><br><span class="line">    print(<span class="number">10</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Print the first num elements of each RDD generated in this DStream. This is an output</span></span><br><span class="line"><span class="comment">   * operator, so this DStream will be registered as an output stream and there materialized.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">print</span></span>(num: <span class="type">Int</span>): <span class="type">Unit</span> = ssc.withScope &#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foreachFunc</span></span>: (<span class="type">RDD</span>[<span class="type">T</span>], <span class="type">Time</span>) =&gt; <span class="type">Unit</span> = &#123;</span><br><span class="line">      (rdd: <span class="type">RDD</span>[<span class="type">T</span>], time: <span class="type">Time</span>) =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> firstNum = rdd.take(num + <span class="number">1</span>)</span><br><span class="line">        <span class="comment">// scalastyle:off println</span></span><br><span class="line">        println(<span class="string">"-------------------------------------------"</span>)</span><br><span class="line">        println(<span class="string">s"Time: <span class="subst">$time</span>"</span>)</span><br><span class="line">        println(<span class="string">"-------------------------------------------"</span>)</span><br><span class="line">        firstNum.take(num).foreach(println)</span><br><span class="line">        <span class="keyword">if</span> (firstNum.length &gt; num) println(<span class="string">"..."</span>)</span><br><span class="line">        println()</span><br><span class="line">        <span class="comment">// scalastyle:on println</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    foreachRDD(context.sparkContext.clean(foreachFunc), displayInnerRDDOps = <span class="literal">false</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Register this streaming as an output stream. This would ensure that RDDs of this</span></span><br><span class="line"><span class="comment">   * DStream will be generated.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[streaming] <span class="function"><span class="keyword">def</span> <span class="title">register</span></span>(): <span class="type">DStream</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    ssc.graph.addOutputStream(<span class="keyword">this</span>)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>与 DStream transform 操作返回一个新的 DStream 不同，output 操作不会返回任何东西，只会创建一个ForEachDStream作为依赖链的终结。<br>至此，生成了完成的依赖链，也就是 DAG，如下图（这里将 ForEachDStream 标为黄色以显示其与众不同）：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/30-FileInputDStream-To-MappedDStream.png?raw=true" alt="30-FileInputDStream-To-MappedDStream"></p><h2 id="ReceiverTracker-与数据导入"><a href="#ReceiverTracker-与数据导入" class="headerlink" title="ReceiverTracker 与数据导入"></a>ReceiverTracker 与数据导入</h2><p>Spark Streaming 在数据接收与导入方面需要满足有以下三个特点：</p><ol><li>兼容众多输入源，包括HDFS, Flume, Kafka, Twitter and ZeroMQ。还可以自定义数据源</li><li>要能为每个 batch 的 RDD 提供相应的输入数据</li><li>为适应 7*24h 不间断运行，要有接收数据挂掉的容错机制<br>有容乃大，兼容众多数据源</li></ol><p>InputDStream是所有 input streams(数据输入流) 的虚基类。该类提供了 start() 和 stop()方法供 streaming 系统来开始和停止接收数据。那些只需要在 driver 端接收数据并转成 RDD 的 input streams 可以直接继承 InputDStream，例如 FileInputDStream是 InputDStream 的子类，它监控一个 HDFS 目录并将新文件转成RDDs。而那些需要在 workers 上运行receiver 来接收数据的 Input DStream，需要继承 ReceiverInputDStream，比如 KafkaReceiver<br>只需在 driver 端接收数据的 input stream 一般比较简单且在生产环境中使用的比较少，本文不作分析，只分析继承了 ReceiverInputDStream 的 input stream 是如何导入数据的。<br>ReceiverInputDStream有一个def getReceiver(): Receiver[T]方法，每个继承了ReceiverInputDStream的 input stream 都必须实现这个方法。该方法用来获取将要分发到各个 worker 节点上用来接收数据的 receiver（接收器）。不同的 ReceiverInputDStream 子类都有它们对应的不同的 receiver，如KafkaInputDStream对应KafkaReceiver，FlumeInputDStream对应FlumeReceiver，TwitterInputDStream对应TwitterReceiver，如果你要实现自己的数据源，也需要定义相应的 receiver。<br>继承 ReceiverInputDStream 并定义相应的 receiver，就是 Spark Streaming 能兼容众多数据源的原因。</p><p>为每个 batch 的 RDD 提供输入数据<br>在 StreamingContext 中，有一个重要的组件叫做 ReceiverTracker，它是 Spark Streaming 作业调度器 JobScheduler 的成员，负责启动、管理各个 receiver 及管理各个 receiver 接收到的数据。</p><p>确定 receiver 要分发到哪些 executors 上执行<br>创建 ReceiverTracker 实例<br>我们来看 StreamingContext#start() 方法部分调用实现，如下：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/31-StreamingContext-start.png?raw=true" alt="31-StreamingContext-start"></p><p>可以看到，StreamingContext#start() 会调用 JobScheduler#start() 方法，在 JobScheduler#start() 中，会创建一个新的 ReceiverTracker 实例 receiverTracker，并调用其 start() 方法。</p><p>ReceiverTracker#start()<br>继续跟进 ReceiverTracker#start()，如下图，它主要做了两件事：</p><ol><li>初始化一个 endpoint: ReceiverTrackerEndpoint，用来接收和处理来自 ReceiverTracker 和 receivers 发送的消息</li><li>调用 launchReceivers 来自将各个 receivers 分发到 executors 上</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/32-ReceiverTracker-start.png?raw=true" alt="32-ReceiverTracker-start"></p><p>ReceiverTracker#launchReceivers()<br>继续跟进 launchReceivers，它也主要干了两件事：</p><ol><li>获取 DStreamGraph.inputStreams 中继承了 ReceiverInputDStream 的 input streams 的 receivers。也就是数据接收器</li><li>给消息接收处理器 endpoint 发送 StartAllReceivers(receivers)消息。直接返回，不等待消息被处理</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/33-launchReceivers.png?raw=true" alt="33-launchReceivers"></p><p>处理StartAllReceivers消息<br>endpoint 在接收到消息后，会先判断消息类型，对不同的消息做不同处理。对于StartAllReceivers消息，处理流程如下：<br>计算每个 receiver 要分发的目的 executors。遵循两条原则：</p><ul><li>将 receiver 分布的尽量均匀</li><li>如果 receiver 的preferredLocation本身不均匀，以preferredLocation为准</li></ul><p>遍历每个 receiver，根据第1步中得到的目的 executors 调用 startReceiver 方法。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/34-receiver.png?raw=true" alt="34-receiver"></p><p>到这里，已经确定了每个 receiver 要分发到哪些 executors 上<br>启动 receivers<br>接上，通过 ReceiverTracker#startReceiver(receiver: Receiver[_], scheduledExecutors: Seq[String]) 来启动 receivers，我们来看具体流程：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/35-startReceiver.png?raw=true" alt="35-startReceiver"></p><p>如上流程图所述，分发和启动 receiver 的方式不可谓不精彩。其中，startReceiverFunc 函数主要实现如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> supervisor = <span class="keyword">new</span> <span class="type">ReceiverSupervisorImpl</span>(</span><br><span class="line">  receiver, <span class="type">SparkEnv</span>.get, serializableHadoopConf.value, checkpointDirOption)</span><br><span class="line">supervisor.start()</span><br><span class="line">supervisor.awaitTermination()</span><br></pre></td></tr></table></figure><p>supervisor.start() 中会调用 receiver#onStart 后立即返回。receiver#onStart 一般自行新建线程或线程池来接收数据，比如在 KafkaReceiver 中，就新建了线程池，在线程池中接收 topics 的数据。<br>supervisor.start() 返回后，由 supervisor.awaitTermination() 阻塞住线程，以让这个 task 一直不退出，从而可以源源不断接收数据。</p><p>数据流转</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/36-Data-Flow.png?raw=true" alt="36-Data-Flow"></p><p>上图为 receiver 接收到的数据的流转过程，让我们来逐一分析<br>Step1: Receiver -&gt; ReceiverSupervisor</p><p>这一步中，Receiver 将接收到的数据源源不断地传给 ReceiverSupervisor。Receiver 调用其 store(…) 方法，store 方法中继续调用 supervisor.pushSingle 或 supervisor.pushArrayBuffer 等方法来传递数据。Receiver#store 有多重形式， ReceiverSupervisor 也有 pushSingle、pushArrayBuffer、pushIterator、pushBytes 方法与不同的 store 对应。</p><ul><li>pushSingle: 对应单条小数据</li><li>pushArrayBuffer: 对应数组形式的数据</li><li>pushIterator: 对应 iterator 形式数据</li><li>pushBytes: 对应 ByteBuffer 形式的块数据</li></ul><p>对于细小的数据，存储时需要 BlockGenerator 聚集多条数据成一块，然后再成块存储；反之就不用聚集，直接成块存储。当然，存储操作并不在 Step1 中执行，只为说明之后不同的操作逻辑。</p><p>Step2.1: ReceiverSupervisor -&gt; BlockManager -&gt; disk/memory</p><p>在这一步中，主要将从 receiver 收到的数据以 block（数据块）的形式存储<br>存储 block 的是receivedBlockHandler: ReceivedBlockHandler，根据参数spark.streaming.receiver.writeAheadLog.enable配置的不同，默认为 false，receivedBlockHandler对象对应的类也不同，如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> receivedBlockHandler: <span class="type">ReceivedBlockHandler</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (<span class="type">WriteAheadLogUtils</span>.enableReceiverLog(env.conf)) &#123;</span><br><span class="line">    <span class="comment">//&lt; 先写 WAL，再存储到 executor 的内存或硬盘</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">WriteAheadLogBasedBlockHandler</span>(env.blockManager, receiver.streamId,</span><br><span class="line">      receiver.storageLevel, env.conf, hadoopConf, checkpointDirOption.get)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">//&lt; 直接存到 executor 的内存或硬盘</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">BlockManagerBasedBlockHandler</span>(env.blockManager, receiver.storageLevel)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动 WAL 的好处就是在application 挂掉之后，可以恢复数据。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//&lt; 调用 receivedBlockHandler.storeBlock 方法存储 block，并得到一个 blockStoreResult</span></span><br><span class="line"><span class="keyword">val</span> blockStoreResult = receivedBlockHandler.storeBlock(blockId, receivedBlock)</span><br><span class="line"><span class="comment">//&lt; 使用blockStoreResult初始化一个ReceivedBlockInfo实例</span></span><br><span class="line"><span class="keyword">val</span> blockInfo = <span class="type">ReceivedBlockInfo</span>(streamId, numRecords, metadataOption, blockStoreResult)</span><br><span class="line"><span class="comment">//&lt; 发送消息通知 ReceiverTracker 新增并存储了 block</span></span><br><span class="line">trackerEndpoint.askWithRetry[<span class="type">Boolean</span>](<span class="type">AddBlock</span>(blockInfo))</span><br></pre></td></tr></table></figure><p>不管是 WriteAheadLogBasedBlockHandler 还是 BlockManagerBasedBlockHandler 最终都是通过 BlockManager 将 block 数据存储 execuor 内存或磁盘或还有 WAL 方式存入。<br>这里需要说明的是 streamId，每个 InputDStream 都有它自己唯一的 id，即 streamId，blockInfo包含 streamId 是为了区分block 是哪个 InputDStream 的数据。之后为 batch 分配 blocks 时，需要知道每个 InputDStream 都有哪些未分配的 blocks。</p><p>Step2.2: ReceiverSupervisor -&gt; ReceiverTracker<br>将 block 存储之后，获得 block 描述信息 blockInfo: ReceivedBlockInfo，这里面包含：streamId、数据位置、数据条数、数据 size 等信息。<br>之后，封装以 block 作为参数的 AddBlock(blockInfo) 消息并发送给 ReceiverTracker 以通知其有新增 block 数据块。</p><p>Step3: ReceiverTracker -&gt; ReceivedBlockTracker</p><p>ReceiverTracker 收到 ReceiverSupervisor 发来的 AddBlock(blockInfo) 消息后，直接调用以下代码将 block 信息传给 ReceivedBlockTracker：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">addBlock</span></span>(receivedBlockInfo: <span class="type">ReceivedBlockInfo</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">   receivedBlockTracker.addBlock(receivedBlockInfo)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>receivedBlockTracker.addBlock中，如果启用了 WAL，会将新增的 block 信息以 WAL 方式保存。</p><p>无论 WAL 是否启用，都会将新增的 block 信息保存到 streamIdToUnallocatedBlockQueues: mutable.HashMap[Int, ReceivedBlockQueue]中，该变量 key 为 InputDStream 的唯一 id，value 为已存储未分配的 block 信息。之后为 batch 分配blocks，会访问该结构来获取每个 InputDStream 对应的未消费的 blocks。</p><h2 id="动态生成-Job"><a href="#动态生成-Job" class="headerlink" title="动态生成 Job"></a>动态生成 Job</h2><p>JobScheduler有两个重要成员，一是ReceiverTracker，负责分发 receivers 及源源不断地接收数据；二是JobGenerator，负责定时的生成 jobs 并 checkpoint。</p><p><strong>定时逻辑</strong></p><p>在 JobScheduler 的主构造函数中，会创建 JobGenerator 对象。在 JobGenerator 的主构造函数中，会创建一个定时器：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> timer = <span class="keyword">new</span> <span class="type">RecurringTimer</span>(clock, ssc.graph.batchDuration.milliseconds,</span><br><span class="line">    longTime =&gt; eventLoop.post(<span class="type">GenerateJobs</span>(<span class="keyword">new</span> <span class="type">Time</span>(longTime))), <span class="string">"JobGenerator"</span>)</span><br></pre></td></tr></table></figure><p>该定时器每隔 ssc.graph.batchDuration.milliseconds 会执行一次 eventLoop.post(GenerateJobs(new Time(longTime))) 向 eventLoop 发送 GenerateJobs(new Time(longTime))消息，<strong>eventLoop**</strong>收到消息后会进行这个 batch<strong> </strong>对应的 jobs<strong> </strong>的生成及提交执行**，eventLoop 是一个消息接收处理器。</p><p>需要注意的是，timer 在创建之后并不会马上启动，将在 StreamingContext#start() 启动 Streaming Application 时间接调用到 timer.start(restartTime.milliseconds)才启动。</p><p><strong>为 batch</strong> <strong>生成 jobs</strong></p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/37-GenerateJobs.png?raw=true" alt="37-GenerateJobs"></p><p>eventLoop 在接收到 GenerateJobs(new Time(longTime))消息后的主要处理流程有以上图中三步：</p><ol><li>将已接收到的 blocks 分配给 batch</li><li>生成该 batch 对应的 jobs</li><li>将 jobs 封装成 JobSet 并提交执行<br>接下来我们就将逐一展开这三步进行分析</li></ol><p>将已接受到的 blocks 分配给 batch</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/38-allocateBlocksToBatch.png?raw=true" alt="38-allocateBlocksToBatch"></p><p>上图是根据源码画出的为 batch 分配 blocks 的流程图，这里对 『获得 batchTime 各个 InputDStream 未分配的 blocks』作进一步说明：</p><p>我们知道了各个 ReceiverInputDStream 对应的 receivers 接收并保存的 blocks 信息会保存在 ReceivedBlockTracker#streamIdToUnallocatedBlockQueues，该成员 key 为 streamId，value 为该 streamId 对应的 InputDStream 已接收保存但尚未分配的 blocks 信息。<br>所以获取某 InputDStream 未分配的 blocks 只要以该 InputDStream 的 streamId 来从 streamIdToUnallocatedBlockQueues 来 get 就好。获取之后，会清楚该 streamId 对应的value，以保证 block 不会被重复分配。<br>在实际调用中，为 batchTime 分配 blocks 时，会从streamIdToUnallocatedBlockQueues取出未分配的 blocks 塞进 timeToAllocatedBlocks: mutable.HashMap[Time, AllocatedBlocks] 中，以在之后作为该 batchTime 对应的 RDD 的输入数据。<br>通过以上步骤，就可以为 batch 的所有 InputDStream 分配 blocks。也就是为 batch 分配了 blocks。</p><p>生成该 batch 对应的 jobs</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/39-GenerateJobs.png?raw=true" alt="39-GenerateJobs"></p><p>为指定 batchTime 生成 jobs 的逻辑如上图所示。你可能会疑惑，为什么 DStreamGraph#generateJobs(time: Time)为什么返回 Seq[Job]，而不是单个 job。这是因为，在一个 batch 内，可能会有多个 OutputStream 执行了多次 output 操作，每次 output 操作都将产生一个 Job，最终就会产生多个 Jobs。<br>我们结合上图对执行流程进一步分析。<br>在DStreamGraph#generateJobs(time: Time)中，对于DStreamGraph成员ArrayBuffer[DStream[_]]的每一项，调用DStream#generateJob(time: Time)来生成这个 outputStream 在该 batchTime 的 job。该生成过程主要有三步：</p><p>Step1: 获取该 outputStream 在该 batchTime 对应的 RDD</p><p>每个 DStream 实例都有一个 generatedRDDs: HashMap[Time, RDD[T]] 成员，用来保存该 DStream 在每个 batchTime 生成的 RDD，当 DStream#getOrCompute(time: Time)调用时</p><ul><li>首先会查看generatedRDDs中是否已经有该 time 对应的 RDD，若有则直接返回</li><li>若无，则调用compute(validTime: Time)来生成 RDD，这一步根据每个 InputDStream继承 compute 的实现不同而不同。例如，对于 FileInputDStream，其 compute 实现逻辑如下：<ol><li>先通过一个 findNewFiles() 方法，找到多个新 file</li><li>对每个新 file，都将其作为参数调用 sc.newAPIHadoopFile(file)，生成一个 RDD 实例</li><li>将 2 中的多个新 file 对应的多个 RDD 实例进行 union，返回一个 union 后的 UnionRDD</li></ol></li></ul><p>Step2: 根据 Step1中得到的 RDD 生成最终 job 要执行的函数 jobFunc<br>jobFunc定义如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> jobFunc = () =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> emptyFunc = &#123; (iterator: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; &#123;&#125; &#125;</span><br><span class="line">  context.sparkContext.runJob(rdd, emptyFunc)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，每个 outputStream 的 output 操作生成的 Job 其实与 RDD action 一样，最终调用 SparkContext#runJob 来提交 RDD DAG 定义的任务。</p><p>Step3: 根据 Step2中得到的 jobFunc 生成最终要执行的 Job 并返回<br>Step2中得到了定义 Job 要干嘛的函数-jobFunc，这里便以 jobFunc及 batchTime 生成 Job 实例：<br>Some(new Job(time, jobFunc))<br>该Job实例将最终封装在 JobHandler 中被执行<br>至此，我们搞明白了 JobScheduler 是如何通过一步步调用来动态生成每个 batchTime 的 jobs。下文我们将分析这些动态生成的 jobs 如何被分发及如何执行。</p><h2 id="Job-的提交与执行"><a href="#Job-的提交与执行" class="headerlink" title="Job 的提交与执行"></a>Job 的提交与执行</h2><p>我们分析了 JobScheduler 是如何动态为每个 batch生成 jobs，那么生成的 jobs 是如何被提交的。<br>在 JobScheduler 生成某个 batch 对应的 Seq[Job] 之后，会将 batch 及 Seq[Job] 封装成一个 JobSet 对象，JobSet 持有某个 batch 内所有的 jobs，并记录各个 job 的运行状态。<br>之后，调用JobScheduler#submitJobSet(jobSet: JobSet)来提交 jobs，在该函数中，除了一些状态更新，主要任务就是执行 jobSet.jobs.foreach(job =&gt; jobExecutor.execute(new JobHandler(job))). 即，对于 jobSet 中的每一个 job，执行jobExecutor.execute(new JobHandler(job))，要搞懂这行代码干了什么，就必须了解 JobHandler 及 jobExecutor。<br>JobHandler<br>JobHandler 继承了 Runnable，为了说明与 job 的关系，其精简后的实现如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">JobHandler</span>(<span class="params">job: <span class="type">Job</span></span>) <span class="keyword">extends</span> <span class="title">Runnable</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  <span class="keyword">import</span> <span class="type">JobScheduler</span>._</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">    _eventLoop.post(<span class="type">JobStarted</span>(job))</span><br><span class="line">    <span class="type">PairRDDFunctions</span>.disableOutputSpecValidation.withValue(<span class="literal">true</span>) &#123;</span><br><span class="line">      job.run()</span><br><span class="line">    &#125;</span><br><span class="line">    _eventLoop = eventLoop</span><br><span class="line">    <span class="keyword">if</span> (_eventLoop != <span class="literal">null</span>) &#123;</span><br><span class="line">      _eventLoop.post(<span class="type">JobCompleted</span>(job))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>JobHandler#run 方法主要执行了 job.run()，该方法最终将调用到<br>『生成该 batch 对应的 jobs的Step2 定义的 jobFunc』，jonFunc 将提交对应 RDD DAG 定义的 job。<br>JobExecutor<br>知道了 JobHandler 是用来执行 job 的，那么 JobHandler 将在哪里执行 job 呢？答案是<br>jobExecutor，jobExecutor为 JobScheduler 成员，是一个线程池，在JobScheduler 主构造函数中创建，如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> numConcurrentJobs = ssc.conf.getInt(<span class="string">"spark.streaming.concurrentJobs"</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> jobExecutor = <span class="type">ThreadUtils</span>.newDaemonFixedThreadPool(numConcurrentJobs, <span class="string">"streaming-job-executor"</span>)</span><br></pre></td></tr></table></figure><p>JobHandler 将最终在 线程池jobExecutor 的线程中被调用，jobExecutor的线程数可通过spark.streaming.concurrentJobs配置，默认为1。若配置多个线程，就能让多个 job 同时运行，若只有一个线程，那么同一时刻只能有一个 job 运行。<br>以上，即 jobs 被执行的逻辑。</p><h2 id="Block-的生成与存储"><a href="#Block-的生成与存储" class="headerlink" title="Block 的生成与存储"></a>Block 的生成与存储</h2><p>ReceiverSupervisorImpl共提供了4个将从 receiver 传递过来的数据转换成 block 并存储的方法，分别是：</p><ul><li>pushSingle: 处理单条数据</li><li>pushArrayBuffer: 处理数组形式数据</li><li>pushIterator: 处理 iterator 形式处理</li><li>pushBytes: 处理 ByteBuffer 形式数据</li></ul><p>其中，pushArrayBuffer、pushIterator、pushBytes最终调用pushAndReportBlock；而pushSingle将调用defaultBlockGenerator.addData(data)，我们分别就这两种形式做说明</p><p>pushAndReportBlock<br>我们针对存储 block 简化 pushAndReportBlock 后的代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pushAndReportBlock</span></span>(</span><br><span class="line">  receivedBlock: <span class="type">ReceivedBlock</span>,</span><br><span class="line">  metadataOption: <span class="type">Option</span>[<span class="type">Any</span>],</span><br><span class="line">  blockIdOption: <span class="type">Option</span>[<span class="type">StreamBlockId</span>]</span><br><span class="line">) &#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">val</span> blockId = blockIdOption.getOrElse(nextBlockId)</span><br><span class="line">  receivedBlockHandler.storeBlock(blockId, receivedBlock)</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先获取一个新的 blockId，之后调用 receivedBlockHandler.storeBlock, receivedBlockHandler 在 ReceiverSupervisorImpl 构造函数中初始化。当启用了 checkpoint 且 spark.streaming.receiver.writeAheadLog.enable 为 true 时，receivedBlockHandler 被初始化为 WriteAheadLogBasedBlockHandler 类型；否则将初始化为 BlockManagerBasedBlockHandler类型。<br>WriteAheadLogBasedBlockHandler#storeBlock 将 ArrayBuffer, iterator, bytes 类型的数据序列化后得到的 serializedBlock</p><ol><li>交由 BlockManager 根据设置的 StorageLevel 存入 executor 的内存或磁盘中</li><li>通过 WAL 再存储一份</li></ol><p>而BlockManagerBasedBlockHandler#storeBlock将 ArrayBuffer, iterator, bytes 类型的数据交由 BlockManager 根据设置的 StorageLevel 存入 executor 的内存或磁盘中，并不再通过 WAL 存储一份<br>pushSingle<br>pushSingle将调用 BlockGenerator#addData(data: Any) 通过积攒的方式来存储数据。接下来对 BlockGenerator 是如何积攒一条一条数据最后写入 block 的逻辑</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Streaming/40-BlockGenerator.png?raw=true" alt="40-BlockGenerator"></p><p>上图为 BlockGenerator 的各个成员，首选对各个成员做介绍：<br>currentBuffer<br>变长数组，当 receiver 接收的一条一条的数据将会添加到该变长数组的尾部</p><ul><li>可能会有一个 receiver 的多个线程同时进行添加数据，这里是同步操作</li><li>添加前，会由 rateLimiter 检查一下速率，是否加入的速度过快。如果过快的话就需要 block 住，等到下一秒再开始添加。最高频率由 spark.streaming.receiver.maxRate 控制，默认值为 Long.MaxValue，具体含义是单个 Receiver 每秒钟允许添加的条数。<br>blockIntervalTimer &amp; blockIntervalMs</li></ul><p>分别是定时器和时间间隔。blockIntervalTimer中有一个线程，每隔blockIntervalMs会执行以下操作：</p><ol><li>将 currentBuffer 赋值给 newBlockBuffer</li><li>将 currentBuffer 指向新的空的 ArrayBuffer 对象</li><li>将 newBlockBuffer 封装成 newBlock</li><li>将 newBlock 添加到 blocksForPushing 队列中blockIntervalMs 由 spark.streaming.blockInterval 控制，默认是 200ms。</li></ol><p>blockPushingThread &amp; blocksForPushing &amp; blockQueueSize<br>blocksForPushing 是一个定长数组，长度由 blockQueueSize 决定，默认为10，可通过 spark.streaming.blockQueueSize 改变。上面分析到，blockIntervalTimer中的线程会定时将 block 塞入该队列。<br>还有另一条线程不断送该队列中取出 block，然后调用 ReceiverSupervisorImpl.pushArrayBuffer(…) 来将 block 存储，这条线程就是blockPushingThread。<br>PS: blocksForPushing为ArrayBlockingQueue类型。ArrayBlockingQueue是一个阻塞队列，能够自定义队列大小，当插入时，如果队列已经没有空闲位置，那么新的插入线程将阻塞到该队列，一旦该队列有空闲位置，那么阻塞的线程将执行插入<br>以上，通过分析各个成员，也说明了 BlockGenerator 是如何存储单条数据的。</p><p>End.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要解析部分 Spark Streaming 的技术点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="http://moqimoqidea.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>数据仓库建设</title>
    <link href="http://moqimoqidea.github.io/2017/08/13/Data-Warehouse-Construction/"/>
    <id>http://moqimoqidea.github.io/2017/08/13/Data-Warehouse-Construction/</id>
    <published>2017-08-13T02:03:25.000Z</published>
    <updated>2018-11-25T14:07:51.504Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍数据仓库的起源和基本搭建。</p><a id="more"></a> <h1 id="什么是数据仓库"><a href="#什么是数据仓库" class="headerlink" title="什么是数据仓库"></a>什么是数据仓库</h1><p>数据仓库，英文名称为Data Warehouse，可简写为DW或DWH。数据仓库，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。它出于分析性报告和决策支持目的而创建。 为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制。<br>数据仓库能干什么？</p><ol><li>年度销售目标的指定，需要根据以往的历史报表进行决策，不能拍脑袋。</li><li>如何优化业务流程？<ol><li>案例1：某公司需要对APP进行推广，考核的主要目标是下载安装，有些第三方渠道会对这些数据造假，比如某个渠道在凌晨批量下载，点赞操作，操作步骤一致。通过数据分析，分析出应用的名称和安装时间，来判断一个渠道的是否优质、是否作假。</li><li>案例2：一个电商网站订单的完成包括：浏览、下单、支付、物流，其中物流环节可能和中通、申通、韵达等快递公司合作。快递公司每派送一个订单，都会有订单派送的确认时间，可以根据订单派送时间来分析哪个快递公司比较快捷高效，从而选择与哪些快递公司合作，剔除哪些快递公司，增加用户友好型。</li></ol></li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/01-Shopping-Website-Order.png?raw=true" alt="01-Shopping-Website-Order"></p><h1 id="数据仓库的特点"><a href="#数据仓库的特点" class="headerlink" title="数据仓库的特点"></a>数据仓库的特点</h1><h2 id="数据是面向主题的"><a href="#数据是面向主题的" class="headerlink" title="数据是面向主题的"></a>数据是面向主题的</h2><p>与传统数据库面向应用进行数据组织的特点相对应，数据仓库中的数据是面向主题进行组织的。什么是主题呢？首先，主题是一个抽象的概念，是较高层次上企业信息系统中的数据综合、归类并进行分析利用的抽象。在逻辑意义上，它是对应企业中某一宏观分析领域所涉及的分析对象。面向主题的数据组织方式，就是在较高层次上对分析对象的数据的一个完整、一致的描述，能完整、统一地刻划各个分析对象所涉及的企业的各项数据，以及数据之间的联系。所谓较高层次是相对面向应用的数据组织方式而言的，是指按照主题进行数据组织的方式具有更高的数据抽象级别。</p><h2 id="数据是集成的"><a href="#数据是集成的" class="headerlink" title="数据是集成的"></a>数据是集成的</h2><p>数据仓库的数据是从原有的分散的数据库数据抽取来的。操作型数据与DSS分析型数据之间差别甚大。第一，数据仓库的每一个主题所对应的源数据在原有的各分散数据库中有许多重复和不一致的地方，且来源于不同的联机系统的数据都和不同的应用逻辑捆绑在一起；第二，数据仓库中的综合数据不能从原有的数据库系统直接得到。因此在数据进入数据仓库之前，必然要经过统一与综合，这一步是数据仓库建设中最关键、最复杂的一步，所要完成的工作有：</p><ol><li>要统一源数据中所有矛盾之处，如字段的同名异义、异名同义、单位不统一、字长不一致，等等。</li><li><p>进行数据综合和计算。数据仓库中的数据综合工作可以在从原有数据库抽取 数据时生成，但许多是在数据仓库内部生成的，即进入数据仓库以后进行综合生成的。</p><h2 id="数据是不可更新的"><a href="#数据是不可更新的" class="headerlink" title="数据是不可更新的"></a>数据是不可更新的</h2><p>数据仓库的数据主要供企业决策分析之用，所涉及的数据操作主要是数据查询，一般情况下并不进行修改操作。数据仓库的数据反映的是一段相当长的时间内历史数据的内容，是不同时点的数据库快照的集合，以及基于这些快照进行统计、综合和重组的导出数据，而不是联机处理的数据。数据库中进行联机处理的数据经过集成输入到数据仓库中，一旦数据仓库存放的数据已经超过数据仓库的数据存储期限，这些数据将从当前的数据仓库中删去。因为数据仓库只进行数据查询操作，所以数据仓库管理系统相比数据库管理系统而言要简单得多。数据库管理系统中许多技术难点，如完整性保护、并发控制等等，在数据仓库的管理中几乎可以省去。但是由于数据仓库的查询数据量往往很大，所以就对数据查询提出了更高的要求，它要求采用各种复杂的索引技术；同时由于数据仓库面向的是商业企业的高层管理者，他们会对数据查询的界面友好性和数据表示提出更高的要求。</p><h2 id="数据是随时间不断变化的"><a href="#数据是随时间不断变化的" class="headerlink" title="数据是随时间不断变化的"></a>数据是随时间不断变化的</h2><p>数据仓库中的数据不可更新是针对应用来说的，也就是说，数据仓库的用户进行分析处理时是不进行数据更新操作的。但并不是说，在从数据集成输入数据仓库开始到最终被删除的整个数据生存周期中，所有的数据仓库数据都是永远不变的。<br>​    数据仓库的数据是随时间的变化而不断变化的，这是数据仓库数据的第四个特征。这一特征表现在以下3方面：</p></li><li><p>数据仓库随时间变化不断增加新的数据内容。数据仓库系统必须不断捕捉OLTP数据库中变化的数据，追加到数据仓库中去，也就是要不断地生成OLTP数据库的快照，经统一集成后增加到数据仓库中去；但对于确实不再变化的数据库快照，如果捕捉到新的变化数据，则只生成一个新的数据库快照增加进去，而不会对原有的数据库快照进行修改。</p></li><li>数据仓库随时间变化不断删去旧的数据内容。数据仓库的数据也有存储期限，一旦超过了这一期限，过期数据就要被删除。只是数据仓库内的数据时限要远远长于操作型环境中的数据时限。在操作型环境中一般只保存有60~90天的数据，而在数据仓库中则需要保存较长时限的数据（如5~10年），以适应DSS进行趋势分析的要求。</li><li>数据仓库中包含有大量的综合数据，这些综合数据中很多跟时间有关，如数据经常按照时间段进行综合，或隔一定的时间片进行抽样等等。这些数据要随着时间的变化不断地进行重新综合。因此，数据仓库的数据特征都包含时间项，以标明数据的历史时期。</li></ol><h1 id="数据仓库的发展历程"><a href="#数据仓库的发展历程" class="headerlink" title="数据仓库的发展历程"></a>数据仓库的发展历程</h1><p>数据仓库的发展大致经历了这样的三个过程：</p><h2 id="简单报表阶段"><a href="#简单报表阶段" class="headerlink" title="简单报表阶段"></a>简单报表阶段</h2><p>这个阶段，系统的主要目标是解决一些日常的工作中业务人员需要的报表，以及生成一些简单的能够帮助领导进行决策所需要的汇总数据。这个阶段的大部分表现形式为数据库和前端报表工具。</p><h2 id="数据集市阶段"><a href="#数据集市阶段" class="headerlink" title="数据集市阶段"></a>数据集市阶段</h2><p>这个阶段，主要是根据某个业务部门的需要，进行一定的数据的采集，整理，按照业务人员的需要，进行多维报表的展现，能够提供对特定业务指导的数据，并且能够提供特定的领导决策数据。</p><h2 id="数据仓库阶段"><a href="#数据仓库阶段" class="headerlink" title="数据仓库阶段"></a>数据仓库阶段</h2><p>这个阶段，主要是按照一定的数据模型，对整个企业的数据进行采集，整理，并且能够按照各个业务部门的需要，提供跨部门的，完全一致的业务报表数据，能够通过数据仓库生成对对业务具有指导性的数据，同时，为领导决策提供全面的数据支持。</p><p>通过数据仓库建设的发展阶段，我们能够看出，数据仓库的建设和数据集市的建设的重要区别就在于数据模型的支持。因此，数据模型的建设，对于我们数据仓库的建设，有着决定性的意义。</p><h1 id="数据仓库和数据库的区别"><a href="#数据仓库和数据库的区别" class="headerlink" title="数据仓库和数据库的区别"></a>数据仓库和数据库的区别</h1><p>了解数据库与数据仓库的区别之前，首先掌握三个概念。数据库软件、数据库、数据仓库。<br>数据库软件：是一种软件，可以看得见，可以操作。用来实现数据库逻辑功能。属于物理层。<br>数据库：是一种逻辑概念，用来存放数据的仓库。通过数据库软件来实现。数据库由很多表组成，表是二维的，一张表里可以有很多字段。字段一字排开，对应的数据就一行一行写入表中。数据库的表，在于能够用二维表现多维关系。目前市面上流行的数据库都是二维数据库。如：Oracle、DB2、MySQL、Sybase、MS SQL Server等。<br>数据仓库：是数据库概念的升级。从逻辑上理解，数据库和数据仓库没有区别，都是通过数据库软件实现的存放数据的地方，只不过从数据量来说，数据仓库要比数据库更庞大得多。数据仓库主要用于数据挖掘和数据分析，辅助领导做决策。<br>在IT的架构体系中，数据库是必须存在的。必须要有地方存放数据。比如现在的网购，淘宝，京东等等。物品的存货数量，货品的价格，用户的账户余额之类的。这些数据都是存放在后台数据库中。或者最简单理解，我们现在微博，QQ等账户的用户名和密码。在后台数据库必然有一张user表，字段起码有两个，即用户名和密码，然后我们的数据就一行一行的存在表上面。当我们登录的时候，我们填写了用户名和密码，这些数据就会被传回到后台去，去跟表上面的数据匹配，匹配成功了，你就能登录了。匹配不成功就会报错说密码错误或者没有此用户名等。这个就是数据库，数据库在生产环境就是用来干活的。凡是跟业务应用挂钩的，我们都使用数据库。<br>数据仓库则是BI下的其中一种技术。由于数据库是跟业务应用挂钩的，所以一个数据库不可能装下一家公司的所有数据。数据库的表设计往往是针对某一个应用进行设计的。比如刚才那个登录的功能，这张user表上就只有这两个字段，没有别的字段了。但是这张表符合应用，没有问题。但是这张表不符合分析。比如我想知道在哪个时间段，用户登录的量最多？哪个用户一年购物最多？诸如此类的指标。那就要重新设计数据库的表结构了。对于数据分析和数据挖掘，我们引入数据仓库概念。数据仓库的表结构是依照分析需求，分析维度，分析指标进行设计的。<br>数据库与数据仓库的区别实际讲的是OLTP与OLAP的区别。<br>操作型处理，叫联机事务处理OLTP（On-Line Transaction Processing），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。<br>分析型处理，叫联机分析处理OLAP（On-Line Analytical Processing）一般针对某些主题的历史数据进行分析，支持管理决策。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/02-OLTP-AND-OLAP.png?raw=true" alt="02-OLTP-AND-OLAP"></p><h1 id="数据仓库架构分层"><a href="#数据仓库架构分层" class="headerlink" title="数据仓库架构分层"></a>数据仓库架构分层</h1><p>数据仓库标准上可以分为四层：ODS（临时存储层）、PDW（数据仓库层）、DM（数据集市层）、APP（应用层）。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/03-DWBI.png?raw=true" alt="03-DWBI"></p><h2 id="ODS层"><a href="#ODS层" class="headerlink" title="ODS层"></a>ODS层</h2><p>为临时存储层，是接口数据的临时存储区域，为后一步的数据处理做准备。一般来说ODS层的数据和源系统的数据是同构的，主要目的是简化后续数据加工处理的工作。从数据粒度上来说ODS层的数据粒度是最细的。ODS层的表通常包括两类，一个用于存储当前需要加载的数据，一个用于存储处理完后的历史数据。历史数据一般保存3-6个月后需要清除，以节省空间。但不同的项目要区别对待，如果源系统的数据量不大，可以保留更长的时间，甚至全量保存；</p><h2 id="PDW层"><a href="#PDW层" class="headerlink" title="PDW层"></a>PDW层</h2><p>为数据仓库层，PDW层的数据应该是一致的、准确的、干净的数据，即对源系统数据进行了清洗（去除了杂质）后的数据。这一层的数据一般是遵循数据库第三范式的，其数据粒度通常和ODS的粒度相同。在PDW层会保存BI系统中所有的历史数据，例如保存10年的数据。</p><h2 id="DM层"><a href="#DM层" class="headerlink" title="DM层"></a>DM层</h2><p>为数据集市层，这层数据是面向主题来组织数据的，通常是星形或雪花结构的数据。从数据粒度来说，这层的数据是轻度汇总级的数据，已经不存在明细数据了。从数据的时间跨度来说，通常是PDW层的一部分，主要的目的是为了满足用户分析的需求，而从分析的角度来说，用户通常只需要分析近几年（如近三年的数据）的即可。从数据的广度来说，仍然覆盖了所有业务数据。</p><h2 id="APP层"><a href="#APP层" class="headerlink" title="APP层"></a>APP层</h2><p>为应用层，这层数据是完全为了满足具体的分析需求而构建的数据，也是星形或雪花结构的数据。从数据粒度来说是高度汇总的数据。从数据的广度来说，则并不一定会覆盖所有业务数据，而是DM层数据的一个真子集，从某种意义上来说是DM层数据的一个重复。从极端情况来说，可以为每一张报表在APP层构建一个模型来支持，达到以空间换时间的目的数据仓库的标准分层只是一个建议性质的标准，实际实施时需要根据实际情况确定数据仓库的分层，不同类型的数据也可能采取不同的分层方法。</p><h2 id="为什么要对数据仓库分层"><a href="#为什么要对数据仓库分层" class="headerlink" title="为什么要对数据仓库分层"></a>为什么要对数据仓库分层</h2><ol><li>用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据；</li><li>如果不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。</li><li>通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。</li></ol><h1 id="数据质量检查"><a href="#数据质量检查" class="headerlink" title="数据质量检查"></a>数据质量检查</h1><p>保证报表数据的正确性、稳定性，通过告警机制尽可能快的发现异常、尽可能快的解决问题。</p><p>检查方法：</p><ol><li>数据行数据的比较。</li><li>行数有变化，但是指标有变化。对重点指标进行筛选。</li><li>发现问题，及时通知相关模块负责人</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/04-Rule.png?raw=true" alt="04-Rule"></p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/05-Exception.png?raw=true" alt="05-Exception"></p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/06-Mail.png?raw=true" alt="06-Mail"></p><h1 id="元数据介绍"><a href="#元数据介绍" class="headerlink" title="元数据介绍"></a>元数据介绍</h1><p>当需要了解某地企业及其提供的服务时，电话黄页的重要性就体现出来了。元数据（Metadata）类似于这样的电话黄页。</p><h2 id="元数据的定义"><a href="#元数据的定义" class="headerlink" title="元数据的定义"></a>元数据的定义</h2><p>   数据仓库的元数据是关于数据仓库中数据的数据。它的作用类似于数据库管理系统的数据字典，保存了逻辑数据结构、文件、地址和索引等信息。广义上讲，在数据仓库中，元数据描述了数据仓库内数据的结构和建立方法的数据。</p><p>   <img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/07-Metadata.png?raw=true" alt="07-Metadata"></p><p>   元数据是数据仓库管理系统的重要组成部分，元数据管理器是企业级数据仓库中的关键组件，贯穿数据仓库构建的整个过程，直接影响着数据仓库的构建、使用和维护。</p><ol><li>构建数据仓库的主要步骤之一是ETL。这时元数据将发挥重要的作用，它定义了源数据系统到数据仓库的映射、数据转换的规则、数据仓库的逻辑结构、数据更新的规则、数据导入历史记录以及装载周期等相关内容。数据抽取和转换的专家以及数据仓库管理员正是通过元数据高效地构建数据仓库。</li><li>用户在使用数据仓库时，通过元数据访问数据，明确数据项的含义以及定制报表。</li><li><p>数据仓库的规模及其复杂性离不开正确的元数据管理，包括增加或移除外部数据源，改变数据清洗方法，控制出错的查询以及安排备份等。</p><p>元数据可分为技术元数据和业务元数据。技术元数据为开发和管理数据仓库的IT 人员使用，它描述了与数据仓库开发、管理和维护相关的数据，包括数据源信息、数据转换描述、数据仓库模型、数据清洗与更新规则、数据映射和访问权限等。而业务元数据为管理层和业务分析人员服务，从业务角度描述数据，包括商务术语、数据仓库中有什么数据、数据的位置和数据的可用性等，帮助业务人员更好地理解数据仓库中哪些数据是可用的以及如何使用。<br>由上可见，元数据不仅定义了数据仓库中数据的模式、来源、抽取和转换规则等，而且是整个数据仓库系统运行的基础，元数据把数据仓库系统中各个松散的组件联系起来，组成了一个有机的整体，如图所示：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/08-Metadata-And-Another.png?raw=true" alt="08-Metadata-And-Another"></p></li></ol><h2 id="元数据的存储方式"><a href="#元数据的存储方式" class="headerlink" title="元数据的存储方式"></a>元数据的存储方式</h2><p>   元数据有两种常见存储方式：一种是以数据集为基础，每一个数据集有对应的元数据文件，每一个元数据文件包含对应数据集的元数据内容；另一种存储方式是以数据库为基础，即元数据库。其中元数据文件由若干项组成，每一项表示元数据的一个要素，每条记录为数据集的元数据内容。上述存储方式各有优缺点，第一种存储方式的优点是调用数据时相应的元数据也作为一个独立的文件被传输，相对数据库有较强的独立性，在对元数据进行检索时可以利用数据库的功能实现，也可以把元数据文件调到其他数据库系统中操作；不足是如果每一数据集都对应一个元数据文档，在规模巨大的数据库中则会有大量的元数据文件，管理不方便。第二种存储方式下，元数据库中只有一个元数据文件，管理比较方便，添加或删除数据集，只要在该文件中添加或删除相应的记录项即可。在获取某数据集的元数据时，因为实际得到的只是关系表格数据的一条记录，所以要求用户系统可以接受这种特定形式的数据。因此推荐使用元数据库的方式。<br>   元数据库用于存储元数据，因此元数据库最好选用主流的关系数据库管理系统。元数据库还包含用于操作和查询元数据的机制。建立元数据库的主要好处是提供统一的数据结构和业务规则，易于把企业内部的多个数据集市有机地集成起来。目前，一些企业倾向建立多个数据集市，而不是一个集中的数据仓库，这时可以考虑在建立数据仓库（或数据集市）之前，先建立一个用于描述数据、服务应用集成的元数据库，做好数据仓库实施的初期支持工作，对后续开发和维护有很大的帮助。元数据库保证了数据仓库数据的一致性和准确性，为企业进行数据质量管理提供基础。</p><h2 id="元数据的作用"><a href="#元数据的作用" class="headerlink" title="元数据的作用"></a>元数据的作用</h2><ol><li>描述哪些数据在数据仓库中，帮助决策分析者对数据仓库的内容定位。</li><li>定义数据进入数据仓库的方式，作为数据汇总、映射和清洗的指南。</li><li>记录业务事件发生而随之进行的数据抽取工作时间安排。</li><li>记录并检测系统数据一致性的要求和执行情况。</li><li>评估数据质量。</li></ol><h1 id="什么是数据模型"><a href="#什么是数据模型" class="headerlink" title="什么是数据模型"></a>什么是数据模型</h1><p>数据模型是抽象描述现实世界的一种工具和方法，是通过抽象的实体及实体之间联系的形式，来表示现实世界中事务的相互关系的一种映射。在这里，数据模型表现的抽象的是实体和实体之间的关系，通过对实体和实体之间关系的定义和描述，来表达实际的业务中具体的业务关系。<br>数据仓库模型是数据模型中针对特定的数据仓库应用系统的一种特定的数据模型，一般的来说，我们数据仓库模型分为几下几个层次，如图所示：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/09-Data-Warehouse-Hierarchy.png?raw=true" alt="09-Data-Warehouse-Hierarchy"></p><p>通过上面的图形，我们能够很容易的看出在整个数据仓库得建模过程中，我们需要经历一般四个过程：</p><ul><li>业务建模，生成业务模型，主要解决业务层面的分解和程序化。</li><li>领域建模，生成领域模型，主要是对业务模型进行抽象处理，生成领域概念模型。</li><li>逻辑建模，生成逻辑模型，主要是将领域模型的概念实体以及实体之间的关系进行数据库层次的逻辑化。</li><li>物理建模，生成物理模型，主要解决，逻辑模型针对不同关系型数据库的物理化以及性能等一些具体的技术问题。</li></ul><p>因此，在整个数据仓库的模型的设计和架构中，既涉及到业务知识，也涉及到了具体的技术，我们既需要了解丰富的行业经验，同时，也需要一定的信息技术来帮助我们实现我们的数据模型，最重要的是，我们还需要一个非常适用的方法论，来指导我们自己针对我们的业务进行抽象，处理，生成各个阶段的模型。</p><h1 id="为什么需要数据模型"><a href="#为什么需要数据模型" class="headerlink" title="为什么需要数据模型"></a>为什么需要数据模型</h1><p>通过数据仓库建设的发展阶段，我们能够看出，数据仓库的建设和数据集市的建设的重要区别就在于数据模型的支持。因此，数据模型的建设，对于我们数据仓库的建设，有着决定性的意义。</p><p>一般来说，数据模型的建设主要能够帮助我们解决以下的一些问题：</p><ol><li>进行全面的业务梳理，改进业务流程。在业务模型建设的阶段，能够帮助我们的企业或者是管理机关对本单位的业务进行全面的梳理。通过业务模型的建设，我们应该能够全面了解该单位的业务架构图和整个业务的运行情况，能够将业务按照特定的规律进行分门别类和程序化，同时，帮助我们进一步的改进业务的流程，提高业务效率，指导我们的业务部门的生产。</li><li>建立全方位的数据视角，消灭信息孤岛和数据差异。通过数据仓库的模型建设，能够为企业提供一个整体的数据视角，不再是各个部门只是关注自己的数据，而且通过模型的建设，勾勒出了部门之间内在的联系，帮助消灭各个部门之间的信息孤岛的问题，更为重要的是，通过数据模型的建设，能够保证整个企业的数据的一致性，各个部门之间数据的差异将会得到有效解决。</li><li>解决业务的变动和数据仓库的灵活性。通过数据模型的建设，能够很好的分离出底层技术的实现和上层业务的展现。当上层业务发生变化时，通过数据模型，底层的技术实现可以非常轻松的完成业务的变动，从而达到整个数据仓库系统的灵活性。</li><li>帮助数据仓库系统本身的建设。通过数据仓库的模型建设，开发人员和业务人员能够很容易的达成系统建设范围的界定，以及长期目标的规划，从而能够使整个项目组明确当前的任务，加快整个系统建设的速度。</li></ol><h1 id="如何建设数据仓库模型"><a href="#如何建设数据仓库模型" class="headerlink" title="如何建设数据仓库模型"></a>如何建设数据仓库模型</h1><p>建设数据模型既然是整个数据仓库建设中一个非常重要的关键部分，那么，怎么建设我们的数据仓库模型就是我们需要解决的一个问题。这里我们将要详细介绍如何创建适合自己的数据模型。</p><h2 id="数据仓库数据模型架构"><a href="#数据仓库数据模型架构" class="headerlink" title="数据仓库数据模型架构"></a>数据仓库数据模型架构</h2><p>数据仓库的数据模型的架构和数据仓库的整体架构是紧密关联在一起的，我们首先来了解一下整个数据仓库的数据模型应该包含的几个部分。从下图我们可以很清楚地看到，整个数据模型的架构分成 5 大部分，每个部分其实都有其独特的功能。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/10-Data-Warehouse-Model-Architecture.png?raw=true" alt="10-Data-Warehouse-Model-Architecture"></p><p>从上图我们可以看出，整个数据仓库的数据模型可以分为大概 5 大部分：</p><ol><li>系统记录域（System of Record）：这部分是主要的数据仓库业务数据存储区，数据模型在这里保证了数据的一致性。</li><li>内部管理域（Housekeeping）：这部分主要存储数据仓库用于内部管理的元数据，数据模型在这里能够帮助进行统一的元数据的管理。</li><li>汇总域（Summary of Area）：这部分数据来自于系统记录域的汇总，数据模型在这里保证了分析域的主题分析的性能，满足了部分的报表查询。</li><li>分析域（Analysis Area）：这部分数据模型主要用于各个业务部分的具体的主题业务分析。这部分数据模型可以单独存储在相应的数据集市中。</li><li>反馈域（Feedback Area）：可选项，这部分数据模型主要用于相应前端的反馈数据，数据仓库可以视业务的需要设置这一区域。</li></ol><p>通过对整个数据仓库模型的数据区域的划分，我们可以了解到，一个好的数据模型，不仅仅是对业务进行抽象划分，而且对实现技术也进行具体的指导，它应该涵盖了从业务到实现技术的各个部分。</p><h2 id="数据仓库建模阶段划分"><a href="#数据仓库建模阶段划分" class="headerlink" title="数据仓库建模阶段划分"></a>数据仓库建模阶段划分</h2><p>我们前面介绍了数据仓库模型的几个层次，下面我们讲一下，针对这几个层次的不同阶段的数据建模的工作的主要内容：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/11-Data-Warehouse-Model-Hierarchy.png?raw=true" alt="11-Data-Warehouse-Model-Hierarchy"></p><h3 id="业务建模"><a href="#业务建模" class="headerlink" title="业务建模"></a>业务建模</h3><ol><li>划分整个单位的业务，一般按照业务部门的划分，进行各个部分之间业务工作的界定，理清各业务部门之间的关系。</li><li>深入了解各个业务部门的内具体业务流程并将其程序化。</li><li>提出修改和改进业务部门工作流程的方法并程序化。</li><li>数据建模的范围界定，整个数据仓库项目的目标和阶段划分。</li></ol><h3 id="领域概念建模"><a href="#领域概念建模" class="headerlink" title="领域概念建模"></a>领域概念建模</h3><ol><li>抽取关键业务概念，并将之抽象化。</li><li>将业务概念分组，按照业务主线聚合类似的分组概念。</li><li>细化分组概念，理清分组概念内的业务流程并抽象化。</li><li>理清分组概念之间的关联，形成完整的领域概念模型。</li></ol><h3 id="逻辑建模"><a href="#逻辑建模" class="headerlink" title="逻辑建模"></a>逻辑建模</h3><ol><li>业务概念实体化，并考虑其具体的属性。</li><li>事件实体化，并考虑其属性内容。</li><li>说明实体化，并考虑其属性内容。</li></ol><h3 id="物理建模"><a href="#物理建模" class="headerlink" title="物理建模"></a>物理建模</h3><ol><li>针对特定物理化平台，做出相应的技术调整。</li><li>针对模型的性能考虑，对特定平台作出相应的调整。</li><li>针对管理的需要，结合特定的平台，做出相应的调整。</li><li>生成最后的执行脚本，并完善之。</li></ol><p>从我们上面对数据仓库的数据建模阶段的各个阶段的划分，我们能够了解到整个数据仓库建模的主要工作和工作量，希望能够对我们在实际的项目建设能够有所帮助。</p><h2 id="数据仓库建模方法"><a href="#数据仓库建模方法" class="headerlink" title="数据仓库建模方法"></a>数据仓库建模方法</h2><p>大千世界，表面看五彩缤纷，实质上，万物都遵循其自有的法则。数据仓库的建模方法同样也有很多种，每一种建模方法其实代表了哲学上的一个观点，代表了一种归纳，概括世界的一种方法。目前业界较为流行的数据仓库的建模方法非常多，这里主要介绍范式建模法，维度建模法，实体建模法等几种方法，每种方法其实从本质上讲就是从不同的角度看我们业务中的问题，不管从技术层面还是业务层面，其实代表的是哲学上的一种世界观。我们下面给大家详细介绍一下这些建模方法。</p><h3 id="范式建模法"><a href="#范式建模法" class="headerlink" title="范式建模法"></a>范式建模法</h3><p>范式建模法（Third Normal Form，3NF）：范式建模法其实是我们在构建数据模型常用的一个方法，该方法的主要由 Inmon 所提倡，主要解决关系型数据库得数据存储，利用的一种技术层面上的方法。目前，我们在关系型数据库中的建模方法，大部分采用的是三范式建模法。<br>范式是数据库逻辑模型设计的基本理论，一个关系模型可以从第一范式到第五范式进行无损分解，这个过程也可称为规范化。在数据仓库的模型设计中目前一般采用第三范式，它有着严格的数学定义。从其表达的含义来看，一个符合第三范式的关系必须具有以下三个条件 :</p><ol><li>每个属性值唯一，不具有多义性 ;</li><li>每个非主属性必须完全依赖于整个主键，而非主键的一部分 ;</li><li>每个非主属性不能依赖于其他关系中的属性，因为这样的话，这种属性应该归到其他关系中去。</li></ol><p>由于范式是基于整个关系型数据库的理论基础之上发展而来的，因此，本人在这里不多做介绍，有兴趣的读者可以通过阅读相应的材料来获得这方面的知识。<br>根据 Inmon 的观点，数据仓库模型得建设方法和业务系统的企业数据模型类似。在业务系统中，企业数据模型决定了数据的来源，而企业数据模型也分为两个层次，即主题域模型和逻辑模型。同样，主题域模型可以看成是业务模型的概念模型，而逻辑模型则是域模型在关系型数据库上的实例。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/12-Business-And-DW.png?raw=true" alt="12-Business-And-DW"></p><p>从业务数据模型转向数据仓库模型时，同样也需要有数据仓库的域模型，即概念模型，同时也存在域模型的逻辑模型。这里，业务模型中的数据模型和数据仓库的模型稍微有一些不同。主要区别在于：</p><ul><li>数据仓库的域模型应该包含企业数据模型的域模型之间的关系，以及各主题域定义。数据仓库的域模型的概念应该比业务系统的主题域模型范围更加广。</li><li>在数据仓库的逻辑模型需要从业务系统的数据模型中的逻辑模型中抽象实体，实体的属性，实体的子类，以及实体的关系等。</li></ul><p>Inmon 的范式建模法的最大优点就是从关系型数据库的角度出发，结合了业务系统的数据模型，能够比较方便的实现数据仓库的建模。但其缺点也是明显的，由于建模方法限定在关系型数据库之上，在某些时候反而限制了整个数据仓库模型的灵活性，性能等，特别是考虑到数据仓库的底层数据向数据集市的数据进行汇总时，需要进行一定的变通才能满足相应的需求。因此，笔者建议读者们在实际的使用中，参考使用这一建模方式。</p><h3 id="维度建模法"><a href="#维度建模法" class="headerlink" title="维度建模法"></a>维度建模法</h3><p>维度建模法，Kimball 最先提出这一概念。其最简单的描述就是，按照事实表，维表来构建数据仓库，数据集市。这种方法的最被人广泛知晓的名字就是星型模式（Star-schema）。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/13-Dimension-Of-DW.png?raw=true" alt="13-Dimension-Of-DW"></p><p>上图的这个架构中是典型的星型架构。星型模式之所以广泛被使用，在于针对各个维作了大量的预处理，如按照维进行预先的统计、分类、排序等。通过这些预处理，能够极大的提升数据仓库的处理能力。特别是针对 3NF 的建模方法，星型模式在性能上占据明显的优势。<br>同时，维度建模法的另外一个优点是，维度建模非常直观，紧紧围绕着业务模型，可以直观的反映出业务模型中的业务问题。不需要经过特别的抽象处理，即可以完成维度建模。这一点也是维度建模的优势。<br>但是，维度建模法的缺点也是非常明显的，由于在构建星型模式之前需要进行大量的数据预处理，因此会导致大量的数据处理工作。而且，当业务发生变化，需要重新进行维度的定义时，往往需要重新进行维度数据的预处理。而在这些与处理过程中，往往会导致大量的数据冗余。<br>另外一个维度建模法的缺点就是，如果只是依靠单纯的维度建模，不能保证数据来源的一致性和准确性，而且在数据仓库的底层，不是特别适用于维度建模的方法。<br>因此以笔者的观点看，维度建模的领域主要适用与数据集市层，它的最大的作用其实是为了解决数据仓库建模中的性能问题。维度建模很难能够提供一个完整地描述真实业务实体之间的复杂关系的抽象方法。</p><h3 id="实体建模法"><a href="#实体建模法" class="headerlink" title="实体建模法"></a>实体建模法</h3><p>实体建模法并不是数据仓库建模中常见的一个方法，它来源于哲学的一个流派。从哲学的意义上说，客观世界应该是可以细分的，客观世界应该可以分成由一个个实体，以及实体与实体之间的关系组成。那么我们在数据仓库的建模过程中完全可以引入这个抽象的方法，将整个业务也可以划分成一个个的实体，而每个实体之间的关系，以及针对这些关系的说明就是我们数据建模需要做的工作。<br>虽然实体法粗看起来好像有一些抽象，其实理解起来很容易。即我们可以将任何一个业务过程划分成 3 个部分，实体，事件和说明，如下图所示：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/14-Entity-Of-Model.png?raw=true" alt="14-Entity-Of-Model"></p><p>上图表述的是一个抽象的含义，如果我们描述一个简单的事实：“小明开车去学校上学”。以这个业务事实为例，我们可以把“小明”，“学校”看成是一个实体，“上学”描述的是一个业务过程，我们在这里可以抽象为一个具体“事件”，而“开车去”则可以看成是事件“上学”的一个说明。<br>从上面的举例我们可以了解，我们使用的抽象归纳方法其实很简单，任何业务可以看成 3 个部分：</p><ul><li>实体，主要指领域模型中特定的概念主体，指发生业务关系的对象。</li><li>事件，主要指概念主体之间完成一次业务流程的过程，特指特定的业务过程。</li><li>说明，主要是针对实体和事件的特殊说明。</li></ul><p>由于实体建模法，能够很轻松的实现业务模型的划分，因此，在业务建模阶段和领域概念建模阶段，实体建模法有着广泛的应用。从笔者的经验来看，再没有现成的行业模型的情况下，我们可以采用实体建模的方法，和客户一起理清整个业务的模型，进行领域概念模型的划分，抽象出具体的业务概念，结合客户的使用特点，完全可以创建出一个符合自己需要的数据仓库模型来。<br>但是，实体建模法也有着自己先天的缺陷，由于实体说明法只是一种抽象客观世界的方法，因此，注定了该建模方法只能局限在业务建模和领域概念建模阶段。因此，到了逻辑建模阶段和物理建模阶段，则是范式建模和维度建模发挥长处的阶段。<br>因此，笔者建议读者在创建自己的数据仓库模型的时候，可以参考使用上述的三种数据仓库得建模方法，在各个不同阶段采用不同的方法，从而能够保证整个数据仓库建模的质量。</p><h1 id="维度建模"><a href="#维度建模" class="headerlink" title="维度建模"></a>维度建模</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在多维分析的商业智能解决方案中，根据事实表和维度表的关系，又可将常见的模型分为星型模型和雪花型模型。在设计逻辑型数据的模型的时候，就应考虑数据是按照星型模型还是雪花型模型进行组织。<br>当所有维表都直接连接到“ 事实表”上时，整个图解就像星星一样，故将该模型称为星型模型，如图 1 。<br>星型架构是一种非正规化的结构，多维数据集的每一个维度都直接与事实表相连接，不存在渐变维度，所以数据有一定的冗余，如在地域维度表中，存在国家 A 省 B 的城市 C 以及国家 A 省 B 的城市 D 两条记录，那么国家 A 和省 B 的信息分别存储了两次，即存在冗余。</p><p>销售数据仓库中的星型模型：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/15-Star-Model.png?raw=true" alt="15-Star-Model"></p><p>当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的 “ 层次 “ 区域，这些被分解的表都连接到主维度表而不是事实表。如图 2，将地域维表又分解为国家，省份，城市等维表。它的优点是 : 通过最大限度地减少数据存储量以及联合较小的维表来改善查询性能。雪花型结构去除了数据冗余。<br>销售数据仓库中的雪花型模型：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/16-Snowflake-Model.png?raw=true" alt="16-Snowflake-Model"></p><p>星型模型因为数据的冗余所以很多统计查询不需要做外部的连接，因此一般情况下效率比雪花型模型要高。星型结构不用考虑很多正规化的因素，设计与实现都比较简单。雪花型模型由于去除了冗余，有些统计就需要通过表的联接才能产生，所以效率不一定有星型模型高。正规化也是一种比较复杂的过程，相应的数据库结构设计、数据的 ETL、以及后期的维护都要复杂一些。因此在冗余可以接受的前提下，实际运用中星型模型使用更多，也更有效率。</p><h2 id="使用选择"><a href="#使用选择" class="headerlink" title="使用选择"></a>使用选择</h2><p>星形模型(Star Schema)和雪花模型(Snowflake Schema)是数据仓库中常用到的两种方式，而它们之间的对比要从四个角度来进行讨论。</p><h3 id="数据优化"><a href="#数据优化" class="headerlink" title="数据优化"></a>数据优化</h3><p>雪花模型使用的是规范化数据，也就是说数据在数据库内部是组织好的，以便消除冗余，因此它能够有效地减少数据量。通过引用完整性，其业务层级和维度都将存储在数据模型之中。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/17-Snowflake-Model.png?raw=true" alt="17-Snowflake-Model"></p><p>相比较而言，星形模型实用的是反规范化数据。在星形模型中，维度直接指的是事实表，业务层级不会通过维度之间的参照完整性来部署。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/18-Star-Model.png?raw=true" alt="18-Star-Model"></p><h3 id="业务模型"><a href="#业务模型" class="headerlink" title="业务模型"></a>业务模型</h3><p>主键是一个单独的唯一键(数据属性)，为特殊数据所选择。在上面的例子中，Advertiser_ID就将是一个主键。外键(参考属性)仅仅是一个表中的字段，用来匹配其他维度表中的主键。在我们所引用的例子中，Advertiser_ID将是Account_dimension的一个外键。<br>在雪花模型中，数据模型的业务层级是由一个不同维度表主键-外键的关系来代表的。而在星形模型中，所有必要的维度表在事实表中都只拥有外键。</p><h3 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h3><p>第三个区别在于性能的不同。雪花模型在维度表、事实表之间的连接很多，因此性能方面会比较低。举个例子，如果你想要知道Advertiser 的详细信息，雪花模型就会请求许多信息，比如Advertiser Name、ID以及那些广告主和客户表的地址需要连接起来，然后再与事实表连接。<br>而星形模型的连接就少的多，在这个模型中，如果你需要上述信息，你只要将Advertiser的维度表和事实表连接即可。</p><h3 id="ETL"><a href="#ETL" class="headerlink" title="ETL"></a>ETL</h3><p>雪花模型加载数据集市，因此ETL操作在设计上更加复杂，而且由于附属模型的限制，不能并行化。<br>星形模型加载维度表，不需要再维度之间添加附属模型，因此ETL就相对简单，而且可以实现高度的并行化。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>雪花模型使得维度分析更加容易，比如“针对特定的广告主，有哪些客户或者公司是在线的?”星形模型用来做指标分析更适合，比如“给定的一个客户他们的收入是多少?”</p><h2 id="缓慢变化维"><a href="#缓慢变化维" class="headerlink" title="缓慢变化维"></a>缓慢变化维</h2><p>维度建模的数据仓库中，有一个概念叫 Slowly Changing Dimensions, 中文一般翻译成”缓慢变化维“，经常被简写为 SCD. 缓慢变化维的提出是因为在现实世界中，维度的属性并不是静态的，它会随着时间的流失发生缓慢的变化。这种随着时间发生变化的维度我们一般称之为缓慢变化维，并且把处理温度表的历史变化信息的问题称为处理缓慢变化维问题，有时也简称为处理 SCD 的问题。</p><p>如何解决缓慢变化维带来的影响？</p><h3 id="第一种方法"><a href="#第一种方法" class="headerlink" title="第一种方法"></a>第一种方法</h3><p>直接在原来维度的基础上进行更新，不会产生新的记录。</p><p>更新前：</p><table><thead><tr><th style="text-align:center">emp_rid</th><th style="text-align:center">emp_id</th><th style="text-align:center">emp_name</th><th style="text-align:center">position</th></tr></thead><tbody><tr><td style="text-align:center">101212</td><td style="text-align:center">12345</td><td style="text-align:center">Jack</td><td style="text-align:center">Developer</td></tr></tbody></table><p>更新后：</p><table><thead><tr><th style="text-align:center">emp_rid</th><th style="text-align:center">emp_id</th><th style="text-align:center">emp_name</th><th style="text-align:center">position</th></tr></thead><tbody><tr><td style="text-align:center">101212</td><td style="text-align:center">12345</td><td style="text-align:center">Jack</td><td style="text-align:center">Manager</td></tr></tbody></table><p>position 发生变化。</p><h3 id="第二种方法"><a href="#第二种方法" class="headerlink" title="第二种方法"></a>第二种方法</h3><p>不修改原有的数据，重新产生一条新的记录，这样就可以追溯所有的历史记录。</p><p>更新前：</p><table><thead><tr><th style="text-align:center">emp_rid</th><th style="text-align:center">emp_id</th><th style="text-align:center">emp_name</th><th style="text-align:center">position</th><th style="text-align:center">start_date</th><th style="text-align:center">end_date</th></tr></thead><tbody><tr><td style="text-align:center">101212</td><td style="text-align:center">12345</td><td style="text-align:center">Jack</td><td style="text-align:center">Developer</td><td style="text-align:center">2010-02-05</td><td style="text-align:center">2012-06-12</td></tr></tbody></table><p>更新后：</p><table><thead><tr><th style="text-align:center">emp_rid</th><th style="text-align:center">emp_id</th><th style="text-align:center">emp_name</th><th style="text-align:center">position</th><th style="text-align:center">start_date</th><th style="text-align:center">end_date</th></tr></thead><tbody><tr><td style="text-align:center">101212</td><td style="text-align:center">12345</td><td style="text-align:center">Jack</td><td style="text-align:center">Manager</td><td style="text-align:center">2012-06-12</td></tr></tbody></table><p>新增了一条记录。</p><h3 id="第三种方法"><a href="#第三种方法" class="headerlink" title="第三种方法"></a>第三种方法</h3><p>直接在原来维度的基础上进行更新，不会产生新的记录但是会记录上一次的。</p><p>更新前：</p><table><thead><tr><th style="text-align:center">emp_rid</th><th style="text-align:center">emp_id</th><th style="text-align:center">emp_name</th><th style="text-align:center">position</th><th style="text-align:center">old_position</th></tr></thead><tbody><tr><td style="text-align:center">101212</td><td style="text-align:center">12345</td><td style="text-align:center">Jack</td><td style="text-align:center">Developer</td><td style="text-align:center">null</td></tr></tbody></table><p>更新后：</p><table><thead><tr><th style="text-align:center">emp_rid</th><th style="text-align:center">emp_id</th><th style="text-align:center">emp_name</th><th style="text-align:center">position</th><th style="text-align:center">old_position</th></tr></thead><tbody><tr><td style="text-align:center">101212</td><td style="text-align:center">12345</td><td style="text-align:center">Jack</td><td style="text-align:center">Manager</td><td style="text-align:center">Developer</td></tr></tbody></table><p>多一个字段，用来存放以前的 position.</p><p>End.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍数据仓库的起源和基本搭建。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Data-Warehouse" scheme="http://moqimoqidea.github.io/tags/Data-Warehouse/"/>
    
  </entry>
  
  <entry>
    <title>Spark SQL 应用解析</title>
    <link href="http://moqimoqidea.github.io/2017/08/10/Spark-SQL-Analysis/"/>
    <id>http://moqimoqidea.github.io/2017/08/10/Spark-SQL-Analysis/</id>
    <published>2017-08-10T12:37:27.000Z</published>
    <updated>2018-11-25T07:38:41.517Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要解析部分 Spark SQL 的技术点。</p><a id="more"></a> <h1 id="Spark-SQL-概述"><a href="#Spark-SQL-概述" class="headerlink" title="Spark SQL 概述"></a>Spark SQL 概述</h1><h2 id="什么是-Spark-SQL"><a href="#什么是-Spark-SQL" class="headerlink" title="什么是 Spark SQL"></a>什么是 Spark SQL</h2><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/01-Spark-SQL-LOGO.png?raw=true" alt="01-Spark-SQL-LOGO"></p><p>Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。<br>我们已经学习了Hive，它是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！</p><p>Spark SQL 有四大特点：</p><ol><li>易整合。</li><li>统一的数据访问方式。</li><li>兼容 Hive.</li><li>标准的数据连接。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/02-Spark-SQL-Architecture.png?raw=true" alt="02-Spark-SQL-Architecture"></p><p>SparkSQL可以看做是一个转换层，向下对接各种不同的结构化数据源，向上提供不同的数据访问方式。</p><h2 id="RDD-vs-DataFrames-vs-DataSet"><a href="#RDD-vs-DataFrames-vs-DataSet" class="headerlink" title="RDD vs DataFrames vs DataSet"></a>RDD vs DataFrames vs DataSet</h2><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/03-RDD-DataFrames-DataSet.png?raw=true" alt="03-RDD-DataFrames-DataSet"></p><p>在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。他们和RDD有什么区别呢？首先从版本的产生上来看：<br>RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt; Dataset(Spark1.6)<br>如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。<br>在后期的Spark版本中，DataSet会逐步取代RDD和DataFrame成为唯一的API接口。</p><h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><ul><li>RDD是一个懒执行的不可变的可以支持Lambda表达式的并行数据集合。</li><li>RDD的最大好处就是简单，API的人性化程度很高。</li><li>RDD的劣势是性能限制，它是一个JVM驻内存对象，这也就决定了存在GC的限制和数据增加时Java序列化成本的升高。</li></ul><h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。由于与R和Pandas的DataFrame类似，Spark DataFrame很好地继承了传统单机数据分析的开发体验。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/04-RDD-Vs-DataFrame.png?raw=true" alt="04-RDD-Vs-DataFrame"></p><p>上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。<br>DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待。<br>DataFrame也是懒执行的。<br>性能上比RDD要高，主要有两方面原因： </p><ol><li>定制化内存管理：数据以二进制的方式存在于非堆内存，节省了大量空间之外，还摆脱了GC的限制。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/05-OnHeap-OffHeap-GC.png?raw=true" alt="05-OnHeap-OffHeap-GC"></p><ol start="2"><li>优化的执行计划：查询计划通过Spark catalyst optimiser进行优化。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/06-Spark-Catalyst.png?raw=true" alt="06-Spark-Catalyst"></p><p>比如下面这个例子：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">users.join(events, users(<span class="string">"id"</span>) === events(<span class="string">"uid"</span>))</span><br><span class="line"> .filter(events(<span class="string">"date"</span>) &gt; <span class="string">"2015-01-01"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/07-Spark-SQL-Optimization.png?raw=true" alt="07-Spark-SQL-Optimization"></p><p>为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。<br>得到的优化执行计划在转换成物 理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。<br>对于普通开发者而言，查询优化器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。<br>Dataframe的劣势在于在编译期缺少类型安全检查，导致运行时出错。</p><h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><ul><li>是Dataframe API的一个扩展，是Spark最新的数据抽象。</li><li>用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。</li><li>Dataset支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。</li><li>样例类被用来在Dataset中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称。</li><li>Dataframe是Dataset的特列，DataFrame=Dataset[Row] ，所以可以通过as方法将Dataframe转换为Dataset。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息我都用Row来表示。</li><li>DataSet是强类型的。比如可以有Dataset[Car]，Dataset[Person].</li></ul><p>DataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比。</p><p>RDD让我们能够决定怎么做，而DataFrame和DataSet让我们决定做什么，控制的粒度不一样。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/08-Comparisons-Them.png?raw=true" alt="08-Comparisons-Them"></p><h3 id="三者的共性"><a href="#三者的共性" class="headerlink" title="三者的共性"></a>三者的共性</h3><ol><li>RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利。</li><li>三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过。</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkconf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"test"</span>).set(<span class="string">"spark.port.maxRetries"</span>,<span class="string">"1000"</span>)</span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkconf).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> rdd=spark.sparkContext.parallelize(<span class="type">Seq</span>((<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">1</span>), (<span class="string">"a"</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="comment">// map不运行</span></span><br><span class="line">rdd.map&#123;line=&gt;</span><br><span class="line">  println(<span class="string">"运行"</span>)</span><br><span class="line">  line._1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="3"><li>三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出。</li><li>三者都有partition的概念。</li><li>三者有许多共同的函数，如filter，排序等。</li><li>在对DataFrame和Dataset进行操作许多操作都需要这个包进行支持。</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><ol start="7"><li>DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型。<br>DataFrame:</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">testDF.map&#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(col1:<span class="type">String</span>,col2:<span class="type">Int</span>)=&gt;</span><br><span class="line">        println(col1);println(col2)</span><br><span class="line">        col1</span><br><span class="line">      <span class="keyword">case</span> _=&gt;</span><br><span class="line">        <span class="string">""</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>   Dataset:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class">    <span class="title">testDS</span>.<span class="title">map</span></span>&#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Coltest</span>(col1:<span class="type">String</span>,col2:<span class="type">Int</span>)=&gt;</span><br><span class="line">        println(col1);println(col2)</span><br><span class="line">        col1</span><br><span class="line">      <span class="keyword">case</span> _=&gt;</span><br><span class="line">        <span class="string">""</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="三者的区别"><a href="#三者的区别" class="headerlink" title="三者的区别"></a>三者的区别</h3><p><strong>RDD:</strong></p><ol><li>RDD一般和spark mlib同时使用。</li><li>RDD不支持spark sql操作。</li></ol><p><strong>DataFrame:</strong></p><ol><li>与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值，如</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">testDF.foreach&#123;</span><br><span class="line">  line =&gt;</span><br><span class="line">    <span class="keyword">val</span> col1=line.getAs[<span class="type">String</span>](<span class="string">"col1"</span>)</span><br><span class="line">    <span class="keyword">val</span> col2=line.getAs[<span class="type">String</span>](<span class="string">"col2"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   每一列的值没法直接访问</p><ol start="2"><li>DataFrame与Dataset一般不与spark ml同时使用。</li><li>DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作，如</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataDF.createOrReplaceTempView(<span class="string">"tmp"</span>)</span><br><span class="line">spark.sql(<span class="string">"select  ROW,DATE from tmp where DATE is not null order by DATE"</span>).show(<span class="number">100</span>,<span class="literal">false</span>)</span><br></pre></td></tr></table></figure><ol start="4"><li>DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然。</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//保存</span></span><br><span class="line"><span class="keyword">val</span> saveoptions = <span class="type">Map</span>(<span class="string">"header"</span> -&gt; <span class="string">"true"</span>, <span class="string">"delimiter"</span> -&gt; <span class="string">"\t"</span>, <span class="string">"path"</span> -&gt; <span class="string">"hdfs://master01:9000/test"</span>)</span><br><span class="line">datawDF.write.format(<span class="string">"com.atguigu.spark.csv"</span>).mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).options(saveoptions).save()</span><br><span class="line"><span class="comment">//读取</span></span><br><span class="line"><span class="keyword">val</span> options = <span class="type">Map</span>(<span class="string">"header"</span> -&gt; <span class="string">"true"</span>, <span class="string">"delimiter"</span> -&gt; <span class="string">"\t"</span>, <span class="string">"path"</span> -&gt; <span class="string">"hdfs://master01:9000/test"</span>)</span><br><span class="line"><span class="keyword">val</span> datarDF= spark.read.options(options).format(<span class="string">"com.atguigu.spark.csv"</span>).load()</span><br></pre></td></tr></table></figure><p>利用这样的保存方式，可以方便的获得字段名和列的对应，而且分隔符（delimiter）可以自由指定。</p><p><strong>Dataset:</strong></p><ol><li>Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同。</li><li>DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的getAS方法或者共性中的第七条提到的模式匹配拿出特定字段。</li><li>而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息。</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class"><span class="title">/**</span></span></span><br><span class="line"><span class="class"> <span class="title">rdd</span></span></span><br><span class="line"><span class="class"> (<span class="params">"a", 1</span>)</span></span><br><span class="line"><span class="class"> (<span class="params">"b", 1</span>)</span></span><br><span class="line"><span class="class"> (<span class="params">"a", 1</span>)</span></span><br><span class="line"><span class="class"><span class="title">**/</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">test</span></span>: <span class="type">Dataset</span>[<span class="type">Coltest</span>]=rdd.map&#123;line=&gt;</span><br><span class="line">      <span class="type">Coltest</span>(line._1,line._2)</span><br><span class="line">    &#125;.toDS</span><br><span class="line">test.map&#123;</span><br><span class="line">      line=&gt;</span><br><span class="line">        println(line.col1)</span><br><span class="line">        println(line.col2)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>可以看出，Dataset在需要访问列中的某个字段时是非常方便的，然而，如果要写一些适配性很强的函数时，如果使用Dataset，行的类型又不确定，可能是各种case class，无法实现适配，这时候用DataFrame即Dataset[Row]就能比较好的解决问题。</p><h1 id="执行-Spark-SQL-查询"><a href="#执行-Spark-SQL-查询" class="headerlink" title="执行 Spark SQL 查询"></a>执行 Spark SQL 查询</h1><h2 id="命令行查询流程"><a href="#命令行查询流程" class="headerlink" title="命令行查询流程"></a>命令行查询流程</h2><p>打开Spark shell<br>例子：查询大于30岁的用户<br>创建如下JSON文件，注意JSON的格式：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Michael"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Andy"</span>, <span class="attr">"age"</span>:<span class="number">30</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Justin"</span>, <span class="attr">"age"</span>:<span class="number">19</span>&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">"/opt/txt_data/json.txt"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">"persons"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">"SELECT * FROM persons"</span>).show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><h2 id="IDEA-创建-Spark-SQL-程序"><a href="#IDEA-创建-Spark-SQL-程序" class="headerlink" title="IDEA 创建 Spark SQL 程序"></a>IDEA 创建 Spark SQL 程序</h2><p>Maven 依赖：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>程序如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.moqi.sparksql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.<span class="type">LoggerFactory</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloWorld</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> logger = <span class="type">LoggerFactory</span>.getLogger(<span class="type">HelloWorld</span>.getClass)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">//创建SparkConf()并设置App名称</span></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"Spark SQL basic example"</span>)</span><br><span class="line">      .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// For implicit conversions like converting RDDs to DataFrames</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">    df.filter($<span class="string">"age"</span> &gt; <span class="number">21</span>).show()</span><br><span class="line"></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"persons"</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">"SELECT * FROM persons where age &gt; 21"</span>).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Spark-SQL-解析"><a href="#Spark-SQL-解析" class="headerlink" title="Spark SQL 解析"></a>Spark SQL 解析</h1><h2 id="新的起点：SparkSession"><a href="#新的起点：SparkSession" class="headerlink" title="新的起点：SparkSession"></a>新的起点：SparkSession</h2><p>在老的版本中，SparkSQL提供两种SQL查询起始点，一个叫SQLContext，用于Spark自己提供的SQL查询，一个叫HiveContext，用于连接Hive的查询，SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">.builder()</span><br><span class="line">.appName(<span class="string">"Spark SQL basic example"</span>)</span><br><span class="line">.config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">// For implicit conversions like converting RDDs to DataFrames</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><p>SparkSession.builder 用于创建一个SparkSession。<br>import spark.implicits._的引入是用于将DataFrames隐式转换成RDD，使df能够使用RDD中的方法。<br>如果需要Hive支持，则需要以下创建语句：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">.builder()</span><br><span class="line">.appName(<span class="string">"Spark SQL basic example"</span>)</span><br><span class="line">.config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">// For implicit conversions like converting RDDs to DataFrames</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><h2 id="创建-DataFrames"><a href="#创建-DataFrames" class="headerlink" title="创建 DataFrames"></a>创建 DataFrames</h2><p>在Spark SQL中SparkSession是创建DataFrames和执行SQL的入口，创建DataFrames有三种方式，一种是可以从一个存在的RDD进行转换，还可以从Hive Table进行查询返回，或者通过Spark的数据源进行创建。<br>从Spark数据源进行创建：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line"><span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><p>从 RDD 进行转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">Michael, 29</span></span><br><span class="line"><span class="comment">Andy, 30</span></span><br><span class="line"><span class="comment">Justin, 19</span></span><br><span class="line"><span class="comment">**/</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> peopleRdd = sc.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</span><br><span class="line">peopleRdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = examples/src/main/resources/people.txt <span class="type">MapPartitionsRDD</span>[<span class="number">18</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> peopleDF3 = peopleRdd.map(_.split(<span class="string">","</span>)).map(paras =&gt; (paras(<span class="number">0</span>),paras(<span class="number">1</span>).trim().toInt)).toDF(<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">peopleDF3: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.show()</span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">|<span class="type">Michael</span>| <span class="number">29</span>|</span><br><span class="line">|   <span class="type">Andy</span>| <span class="number">30</span>|</span><br><span class="line">| <span class="type">Justin</span>| <span class="number">19</span>|</span><br><span class="line">+-------+---+</span><br></pre></td></tr></table></figure><p>Hive 部分在后面数据源介绍。</p><h2 id="DataFrame-常用操作"><a href="#DataFrame-常用操作" class="headerlink" title="DataFrame 常用操作"></a>DataFrame 常用操作</h2><h3 id="DSL-风格语法"><a href="#DSL-风格语法" class="headerlink" title="DSL 风格语法"></a>DSL 风格语法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This import is needed to use the $-notation</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="comment">// Print the schema in a tree format</span></span><br><span class="line">df.printSchema()</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">// |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select only the "name" column</span></span><br><span class="line">df.select(<span class="string">"name"</span>).show()</span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |   name|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |Michael|</span></span><br><span class="line"><span class="comment">// |   Andy|</span></span><br><span class="line"><span class="comment">// | Justin|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select everybody, but increment the age by 1</span></span><br><span class="line">df.select($<span class="string">"name"</span>, $<span class="string">"age"</span> + <span class="number">1</span>).show()</span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |   name|(age + 1)|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |Michael|     null|</span></span><br><span class="line"><span class="comment">// |   Andy|       31|</span></span><br><span class="line"><span class="comment">// | Justin|       20|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select people older than 21</span></span><br><span class="line">df.filter($<span class="string">"age"</span> &gt; <span class="number">21</span>).show()</span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// |age|name|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// | 30|Andy|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Count people by age</span></span><br><span class="line">df.groupBy(<span class="string">"age"</span>).count().show()</span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// | age|count|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// |  19|    1|</span></span><br><span class="line"><span class="comment">// |null|    1|</span></span><br><span class="line"><span class="comment">// |  30|    1|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br></pre></td></tr></table></figure><h3 id="SQL-风格语法"><a href="#SQL-风格语法" class="headerlink" title="SQL 风格语法"></a>SQL 风格语法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Register the DataFrame as a SQL temporary view</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM people"</span>)</span><br><span class="line">sqlDF.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Register the DataFrame as a global temporary view</span></span><br><span class="line">df.createGlobalTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Global temporary view is tied to a system preserved database `global_temp`</span></span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Global temporary view is cross-session</span></span><br><span class="line">spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><p>临时表是Session范围内的，Session退出后，表就失效了。如果想应用范围内有效，可以使用全局表。注意使用全局表时需要全路径访问，如：global_temp.people.</p><h2 id="创建-DataSet"><a href="#创建-DataSet" class="headerlink" title="创建 DataSet"></a>创建 DataSet</h2><p>Dataset是具有强类型的数据集合，需要提供对应的类型信息。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,</span></span><br><span class="line"><span class="comment">// you can use custom classes that implement the Product interface</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">Encoders</span> <span class="title">are</span> <span class="title">created</span> <span class="title">for</span> <span class="title">case</span> <span class="title">classes</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">caseClassDS</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Andy"</span>, <span class="number">32</span>)).toDS()</span><br><span class="line">caseClassDS.show()</span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"><span class="comment">// |name|age|</span></span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"><span class="comment">// |Andy| 32|</span></span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Encoders for most common types are automatically provided by importing spark.implicits._</span></span><br><span class="line"><span class="keyword">val</span> primitiveDS = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).toDS()</span><br><span class="line">primitiveDS.map(_ + <span class="number">1</span>).collect() <span class="comment">// Returns: Array(2, 3, 4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name</span></span><br><span class="line"><span class="keyword">val</span> path = <span class="string">"examples/src/main/resources/people.json"</span></span><br><span class="line"><span class="keyword">val</span> peopleDS = spark.read.json(path).as[<span class="type">Person</span>]</span><br><span class="line">peopleDS.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><h2 id="DataSet-和-RDD-互操作"><a href="#DataSet-和-RDD-互操作" class="headerlink" title="DataSet 和 RDD 互操作"></a>DataSet 和 RDD 互操作</h2><p>Spark SQL支持通过两种方式将存在的RDD转换为Dataset，转换的过程中需要让Dataset获取RDD中的Schema信息，主要有两种方式，一种是通过反射来获取RDD中的Schema信息。这种方式适合于列名已知的情况下。第二种是通过编程接口的方式将Schema信息应用于RDD，这种方式可以处理那种在运行时才能知道列的方式。</p><h3 id="通过反射获取-Schema"><a href="#通过反射获取-Schema" class="headerlink" title="通过反射获取 Schema"></a>通过反射获取 Schema</h3><p>SparkSQL能够自动将包含有case类的RDD转换成DataFrame，case类定义了table的结构，case类属性通过反射变成了表的列名。Case类可以包含诸如Seqs或者Array等复杂的结构。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// For implicit conversions from RDDs to DataFrames</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD of Person objects from a text file, convert it to a Dataframe</span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.sparkContext</span><br><span class="line">.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</span><br><span class="line">.map(_.split(<span class="string">","</span>))</span><br><span class="line">.map(attributes =&gt; <span class="type">Person</span>(attributes(<span class="number">0</span>), attributes(<span class="number">1</span>).trim.toInt))</span><br><span class="line">.toDF()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Register the DataFrame as a temporary view</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by Spark</span></span><br><span class="line"><span class="keyword">val</span> teenagersDF = spark.sql(<span class="string">"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index  ROW object</span></span><br><span class="line">teenagersDF.map(teenager =&gt; <span class="string">"Name: "</span> + teenager(<span class="number">0</span>)).show()</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// or by field name</span></span><br><span class="line">teenagersDF.map(teenager =&gt; <span class="string">"Name: "</span> + teenager.getAs[<span class="type">String</span>](<span class="string">"name"</span>)).show()</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// No pre-defined encoders for Dataset[Map[K,V]], define explicitly</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> mapEncoder = org.apache.spark.sql.<span class="type">Encoders</span>.kryo[<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Any</span>]]</span><br><span class="line"><span class="comment">// Primitive types and case classes can be also defined as</span></span><br><span class="line"><span class="comment">// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span></span><br><span class="line">teenagersDF.map(teenager =&gt; teenager.getValuesMap[<span class="type">Any</span>](<span class="type">List</span>(<span class="string">"name"</span>, <span class="string">"age"</span>))).collect()</span><br><span class="line"><span class="comment">// Array(Map("name" -&gt; "Justin", "age" -&gt; 19))</span></span><br></pre></td></tr></table></figure><h3 id="通过编程设置-Schema"><a href="#通过编程设置-Schema" class="headerlink" title="通过编程设置 Schema"></a>通过编程设置 Schema</h3><p>如果case类不能够提前定义，可以通过下面三个步骤定义一个DataFrame<br>创建一个多行结构的RDD;<br>创建用StructType来表示的行结构信息。<br>通过SparkSession提供的createDataFrame方法来应用Schema.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD</span></span><br><span class="line"><span class="keyword">val</span> peopleRDD = spark.sparkContext.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The schema is encoded in a string,应该是动态通过程序生成的</span></span><br><span class="line"><span class="keyword">val</span> schemaString = <span class="string">"name age"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Generate the schema based on the string of schema   Array[StructFiled]</span></span><br><span class="line"><span class="keyword">val</span> fields = schemaString.split(<span class="string">" "</span>)</span><br><span class="line">.map(fieldName =&gt; <span class="type">StructField</span>(fieldName, <span class="type">StringType</span>, nullable = <span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// val filed = schemaString.split(" ").map(filename=&gt; filename match&#123; case "name"=&gt; StructField(filename,StringType,nullable = true); case "age"=&gt;StructField(filename, IntegerType,nullable = true)&#125; )</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(fields)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Convert records of the RDD (people) to Rows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"><span class="keyword">val</span> rowRDD = peopleRDD</span><br><span class="line">.map(_.split(<span class="string">","</span>))</span><br><span class="line">.map(attributes =&gt; <span class="type">Row</span>(attributes(<span class="number">0</span>), attributes(<span class="number">1</span>).trim))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Apply the schema to the RDD</span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL can be run over a temporary view created using DataFrames</span></span><br><span class="line"><span class="keyword">val</span> results = spark.sql(<span class="string">"SELECT name FROM people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations</span></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index or by field name</span></span><br><span class="line">results.map(attributes =&gt; <span class="string">"Name: "</span> + attributes(<span class="number">0</span>)).show()</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        value|</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |Name: Michael|</span></span><br><span class="line"><span class="comment">// |   Name: Andy|</span></span><br><span class="line"><span class="comment">// | Name: Justin|</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br></pre></td></tr></table></figure><h2 id="类型之间的转换总结"><a href="#类型之间的转换总结" class="headerlink" title="类型之间的转换总结"></a>类型之间的转换总结</h2><p>RDD、DataFrame、Dataset三者有许多共性，有各自适用的场景常常需要在三者之间转换<br><strong>DataFrame/Dataset转RDD：</strong><br>这个转换很简单：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1=testDF.rdd</span><br><span class="line"><span class="keyword">val</span> rdd2=testDS.rdd</span><br></pre></td></tr></table></figure><p><strong>RDD转DataFrame：</strong></p><p>一般用元组把一行的数据写在一起，然后在toDF中指定字段名。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> testDF = rdd.map &#123;line=&gt;</span><br><span class="line">      (line._1,line._2)</span><br><span class="line">    &#125;.toDF(<span class="string">"col1"</span>,<span class="string">"col2"</span>)</span><br></pre></td></tr></table></figure><p><strong>RDD转DataSet：</strong></p><p>可以注意到，定义每一行的类型（case class）时，已经给出了字段名和类型，后面只要往case class里面添加值即可。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">testDS</span> </span>= rdd.map &#123;line=&gt;</span><br><span class="line">      <span class="type">Coltest</span>(line._1,line._2)</span><br><span class="line">    &#125;.toDS</span><br></pre></td></tr></table></figure><p><strong>DataSet转DataFrame：</strong></p><p>这个也很简单，因为只是把case class封装成Row.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> testDF = testDS.toDF</span><br></pre></td></tr></table></figure><p><strong>DataFrame转DataSet：</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">testDS</span> </span>= testDF.as[<span class="type">Coltest</span>]</span><br></pre></td></tr></table></figure><p>这种方法就是在给出每一列的类型后，使用as方法，转成Dataset，这在数据类型是DataFrame又需要针对各个字段处理时极为方便。<br>在使用一些特殊的操作时，一定要加上 import spark.implicits._ 不然toDF、toDS无法使用。</p><h2 id="用户自定义函数"><a href="#用户自定义函数" class="headerlink" title="用户自定义函数"></a>用户自定义函数</h2><p>通过spark.udf功能用户可以自定义函数。</p><h3 id="用户自定义UDF函数"><a href="#用户自定义UDF函数" class="headerlink" title="用户自定义UDF函数"></a>用户自定义UDF函数</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; spark.udf.register(<span class="string">"addName"</span>, (x:<span class="type">String</span>)=&gt; <span class="string">"Name:"</span>+x)</span><br><span class="line">res5: org.apache.spark.sql.expressions.<span class="type">UserDefinedFunction</span> = <span class="type">UserDefinedFunction</span>(&lt;function1&gt;,<span class="type">StringType</span>,<span class="type">Some</span>(<span class="type">List</span>(<span class="type">StringType</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">"Select addName(name), age from people"</span>).show()</span><br><span class="line">+-----------------+----+</span><br><span class="line">|<span class="type">UDF</span>:addName(name)| age|</span><br><span class="line">+-----------------+----+</span><br><span class="line">|     <span class="type">Name</span>:<span class="type">Michael</span>|<span class="literal">null</span>|</span><br><span class="line">|        <span class="type">Name</span>:<span class="type">Andy</span>|  <span class="number">30</span>|</span><br><span class="line">|      <span class="type">Name</span>:<span class="type">Justin</span>|  <span class="number">19</span>|</span><br><span class="line">+-----------------+----+</span><br></pre></td></tr></table></figure><h3 id="用户自定义聚合函数"><a href="#用户自定义聚合函数" class="headerlink" title="用户自定义聚合函数"></a>用户自定义聚合函数</h3><p>强类型的Dataset和弱类型的DataFrame都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。</p><h4 id="弱类型"><a href="#弱类型" class="headerlink" title="弱类型"></a>弱类型</h4><p>通过继承UserDefinedAggregateFunction来实现用户自定义聚合函数。下面展示一个求平均工资的自定义聚合函数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">MutableAggregationBuffer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">UserDefinedAggregateFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line"><span class="comment">// 聚合函数输入参数的数据类型 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"inputColumn"</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span><br><span class="line"><span class="comment">// 聚合缓冲区中值得数据类型 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line"><span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"sum"</span>, <span class="type">LongType</span>) :: <span class="type">StructField</span>(<span class="string">"count"</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 返回值的数据类型 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"><span class="comment">// 对于相同的输入是否一直返回相同的输出。 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">// 初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="comment">// 存工资的总额</span></span><br><span class="line">buffer(<span class="number">0</span>) = <span class="number">0</span>L</span><br><span class="line"><span class="comment">// 存工资的个数</span></span><br><span class="line">buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 相同Execute间的数据合并。 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>)</span><br><span class="line">buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 不同Execute间的数据合并 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>)</span><br><span class="line">buffer1(<span class="number">1</span>) = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 计算最终结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Double</span> = buffer.getLong(<span class="number">0</span>).toDouble / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册函数</span></span><br><span class="line">spark.udf.register(<span class="string">"myAverage"</span>, <span class="type">MyAverage</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">"examples/src/main/resources/employees.json"</span>)</span><br><span class="line">df.createOrReplaceTempView(<span class="string">"employees"</span>)</span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |   name|salary|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |Michael|  3000|</span></span><br><span class="line"><span class="comment">// |   Andy|  4500|</span></span><br><span class="line"><span class="comment">// | Justin|  3500|</span></span><br><span class="line"><span class="comment">// |  Berta|  4000|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> result = spark.sql(<span class="string">"SELECT myAverage(salary) as average_salary FROM employees"</span>)</span><br><span class="line">result.show()</span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |average_salary|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |        3750.0|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br></pre></td></tr></table></figure><h4 id="强类型"><a href="#强类型" class="headerlink" title="强类型"></a>强类型</h4><p>通过继承Aggregator来实现强类型自定义聚合函数，同样是求平均工资。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Encoder</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Encoders</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">// 既然是强类型，可能有case类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span>(<span class="params">name: <span class="type">String</span>, salary: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Average</span>(<span class="params">var sum: <span class="type">Long</span>, var count: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Employee</span>, <span class="type">Average</span>, <span class="type">Double</span>] </span>&#123;</span><br><span class="line"><span class="comment">// 定义一个数据结构，保存工资总数和工资总个数，初始都为0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Average</span> = <span class="type">Average</span>(<span class="number">0</span>L, <span class="number">0</span>L)</span><br><span class="line"><span class="comment">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span></span><br><span class="line"><span class="comment">// and return it instead of constructing a new object</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buffer: <span class="type">Average</span>, employee: <span class="type">Employee</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">buffer.sum += employee.salary</span><br><span class="line">buffer.count += <span class="number">1</span></span><br><span class="line">buffer</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 聚合不同execute的结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">Average</span>, b2: <span class="type">Average</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">b1.sum += b2.sum</span><br><span class="line">b1.count += b2.count</span><br><span class="line">b1</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 计算输出</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">Average</span>): <span class="type">Double</span> = reduction.sum.toDouble / reduction.count</span><br><span class="line"><span class="comment">// 设定之间值类型的编码器，要转换成case类</span></span><br><span class="line"><span class="comment">// Encoders.product是进行scala元组和case类转换的编码器 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Average</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"><span class="comment">// 设定最终输出值的编码器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds = spark.read.json(<span class="string">"examples/src/main/resources/employees.json"</span>).as[<span class="type">Employee</span>]</span><br><span class="line">ds.show()</span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |   name|salary|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |Michael|  3000|</span></span><br><span class="line"><span class="comment">// |   Andy|  4500|</span></span><br><span class="line"><span class="comment">// | Justin|  3500|</span></span><br><span class="line"><span class="comment">// |  Berta|  4000|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Convert the function to a `TypedColumn` and give it a name</span></span><br><span class="line"><span class="keyword">val</span> averageSalary = <span class="type">MyAverage</span>.toColumn.name(<span class="string">"average_salary"</span>)</span><br><span class="line"><span class="keyword">val</span> result = ds.select(averageSalary)</span><br><span class="line">result.show()</span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |average_salary|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |        3750.0|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br></pre></td></tr></table></figure><h1 id="Spark-SQL-数据源"><a href="#Spark-SQL-数据源" class="headerlink" title="Spark SQL 数据源"></a>Spark SQL 数据源</h1><h2 id="通用加载-保存方法"><a href="#通用加载-保存方法" class="headerlink" title="通用加载 / 保存方法"></a>通用加载 / 保存方法</h2><h3 id="手动指定选项"><a href="#手动指定选项" class="headerlink" title="手动指定选项"></a>手动指定选项</h3><p>Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。<br>Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。修改配置项spark.sql.sources.default，可修改默认数据源格式。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.load(<span class="string">"examples/src/main/resources/users.parquet"</span>) df.select(<span class="string">"name"</span>, <span class="string">"favorite_color"</span>).write.save(<span class="string">"namesAndFavColors.parquet"</span>)</span><br></pre></td></tr></table></figure><p>当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：org.apache.spark.sql.parquet），如果数据源格式为内置格式，则只需要指定简称定json, parquet, jdbc, orc, libsvm, csv, text来指定数据的格式。<br>可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> peopleDF = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line">peopleDF.write.format(<span class="string">"parquet"</span>).save(<span class="string">"hdfs://master01:9000/namesAndAges.parquet"</span>)</span><br></pre></td></tr></table></figure><p>除此之外，可以直接运行SQL在文件上：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM parquet.`hdfs://master01:9000/namesAndAges.parquet`"</span>)</span><br><span class="line">sqlDF.show()</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> peopleDF = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line">peopleDF: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.write.format(<span class="string">"parquet"</span>).save(<span class="string">"hdfs://master01:9000/namesAndAges.parquet"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM parquet.`hdfs://master01:9000/namesAndAges.parquet`"</span>)</span><br><span class="line"><span class="number">17</span>/<span class="number">09</span>/<span class="number">05</span> <span class="number">04</span>:<span class="number">21</span>:<span class="number">11</span> <span class="type">WARN</span> <span class="type">ObjectStore</span>: <span class="type">Failed</span> to get database parquet, returning <span class="type">NoSuchObjectException</span></span><br><span class="line">sqlDF: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; sqlDF.show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><h3 id="文件保存选项"><a href="#文件保存选项" class="headerlink" title="文件保存选项"></a>文件保存选项</h3><p>可以采用SaveMode执行存储操作，SaveMode定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。SaveMode详细介绍如下表：</p><table><thead><tr><th><strong>Scala/Java</strong></th><th><strong>Any Language</strong></th><th><strong>Meaning</strong></th></tr></thead><tbody><tr><td><strong>SaveMode.ErrorIfExists(default)</strong></td><td>“error”(default)</td><td>如果文件存在，则报错</td></tr><tr><td><strong>SaveMode.Append</strong></td><td>“append”</td><td>追加</td></tr><tr><td><strong>SaveMode.Overwrite</strong></td><td>“overwrite”</td><td>覆写</td></tr><tr><td><strong>SaveMode.Ignore</strong></td><td>“ignore”</td><td>数据存在，则忽略</td></tr></tbody></table><h2 id="Parquet-文件"><a href="#Parquet-文件" class="headerlink" title="Parquet 文件"></a>Parquet 文件</h2><p>Parquet是一种流行的列式存储格式，可以高效地存储具有嵌套字段的记录。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/09-Parquet-File.png?raw=true" alt="09-Parquet-File"></p><h3 id="Parquet-读写"><a href="#Parquet-读写" class="headerlink" title="Parquet 读写"></a>Parquet 读写</h3><p>Parquet格式经常在Hadoop生态圈中被使用，它也支持Spark SQL的全部数据类型。Spark SQL 提供了直接读取和存储 Parquet 格式文件的方法。 </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Encoders for most common types are automatically provided by importing spark.implicits._</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrames can be saved as Parquet files, maintaining the schema information</span></span><br><span class="line">peopleDF.write.parquet(<span class="string">"hdfs://master01:9000/people.parquet"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read in the parquet file created above</span></span><br><span class="line"><span class="comment">// Parquet files are self-describing so the schema is preserved</span></span><br><span class="line"><span class="comment">// The result of loading a Parquet file is also a DataFrame</span></span><br><span class="line"><span class="keyword">val</span> parquetFileDF = spark.read.parquet(<span class="string">"hdfs://master01:9000/people.parquet"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Parquet files can also be used to create a temporary view and then used in SQL statements</span></span><br><span class="line">parquetFileDF.createOrReplaceTempView(<span class="string">"parquetFile"</span>)</span><br><span class="line"><span class="keyword">val</span> namesDF = spark.sql(<span class="string">"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19"</span>)</span><br><span class="line">namesDF.map(attributes =&gt; <span class="string">"Name: "</span> + attributes(<span class="number">0</span>)).show()</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br></pre></td></tr></table></figure><h3 id="解析分区信息"><a href="#解析分区信息" class="headerlink" title="解析分区信息"></a>解析分区信息</h3><p>对表进行分区是对数据进行优化的方式之一。在分区的表内，数据通过分区列将数据存储在不同的目录下。Parquet数据源现在能够自动发现并解析分区信息。例如，对人口数据进行分区存储，分区列为gender和country，使用下面的目录结构：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">path</span><br><span class="line">└── to</span><br><span class="line">    └── table</span><br><span class="line">        ├── gender=male</span><br><span class="line">        │   ├── ...</span><br><span class="line">        │   │</span><br><span class="line">        │   ├── country=US</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   ├── country=CN</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   └── ...</span><br><span class="line">        └── gender=female</span><br><span class="line">            ├── ...</span><br><span class="line">            │</span><br><span class="line">            ├── country=US</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            ├── country=CN</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            └── ...</span><br></pre></td></tr></table></figure><p>通过传递path/to/table给 SQLContext.read.parquet或SQLContext.read.load，Spark SQL将自动解析分区信息。返回的DataFrame的Schema如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- name: string (nullable = <span class="literal">true</span>)</span><br><span class="line">|-- age: long (nullable = <span class="literal">true</span>)</span><br><span class="line">|-- gender: string (nullable = <span class="literal">true</span>)</span><br><span class="line">|-- country: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure><p>需要注意的是，数据的分区列的数据类型是自动解析的。当前，支持数值类型和字符串类型。自动解析分区类型的参数为：spark.sql.sources.partitionColumnTypeInference.enabled，默认值为true。如果想关闭该功能，直接将该参数设置为disabled。此时，分区列数据格式将被默认设置为string类型，不再进行类型解析。</p><h3 id="Schema-合并"><a href="#Schema-合并" class="headerlink" title="Schema 合并"></a>Schema 合并</h3><p>像ProtocolBuffer、Avro和Thrift那样，Parquet也支持Schema evolution（Schema演变）。用户可以先定义一个简单的Schema，然后逐渐的向Schema中增加列描述。通过这种方式，用户可以获取多个有不同Schema但相互兼容的Parquet文件。现在Parquet数据源能自动检测这种情况，并合并这些文件的schemas。<br>因为Schema合并是一个高消耗的操作，在大多数情况下并不需要，所以Spark SQL从1.5.0开始默认关闭了该功能。可以通过下面两种方式开启该功能：<br>当数据源为Parquet文件时，将数据源选项mergeSchema设置为true<br>设置全局SQL选项spark.sql.parquet.mergeSchema为true<br>示例如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sqlContext from the previous example is used in this example.</span></span><br><span class="line"><span class="comment">// This is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a simple DataFrame, stored into a partition directory</span></span><br><span class="line"><span class="keyword">val</span> df1 = sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>).map(i =&gt; (i, i * <span class="number">2</span>)).toDF(<span class="string">"single"</span>, <span class="string">"double"</span>)</span><br><span class="line">df1.write.parquet(<span class="string">"hdfs://master01:9000/data/test_table/key=1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create another DataFrame in a new partition directory,</span></span><br><span class="line"><span class="comment">// adding a new column and dropping an existing column</span></span><br><span class="line"><span class="keyword">val</span> df2 = sc.makeRDD(<span class="number">6</span> to <span class="number">10</span>).map(i =&gt; (i, i * <span class="number">3</span>)).toDF(<span class="string">"single"</span>, <span class="string">"triple"</span>)</span><br><span class="line">df2.write.parquet(<span class="string">"hdfs://master01:9000/data/test_table/key=2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read the partitioned table</span></span><br><span class="line"><span class="keyword">val</span> df3 = spark.read.option(<span class="string">"mergeSchema"</span>, <span class="string">"true"</span>).parquet(<span class="string">"hdfs://master01:9000/data/test_table"</span>)</span><br><span class="line">df3.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment">// The final schema consists of all 3 columns in the Parquet files together</span></span><br><span class="line"><span class="comment">// with the partitioning column appeared in the partition directory paths.</span></span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">// |-- single: int (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- double: int (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- triple: int (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- key : int (nullable = true)</span></span><br></pre></td></tr></table></figure><h2 id="Hive-数据库"><a href="#Hive-数据库" class="headerlink" title="Hive 数据库"></a>Hive 数据库</h2><p>Apache Hive是Hadoop上的SQL引擎，Spark SQL编译时可以包含Hive支持，也可以不包含。包含Hive支持的Spark SQL可以支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的 一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。<br>若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好Hive，Spark SQL也可以运行。 需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span>(<span class="params">key: <span class="type">Int</span>, value: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">warehouseLocation</span> <span class="title">points</span> <span class="title">to</span> <span class="title">the</span> <span class="title">default</span> <span class="title">location</span> <span class="title">for</span> <span class="title">managed</span> <span class="title">databases</span> <span class="title">and</span> <span class="title">tables</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">warehouseLocation</span> </span>= <span class="keyword">new</span> <span class="type">File</span>(<span class="string">"spark-warehouse"</span>).getAbsolutePath</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">.builder()</span><br><span class="line">.appName(<span class="string">"Spark Hive Example"</span>)</span><br><span class="line">.config(<span class="string">"spark.sql.warehouse.dir"</span>, warehouseLocation)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">import</span> spark.sql</span><br><span class="line"></span><br><span class="line">sql(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)"</span>)</span><br><span class="line">sql(<span class="string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries are expressed in HiveQL</span></span><br><span class="line">sql(<span class="string">"SELECT * FROM src"</span>).show()</span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |key|  value|</span></span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |238|val_238|</span></span><br><span class="line"><span class="comment">// | 86| val_86|</span></span><br><span class="line"><span class="comment">// |311|val_311|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Aggregation queries are also supported.</span></span><br><span class="line">sql(<span class="string">"SELECT COUNT(*) FROM src"</span>).show()</span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |count(1)|</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |    500 |</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are themselves DataFrames and support all normal functions.</span></span><br><span class="line"><span class="keyword">val</span> sqlDF = sql(<span class="string">"SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span></span><br><span class="line"><span class="keyword">val</span> stringsDS = sqlDF.map &#123;</span><br><span class="line"><span class="keyword">case</span> <span class="type">Row</span>(key: <span class="type">Int</span>, value: <span class="type">String</span>) =&gt; <span class="string">s"Key: <span class="subst">$key</span>, Value: <span class="subst">$value</span>"</span></span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |               value|</span></span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// You can also use DataFrames to create temporary views within a SparkSession.</span></span><br><span class="line"><span class="keyword">val</span> recordsDF = spark.createDataFrame((<span class="number">1</span> to <span class="number">100</span>).map(i =&gt; <span class="type">Record</span>(i, <span class="string">s"val_<span class="subst">$i</span>"</span>)))</span><br><span class="line">recordsDF.createOrReplaceTempView(<span class="string">"records"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries can then join DataFrame data with data stored in Hive.</span></span><br><span class="line">sql(<span class="string">"SELECT * FROM records r JOIN src s ON r.key = s.key"</span>).show()</span><br><span class="line"><span class="comment">// +---+------+---+------+</span></span><br><span class="line"><span class="comment">// |key| value|key| value|</span></span><br><span class="line"><span class="comment">// +---+------+---+------+</span></span><br><span class="line"><span class="comment">// |  2| val_2|  2| val_2|</span></span><br><span class="line"><span class="comment">// |  4| val_4|  4| val_4|</span></span><br><span class="line"><span class="comment">// |  5| val_5|  5| val_5|</span></span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></table></figure><h3 id="内联-Hive-应用"><a href="#内联-Hive-应用" class="headerlink" title="内联 Hive 应用"></a>内联 Hive 应用</h3><p>如果要使用内嵌的Hive，什么都不用做，直接用就可以了。 –conf : spark.sql.warehouse.dir=</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">"show tables"</span>).show</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|        |  persons|       <span class="literal">true</span>|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">"SELECT * FROM persons"</span>).show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><p>注意：如果你使用的是内部的Hive，在Spark2.0之后，spark.sql.warehouse.dir用于指定数据仓库的地址，如果你需要是用HDFS作为路径，那么需要将core-site.xml和hdfs-site.xml 加入到Spark conf目录，否则只会创建master节点上的warehouse目录，查询时会出现文件找不到的问题。所以如果需要使用HDFS，则需要将metastore删除，重启集群。</p><h3 id="外部-Hive-应用"><a href="#外部-Hive-应用" class="headerlink" title="外部 Hive 应用"></a>外部 Hive 应用</h3><p>如果想连接外部已经部署好的Hive，需要通过以下几个步骤。</p><ol><li>将Hive中的hive-site.xml拷贝或者软连接到Spark安装目录下的conf目录下。</li><li>打开spark shell，注意带上访问Hive元数据库的JDBC客户端。</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master spark://master01:7077 --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure><h2 id="JSON-数据集"><a href="#JSON-数据集" class="headerlink" title="JSON 数据集"></a>JSON 数据集</h2><p>Spark SQL 能够自动推测 JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载一个 Dataset[String]或者一个JSON 文件.注意，这个JSON文件不是一个传统的JSON文件，每一行都得是一个JSON串。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Michael"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Andy"</span>, <span class="attr">"age"</span>:<span class="number">30</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Justin"</span>, <span class="attr">"age"</span>:<span class="number">19</span>&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Primitive types (Int, String, etc) and Product types (case classes) encoders are</span></span><br><span class="line"><span class="comment">// supported by importing this when creating a Dataset.</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// A JSON dataset is pointed to by path.</span></span><br><span class="line"><span class="comment">// The path can be either a single text file or a directory storing text files</span></span><br><span class="line"><span class="keyword">val</span> path = <span class="string">"examples/src/main/resources/people.json"</span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.read.json(path)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The inferred schema can be visualized using the printSchema() method</span></span><br><span class="line">peopleDF.printSchema()</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by spark</span></span><br><span class="line"><span class="keyword">val</span> teenagerNamesDF = spark.sql(<span class="string">"SELECT name FROM people WHERE age BETWEEN 13 AND 19"</span>)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line"><span class="comment">// +------+</span></span><br><span class="line"><span class="comment">// |  name|</span></span><br><span class="line"><span class="comment">// +------+</span></span><br><span class="line"><span class="comment">// |Justin|</span></span><br><span class="line"><span class="comment">// +------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span></span><br><span class="line"><span class="comment">// a Dataset[String] storing one JSON object per string</span></span><br><span class="line"><span class="keyword">val</span> otherPeopleDataset = spark.createDataset(</span><br><span class="line"><span class="string">""</span><span class="string">"&#123;"</span><span class="string">name":"</span><span class="type">Yin</span><span class="string">","</span><span class="string">address":&#123;"</span><span class="string">city":"</span><span class="type">Columbus</span><span class="string">","</span><span class="string">state":"</span><span class="type">Ohio</span><span class="string">"&#125;&#125;"</span><span class="string">""</span> :: <span class="type">Nil</span>)</span><br><span class="line"><span class="keyword">val</span> otherPeople = spark.read.json(otherPeopleDataset)</span><br><span class="line">otherPeople.show()</span><br><span class="line"><span class="comment">// +---------------+----+</span></span><br><span class="line"><span class="comment">// |        address|name|</span></span><br><span class="line"><span class="comment">// +---------------+----+</span></span><br><span class="line"><span class="comment">// |[Columbus,Ohio]| Yin|</span></span><br><span class="line"><span class="comment">// +---------------+----+</span></span><br></pre></td></tr></table></figure><h2 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h2><p>Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。<br>注意，需要将相关的数据库驱动放到spark的类路径下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master spark://master01:7077 --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span></span><br><span class="line"><span class="comment">// Loading data from a JDBC source</span></span><br><span class="line"><span class="keyword">val</span> jdbcDF = spark.read.format(<span class="string">"jdbc"</span>).option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://master01:3306/rdd"</span>).option(<span class="string">"dbtable"</span>, <span class="string">" rddtable"</span>).option(<span class="string">"user"</span>, <span class="string">"root"</span>).option(<span class="string">"password"</span>, <span class="string">"hive"</span>).load()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> connectionProperties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">connectionProperties.put(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">connectionProperties.put(<span class="string">"password"</span>, <span class="string">"hive"</span>)</span><br><span class="line"><span class="keyword">val</span> jdbcDF2 = spark.read</span><br><span class="line">.jdbc(<span class="string">"jdbc:mysql://master01:3306/rdd"</span>, <span class="string">"rddtable"</span>, connectionProperties)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Saving data to a JDBC source</span></span><br><span class="line">jdbcDF.write</span><br><span class="line">.format(<span class="string">"jdbc"</span>)</span><br><span class="line">.option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://master01:3306/rdd"</span>)</span><br><span class="line">.option(<span class="string">"dbtable"</span>, <span class="string">"rddtable2"</span>)</span><br><span class="line">.option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">.option(<span class="string">"password"</span>, <span class="string">"hive"</span>)</span><br><span class="line">.save()</span><br><span class="line"></span><br><span class="line">jdbcDF2.write</span><br><span class="line">.jdbc(<span class="string">"jdbc:mysql://master01:3306/mysql"</span>, <span class="string">"db"</span>, connectionProperties)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Specifying create table column data types on write</span></span><br><span class="line">jdbcDF.write</span><br><span class="line">.option(<span class="string">"createTableColumnTypes"</span>, <span class="string">"name CHAR(64), comments VARCHAR(1024)"</span>)</span><br><span class="line">.jdbc(<span class="string">"jdbc:mysql://master01:3306/mysql"</span>, <span class="string">"db"</span>, connectionProperties)</span><br></pre></td></tr></table></figure><h1 id="JDBC-ODBC-服务器"><a href="#JDBC-ODBC-服务器" class="headerlink" title="JDBC / ODBC 服务器"></a>JDBC / ODBC 服务器</h1><p>Spark SQL也提供JDBC连接支持，这对于让商业智能(BI)工具连接到Spark集群上以 及在多用户间共享一个集群的场景都非常有用。JDBC 服务器作为一个独立的 Spark 驱动 器程序运行，可以在多用户之间共享。任意一个客户端都可以在内存中缓存数据表，对表 进行查询。集群的资源以及缓存数据都在所有用户之间共享。<br>Spark SQL的JDBC服务器与Hive中的HiveServer2相一致。由于使用了Thrift通信协议，它也被称为“Thrift server”。<br>服务器可以通过 Spark 目录中的 sbin/start-thriftserver.sh 启动。这个 脚本接受的参数选项大多与 spark-submit 相同。默认情况下，服务器会在 localhost:10000 上进行监听，我们可以通过环境变量(HIVE_SERVER2_THRIFT_PORT 和 HIVE_SERVER2_THRIFT_BIND_HOST)修改这些设置，也可以通过 Hive配置选项(hive. server2.thrift.port 和 hive.server2.thrift.bind.host)来修改。你也可以通过命令行参 数–hiveconf property=value来设置Hive选项。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-thriftserver.sh \</span><br><span class="line">--hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \</span><br><span class="line">--hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \</span><br><span class="line">--master &lt;master-uri&gt;</span><br><span class="line">...</span><br><span class="line">./bin/beeline</span><br><span class="line">beeline&gt; !connect jdbc:hive2://master01:10000</span><br></pre></td></tr></table></figure><p>在 Beeline 客户端中，你可以使用标准的 HiveQL 命令来创建、列举以及查询数据表。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[bigdata<span class="meta">@master</span>01 spark<span class="number">-2.1</span><span class="number">.1</span>-bin-hadoop2<span class="number">.7</span>]$ ./sbin/start-thriftserver.sh</span><br><span class="line">starting org.apache.spark.sql.hive.thriftserver.<span class="type">HiveThriftServer2</span>, logging to /home/bigdata/hadoop/spark<span class="number">-2.1</span><span class="number">.1</span>-bin-hadoop2<span class="number">.7</span>/logs/spark-bigdata-org.apache.spark.sql.hive.thriftserver.<span class="type">HiveThriftServer2</span><span class="number">-1</span>-master01.out</span><br><span class="line"></span><br><span class="line">[bigdata<span class="meta">@master</span>01 spark<span class="number">-2.1</span><span class="number">.1</span>-bin-hadoop2<span class="number">.7</span>]$ ./bin/beeline</span><br><span class="line"><span class="type">Beeline</span> version <span class="number">1.2</span><span class="number">.1</span>.spark2 by <span class="type">Apache</span> <span class="type">Hive</span></span><br><span class="line"></span><br><span class="line">beeline&gt; !connect jdbc:hive2:<span class="comment">//master01:10000</span></span><br><span class="line"><span class="type">Connecting</span> to jdbc:hive2:<span class="comment">//master01:10000</span></span><br><span class="line"><span class="type">Enter</span> username <span class="keyword">for</span> jdbc:hive2:<span class="comment">//master01:10000: bigdata</span></span><br><span class="line"><span class="type">Enter</span> password <span class="keyword">for</span> jdbc:hive2:<span class="comment">//master01:10000: *******</span></span><br><span class="line">log4j:<span class="type">WARN</span> <span class="type">No</span> appenders could be found <span class="keyword">for</span> logger (org.apache.hive.jdbc.<span class="type">Utils</span>).</span><br><span class="line">log4j:<span class="type">WARN</span> <span class="type">Please</span> initialize the log4j system properly.</span><br><span class="line">log4j:<span class="type">WARN</span> <span class="type">See</span> http:<span class="comment">//logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span></span><br><span class="line"><span class="type">Connected</span> to: <span class="type">Spark</span> <span class="type">SQL</span> (version <span class="number">2.1</span><span class="number">.1</span>)</span><br><span class="line"><span class="type">Driver</span>: <span class="type">Hive</span> <span class="type">JDBC</span> (version <span class="number">1.2</span><span class="number">.1</span>.spark2)</span><br><span class="line"><span class="type">Transaction</span> isolation: <span class="type">TRANSACTION_REPEATABLE_READ</span></span><br><span class="line"></span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="comment">//master01:10000&gt; show tables;</span></span><br><span class="line">+-----------+------------+--------------+--+</span><br><span class="line">| database  | tableName  | isTemporary  |</span><br><span class="line">+-----------+------------+--------------+--+</span><br><span class="line">| <span class="keyword">default</span>   | src        | <span class="literal">false</span>        |</span><br><span class="line">+-----------+------------+--------------+--+</span><br><span class="line"><span class="number">1</span> row selected (<span class="number">0.726</span> seconds)</span><br><span class="line"></span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="comment">//master01:10000&gt;</span></span><br></pre></td></tr></table></figure><h1 id="Spark-SQL-CLI"><a href="#Spark-SQL-CLI" class="headerlink" title="Spark SQL CLI"></a>Spark SQL CLI</h1><p>Spark SQL CLI可以很方便的在本地运行Hive元数据服务以及从命令行执行查询任务。需要注意的是，Spark SQL CLI不能与Thrift JDBC服务交互。在Spark目录下执行如下命令启动Spark SQL CLI：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-sql</span><br></pre></td></tr></table></figure><p>配置Hive需要替换 conf/ 下的 hive-site.xml 。</p><h1 id="Spark-SQL-的运行原理"><a href="#Spark-SQL-的运行原理" class="headerlink" title="Spark SQL 的运行原理"></a>Spark SQL 的运行原理</h1><h2 id="Spark-SQL-运行架构"><a href="#Spark-SQL-运行架构" class="headerlink" title="Spark SQL 运行架构"></a>Spark SQL 运行架构</h2><p>Spark SQL对SQL语句的处理和关系型数据库类似，即词法/语法解析、绑定、优化、执行。Spark SQL会先将SQL语句解析成一棵树，然后使用规则(Rule)对Tree进行绑定、优化等处理过程。Spark SQL由Core、Catalyst、Hive、Hive-ThriftServer四部分构成：<br>Core: 负责处理数据的输入和输出，如获取数据，查询结果输出成DataFrame等<br>Catalyst: 负责处理整个查询过程，包括解析、绑定、优化等<br>Hive: 负责对Hive数据进行处理<br>Hive-ThriftServer: 主要用于对hive的访问</p><h3 id="TreeNode"><a href="#TreeNode" class="headerlink" title="TreeNode"></a>TreeNode</h3><p>逻辑计划、表达式等都可以用tree来表示，它只是在内存中维护，并不会进行磁盘的持久化，分析器和优化器对树的修改只是替换已有节点。<br>TreeNode有2个直接子类，QueryPlan和Expression。QueryPlam下又有LogicalPlan和SparkPlan. Expression是表达式体系，不需要执行引擎计算而是可以直接处理或者计算的节点，包括投影操作，操作符运算等。</p><h3 id="Rule-amp-RuleExecutor"><a href="#Rule-amp-RuleExecutor" class="headerlink" title="Rule &amp; RuleExecutor"></a>Rule &amp; RuleExecutor</h3><p>Rule就是指对逻辑计划要应用的规则，以到达绑定和优化。他的实现类就是RuleExecutor。优化器和分析器都需要继承RuleExecutor。每一个子类中都会定义Batch、Once、FixPoint. 其中每一个Batch代表着一套规则，Once表示对树进行一次操作，FixPoint表示对树进行多次的迭代操作。RuleExecutor内部提供一个Seq[Batch]属性，里面定义的是RuleExecutor的处理逻辑，具体的处理逻辑由具体的Rule子类实现。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/10-Rule-And-RuleExecutor.png?raw=true" alt="10-Rule-And-RuleExecutor"></p><p>整个流程架构图：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/11-Spark-SQL-Run-Architecture.png?raw=true" alt="11-Spark-SQL-Run-Architecture"></p><h2 id="Spark-SQL-运行原理"><a href="#Spark-SQL-运行原理" class="headerlink" title="Spark SQL 运行原理"></a>Spark SQL 运行原理</h2><ol><li><strong>使用SessionCatalog保存元数据</strong>：在解析SQL语句之前，会创建SparkSession，或者如果是2.0之前的版本初始化SQLContext，SparkSession只是封装了SparkContext和SQLContext的创建而已。会把元数据保存在SessionCatalog中，涉及到表名，字段名称和字段类型。创建临时表或者视图，其实就会往SessionCatalog注册。</li><li><strong>解析SQL,使用ANTLR生成未绑定的逻辑计划</strong>：当调用SparkSession的sql或者SQLContext的sql方法，我们以2.0为准，就会使用SparkSqlParser进行解析SQL. 使用的ANTLR进行词法解析和语法解析。它分为2个步骤来生成Unresolved LogicalPlan：(1)词法分析：Lexical Analysis，负责将token分组成符号类； (2)构建一个分析树或者语法树AST。</li><li><strong>使用分析器Analyzer绑定逻辑计划</strong>：在该阶段，Analyzer会使用Analyzer Rules，并结合SessionCatalog，对未绑定的逻辑计划进行解析，生成已绑定的逻辑计划。</li><li><strong>使用优化器Optimizer优化逻辑计划</strong>：优化器也是会定义一套Rules，利用这些Rule对逻辑计划和Exepression进行迭代处理，从而使得树的节点进行和并和优化。</li><li><strong>使用SparkPlanner生成物理计划</strong>：SparkSpanner使用Planning Strategies，对优化后的逻辑计划进行转换，生成可以执行的物理计划SparkPlan.</li><li><strong>使用QueryExecution执行物理计划</strong>：此时调用SparkPlan的execute方法，底层其实已经再触发JOB了，然后返回RDD.</li></ol><p>End. </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要解析部分 Spark SQL 的技术点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="http://moqimoqidea.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark Core 应用解析</title>
    <link href="http://moqimoqidea.github.io/2017/08/04/Spark-Core-Analysis/"/>
    <id>http://moqimoqidea.github.io/2017/08/04/Spark-Core-Analysis/</id>
    <published>2017-08-04T05:29:46.000Z</published>
    <updated>2018-11-24T12:21:42.396Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要解析部分 Spark Core 的技术点。</p><a id="more"></a> <h1 id="RDD-概念"><a href="#RDD-概念" class="headerlink" title="RDD 概念"></a>RDD 概念</h1><h2 id="RDD-为什么会产生"><a href="#RDD-为什么会产生" class="headerlink" title="RDD 为什么会产生"></a>RDD 为什么会产生</h2><p>RDD是Spark的基石，是实现Spark数据处理的核心抽象。那么RDD为什么会产生呢？</p><p>Hadoop的MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后写入物理存储设备。数据更多面临的是一次性处理。</p><p>MR的这种方式对数据领域两种常见的操作不是很高效。第一种是迭代式的算法。比如机器学习中ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR这种模式不太合适，即使多MR串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。</p><p>MR中的迭代：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/01-Iteration-In-MapReduce.png?raw=true" alt="01-Iteration-In-MapReduce"></p><p>Spark中的迭代：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/02-Iteration-In-Spark.png?raw=true" alt="02-Iteration-In-Spark"></p><p>我们需要一个效率非常快，且能够支持迭代计算和有效数据共享的模型，Spark应运而生。RDD是基于工作集的工作模式，更多的是面向工作流。</p><p>但是无论是MR还是RDD都应该具有类似位置感知、容错和负载均衡等特性。</p><h2 id="RDD-概述"><a href="#RDD-概述" class="headerlink" title="RDD 概述"></a>RDD 概述</h2><h3 id="什么是-RDD"><a href="#什么是-RDD" class="headerlink" title="什么是 RDD"></a>什么是 RDD</h3><p>RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。在 Spark 中，对数据的所有操作不外乎创建 RDD、转化已有RDD 以及调用 RDD 操作进行求值。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。RDD 可以包含 Python、Java、Scala 中任意类型的对象， 甚至可以包含用户自定义的对象。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。<br>RDD支持两种操作:转化操作和行动操作。RDD 的转化操作是返回一个新的 RDD的操作，比如 map()和 filter()，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作。比如 count() 和 first()。<br>Spark采用惰性计算模式，RDD只有第一次在一个行动操作中用到时，才会真正计算。Spark可以优化整个计算过程。默认情况下，Spark 的 RDD 会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个 RDD，可以使用 RDD.persist() 让 Spark 把这个 RDD 缓存下来。</p><h3 id="RDD-的属性"><a href="#RDD-的属性" class="headerlink" title="RDD 的属性"></a>RDD 的属性</h3><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/03-RDD-Attributes.png?raw=true" alt="03-RDD-Attributes"></p><ol><li>一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</li><li>一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</li><li>RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</li><li>一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</li><li>一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</li></ol><p>RDD是一个应用层面的逻辑概念。一个RDD多个分片。RDD就是一个元数据记录集，记录了RDD内存所有的关系数据。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/04-Native-Data-Space-And-Spark-RDD.png?raw=true" alt="04-Native-Data-Space-And-Spark-RDD"></p><h2 id="RDD-弹性"><a href="#RDD-弹性" class="headerlink" title="RDD 弹性"></a>RDD 弹性</h2><ol><li>自动进行内存和磁盘数据存储的切换：Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换</li><li>基于血统的高效容错机制：在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。</li><li>Task如果失败会自动进行特定次数的重试：RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。</li><li>Stage如果失败会自动进行特定次数的重试：如果Job的某个Stage阶段计算失败，框架也会自动进行任务的重新计算，默认次数也是4次。</li><li>Checkpoint和Persist可主动或被动触发：RDD可以通过Persist持久化将RDD缓存到内存或者磁盘，当再次用到该RDD时直接读取就行。也可以将RDD进行检查点，检查点会将数据存储在HDFS中，该RDD的所有父RD都会被移除。</li><li>数据调度弹性：Spark把这个JOB执行模型抽象为通用的有向无环图DAG，可以将多Stage的任务串联或并行执行，调度引擎自动处理Stage的失败以及Task的失败。</li><li>数据分片的高度弹性：可以根据业务的特征，动态调整数据分片的个数，提升整体的应用执行效率。</li></ol><p>RDD全称叫做弹性分布式数据集(Resilient Distributed Datasets)一种分布式的内存抽象，表示一个只读的记录分区的集合，它只能通过其他RDD转换而创建，为此，RDD支持丰富的转换操作(如map, join, filter, groupBy等过这种转换操作，新的RDD则包含了如何从其他RDDs衍生所必需的信息，所以说RDDs之间是有依赖关系的。基于RDDs之间的依赖，RDDs会形成一个有向无环图DAG，G描述了整个流式计算的流程，实际执行的时候，RDD是通过血缘关系(Lineage)成的，即使出现数据分区丢失，也可以通过血缘关系重建分区，总结起来，基于RDD的流式计算任务可描述为：从稳定的物理存储(如分布式文件系统)中加载记录，记入由一组确定性操作构成的DAG，然后写回稳定存储。另外RDD还可以将数据集缓存到内存中，使得在多个操作之间可以重用数据集，基于这个特点可以很方便地构建应用(图计算、机器学习等)或者交互式数据分析应用。可以说Spark最初也就是实现RDD的一个分布式系统，后面通过不断发展壮大成为现在较为完善的大数据生态系单来讲，Spark-RDD的关系类似于Hadoop-MapReduce关系。</p><h2 id="RDD-特点"><a href="#RDD-特点" class="headerlink" title="RDD 特点"></a>RDD 特点</h2><p>RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。</p><h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><p>RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/05-RDD-Partition.png?raw=true" alt="05-RDD-Partition"></p><h3 id="只读"><a href="#只读" class="headerlink" title="只读"></a>只读</h3><p>如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/06-RDD-Read-Only.png?raw=true" alt="06-RDD-Read-Only"></p><p>由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/07-Variety-Of-Operators.png?raw=true" alt="07-Variety-Of-Operators"></p><p>RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/08-Transformation-Operations-And-Actions-Operations.png?raw=true" alt="08-Transformation-Operations-And-Actions-Operations"></p><h3 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h3><p>RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是一一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/09-Dependencies-Between-RDD.png?raw=true" alt="09-Dependencies-Between-RDD"></p><p>通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，如下图所示，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/10-DAG.png?raw=true" alt="10-DAG"></p><h3 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h3><p>如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/11-RDD-Cache.png?raw=true" alt="11-RDD-Cache"></p><h3 id="CheckPoint"><a href="#CheckPoint" class="headerlink" title="CheckPoint"></a>CheckPoint</h3><p>虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。</p><p>给定一个RDD我们至少可以知道如下几点信息：1、分区数以及分区方式；2、由父RDDs衍生而来的相关依赖信息；3、计算每个分区的数据，计算步骤为：1）如果被缓存，则从缓存中取的分区的数据；2）如果被checkpoint，则从checkpoint处恢复数据；3）根据血缘关系计算分区的数据。</p><h1 id="RDD-编程"><a href="#RDD-编程" class="headerlink" title="RDD 编程"></a>RDD 编程</h1><h2 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h2><p>在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。<br>要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/12-Driver-And-Worker.png?raw=true" alt="12-Driver-And-Worker"></p><p>那么这其中的 Dirver、SparkContext、Executor、Master、Worker分别是什么？</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/13-MasterNode-And-WorkerNode.png?raw=true" alt="13-MasterNode-And-WorkerNode"></p><h2 id="创建-RDD"><a href="#创建-RDD" class="headerlink" title="创建 RDD"></a>创建 RDD</h2><p>在Spark中创建RDD的创建方式大概可以分为三种：（1）、从集合中创建RDD；（2）、从外部存储创建RDD；（3）、从其他RDD创建。</p><ol><li>由一个已经存在的Scala集合创建，集合并行化。val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))；而从集合中创建RDD，Spark主要提供了两种函数：parallelize和makeRDD。我们可以先看看这两个函数的声明：</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[(<span class="type">T</span>, <span class="type">Seq</span>[<span class="type">String</span>])]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p>我们可以从上面看出makeRDD有两种实现，而且第一个makeRDD函数接收的参数和parallelize完全一致。其实第一种makeRDD函数实现是依赖了parallelize函数的实现，来看看Spark中是怎么实现这个makeRDD函数的：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  parallelize(seq, numSlices)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们可以看出，这个makeRDD函数完全和parallelize函数一致。但是我们得看看第二种makeRDD函数函数实现了，它接收的参数类型是Seq[(T, Seq[String])]，Spark文档的说明是：<br>Distribute a local Scala collection to form an RDD, with one or more location preferences (hostnames of Spark nodes) for each object. Create a new partition for each collection item.<br>原来，这个函数还为数据提供了位置信息，来看看我们怎么使用：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> guigu1= sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">guigu1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> guigu2 = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">guigu2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">11</span>] at makeRDD at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> seq = <span class="type">List</span>((<span class="number">1</span>, <span class="type">List</span>(<span class="string">"slave01"</span>)),</span><br><span class="line">     | (<span class="number">2</span>, <span class="type">List</span>(<span class="string">"slave02"</span>)))</span><br><span class="line">seq: <span class="type">List</span>[(<span class="type">Int</span>, <span class="type">List</span>[<span class="type">String</span>])] = <span class="type">List</span>((<span class="number">1</span>,<span class="type">List</span>(slave01)),</span><br><span class="line"> (<span class="number">2</span>,<span class="type">List</span>(slave02)))</span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> guigu3 = sc.makeRDD(seq)</span><br><span class="line">guigu3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at makeRDD at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"> </span><br><span class="line">scala&gt; guigu3.preferredLocations(guigu3.partitions(<span class="number">1</span>))</span><br><span class="line">res26: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>(slave02)</span><br><span class="line"> </span><br><span class="line">scala&gt; guigu3.preferredLocations(guigu3.partitions(<span class="number">0</span>))</span><br><span class="line">res27: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>(slave01)</span><br><span class="line"> </span><br><span class="line">scala&gt; guigu1.preferredLocations(guigu1.partitions(<span class="number">0</span>))</span><br><span class="line">res28: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>()</span><br></pre></td></tr></table></figure><p>我们可以看到，makeRDD函数有两种实现，第一种实现其实完全和parallelize一致；而第二种实现可以为数据提供位置信息，而除此之外的实现和parallelize函数也是一致的，如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq, numSlices, <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">String</span>]]())</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[(<span class="type">T</span>, <span class="type">Seq</span>[<span class="type">String</span>])]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">val</span> indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq.map(_._1), seq.size, indexToPrefs)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>都是返回ParallelCollectionRDD，而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。</p><ol start="2"><li>由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等。</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://slave1:9000/txtFile"</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = hdfs:<span class="comment">//slave1:9000/RELEASE MapPartitionsRDD[4] at textFile at &lt;console&gt;:24</span></span><br></pre></td></tr></table></figure><h2 id="RDD-编程-1"><a href="#RDD-编程-1" class="headerlink" title="RDD 编程"></a>RDD 编程</h2><p>RDD一般分为数值RDD和键值对RDD，本章不进行具体区分，先统一来看，下一章会对键值对RDD做专门说明。</p><h3 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h3><p>RDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。常用的 Transformation：</p><ul><li>map(func): 返回通过函数func传递源的每个元素形成的新分布式数据集。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> source  = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">source: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">8</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; source.collect()</span><br><span class="line">res7: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> mapadd = source.map(_ * <span class="number">2</span>)</span><br><span class="line">mapadd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">9</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; mapadd.collect()</span><br><span class="line">res8: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">18</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure><ul><li>filter(func): 返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> sourceFilter = sc.parallelize(<span class="type">Array</span>(<span class="string">"xiaoming"</span>,<span class="string">"xiaojiang"</span>,<span class="string">"xiaohe"</span>,<span class="string">"dazhi"</span>))</span><br><span class="line">sourceFilter: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> filter = sourceFilter.filter(_.contains(<span class="string">"xiao"</span>))</span><br><span class="line">filter: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">11</span>] at filter at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; sourceFilter.collect()</span><br><span class="line">res9: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(xiaoming, xiaojiang, xiaohe, dazhi)</span><br><span class="line"></span><br><span class="line">scala&gt; filter.collect()</span><br><span class="line">res10: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(xiaoming, xiaojiang, xiaohe)</span><br></pre></td></tr></table></figure><ul><li>flatMap(func): 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> sourceFlat = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">sourceFlat: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; sourceFlat.collect()</span><br><span class="line">res11: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> flatMap = sourceFlat.flatMap(<span class="number">1</span> to _)</span><br><span class="line">flatMap: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">13</span>] at flatMap at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; flatMap.collect()</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure><ul><li>mapPartitions(func): 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"kpop"</span>,<span class="string">"female"</span>),(<span class="string">"zorro"</span>,<span class="string">"male"</span>),(<span class="string">"mobin"</span>,<span class="string">"male"</span>),(<span class="string">"lucy"</span>,<span class="string">"female"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">16</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionsFun</span></span>(iter : <span class="type">Iterator</span>[(<span class="type">String</span>,<span class="type">String</span>)]) : <span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">  <span class="keyword">var</span> woman = <span class="type">List</span>[<span class="type">String</span>]()</span><br><span class="line">  <span class="keyword">while</span> (iter.hasNext)&#123;</span><br><span class="line">    <span class="keyword">val</span> next = iter.next()</span><br><span class="line">    next <span class="keyword">match</span> &#123;</span><br><span class="line">       <span class="keyword">case</span> (_,<span class="string">"female"</span>) =&gt; woman = next._1 :: woman</span><br><span class="line">       <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  woman.iterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line"></span><br><span class="line">partitionsFun: (iter: <span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">String</span>)])<span class="type">Iterator</span>[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> result = rdd.mapPartitions(partitionsFun)</span><br><span class="line">result: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">17</span>] at mapPartitions at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; result.collect()</span><br><span class="line">res13: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(kpop, lucy)</span><br></pre></td></tr></table></figure><ul><li>mapPartitionsWithIndex(func): 类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"kpop"</span>,<span class="string">"female"</span>),(<span class="string">"zorro"</span>,<span class="string">"male"</span>),(<span class="string">"mobin"</span>,<span class="string">"male"</span>),(<span class="string">"lucy"</span>,<span class="string">"female"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">18</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionsFun</span></span>(index : <span class="type">Int</span>, iter : <span class="type">Iterator</span>[(<span class="type">String</span>,<span class="type">String</span>)]) : <span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">  <span class="keyword">var</span> woman = <span class="type">List</span>[<span class="type">String</span>]()</span><br><span class="line">  <span class="keyword">while</span> (iter.hasNext)&#123;</span><br><span class="line">    <span class="keyword">val</span> next = iter.next()</span><br><span class="line">    next <span class="keyword">match</span> &#123;</span><br><span class="line">       <span class="keyword">case</span> (_,<span class="string">"female"</span>) =&gt; woman = <span class="string">"["</span>+index+<span class="string">"]"</span>+next._1 :: woman</span><br><span class="line">       <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  woman.iterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line"></span><br><span class="line">partitionsFun: (index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">String</span>)])<span class="type">Iterator</span>[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> result = rdd.mapPartitionsWithIndex(partitionsFun)</span><br><span class="line">result: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">19</span>] at mapPartitionsWithIndex at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; result.collect()</span><br><span class="line">res14: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>([<span class="number">0</span>]kpop, [<span class="number">3</span>]lucy)</span><br></pre></td></tr></table></figure><ul><li>sample(withReplacement, fraction, seed): 以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。例子从RDD中随机且有放回的抽出50%的数据，随机种子值为3（即可能以1 2 3的其中一个起始值）。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">20</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect()</span><br><span class="line">res15: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> sample1 = rdd.sample(<span class="literal">true</span>,<span class="number">0.4</span>,<span class="number">2</span>)</span><br><span class="line">sample1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">PartitionwiseSampledRDD</span>[<span class="number">21</span>] at sample at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; sample1.collect()</span><br><span class="line">res16: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> sample2 = rdd.sample(<span class="literal">false</span>,<span class="number">0.2</span>,<span class="number">3</span>)</span><br><span class="line">sample2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">PartitionwiseSampledRDD</span>[<span class="number">22</span>] at sample at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; sample2.collect()</span><br><span class="line">res17: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">9</span>)</span><br></pre></td></tr></table></figure><ul><li>union(otherDataset): 对源RDD和参数RDD求并集后返回一个新的RDD。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">23</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">5</span> to <span class="number">10</span>)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">24</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = rdd1.union(rdd2)</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">UnionRDD</span>[<span class="number">25</span>] at union at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.collect()</span><br><span class="line">res18: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure><ul><li>intersection(otherDataset): 对源RDD和参数RDD求交集后返回一个新的RDD。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">7</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">26</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">5</span> to <span class="number">10</span>)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">27</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = rdd1.intersection(rdd2)</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">33</span>] at intersection at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.collect()</span><br><span class="line">res19: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure><ul><li>distinct([numTasks])): 对源RDD进行去重后返回一个新的RDD. 默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> distinctRdd = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">9</span>,<span class="number">6</span>,<span class="number">1</span>))</span><br><span class="line">distinctRdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">34</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> unionRDD = distinctRdd.distinct()</span><br><span class="line">unionRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">37</span>] at distinct at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; unionRDD.collect()</span><br><span class="line">res20: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> unionRDD = distinctRdd.distinct(<span class="number">2</span>)</span><br><span class="line">unionRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">40</span>] at distinct at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; unionRDD.collect()</span><br><span class="line">res21: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">6</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure><ul><li>partitionBy(partitioner): 对RDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区， 否则会生成ShuffleRDD。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"aaa"</span>),(<span class="number">2</span>,<span class="string">"bbb"</span>),(<span class="number">3</span>,<span class="string">"ccc"</span>),(<span class="number">4</span>,<span class="string">"ddd"</span>)),<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">44</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res24: <span class="type">Int</span> = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> rdd2 = rdd.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">45</span>] at partitionBy at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.partitions.size</span><br><span class="line">res25: <span class="type">Int</span> = <span class="number">2</span></span><br></pre></td></tr></table></figure><ul><li>reduceByKey(func, [numTasks]): 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"female"</span>,<span class="number">1</span>),(<span class="string">"male"</span>,<span class="number">5</span>),(<span class="string">"female"</span>,<span class="number">5</span>),(<span class="string">"male"</span>,<span class="number">2</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">46</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> reduce = rdd.reduceByKey((x,y) =&gt; x+y)</span><br><span class="line">reduce: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">47</span>] at reduceByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; reduce.collect()</span><br><span class="line">res29: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((female,<span class="number">6</span>), (male,<span class="number">7</span>))</span><br></pre></td></tr></table></figure><ul><li>groupByKey(): groupByKey也是对每个key进行操作，但只生成一个sequence。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> words = <span class="type">Array</span>(<span class="string">"one"</span>, <span class="string">"two"</span>, <span class="string">"two"</span>, <span class="string">"three"</span>, <span class="string">"three"</span>, <span class="string">"three"</span>)</span><br><span class="line">words: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(one, two, two, three, three, three)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> wordPairsRDD = sc.parallelize(words).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">wordPairsRDD: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">4</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> group = wordPairsRDD.groupByKey()</span><br><span class="line">group: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">ShuffledRDD</span>[<span class="number">5</span>] at groupByKey at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; group.collect()</span><br><span class="line">res1: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">Array</span>((two,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">1</span>)), (one,<span class="type">CompactBuffer</span>(<span class="number">1</span>)), (three,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; group.map(t =&gt; (t._1, t._2.sum))</span><br><span class="line">res2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">6</span>] at map at &lt;console&gt;:<span class="number">31</span></span><br><span class="line"></span><br><span class="line">scala&gt; res2.collect()</span><br><span class="line">res3: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((two,<span class="number">2</span>), (one,<span class="number">1</span>), (three,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> map = group.map(t =&gt; (t._1, t._2.sum))</span><br><span class="line">map: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">7</span>] at map at &lt;console&gt;:<span class="number">30</span></span><br><span class="line"></span><br><span class="line">scala&gt; map.collect()</span><br><span class="line">res4: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((two,<span class="number">2</span>), (one,<span class="number">1</span>), (three,<span class="number">3</span>))</span><br></pre></td></tr></table></figure><ul><li>combineByKey[C](  createCombiner: V =&gt; C,  mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): 对相同K，把V合并成一个集合.<ul><li>createCombiner: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。如果这是一个新的元素,combineByKey() 会使用一个叫作 createCombiner() 的函数来创建 那个键对应的累加器的初始值。</li><li>mergeValue: 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并。</li><li>mergeCombiners: 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。</li></ul></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> scores = <span class="type">Array</span>((<span class="string">"Fred"</span>, <span class="number">88</span>), (<span class="string">"Fred"</span>, <span class="number">95</span>), (<span class="string">"Fred"</span>, <span class="number">91</span>), (<span class="string">"Wilma"</span>, <span class="number">93</span>), (<span class="string">"Wilma"</span>, <span class="number">95</span>), (<span class="string">"Wilma"</span>, <span class="number">98</span>))</span><br><span class="line">scores: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="type">Fred</span>,<span class="number">88</span>), (<span class="type">Fred</span>,<span class="number">95</span>), (<span class="type">Fred</span>,<span class="number">91</span>), (<span class="type">Wilma</span>,<span class="number">93</span>), (<span class="type">Wilma</span>,<span class="number">95</span>), (<span class="type">Wilma</span>,<span class="number">98</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> input = sc.parallelize(scores)</span><br><span class="line">input: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">52</span>] at parallelize at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> combine = input.combineByKey(</span><br><span class="line">     |     (v)=&gt;(v,<span class="number">1</span>),</span><br><span class="line">     |     (acc:(<span class="type">Int</span>,<span class="type">Int</span>),v)=&gt;(acc._1+v,acc._2+<span class="number">1</span>),</span><br><span class="line">     |     (acc1:(<span class="type">Int</span>,<span class="type">Int</span>),acc2:(<span class="type">Int</span>,<span class="type">Int</span>))=&gt;(acc1._1+acc2._1,acc1._2+acc2._2))</span><br><span class="line">combine: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = <span class="type">ShuffledRDD</span>[<span class="number">53</span>] at combineByKey at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> result = combine.map&#123;</span><br><span class="line">     |     <span class="keyword">case</span> (key,value) =&gt; (key,value._1/value._2.toDouble)&#125;</span><br><span class="line">result: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">54</span>] at map at &lt;console&gt;:<span class="number">30</span></span><br><span class="line"></span><br><span class="line">scala&gt; result.collect()</span><br><span class="line">res33: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = <span class="type">Array</span>((<span class="type">Wilma</span>,<span class="number">95.33333333333333</span>), (<span class="type">Fred</span>,<span class="number">91.33333333333333</span>))</span><br></pre></td></tr></table></figure><ul><li>aggregateByKey(zeroValue:U,[partitioner: Partitioner]) (seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U): 在kv对的RDD中，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。seqOp函数用于在每一个分区中用初始值逐步迭代value，combOp函数用于合并每个分区中的结果。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">3</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> agg = rdd.aggregateByKey(<span class="number">0</span>)(math.max(_,_),_+_)</span><br><span class="line">agg: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">13</span>] at aggregateByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; agg.collect()</span><br><span class="line">res7: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">3</span>,<span class="number">8</span>), (<span class="number">1</span>,<span class="number">7</span>), (<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; agg.partitions.size</span><br><span class="line">res8: <span class="type">Int</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">1</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> agg = rdd.aggregateByKey(<span class="number">0</span>)(math.max(_,_),_+_).collect()</span><br><span class="line">agg: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>), (<span class="number">3</span>,<span class="number">8</span>), (<span class="number">2</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure><ul><li>foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]: aggregateByKey的简化操作，seqop和combop相同。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">3</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">91</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> agg = rdd.foldByKey(<span class="number">0</span>)(_+_)</span><br><span class="line">agg: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">92</span>] at foldByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; agg.collect()</span><br><span class="line">res61: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">3</span>,<span class="number">14</span>), (<span class="number">1</span>,<span class="number">9</span>), (<span class="number">2</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure><ul><li>sortByKey([ascending], [numTasks]): 在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">3</span>,<span class="string">"aa"</span>),(<span class="number">6</span>,<span class="string">"cc"</span>),(<span class="number">2</span>,<span class="string">"bb"</span>),(<span class="number">1</span>,<span class="string">"dd"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">14</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortByKey(<span class="literal">true</span>).collect()</span><br><span class="line">res9: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">1</span>,dd), (<span class="number">2</span>,bb), (<span class="number">3</span>,aa), (<span class="number">6</span>,cc))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortByKey(<span class="literal">false</span>).collect()</span><br><span class="line">res10: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">6</span>,cc), (<span class="number">3</span>,aa), (<span class="number">2</span>,bb), (<span class="number">1</span>,dd))</span><br></pre></td></tr></table></figure><ul><li>sortBy(func,[ascending], [numTasks]): 与sortByKey类似，但是更灵活,可以用func先对数据进行处理，按照处理后的数据比较结果排序。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">21</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortBy(x =&gt; x).collect()</span><br><span class="line">res11: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortBy(x =&gt; x%<span class="number">3</span>).collect()</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><ul><li>join(otherDataset, [numTasks]): 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">32</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">33</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.join(rdd1).collect()</span><br><span class="line">res13: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">String</span>, <span class="type">Int</span>))] = <span class="type">Array</span>((<span class="number">1</span>,(a,<span class="number">4</span>)), (<span class="number">2</span>,(b,<span class="number">5</span>)), (<span class="number">3</span>,(c,<span class="number">6</span>)))</span><br></pre></td></tr></table></figure><ul><li>cogroup(otherDataset, [numTasks]): 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable\&lt;V>,Iterable\&lt;W>))类型的RDD。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">37</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">38</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.cogroup(rdd1).collect()</span><br><span class="line">res14: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = <span class="type">Array</span>((<span class="number">1</span>,(<span class="type">CompactBuffer</span>(a),<span class="type">CompactBuffer</span>(<span class="number">4</span>))), (<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="number">5</span>))), (<span class="number">3</span>,(<span class="type">CompactBuffer</span>(c),<span class="type">CompactBuffer</span>(<span class="number">6</span>))))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">Array</span>((<span class="number">4</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">41</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.cogroup(rdd2).collect()</span><br><span class="line">res15: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = <span class="type">Array</span>((<span class="number">4</span>,(<span class="type">CompactBuffer</span>(),<span class="type">CompactBuffer</span>(<span class="number">4</span>))), (<span class="number">1</span>,(<span class="type">CompactBuffer</span>(a),<span class="type">CompactBuffer</span>())), (<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="number">5</span>))), (<span class="number">3</span>,(<span class="type">CompactBuffer</span>(c),<span class="type">CompactBuffer</span>(<span class="number">6</span>))))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">1</span>,<span class="string">"d"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">44</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.cogroup(rdd2).collect()</span><br><span class="line">res16: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = <span class="type">Array</span>((<span class="number">4</span>,(<span class="type">CompactBuffer</span>(),<span class="type">CompactBuffer</span>(<span class="number">4</span>))), (<span class="number">1</span>,(<span class="type">CompactBuffer</span>(d, a),<span class="type">CompactBuffer</span>())), (<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="number">5</span>))), (<span class="number">3</span>,(<span class="type">CompactBuffer</span>(c),<span class="type">CompactBuffer</span>(<span class="number">6</span>))))</span><br></pre></td></tr></table></figure><ul><li>cartesian(otherDataset): 笛卡尔积。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">3</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">47</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">2</span> to <span class="number">5</span>)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">48</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.cartesian(rdd2).collect()</span><br><span class="line">res17: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="number">2</span>), (<span class="number">1</span>,<span class="number">3</span>), (<span class="number">1</span>,<span class="number">4</span>), (<span class="number">1</span>,<span class="number">5</span>), (<span class="number">2</span>,<span class="number">2</span>), (<span class="number">2</span>,<span class="number">3</span>), (<span class="number">2</span>,<span class="number">4</span>), (<span class="number">2</span>,<span class="number">5</span>), (<span class="number">3</span>,<span class="number">2</span>), (<span class="number">3</span>,<span class="number">3</span>), (<span class="number">3</span>,<span class="number">4</span>), (<span class="number">3</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure><ul><li>pipe(command, [envVars]): 对于每个分区，都执行一个perl或者shell脚本，返回输出的RDD。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="string">"hi"</span>,<span class="string">"Hello"</span>,<span class="string">"how"</span>,<span class="string">"are"</span>,<span class="string">"you"</span>),<span class="number">1</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">50</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.pipe(<span class="string">"/home/spark_shell/pipe.sh"</span>).collect()</span><br><span class="line">res18: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="type">AA</span>, &gt;&gt;&gt;hi, &gt;&gt;&gt;<span class="type">Hello</span>, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="string">"hi"</span>,<span class="string">"Hello"</span>,<span class="string">"how"</span>,<span class="string">"are"</span>,<span class="string">"you"</span>),<span class="number">2</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">52</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.pipe(<span class="string">"/home/spark_shell/pipe.sh"</span>).collect()</span><br><span class="line">res19: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="type">AA</span>, &gt;&gt;&gt;hi, &gt;&gt;&gt;<span class="type">Hello</span>, <span class="type">AA</span>, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pipe.sh:</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">echo "AA"</span><br><span class="line">while read LINE; do</span><br><span class="line">   echo "&gt;&gt;&gt;"$&#123;LINE&#125;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><ul><li>coalesce(numPartitions): 缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">16</span>,<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">54</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res20: <span class="type">Int</span> = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> coalesceRDD = rdd.coalesce(<span class="number">3</span>)</span><br><span class="line">coalesceRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">CoalescedRDD</span>[<span class="number">55</span>] at coalesce at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; coalesceRDD.partitions.size</span><br><span class="line">res21: <span class="type">Int</span> = <span class="number">3</span></span><br></pre></td></tr></table></figure><ul><li>repartition(numPartitions): 根据分区数，从新通过网络随机洗牌所有数据。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">16</span>,<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">56</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res22: <span class="type">Int</span> = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rerdd = rdd.repartition(<span class="number">2</span>)</span><br><span class="line">rerdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">60</span>] at repartition at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rerdd.partitions.size</span><br><span class="line">res23: <span class="type">Int</span> = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rerdd = rdd.repartition(<span class="number">4</span>)</span><br><span class="line">rerdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">64</span>] at repartition at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rerdd.partitions.size</span><br><span class="line">res24: <span class="type">Int</span> = <span class="number">4</span></span><br></pre></td></tr></table></figure><ul><li>glom(): 将每一个分区形成一个数组，形成新的RDD类型时RDD[Array[T]]。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">16</span>,<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">65</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.glom().collect()</span><br><span class="line">res25: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="type">Array</span>(<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>), <span class="type">Array</span>(<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>), <span class="type">Array</span>(<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>))</span><br></pre></td></tr></table></figure><ul><li>mapValues(func):    针对于(K,V)形式的类型只对V进行操作。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">1</span>,<span class="string">"d"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">67</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.mapValues(_+<span class="string">"|||"</span>).collect()</span><br><span class="line">res26: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">1</span>,a|||), (<span class="number">1</span>,d|||), (<span class="number">2</span>,b|||), (<span class="number">3</span>,c|||))</span><br></pre></td></tr></table></figure><ul><li>subtract(other:RDD): 计算差的一种函数去除两个RDD中相同的元素，不同的RDD将保留下来 。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">3</span> to <span class="number">8</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">70</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">71</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.subtract(rdd1).collect()</span><br><span class="line">res27: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">8</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure><h3 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h3><p>常用的 Action 如下：</p><ul><li>reduce(func): 通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">85</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.reduce(_+_)</span><br><span class="line">res50: <span class="type">Int</span> = <span class="number">55</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="type">Array</span>((<span class="string">"a"</span>,<span class="number">1</span>),(<span class="string">"a"</span>,<span class="number">3</span>),(<span class="string">"c"</span>,<span class="number">3</span>),(<span class="string">"d"</span>,<span class="number">5</span>)))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">86</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.reduce((x,y)=&gt;(x._1 + y._1,x._2 + y._2))</span><br><span class="line">res51: (<span class="type">String</span>, <span class="type">Int</span>) = (adca,<span class="number">12</span>)</span><br></pre></td></tr></table></figure><ul><li>collect(): 在驱动程序中，以数组的形式返回数据集的所有元素。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">17</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect()</span><br><span class="line">res7: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure><ul><li>count(): 返回RDD的元素个数。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">18</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count()</span><br><span class="line">res8: <span class="type">Long</span> = <span class="number">10</span></span><br></pre></td></tr></table></figure><ul><li>first(): 返回RDD的第一个元素（类似于take(1)）。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">19</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.first()</span><br><span class="line">res9: <span class="type">Int</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure><ul><li>take(n): 返回一个由数据集的前n个元素组成的数组。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">20</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.take(<span class="number">7</span>)</span><br><span class="line">res10: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure><ul><li>takeSample(withReplacement,num, [seed]): 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">21</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.takeSample(<span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">res11: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">6</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><ul><li>takeOrdered(n): 返回前几个的排序。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Seq</span>(<span class="number">10</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">19</span>, <span class="number">4</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">23</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.top(<span class="number">2</span>)</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">19</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.takeOrdered(<span class="number">2</span>)</span><br><span class="line">res13: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure><ul><li>aggregate (zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U): aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">88</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(<span class="number">1</span>)(</span><br><span class="line">     | &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x + y&#125;,</span><br><span class="line">     | &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">     | )</span><br><span class="line">res56: <span class="type">Int</span> = <span class="number">58</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(<span class="number">1</span>)(</span><br><span class="line">     | &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x * y&#125;,</span><br><span class="line">     | &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">     | )</span><br><span class="line">res57: <span class="type">Int</span> = <span class="number">30361</span></span><br></pre></td></tr></table></figure><ul><li>fold(num)(func): 折叠操作，aggregate的简化操作，seqop和combop一样。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">90</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(<span class="number">1</span>)(</span><br><span class="line">     | &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x + y&#125;,</span><br><span class="line">     | &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">     | )</span><br><span class="line">res59: <span class="type">Int</span> = <span class="number">13</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.fold(<span class="number">1</span>)(_+_)</span><br><span class="line">res60: <span class="type">Int</span> = <span class="number">13</span></span><br></pre></td></tr></table></figure><ul><li>countByKey(): 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">3</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">95</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.countByKey()</span><br><span class="line">res63: scala.collection.<span class="type">Map</span>[<span class="type">Int</span>,<span class="type">Long</span>] = <span class="type">Map</span>(<span class="number">3</span> -&gt; <span class="number">2</span>, <span class="number">1</span> -&gt; <span class="number">3</span>, <span class="number">2</span> -&gt; <span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li>foreach(func): 在数据集的每一个元素上，运行函数func进行更新。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">30</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.collect().foreach(println)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure><ul><li>saveAsTextFile(path): 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本。</li><li>saveAsSequenceFile(path): 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。</li><li>saveAsObjectFile(path): 用于将RDD中的元素序列化成对象，存储到文件中。</li></ul><h3 id="数值-RDD-的统计操作"><a href="#数值-RDD-的统计操作" class="headerlink" title="数值 RDD 的统计操作"></a>数值 RDD 的统计操作</h3><p>Spark对包含数值数据的RDD提供了一些描述性的统计操作。Spark的数值操作是通过流式算法实现的，允许以每次一个元素的方式构建出模型。这些统计数据都会在调用stats()时通过一次遍历数据计算出来，并以StatsCounter对象返回。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/14-RDD-Statistical-Operation.png?raw=true" alt="14-RDD-Statistical-Operation"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">42</span>] at makeRDD at &lt;console&gt;:<span class="number">32</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.sum()</span><br><span class="line">res34: <span class="type">Double</span> = <span class="number">5050.0</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.max()</span><br><span class="line">res35: <span class="type">Int</span> = <span class="number">100</span></span><br></pre></td></tr></table></figure><h3 id="向RDD操作传递函数注意"><a href="#向RDD操作传递函数注意" class="headerlink" title="向RDD操作传递函数注意"></a>向RDD操作传递函数注意</h3><p>Spark的大部分转化操作和一部分行动操作，都需要依赖用户传递的函数来计算。在Scala中，我们可以把定义的内联函数、方法的引用或静态方法传递给Spark，就像Scala的其他函数式API一样。我们还要考虑其他一些细节，比如所传递的函数及其引用的数据需要是可序列化的(实现了Java的Serializable接口)。传递一个对象的方法或者字段时，会包含对整个对象的引用。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SearchFunctions</span>(<span class="params">val query: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    s.contains(query)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatchesFunctionReference</span></span>(rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>]): org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="comment">// 问题:"isMatch"表示"this.isMatch"，因此我们要传递整个"this" </span></span><br><span class="line">    rdd.filter(isMatch)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatchesFieldReference</span></span>(rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>]): org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = &#123; </span><br><span class="line"><span class="comment">// 问题:"query"表示"this.query"，因此我们要传递整个"this" </span></span><br><span class="line">rdd.filter(x =&gt; x.contains(query)) </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatchesNoReference</span></span>(rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>]): org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="comment">// 安全:只把我们需要的字段拿出来放入局部变量中 </span></span><br><span class="line"><span class="keyword">val</span> query_ = <span class="keyword">this</span>.query</span><br><span class="line">    rdd.filter(x =&gt; x.contains(query_))</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果在Scala中出现了NotSerializableException，通常问题就在于我们传递了一个不可序列化的类中的函数或字段。</p><h3 id="在不同-RDD-类型间转换"><a href="#在不同-RDD-类型间转换" class="headerlink" title="在不同 RDD 类型间转换"></a>在不同 RDD 类型间转换</h3><p>有些函数只能用于特定类型的RDD，比如mean()和variance()只能用在数值RDD上，而join()只能用在键值对RDD上。在Scala和Java中，这些函数都没有定义在标准的RDD类中，所以要访问这些附加功能，必须要确保获得了正确的专用RDD类。<br>在Scala中，将RDD转为有特定函数的RDD(比如在RDD[Double]上进行数值操作)是由隐式转换来自动处理的。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/15-RDD-In-IDEA.png?raw=true" alt="15-RDD-In-IDEA"></p><h2 id="RDD-持久化"><a href="#RDD-持久化" class="headerlink" title="RDD 持久化"></a>RDD 持久化</h2><h3 id="RDD-的缓存"><a href="#RDD-的缓存" class="headerlink" title="RDD 的缓存"></a>RDD 的缓存</h3><p>Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存个数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。如果一个有持久化数据的节点发生故障，Spark会在需要用到缓存的数据时重算丢失的数据分区。如果希望节点故障的情况不会拖累我们的执行速度，也可以把数据备份到多个节点上。</p><h3 id="RDD-的缓存方式"><a href="#RDD-的缓存方式" class="headerlink" title="RDD 的缓存方式"></a>RDD 的缓存方式</h3><p>RDD通过persist方法或cache方法可以将前面的计算结果缓存，默认情况下persist()会把数据以序列化的形式缓存在JVM的堆空间中。<br>但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/16-RDD-Persist.png?raw=true" alt="16-RDD-Persist"></p><p>通过查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在objectStorageLevel中定义的。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">19</span>] at makeRDD at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> nocache = rdd.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">nocache: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">20</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> cache =  rdd.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">cache: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">21</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; cache.cache</span><br><span class="line">res24: cache.<span class="keyword">type</span> = <span class="type">MapPartitionsRDD</span>[<span class="number">21</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; nocache.collect</span><br><span class="line">res25: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479375155</span>], <span class="number">2</span>[<span class="number">1505479374674</span>], <span class="number">3</span>[<span class="number">1505479374674</span>], <span class="number">4</span>[<span class="number">1505479375153</span>], <span class="number">5</span>[<span class="number">1505479375153</span>], <span class="number">6</span>[<span class="number">1505479374675</span>], <span class="number">7</span>[<span class="number">1505479375154</span>], <span class="number">8</span>[<span class="number">1505479375154</span>], <span class="number">9</span>[<span class="number">1505479374676</span>], <span class="number">10</span>[<span class="number">1505479374676</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; nocache.collect</span><br><span class="line">res26: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479375679</span>], <span class="number">2</span>[<span class="number">1505479376157</span>], <span class="number">3</span>[<span class="number">1505479376157</span>], <span class="number">4</span>[<span class="number">1505479375680</span>], <span class="number">5</span>[<span class="number">1505479375680</span>], <span class="number">6</span>[<span class="number">1505479376159</span>], <span class="number">7</span>[<span class="number">1505479375680</span>], <span class="number">8</span>[<span class="number">1505479375680</span>], <span class="number">9</span>[<span class="number">1505479376158</span>], <span class="number">10</span>[<span class="number">1505479376158</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; nocache.collect</span><br><span class="line">res27: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479376743</span>], <span class="number">2</span>[<span class="number">1505479377218</span>], <span class="number">3</span>[<span class="number">1505479377218</span>], <span class="number">4</span>[<span class="number">1505479376745</span>], <span class="number">5</span>[<span class="number">1505479376745</span>], <span class="number">6</span>[<span class="number">1505479377219</span>], <span class="number">7</span>[<span class="number">1505479376747</span>], <span class="number">8</span>[<span class="number">1505479376747</span>], <span class="number">9</span>[<span class="number">1505479377218</span>], <span class="number">10</span>[<span class="number">1505479377218</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; cache.collect</span><br><span class="line">res28: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479382745</span>], <span class="number">2</span>[<span class="number">1505479382253</span>], <span class="number">3</span>[<span class="number">1505479382253</span>], <span class="number">4</span>[<span class="number">1505479382748</span>], <span class="number">5</span>[<span class="number">1505479382748</span>], <span class="number">6</span>[<span class="number">1505479382257</span>], <span class="number">7</span>[<span class="number">1505479382747</span>], <span class="number">8</span>[<span class="number">1505479382747</span>], <span class="number">9</span>[<span class="number">1505479382253</span>], <span class="number">10</span>[<span class="number">1505479382253</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; cache.collect</span><br><span class="line">res29: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479382745</span>], <span class="number">2</span>[<span class="number">1505479382253</span>], <span class="number">3</span>[<span class="number">1505479382253</span>], <span class="number">4</span>[<span class="number">1505479382748</span>], <span class="number">5</span>[<span class="number">1505479382748</span>], <span class="number">6</span>[<span class="number">1505479382257</span>], <span class="number">7</span>[<span class="number">1505479382747</span>], <span class="number">8</span>[<span class="number">1505479382747</span>], <span class="number">9</span>[<span class="number">1505479382253</span>], <span class="number">10</span>[<span class="number">1505479382253</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; cache.collect</span><br><span class="line">res30: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479382745</span>], <span class="number">2</span>[<span class="number">1505479382253</span>], <span class="number">3</span>[<span class="number">1505479382253</span>], <span class="number">4</span>[<span class="number">1505479382748</span>], <span class="number">5</span>[<span class="number">1505479382748</span>], <span class="number">6</span>[<span class="number">1505479382257</span>], <span class="number">7</span>[<span class="number">1505479382747</span>], <span class="number">8</span>[<span class="number">1505479382747</span>], <span class="number">9</span>[<span class="number">1505479382253</span>], <span class="number">10</span>[<span class="number">1505479382253</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; cache.persist(org.apache.spark.storage.<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br></pre></td></tr></table></figure><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/17-RDD-StorageLevel.png?raw=true" alt="17-RDD-StorageLevel"></p><p>在存储级别的末尾加上“_2”来把持久化数据存为两份</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/18-RDD-StorageLevel-Description.png?raw=true" alt="18-RDD-StorageLevel-Description"></p><p>缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。</p><p>注意：使用Tachyon可以实现堆外缓存。</p><h2 id="RDD-CheckPoint-机制"><a href="#RDD-CheckPoint-机制" class="headerlink" title="RDD CheckPoint 机制"></a>RDD CheckPoint 机制</h2><p>Spark中对于数据的保存除了持久化操作之外，还提供了一种CheckPoint的机制，CheckPoint（本质是通过将RDD写入Disk做CheckPoint）是为了通过lineage做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做CheckPoint容错，如果之后有节点出现问题而丢失分区，从做CheckPoint的RDD开始重做Lineage，就会减少开销。CheckPoint通过将数据写入到HDFS文件系统实现了RDD的CheckPoint功能。<br>cache和CheckPoint是有显著区别的，缓存把RDD计算出来然后放在内存中，但是RDD的依赖链（相当于数据库中的redo日志），也不能丢掉，当某个点某个executor宕了，上面cache的RDD就会丢掉，需要通过依赖链重放计算出来，不同的是，CheckPoint是把RDD保存在HDFS中，是多副本可靠存储，所以依赖链就可以丢掉了，就斩断了依赖链，是通过复制实现的高容错。<br>如果存在以下场景，则比较适合使用CheckPoint机制：</p><ol><li>DAG中的Lineage过长，如果重算，则开销太大（如在PageRank中）。</li><li>在宽依赖上做Checkpoint获得的收益更大。</li></ol><p>为当前RDD设置 CheckPoint。该函数将会创建一个二进制的文件，并存储到checkpoint目录中，该目录是用SparkContext.setCheckpointDir()设置的。在checkpoint的过程中，该RDD的所有依赖于父RDD中的信息将全部被移出。对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">100</span> , <span class="number">5</span>)</span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] =</span><br><span class="line">　　<span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at parallelize at &lt;console&gt;:<span class="number">12</span></span><br><span class="line"> </span><br><span class="line">scala&gt; sc.setCheckpointDir(<span class="string">"hdfs://slave1:9000/checkpoint"</span>)</span><br><span class="line"> </span><br><span class="line">scala&gt; data.checkpoint</span><br><span class="line"> </span><br><span class="line">scala&gt; data.count</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ch1 = sc.parallelize(<span class="number">1</span> to <span class="number">2</span>)</span><br><span class="line">ch1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">33</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ch2 = ch1.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">ch2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">36</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ch3 = ch1.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">ch3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">37</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; ch3.checkpoint</span><br><span class="line"></span><br><span class="line">scala&gt; ch2.collect</span><br><span class="line">res62: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480940726</span>], <span class="number">2</span>[<span class="number">1505480940243</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; ch2.collect</span><br><span class="line">res63: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480941957</span>], <span class="number">2</span>[<span class="number">1505480941480</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; ch2.collect</span><br><span class="line">res64: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480942736</span>], <span class="number">2</span>[<span class="number">1505480942257</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; ch3.collect</span><br><span class="line">res65: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480949080</span>], <span class="number">2</span>[<span class="number">1505480948603</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; ch3.collect</span><br><span class="line">res66: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480948683</span>], <span class="number">2</span>[<span class="number">1505480949161</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; ch3.collect</span><br><span class="line">res67: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480948683</span>], <span class="number">2</span>[<span class="number">1505480949161</span>])</span><br></pre></td></tr></table></figure><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/19-RDD-CheckPoint.png?raw=true" alt="19-RDD-CheckPoint"></p><h3 id="CheckPoint-写流程"><a href="#CheckPoint-写流程" class="headerlink" title="CheckPoint 写流程"></a>CheckPoint 写流程</h3><p>RDD checkpoint 过程中会经过以下几个状态：[ Initialized → marked for checkpointing → checkpointing in progress → checkpointed ]，转换流程如下：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/20-RDD-CheckPoint-Transforme.png?raw=true" alt="20-RDD-CheckPoint-Transforme"></p><ol><li>data.CheckPoint这个函数调用中，设置的目录中，所有依赖的RDD都会被删除，函数必须在job运行之前调用执行，强烈建议RDD缓存在内存中（又提到一次，千万要注意哟），否则保存到文件的时候需要从头计算。初始化RDD的CheckPointData变量为ReliableRDDCheckpointData。这时候标记为Initialized状态。</li><li>在所有jobaction的时候，runJob方法中都会调用rdd.doCheckpoint,这个会向前递归调用所有的依赖的RDD，看看需不需要CheckPoint。需要需要CheckPoint，然后调用CheckPointData.get.CheckPoint()，里面标记状态为CheckpointingInProgress，里面调用具体实现类的ReliableRDDCheckpointData的doCheckpoint方法。</li><li>doCheckpoint-&gt;writeRDDToCheckpointDirectory，注意这里会把job再运行一次，如果已经cache了，就可以直接使用缓存中的RDD了，就不需要重头计算一遍了（怎么又说了一遍），这时候直接把RDD，输出到hdfs，每个分区一个文件，会先写到一个临时文件，如果全部输出完，进行rename，如果输出失败，就回滚delete。</li><li>标记状态为Checkpointed，markCheckpointed方法中清除所有的依赖，怎么清除依赖的呢，就是把RDD变量的强引用设置为null，垃圾回收了，会触发ContextCleaner里面监听清除实际BlockManager缓存中的数据。</li></ol><h3 id="CheckPoint-读流程"><a href="#CheckPoint-读流程" class="headerlink" title="CheckPoint 读流程"></a>CheckPoint 读流程</h3><p>如果一个RDD我们已经CheckPoint了那么是什么时候用呢，CheckPoint将RDD持久化到HDFS或本地文件夹，如果不被手动remove掉，是一直存在的，也就是说可以被下一个driverprogram使用。比如sparkstreaming挂掉了，重启后就可以使用之前CheckPoint的数据进行recover,当然在同一个driverprogram也可以使用。我们讲下在同一个driverprogram中是怎么使用CheckPoint数据的。<br>如果一个RDD被CheckPoint了，当这个RDD上有action操作时候，或者回溯的这个RDD的时候，触发这个RDD进行计算，里面判断是否CheckPoint过，对分区和依赖的处理都是使用的RDD内部的CheckPointRDD变量。<br>具体细节如下：如果一个RDD被CheckPoint了，那么这个RDD中对分区和依赖的处理都是使用的RDD内部的CheckPointRDD变量，具体实现是ReliableCheckpointRDD类型。这个是在CheckPoint写流程中创建的。依赖和获取分区方法中先判断是否已经CheckPoint，如果已经CheckPoint了，就斩断依赖，使用ReliableCheckpointRDD，来处理依赖和获取分区。如果没有，才往前回溯依赖。依赖就是没有依赖，因为已经斩断了依赖，获取分区数据就是读取CheckPoint到HDFS目录中不同分区保存下来的文件。</p><h2 id="RDD-的依赖关系"><a href="#RDD-的依赖关系" class="headerlink" title="RDD 的依赖关系"></a>RDD 的依赖关系</h2><p>RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/21-RDD-Narrow-And-Wide-Dependencies.png?raw=true" alt="21-RDD-Narrow-And-Wide-Dependencies"></p><ol><li>窄依赖：窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用。总结：窄依赖我们形象的比喻为独生子女。</li><li>宽依赖：宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition，会引起shuffle。总结：宽依赖我们形象的比喻为超生。</li><li>Lineage：RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/22-RDD-Lineage.png?raw=true" alt="22-RDD-Lineage"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> text = sc.textFile(<span class="string">"README.md"</span>)</span><br><span class="line">text: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">README</span>.md <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> words = text.flatMap(_.split)</span><br><span class="line">split   splitAt</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> words = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">words: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at flatMap at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; words.map((_,<span class="number">1</span>))</span><br><span class="line">res0: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at &lt;console&gt;:<span class="number">29</span></span><br><span class="line"></span><br><span class="line">scala&gt; res0.reduceByKey</span><br><span class="line">reduceByKey   reduceByKeyLocally</span><br><span class="line"></span><br><span class="line">scala&gt; res0.reduceByKey(_+_)</span><br><span class="line">res1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">4</span>] at reduceByKey at &lt;console&gt;:<span class="number">31</span></span><br><span class="line"></span><br><span class="line">scala&gt; res1.dependencies</span><br><span class="line">res2: <span class="type">Seq</span>[org.apache.spark.<span class="type">Dependency</span>[_]] = <span class="type">List</span>(org.apache.spark.<span class="type">ShuffleDependency</span>@<span class="number">6</span>cfe48a4)</span><br><span class="line"></span><br><span class="line">scala&gt; res0.dependencies</span><br><span class="line">res3: <span class="type">Seq</span>[org.apache.spark.<span class="type">Dependency</span>[_]] = <span class="type">List</span>(org.apache.spark.<span class="type">OneToOneDependency</span>@<span class="number">6</span>c9e24c4)</span><br></pre></td></tr></table></figure><h2 id="DAG-的生成"><a href="#DAG-的生成" class="headerlink" title="DAG 的生成"></a>DAG 的生成</h2><p>DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/23-DAG.png?raw=true" alt="23-DAG"></p><h2 id="RDD-相关概念关系"><a href="#RDD-相关概念关系" class="headerlink" title="RDD 相关概念关系"></a>RDD 相关概念关系</h2><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/24-RDD-Partition.png?raw=true" alt="24-RDD-Partition"></p><p>输入可能以多个文件的形式存储在HDFS上，每个File都包含了很多块，称为Block。<br>当Spark读取这些文件作为输入时，会根据具体数据格式对应的InputFormat进行解析，一般是将若干个Block合并成一个输入分片，称为InputSplit，注意InputSplit不能跨越文件。<br>随后将为这些输入分片生成具体的Task。InputSplit与Task是一一对应的关系。<br>随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。</p><ol><li>每个节点可以起一个或多个Executor。</li><li>每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。</li><li>每个Task执行的结果就是生成了目标RDD的一个partiton。</li></ol><p>注意: 这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。<br>而Task被执行的并发度 = Executor数目 * 每个Executor核数。<br>至于partition的数目：</p><ol><li>对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。</li><li>在Map阶段partition数目保持不变。</li><li>在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。</li></ol><p>RDD在计算的时候，每个分区都会起一个task，所以rdd的分区数目决定了总的的task数目。<br>申请的计算节点（Executor）数目和每个计算节点核数，决定了你同一时刻可以并行执行的task。<br>比如的RDD有100个分区，那么计算的时候就会生成100个task，你的资源配置为10个计算节点，每个两2个核，同一时刻可以并行的task数目为20，计算这个RDD就需要5个轮次。<br>如果计算资源不变，你有101个task的话，就需要6个轮次，在最后一轮中，只有一个task在执行，其余核都在空转。<br>如果资源不变，你的RDD只有2个分区，那么同一时刻只有2个task运行，其余18个核空转，造成资源浪费。这就是在spark调优中，增大RDD分区数目，增大任务并行度的做法。</p><h1 id="键值对-RDD"><a href="#键值对-RDD" class="headerlink" title="键值对 RDD"></a>键值对 RDD</h1><p>键值对RDD是Spark中许多操作所需要的常见数据类型。本章做特别讲解。除了在基础RDD类中定义的操作之外，Spark为包含键值对类型的RDD提供了一些专有的操作在PairRDDFunctions专门进行了定义。这些RDD被称为pairRDD。<br>有很多种方式创建pairRDD，在输入输出章节会讲解。一般如果从一个普通的RDD转为pairRDD时，可以调用map()函数来实现，传递的函数需要返回键值对。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pairs = lines.map(x =&gt; (x.split(<span class="string">" "</span>)(<span class="number">0</span>), x))</span><br></pre></td></tr></table></figure><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/25-PairRDDFunctions-In-IDEA.png?raw=true" alt="25-PairRDDFunctions-In-IDEA"></p><h2 id="Pair-RDD-的-Transformation-操作"><a href="#Pair-RDD-的-Transformation-操作" class="headerlink" title="Pair RDD 的 Transformation 操作"></a>Pair RDD 的 Transformation 操作</h2><h3 id="转化操作"><a href="#转化操作" class="headerlink" title="转化操作"></a>转化操作</h3><p>上一章进行了练习，这一章会重点讲解。针对一个Pair RDD的转化操作：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/26-PairRDD-Transformation-1.png?raw=true" alt="26-PairRDD-Transformation-1"></p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/27-PairRDD-Transformation-2.png?raw=true" alt="27-PairRDD-Transformation-2"></p><h3 id="聚合操作"><a href="#聚合操作" class="headerlink" title="聚合操作"></a>聚合操作</h3><p>当数据集以键值对形式组织的时候，聚合具有相同键的元素进行一些统计是很常见的操作。之前讲解过基础RDD上的fold()、combine()、reduce()等行动操作，pairRDD上则有相应的针对键的转化操作。Spark有一组类似的操作，可以组合具有相同键的值。这些操作返回RDD，因此它们是转化操作而不是行动操作。<br>reduceByKey()与reduce()相当类似;它们都接收一个函数，并使用该函数对值进行合并。reduceByKey()会为数据集中的每个键进行并行的归约操作，每个归约操作会将键相同的值合并起来。因为数据集中可能有大量的键，所以reduceByKey()没有被实现为向用户程序返回一个值的行动操作。实际上，它会返回一个由各键和对应键归约出来的结果值组成的新的RDD。<br>foldByKey()则与fold()相当类似;它们都使用一个与RDD和合并函数中的数据类型相同的零值作为初始值。与fold()一样，foldByKey()操作所使用的合并函数对零值与另一个元素进行合并，结果仍为该元素。<br>求均值操作：版本一</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input.mapValues(x =&gt; (x, <span class="number">1</span>)).reduceByKey((x, y) =&gt; (x._1 + y._1, x._2 + y._2)).map&#123; <span class="keyword">case</span> (key, value) =&gt; (key, value._1 / value._2.toFloat) &#125;</span><br></pre></td></tr></table></figure><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/28-PairRDD-MapReduce.png?raw=true" alt="28-PairRDD-MapReduce"></p><p>combineByKey()是最为常用的基于键进行聚合的函数。大多数基于键聚合的函数都是用它实现的。和aggregate()一样，combineByKey()可以让用户返回与输入数据的类型不同的返回值。<br>要理解combineByKey()，要先理解它在处理数据时是如何处理每个元素的。由于combineByKey()会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。<br>如果这是一个新的元素，combineByKey()会使用一个叫作createCombiner()的函数来创建那个键对应的累加器的初始值。需要注意的是，这一过程会在每个分区中第一次出现各个键时发生，而不是在整个RDD中第一次出现一个键时发生。<br>如果这是一个在处理当前分区之前已经遇到的键，它会使用mergeValue()方法将该键的累加器对应的当前值与这个新的值进行合并。<br>由于每个分区都是独立处理的，因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器，就需要使用用户提供的mergeCombiners()方法将各个分区的结果进行合并。</p><p>求均值操作：版本二</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> result = input.combineByKey(</span><br><span class="line">  (v) =&gt; (v, <span class="number">1</span>),</span><br><span class="line">  (acc: (<span class="type">Int</span>, <span class="type">Int</span>), v) =&gt; (acc._1 + v, acc._2 + <span class="number">1</span>),</span><br><span class="line">  (acc1: (<span class="type">Int</span>, <span class="type">Int</span>), acc2: (<span class="type">Int</span>, <span class="type">Int</span>)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)</span><br><span class="line">).map&#123; <span class="keyword">case</span> (key, value) =&gt; (key, value._1 / value._2.toFloat) &#125;</span><br><span class="line"></span><br><span class="line">result.collectAsMap().map(println(_))</span><br></pre></td></tr></table></figure><h3 id="数据分组"><a href="#数据分组" class="headerlink" title="数据分组"></a>数据分组</h3><p>如果数据已经以预期的方式提取了键，groupByKey()就会使用RDD中的键来对数据进行分组。对于一个由类型K的键和类型V的值组成的RDD，所得到的结果RDD类型会是[K,Iterable[V]]。<br>groupBy()可以用于未成对的数据上，也可以根据除键相同以外的条件进行分组。它可以接收一个函数，对源RDD中的每个元素使用该函数，将返回结果作为键再进行分组。<br>多个RDD分组，可以使用cogroup函数，cogroup()的函数对多个共享同一个键的RDD进行分组。对两个键的类型均为K而值的类型分别为V和W的RDD进行cogroup()时，得到的结果RDD类型为[(K,(Iterable[V],Iterable[W]))]。如果其中的一个RDD对于另一个RDD中存在的某个键没有对应的记录，那么对应的迭代器则为空。cogroup()提供了为多个RDD进行数据分组的方法。</p><h3 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h3><p>连接主要用于多个PairRDD的操作，连接方式多种多样:右外连接、左外连接、交叉连接以及内连接。<br>普通的join操作符表示内连接2。只有在两个pairRDD中都存在的键才叫输出。当一个输入对应的某个键有多个值时，生成的pairRDD会包括来自两个输入RDD的每一组相对应的记录。<br>leftOuterJoin()产生的pairRDD中，源RDD的每一个键都有对应的记录。每个键相应的值是由一个源RDD中的值与一个包含第二个RDD的值的Option(在Java中为Optional)对象组成的二元组。<br>rightOuterJoin()几乎与leftOuterJoin()完全一样，只不过预期结果中的键必须出现在第二个RDD中，而二元组中的可缺失的部分则来自于源RDD而非第二个RDD。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/29-PairRDD-Connection.png?raw=true" alt="29-PairRDD-Connection"></p><h3 id="数据排序"><a href="#数据排序" class="headerlink" title="数据排序"></a>数据排序</h3><p>sortByKey()函数接收一个叫作ascending的参数，表示我们是否想要让结果按升序排序(默认值为true)。</p><h2 id="Pair-RDD-的-Action-操作"><a href="#Pair-RDD-的-Action-操作" class="headerlink" title="Pair RDD 的 Action 操作"></a>Pair RDD 的 Action 操作</h2><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/28-PairRDD-Actions.png?raw=true" alt="28-PairRDD-Actions"></p><h2 id="Pair-RDD-的数据分区"><a href="#Pair-RDD-的数据分区" class="headerlink" title="Pair RDD 的数据分区"></a>Pair RDD 的数据分区</h2><p>Spark目前支持Hash分区和Range分区，用户也可以自定义分区，Hash分区为当前的默认分区，Spark中分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle过程属于哪个分区和Reduce的个数。注意：</p><ol><li>只有Key-Value类型的RDD才有分区的，非Key-Value类型的RDD分区的值是None.</li><li>每个RDD的分区ID范围：0~numPartitions-1，决定这个值是属于那个分区的。</li></ol><h3 id="获取-RDD-的分区方式"><a href="#获取-RDD-的分区方式" class="headerlink" title="获取 RDD 的分区方式"></a>获取 RDD 的分区方式</h3><p>可以通过使用RDD的partitioner属性来获取RDD的分区方式。它会返回一个scala.Option对象，通过get方法获取其中的值。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> pairs = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">3</span>)))</span><br><span class="line">pairs: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">33</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; pairs.partitioner</span><br><span class="line">res26: <span class="type">Option</span>[org.apache.spark.<span class="type">Partitioner</span>] = <span class="type">None</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> partitioned = pairs.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">partitioned: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">34</span>] at partitionBy at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; partitioned.partitioner</span><br><span class="line">res27: <span class="type">Option</span>[org.apache.spark.<span class="type">Partitioner</span>] = <span class="type">Some</span>(org.apache.spark.<span class="type">HashPartitioner</span>@<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h3 id="Hash-分区方式"><a href="#Hash-分区方式" class="headerlink" title="Hash 分区方式"></a>Hash 分区方式</h3><p>HashPartitioner分区的原理：对于给定的key，计算其hashCode，并除于分区的个数取余，如果余数小于0，则用余数+分区的个数，最后返回的值就是这个key所属的分区ID。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> nopar = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">2</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">8</span>)</span><br><span class="line">nopar: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; nopar.partitioner</span><br><span class="line">res20: <span class="type">Option</span>[org.apache.spark.<span class="type">Partitioner</span>] = <span class="type">None</span></span><br><span class="line"></span><br><span class="line">scala&gt;nopar.mapPartitionsWithIndex((index,iter)=&gt;&#123; <span class="type">Iterator</span>(index.toString+<span class="string">" : "</span>+iter.mkString(<span class="string">"|"</span>)) &#125;).collect</span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="string">"0 : "</span>, <span class="number">1</span> : (<span class="number">1</span>,<span class="number">3</span>), <span class="number">2</span> : (<span class="number">1</span>,<span class="number">2</span>), <span class="number">3</span> : (<span class="number">2</span>,<span class="number">4</span>), <span class="string">"4 : "</span>, <span class="number">5</span> : (<span class="number">2</span>,<span class="number">3</span>), <span class="number">6</span> : (<span class="number">3</span>,<span class="number">6</span>), <span class="number">7</span> : (<span class="number">3</span>,<span class="number">8</span>)) </span><br><span class="line">scala&gt; <span class="keyword">val</span> hashpar = nopar.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">7</span>))</span><br><span class="line">hashpar: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">12</span>] at partitionBy at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; hashpar.count</span><br><span class="line">res18: <span class="type">Long</span> = <span class="number">6</span></span><br><span class="line"></span><br><span class="line">scala&gt; hashpar.partitioner</span><br><span class="line">res21: <span class="type">Option</span>[org.apache.spark.<span class="type">Partitioner</span>] = <span class="type">Some</span>(org.apache.spark.<span class="type">HashPartitioner</span>@<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; hashpar.mapPartitions(iter =&gt; <span class="type">Iterator</span>(iter.length)).collect()</span><br><span class="line">res19: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/30-PairRDD-HashPartition.png?raw=true" alt="30-PairRDD-HashPartition"></p><h3 id="Range-分区方式"><a href="#Range-分区方式" class="headerlink" title="Range 分区方式"></a>Range 分区方式</h3><p>HashPartitioner分区弊端：可能导致每个分区中数据量的不均匀，极端情况下会导致某些分区拥有RDD的全部数据。<br>RangePartitioner分区优势：尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大；<br>但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。<br>RangePartitioner作用：将一定范围内的数映射到某一个分区内，在实现中，分界的算法尤为重要。用到了水塘抽样算法。</p><h3 id="自定义分区方式"><a href="#自定义分区方式" class="headerlink" title="自定义分区方式"></a>自定义分区方式</h3><p>要实现自定义的分区器，你需要继承org.apache.spark.Partitioner类并实现下面三个方法。</p><ul><li>numPartitions:Int:返回创建出来的分区数。 </li><li>getPartition(key:Any):Int:返回给定键的分区编号(0到numPartitions-1)。</li><li>equals():Java判断相等性的标准方法。这个方法的实现非常重要，Spark需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样Spark才可以判断两个RDD的分区方式是否相同。 </li></ul><p>假设我们需要将相同后缀的数据写入相同的文件，我们通过将相同后缀的数据分区到相同的分区并保存输出来实现。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.moqi.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">Partitioner</span>, <span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomerPartitioner</span>(<span class="params">numParts:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//覆盖分区数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = numParts</span><br><span class="line"></span><br><span class="line">  <span class="comment">//覆盖分区号获取函数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> ckey: <span class="type">String</span> = key.toString</span><br><span class="line">    ckey.substring(ckey.length<span class="number">-1</span>).toInt%numParts</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CustomerPartitioner</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"partitioner"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> data=sc.parallelize(<span class="type">List</span>(<span class="string">"aa.2"</span>,<span class="string">"bb.2"</span>,<span class="string">"cc.3"</span>,<span class="string">"dd.3"</span>,<span class="string">"ee.5"</span>))</span><br><span class="line"></span><br><span class="line">    data.map((_,<span class="number">1</span>)).partitionBy(<span class="keyword">new</span> <span class="type">CustomerPartitioner</span>(<span class="number">5</span>)).keys.saveAsTextFile(<span class="string">"hdfs://slave1:9000/partitioner"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> data=sc.parallelize(<span class="type">List</span>(<span class="string">"aa.2"</span>,<span class="string">"bb.2"</span>,<span class="string">"cc.3"</span>,<span class="string">"dd.3"</span>,<span class="string">"ee.5"</span>).zipWithIndex,<span class="number">2</span>)</span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">7</span>] at parallelize at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; data.collect</span><br><span class="line">res4: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((aa<span class="number">.2</span>,<span class="number">0</span>), (bb<span class="number">.2</span>,<span class="number">1</span>), (cc<span class="number">.3</span>,<span class="number">2</span>), (dd<span class="number">.3</span>,<span class="number">3</span>), (ee<span class="number">.5</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; data.mapPartitionsWithIndex((index,iter)=&gt;<span class="type">Iterator</span>(index.toString +<span class="string">" : "</span>+ iter.mkString(<span class="string">"|"</span>))).collect</span><br><span class="line">res5: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">0</span> : (aa<span class="number">.2</span>,<span class="number">0</span>)|(bb<span class="number">.2</span>,<span class="number">1</span>), <span class="number">1</span> : (cc<span class="number">.3</span>,<span class="number">2</span>)|(dd<span class="number">.3</span>,<span class="number">3</span>)|(ee<span class="number">.5</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomerPartitioner</span>(<span class="params">numParts:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">Partitioner</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//覆盖分区数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = numParts</span><br><span class="line"></span><br><span class="line">  <span class="comment">//覆盖分区号获取函数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> ckey: <span class="type">String</span> = key.toString</span><br><span class="line">    ckey.substring(ckey.length<span class="number">-1</span>).toInt%numParts</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line"></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">CustomerPartitioner</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">data</span>.<span class="title">partitionBy</span>(<span class="params">new <span class="type">CustomerPartitioner</span>(4</span>))</span></span><br><span class="line"><span class="class"><span class="title">res7</span></span>: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">9</span>] at partitionBy at &lt;console&gt;:<span class="number">31</span></span><br><span class="line"></span><br><span class="line">scala&gt; res7.mapPartitionsWithIndex((index,iter)=&gt;<span class="type">Iterator</span>(index.toString +<span class="string">" : "</span>+ iter.mkString(<span class="string">"|"</span>))).collect</span><br><span class="line">res8: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="string">"0 : "</span>, <span class="number">1</span> : (ee<span class="number">.5</span>,<span class="number">4</span>), <span class="number">2</span> : (aa<span class="number">.2</span>,<span class="number">0</span>)|(bb<span class="number">.2</span>,<span class="number">1</span>), <span class="number">3</span> : (cc<span class="number">.3</span>,<span class="number">2</span>)|(dd<span class="number">.3</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>使用自定义的Partitioner是很容易的:只要把它传给partitionBy()方法即可。Spark中有许多依赖于数据混洗的方法，比如join()和groupByKey()，它们也可以接收一个可选的Partitioner对象来控制输出数据的分区方式。</p><h3 id="分区-Shuffle-优化"><a href="#分区-Shuffle-优化" class="headerlink" title="分区 Shuffle 优化"></a>分区 Shuffle 优化</h3><p>在分布式程序中，通信的代价是很大的，因此控制数据分布以获得最少的网络传输可以极大地提升整体性能。<br>Spark中所有的键值对RDD都可以进行分区。系统会根据一个针对键的函数对元素进行分组。主要有哈希分区和范围分区，当然用户也可以自定义分区函数。<br>通过分区可以有效提升程序性能。如下例子：<br>分析这样一个应用，它在内存中保存着一张很大的用户信息表——也就是一个由(UserID,UserInfo)对组成的RDD，其中UserInfo包含一个该用户所订阅的主题的列表。该应用会周期性地将这张表与一个小文件进行组合，这个小文件中存着过去五分钟内发生的事件——其实就是一个由(UserID,LinkInfo)对组成的表，存放着过去五分钟内某网站各用户的访问情况。例如，我们可能需要对用户访问其未订阅主题的页面的情况进行统计。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始化代码，从 HDFS 的一个 Hadoop SequenceFile 读取用户信息</span></span><br><span class="line"><span class="comment">// userData 中的元素会根据它们被读取时的来源，即 HDFS 块所在的节点来分布</span></span><br><span class="line"><span class="comment">// Spark 此时无法获知某个特定的 User ID 对应的记录位于那个节点上</span></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(...)</span><br><span class="line"><span class="keyword">val</span> userData = sc.sequenceFile[<span class="type">UserId</span>, <span class="type">UserInfo</span>](<span class="string">"hdfs://..."</span>).persist()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 周期性调用函数来处理过去五分钟产生的事件日志</span></span><br><span class="line"><span class="comment">// 假设这是一个包含(UserID, LinkInfo)键值对的 SequenceFile</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">processNewLogs</span></span>(logFileName: <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> events = sc.sequenceFile[<span class="type">UserID</span>, <span class="type">LinkInfo</span>](logFileName)</span><br><span class="line">    <span class="keyword">val</span> joined = userData.join(events) <span class="comment">// RDD of (UserID, (UserInfo, LinkInfo)) pairs</span></span><br><span class="line">    <span class="keyword">val</span> offTopicVisits = joined.filter &#123;</span><br><span class="line">        <span class="comment">// Expand the tuple into tis components</span></span><br><span class="line">        <span class="keyword">case</span> (userId, (userInfo, linkInfo)) =&gt; !userInfo.topics.contains(linkInfo.topic)</span><br><span class="line">    &#125;.count()</span><br><span class="line">    println(<span class="string">"Number of visits to non-subscribed topics: "</span> + offTopicVisits)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/31-PairRDD-Partition-Shuffle-Optimization-1.png?raw=true" alt="31-PairRDD-Partition-Shuffle-Optimization-1"></p><p>这段代码可以正确运行，但是不够高效。这是因为在每次调用processNewLogs()时都会用到join()操作，而我们对数据集是如何分区的却一无所知。默认情况下，连接操作会将两个数据集中的所有键的哈希值都求出来，将该哈希值相同的记录通过网络传到同一台机器上，然后在那台机器上对所有键相同的记录进行连接操作。因为userData表比每五分钟出现的访问日志表events要大得多，所以要浪费时间做很多额外工作:在每次调用时都对userData表进行哈希值计算和跨节点数据混洗，降低了程序的执行效率。<br>优化方法：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(...)</span><br><span class="line"><span class="keyword">val</span> userData = sc.sequenceFile[<span class="type">UserID</span>, <span class="type">UserInfo</span>](<span class="string">"hdfs://..."</span>)</span><br><span class="line"> .partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">100</span>)) <span class="comment">// 构造 100 个分区</span></span><br><span class="line"> .persist()</span><br></pre></td></tr></table></figure><p>我们在构建userData时调用了partitionBy()，Spark就知道了该RDD是根据键的哈希值来分区的，这样在调用join()时，Spark就会利用到这一点。具体来说，当调用userData.join(events)时，Spark只会对events进行数据混洗操作，将events中特定UserID的记录发送到userData的对应分区所在的那台机器上。这样，需要通过网络传输的数据就大大减少了，程序运行速度也可以显著提升了。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/32-PairRDD-Partition-Shuffle-Optimization-2.png?raw=true" alt="32-PairRDD-Partition-Shuffle-Optimization-2"></p><h3 id="基于分区进行操作"><a href="#基于分区进行操作" class="headerlink" title="基于分区进行操作"></a>基于分区进行操作</h3><p>基于分区对数据进行操作可以让我们避免为每个数据元素进行重复的配置工作。诸如打开数据库连接或创建随机数生成器等操作，都是我们应当尽量避免为每个元素都配置一次的工作。Spark提供基于分区的mapPartition和foreachPartition，让你的部分代码只对RDD的每个分区运行一次，这样可以帮助降低这些操作的代价。</p><h3 id="从分区中获益的操作"><a href="#从分区中获益的操作" class="headerlink" title="从分区中获益的操作"></a>从分区中获益的操作</h3><p>能够从数据分区中获得性能提升的操作有cogroup()、groupWith()、join()、leftOuterJoin()、rightOuterJoin()、groupByKey()、reduceByKey()、combineByKey()以及lookup()等。</p><h1 id="数据读取与保存主要方式"><a href="#数据读取与保存主要方式" class="headerlink" title="数据读取与保存主要方式"></a>数据读取与保存主要方式</h1><h2 id="文本文件"><a href="#文本文件" class="headerlink" title="文本文件"></a>文本文件</h2><p>当我们将一个文本文件读取为RDD时，输入的每一行都会成为RDD的一个元素。也可以将多个完整的文本文件一次性读取为一个pairRDD，其中键是文件名，值是文件内容。val input = sc.textFile(“./README.md”). 如果传递目录，则将目录下的所有文件读取作为RDD。文件路径支持通配符。通过wholeTextFiles()对于大量的小文件读取效率比较高，大文件效果没有那么高。<br>Spark通过saveAsTextFile()进行文本文件的输出，该方法接收一个路径，并将RDD中的内容都输入到路径对应的文件中。Spark将传入的路径作为目录对待，会在那个目录下输出多个文件。这样，Spark就可以从多个节点上并行输出了。result.saveAsTextFile(outputFile).</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.textFile(<span class="string">"./README.md"</span>)</span><br><span class="line">res6: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = ./<span class="type">README</span>.md <span class="type">MapPartitionsRDD</span>[<span class="number">7</span>] at textFile at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> readme = sc.textFile(<span class="string">"./README.md"</span>)</span><br><span class="line">readme: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = ./<span class="type">README</span>.md <span class="type">MapPartitionsRDD</span>[<span class="number">9</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; readme.collect()</span><br><span class="line">res7: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(# <span class="type">Apache</span> <span class="type">Spark</span>, <span class="string">""</span>, <span class="type">Spark</span> is a fast and general cluster...</span><br><span class="line">scala&gt; readme.saveAsTextFile(<span class="string">"hdfs://slave1:9000/test"</span>)</span><br></pre></td></tr></table></figure><h2 id="JSON-文件"><a href="#JSON-文件" class="headerlink" title="JSON 文件"></a>JSON 文件</h2><p>如果JSON文件中每一行就是一个JSON记录，那么可以通过将JSON文件当做文本文件来读取，然后利用相关的JSON库对每一条数据进行JSON解析。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> org.json4s._  </span><br><span class="line"><span class="keyword">import</span> org.json4s._</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">import</span> org.json4s.jackson.<span class="type">JsonMethods</span>._  </span><br><span class="line"><span class="keyword">import</span> org.json4s.jackson.<span class="type">JsonMethods</span>._</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">import</span> org.json4s.jackson.<span class="type">Serialization</span>  </span><br><span class="line"><span class="keyword">import</span> org.json4s.jackson.<span class="type">Serialization</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> result = sc.textFile(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line">result: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = examples/src/main/resources/people.json <span class="type">MapPartitionsRDD</span>[<span class="number">7</span>] at textFile at &lt;console&gt;:<span class="number">47</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">implicit</span> <span class="keyword">val</span> formats = <span class="type">Serialization</span>.formats(<span class="type">ShortTypeHints</span>(<span class="type">List</span>())) </span><br><span class="line">formats: org.json4s.<span class="type">Formats</span>&#123;<span class="keyword">val</span> dateFormat: org.json4s.<span class="type">DateFormat</span>; <span class="keyword">val</span> typeHints: org.json4s.<span class="type">TypeHints</span>&#125; = org.json4s.<span class="type">Serialization</span>$$anon$<span class="number">1</span>@<span class="number">61</span>f2c1da</span><br><span class="line"></span><br><span class="line">scala&gt; result.collect().foreach(x =&gt; &#123;<span class="keyword">var</span> c = parse(x).extract[<span class="type">Person</span>];println(c.name + <span class="string">","</span> + c.age)&#125;)  </span><br><span class="line"><span class="type">Michael</span>,<span class="number">30</span></span><br><span class="line"><span class="type">Andy</span>,<span class="number">30</span></span><br><span class="line"><span class="type">Justin</span>,<span class="number">19</span></span><br></pre></td></tr></table></figure><p>如果JSON数据是跨行的，那么只能读入整个文件，然后对每个文件进行解析。JSON数据的输出主要是通过在输出之前将由结构化数据组成的RDD转为字符串RDD，然后使用Spark的文本文件API写出去。说白了还是以文本文件的形式存，只是文本的格式已经在程序中转换为JSON。</p><h2 id="CSV-文件"><a href="#CSV-文件" class="headerlink" title="CSV 文件"></a>CSV 文件</h2><p>读取CSV/TSV数据和读取JSON数据相似，都需要先把文件当作普通文本文件来读取数据，然后通过将每一行进行解析实现对CSV的读取。CSV/TSV数据的输出也是需要将结构化RDD通过相关的库转换成字符串RDD，然后使用Spark的文本文件API写出去。</p><h2 id="SequenceFile-文件"><a href="#SequenceFile-文件" class="headerlink" title="SequenceFile 文件"></a>SequenceFile 文件</h2><p>SequenceFile文件是Hadoop用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。Spark有专门用来读取SequenceFile的接口。在SparkContext中，可以调用sequenceFile[keyClass, valueClass](path).</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/33-Type-In-Scala-Java-Hadoop.png?raw=true" alt="33-Type-In-Scala-Java-Hadoop"></p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/34-SequenceFile-Structures.png?raw=true" alt="34-SequenceFile-Structures"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> data=sc.parallelize(<span class="type">List</span>((<span class="number">2</span>,<span class="string">"aa"</span>),(<span class="number">3</span>,<span class="string">"bb"</span>),(<span class="number">4</span>,<span class="string">"cc"</span>),(<span class="number">5</span>,<span class="string">"dd"</span>),(<span class="number">6</span>,<span class="string">"ee"</span>)))</span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">16</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; data.saveAsSequenceFile(<span class="string">"hdfs://slave1:9000/sequdata"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> sdata = sc.sequenceFile[<span class="type">Int</span>,<span class="type">String</span>](<span class="string">"hdfs://slave1:9000/sdata/p*"</span>)</span><br><span class="line">sdata: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">19</span>] at sequenceFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; sdata.collect()</span><br><span class="line">res14: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">2</span>,aa), (<span class="number">3</span>,bb), (<span class="number">4</span>,cc), (<span class="number">5</span>,dd), (<span class="number">6</span>,ee))</span><br></pre></td></tr></table></figure><p>可以直接调用saveAsSequenceFile(path)保存你的PairRDD，它会帮你写出数据。需要键和值能够自动转为Writable类型。</p><h2 id="对象文件"><a href="#对象文件" class="headerlink" title="对象文件"></a>对象文件</h2><p>对象文件是将对象序列化后保存的文件，采用Java的序列化机制。可以通过objectFile<a href="path">k,v</a>函数接收一个路径，读取对象文件，返回对应的RDD，也可以通过调用saveAsObjectFile()实现对对象文件的输出。因为是序列化所以要指定类型。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> data=sc.parallelize(<span class="type">List</span>((<span class="number">2</span>,<span class="string">"aa"</span>),(<span class="number">3</span>,<span class="string">"bb"</span>),(<span class="number">4</span>,<span class="string">"cc"</span>),(<span class="number">5</span>,<span class="string">"dd"</span>),(<span class="number">6</span>,<span class="string">"ee"</span>)))</span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">20</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; data.saveAsObjectFile(<span class="string">"hdfs://slave1:9000/objfile"</span>)</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> objrdd:<span class="type">RDD</span>[(<span class="type">Int</span>,<span class="type">String</span>)] = sc.objectFile[(<span class="type">Int</span>,<span class="type">String</span>)](<span class="string">"hdfs://slave1:9000/objfile/p*"</span>)</span><br><span class="line">objrdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">28</span>] at objectFile at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; objrdd.collect()</span><br><span class="line">res20: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">2</span>,aa), (<span class="number">3</span>,bb), (<span class="number">4</span>,cc), (<span class="number">5</span>,dd), (<span class="number">6</span>,ee))</span><br></pre></td></tr></table></figure><h2 id="Hadoop-文件"><a href="#Hadoop-文件" class="headerlink" title="Hadoop 文件"></a>Hadoop 文件</h2><p>Spark的整个生态系统与Hadoop是完全兼容的,所以对于Hadoop所支持的文件类型或者数据库类型,Spark也同样支持.另外,由于Hadoop的API有新旧两个版本,所以Spark为了能够兼容Hadoop所有的版本,也提供了两套创建操作接口.对于外部存储创建操作而言,hadoopRDD和newHadoopRDD是最为抽象的两个函数接口,主要包含以下四个参数。</p><ol><li>输入格式(InputFormat): 制定数据输入的类型,如TextInputFormat等,新旧两个版本所引用的版本分别是org.apache.hadoop.mapred.InputFormat和org.apache.hadoop.mapreduce.InputFormat(NewInputFormat)</li><li>键类型: 指定[K,V]键值对中K的类型</li><li>值类型: 指定[K,V]键值对中V的类型</li><li>分区值: 指定由外部存储生成的RDD的partition数量的最小值,如果没有指定,系统会使用默认值defaultMinSplits</li></ol><p>其他创建操作的API接口都是为了方便最终的Spark程序开发者而设置的,是这两个接口的高效实现版本.例如,对于textFile而言,只有path这个指定文件路径的参数,其他参数在系统内部指定了默认值。</p><p>注意:</p><ol><li>在Hadoop中以压缩形式存储的数据,不需要指定解压方式就能够进行读取,因为Hadoop本身有一个解压器会根据压缩文件的后缀推断解压算法进行解压。</li><li>如果用Spark从Hadoop中读取某种类型的数据不知道怎么读取的时候,上网查找一个使用map-reduce的时候是怎么读取这种这种数据的,然后再将对应的读取方式改写成兼容版本的的hadoopRDD和newAPIHadoopRDD两个类就行了。</li></ol><p>读取示例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> data = sc.parallelize(<span class="type">Array</span>((<span class="number">30</span>,<span class="string">"hadoop"</span>), (<span class="number">71</span>,<span class="string">"hive"</span>), (<span class="number">11</span>,<span class="string">"cat"</span>)))</span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">47</span>] at parallelize at &lt;console&gt;:<span class="number">35</span></span><br><span class="line"></span><br><span class="line">scala&gt; data.saveAsNewAPIHadoopFile(<span class="string">"hdfs://slave1:9000/output4/"</span>,classOf[<span class="type">LongWritable</span>] ,classOf[<span class="type">Text</span>] ,classOf[org.apache.hadoop.mapreduce.lib.output.<span class="type">TextOutputFormat</span>[<span class="type">LongWritable</span>, <span class="type">Text</span>]])</span><br></pre></td></tr></table></figure><p>对于RDD最后的归宿除了返回为集合和标量，也可以将RDD存储到外部文件系统或者数据库中，Spark系统与Hadoop是完全兼容的，所以MapReduce所支持的读写文件或者数据库类型，Spark也同样支持。另外，由于Hadoop的API有新旧两个版本，所以Spark为了能够兼容Hadoop所有的版本，也提供了两套API。将RDD保存到HDFS中在通常情况下需要关注或者设置五个参数，即文件保存的路径，key值的class类型，Value值的class类型，RDD的输出格式(OutputFormat，如TextOutputFormat/SequenceFileOutputFormat)，以及最后一个相关的参数codec(这个参数表示压缩存储的压缩形式，如DefaultCodec，Gzip，Codec等等)。</p><table><thead><tr><th>兼容旧版API</th></tr></thead><tbody><tr><td>saveAsObjectFile(path:   String): Unit</td></tr><tr><td>saveAsTextFile(path:   String, codec: Class[_ &lt;: CompressionCodec]): Unit</td></tr><tr><td>saveAsTextFile(path:   String): Unit</td></tr><tr><td>saveAsHadoopFile[F   &lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unit</td></tr><tr><td>saveAsHadoopFile[F   &lt;: OutputFormat[K, V]](path: String, codec: Class[_ &lt;:   CompressionCodec])(implicit fm: ClassTag[F]): Unit</td></tr><tr><td>saveAsHadoopFile(path:   String, keyClass: Class[<em>], valueClass: Class[</em>], outputFormatClass: Class[<em>&lt;: OutputFormat[</em>, <em>]], codec: Class[</em> &lt;: CompressionCodec]): Unit</td></tr><tr><td>saveAsHadoopDataset(conf:   JobConf): Unit</td></tr></tbody></table><p>这里列出的API，前面6个都是saveAsHadoopDataset的简易实现版本，仅仅支持将RDD存储到HDFS中，而saveAsHadoopDataset的参数类型是JobConf，所以其不仅能够将RDD存储到HDFS中，也可以将RDD存储到其他数据库中，如Hbase，MangoDB，Cassandra等。</p><table><thead><tr><th>兼容新版API</th></tr></thead><tbody><tr><td>saveAsNewAPIHadoopFile(path:   String, keyClass: Class[<em>], valueClass: Class[</em>], outputFormatClass: Class[_   &lt;: OutputFormat[_, _]], conf: Configuration =   self.context.hadoopConfiguration): Unit</td></tr><tr><td>saveAsNewAPIHadoopFile[F   &lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unit</td></tr><tr><td>saveAsNewAPIHadoopDataset(conf:   Configuration): Unit</td></tr></tbody></table><p>同样的，前2个API是saveAsNewAPIHadoopDataset的简易实现，只能将RDD存到HDFS中，而saveAsNewAPIHadoopDataset比较灵活.新版的API没有codec的参数，所以要压缩存储文件到HDFS中每需要使用hadoopConfiguration参数，设置对应mapreduce.map.output.compress.codec参数和mapreduce.map.output.compress参数。<br>注意：如果不知道怎么将RDD存储到Hadoop生态的系统中，主要上网搜索一下对应的map-reduce是怎么将数据存储进去的，然后改写成对应的saveAsHadoopDataset或saveAsNewAPIHadoopDataset就可以了。</p><p>写入示例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> read =  sc.newAPIHadoopFile[<span class="type">LongWritable</span>, <span class="type">Text</span>, org.apache.hadoop.mapreduce.lib.input.<span class="type">TextInputFormat</span>](<span class="string">"hdfs://slave1:9000/output3/part*"</span>, classOf[org.apache.hadoop.mapreduce.lib.input.<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>])</span><br><span class="line">read: org.apache.spark.rdd.<span class="type">RDD</span>[(org.apache.hadoop.io.<span class="type">LongWritable</span>, org.apache.hadoop.io.<span class="type">Text</span>)] = hdfs:<span class="comment">//slave1:9000/output3/part* NewHadoopRDD[48] at newAPIHadoopFile at &lt;console&gt;:35</span></span><br><span class="line"></span><br><span class="line">scala&gt; read.map&#123;<span class="keyword">case</span> (k, v) =&gt; v.toString&#125;.collect</span><br><span class="line">res44: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">30</span> hadoop, <span class="number">71</span> hive, <span class="number">11</span> cat)</span><br></pre></td></tr></table></figure><h2 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h2><p>Spark支持读写很多种文件系统，像本地文件系统、Amazon S3、HDFS 等。</p><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><h3 id="关系型数据库连接"><a href="#关系型数据库连接" class="headerlink" title="关系型数据库连接"></a>关系型数据库连接</h3><p>支持通过Java JDBC访问关系型数据库。需要通过Jdbc RDD进行，示例如下:<br>MySQL 读取：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span> </span>(args: <span class="type">Array</span>[<span class="type">String</span>] ) &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span> ().setMaster (<span class="string">"local[2]"</span>).setAppName (<span class="string">"JdbcApp"</span>)</span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span> (sparkConf)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> rdd = <span class="keyword">new</span> org.apache.spark.rdd.<span class="type">JdbcRDD</span> (</span><br><span class="line">    sc,</span><br><span class="line">    () =&gt; &#123;</span><br><span class="line">      <span class="type">Class</span>.forName (<span class="string">"com.mysql.jdbc.Driver"</span>).newInstance()</span><br><span class="line">      java.sql.<span class="type">DriverManager</span>.getConnection (<span class="string">"jdbc:mysql://slave1:3306/rdd"</span>, <span class="string">"root"</span>, <span class="string">"hive"</span>)</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"select * from rddtable where id &gt;= ? and id &lt;= ?;"</span>,</span><br><span class="line">    <span class="number">1</span>,</span><br><span class="line">    <span class="number">10</span>,</span><br><span class="line">    <span class="number">1</span>,</span><br><span class="line">    r =&gt; (r.getInt(<span class="number">1</span>), r.getString(<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">  println (rdd.count () )</span><br><span class="line">  rdd.foreach (println (_) )</span><br><span class="line">  sc.stop ()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>MySQL 写入：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"HBaseApp"</span>)</span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">  <span class="keyword">val</span> data = sc.parallelize(<span class="type">List</span>(<span class="string">"Female"</span>, <span class="string">"Male"</span>,<span class="string">"Female"</span>))</span><br><span class="line"></span><br><span class="line">  data.foreachPartition(insertData)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertData</span></span>(iterator: <span class="type">Iterator</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="type">Class</span>.forName (<span class="string">"com.mysql.jdbc.Driver"</span>).newInstance()</span><br><span class="line">  <span class="keyword">val</span> conn = java.sql.<span class="type">DriverManager</span>.getConnection(<span class="string">"jdbc:mysql://slave1:3306/rdd"</span>, <span class="string">"root"</span>, <span class="string">"hive"</span>)</span><br><span class="line">  iterator.foreach(data =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> ps = conn.prepareStatement(<span class="string">"insert into rddtable(name) values (?)"</span>)</span><br><span class="line">    ps.setString(<span class="number">1</span>, data) </span><br><span class="line">    ps.executeUpdate()</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>JdbcRDD 接收这样几个参数：</p><ul><li>首先，要提供一个用于对数据库创建连接的函数。这个函数让每个节点在连接必要的配 置后创建自己读取数据的连接。 </li><li>接下来，要提供一个可以读取一定范围内数据的查询，以及查询参数中lowerBound和 upperBound 的值。这些参数可以让 Spark 在不同机器上查询不同范围的数据，这样就不 会因尝试在一个节点上读取所有数据而遭遇性能瓶颈。</li><li>这个函数的最后一个参数是一个可以将输出结果从转为对操作数据有用的格式的函数。如果这个参数空缺，Spark会自动将每行结果转为一个对象数组。 </li></ul><h3 id="HBase-数据库"><a href="#HBase-数据库" class="headerlink" title="HBase 数据库"></a>HBase 数据库</h3><p>由于 org.apache.hadoop.hbase.mapreduce.TableInputFormat 类的实现，Spark 可以通过Hadoop输入格式访问HBase。这个输入格式会返回键值对数据，其中键的类型为org.apache.hadoop.hbase.io.ImmutableBytesWritable，而值的类型为org.apache.hadoop.hbase.client.Result.<br>HBase 读取：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"HBaseApp"</span>)</span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">  <span class="comment">//HBase中的表名</span></span><br><span class="line">  conf.set(<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>, <span class="string">"fruit"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> hBaseRDD = sc.newAPIHadoopRDD(conf, classOf[<span class="type">TableInputFormat</span>],</span><br><span class="line">    classOf[org.apache.hadoop.hbase.io.<span class="type">ImmutableBytesWritable</span>],</span><br><span class="line">    classOf[org.apache.hadoop.hbase.client.<span class="type">Result</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> count = hBaseRDD.count()</span><br><span class="line">  println(<span class="string">"hBaseRDD RDD Count:"</span>+ count)</span><br><span class="line">  hBaseRDD.cache()</span><br><span class="line">  hBaseRDD.foreach &#123;</span><br><span class="line">    <span class="keyword">case</span> (_, result) =&gt;</span><br><span class="line">      <span class="keyword">val</span> key = <span class="type">Bytes</span>.toString(result.getRow)</span><br><span class="line">      <span class="keyword">val</span> name = <span class="type">Bytes</span>.toString(result.getValue(<span class="string">"info"</span>.getBytes, <span class="string">"name"</span>.getBytes))</span><br><span class="line">      <span class="keyword">val</span> color = <span class="type">Bytes</span>.toString(result.getValue(<span class="string">"info"</span>.getBytes, <span class="string">"color"</span>.getBytes))</span><br><span class="line">      println(<span class="string">"Row key:"</span> + key + <span class="string">" Name:"</span> + name + <span class="string">" Color:"</span> + color)</span><br><span class="line">  &#125;</span><br><span class="line">  sc.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>HBase 写入：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"HBaseApp"</span>)</span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">  <span class="keyword">val</span> jobConf = <span class="keyword">new</span> <span class="type">JobConf</span>(conf)</span><br><span class="line">  jobConf.setOutputFormat(classOf[<span class="type">TableOutputFormat</span>])</span><br><span class="line">  jobConf.set(<span class="type">TableOutputFormat</span>.<span class="type">OUTPUT_TABLE</span>, <span class="string">"fruit_spark"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fruitTable = <span class="type">TableName</span>.valueOf(<span class="string">"fruit_spark"</span>)</span><br><span class="line">  <span class="keyword">val</span> tableDescr = <span class="keyword">new</span> <span class="type">HTableDescriptor</span>(fruitTable)</span><br><span class="line">  tableDescr.addFamily(<span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(<span class="string">"info"</span>.getBytes))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> admin = <span class="keyword">new</span> <span class="type">HBaseAdmin</span>(conf)</span><br><span class="line">  <span class="keyword">if</span> (admin.tableExists(fruitTable)) &#123;</span><br><span class="line">    admin.disableTable(fruitTable)</span><br><span class="line">    admin.deleteTable(fruitTable)</span><br><span class="line">  &#125;</span><br><span class="line">  admin.createTable(tableDescr)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">convert</span></span>(triple: (<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>)) = &#123;</span><br><span class="line">    <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(<span class="type">Bytes</span>.toBytes(triple._1))</span><br><span class="line">    put.addImmutable(<span class="type">Bytes</span>.toBytes(<span class="string">"info"</span>), <span class="type">Bytes</span>.toBytes(<span class="string">"name"</span>), <span class="type">Bytes</span>.toBytes(triple._2))</span><br><span class="line">    put.addImmutable(<span class="type">Bytes</span>.toBytes(<span class="string">"info"</span>), <span class="type">Bytes</span>.toBytes(<span class="string">"price"</span>), <span class="type">Bytes</span>.toBytes(triple._3))</span><br><span class="line">    (<span class="keyword">new</span> <span class="type">ImmutableBytesWritable</span>, put)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> initialRDD = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="string">"apple"</span>,<span class="number">11</span>), (<span class="number">2</span>,<span class="string">"banana"</span>,<span class="number">12</span>), (<span class="number">3</span>,<span class="string">"pear"</span>,<span class="number">13</span>)))</span><br><span class="line">  <span class="keyword">val</span> localData = initialRDD.map(convert)</span><br><span class="line"></span><br><span class="line">  localData.saveAsHadoopDataset(jobConf)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Cassandra-和-ElasticSearch"><a href="#Cassandra-和-ElasticSearch" class="headerlink" title="Cassandra 和 ElasticSearch"></a>Cassandra 和 ElasticSearch</h3><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/35-Cassandra-And-ElasticSearch.png?raw=true" alt="35-Cassandra-And-ElasticSearch"></p><h1 id="RDD-编程进阶"><a href="#RDD-编程进阶" class="headerlink" title="RDD 编程进阶"></a>RDD 编程进阶</h1><h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><p>累加器用来对信息进行聚合，通常在向Spark传递函数时，比如使用map()函数或者用filter()传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，更新这些副本的值也不会影响驱动器中的对应变量。如果我们想实现所有分片处理时更新共享变量的功能，那么累加器可以实现我们想要的效果。<br>针对一个输入的日志文件，如果我们想计算文件中所有空行的数量，我们可以编写以下程序：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> notice = sc.textFile(<span class="string">"./NOTICE"</span>)</span><br><span class="line">notice: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = ./<span class="type">NOTICE</span> <span class="type">MapPartitionsRDD</span>[<span class="number">40</span>] at textFile at &lt;console&gt;:<span class="number">32</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> blanklines = sc.accumulator(<span class="number">0</span>)</span><br><span class="line">warning: there were two deprecation warnings; re-run <span class="keyword">with</span> -deprecation <span class="keyword">for</span> details</span><br><span class="line">blanklines: org.apache.spark.<span class="type">Accumulator</span>[<span class="type">Int</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> tmp = notice.flatMap(line =&gt; &#123;</span><br><span class="line">     |    <span class="keyword">if</span> (line == <span class="string">""</span>) &#123;</span><br><span class="line">     |       blanklines += <span class="number">1</span></span><br><span class="line">     |    &#125;</span><br><span class="line">     |    line.split(<span class="string">" "</span>)</span><br><span class="line">     | &#125;)</span><br><span class="line">tmp: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">41</span>] at flatMap at &lt;console&gt;:<span class="number">36</span></span><br><span class="line"></span><br><span class="line">scala&gt; tmp.count()</span><br><span class="line">res31: <span class="type">Long</span> = <span class="number">3213</span></span><br><span class="line"></span><br><span class="line">scala&gt; blanklines.value</span><br><span class="line">res32: <span class="type">Int</span> = <span class="number">171</span></span><br></pre></td></tr></table></figure><p>累加器的用法如下所示。<br>通过在驱动器中调用SparkContext.accumulator(initialValue)方法，创建出存有初始值的累加器。返回值为org.apache.spark.Accumulator[T]对象，其中T是初始值initialValue的类型。<br>Spark闭包里的执行器代码可以使用累加器的+=方法(在Java中是add)增加累加器的值。 <br>驱动器程序可以调用累加器的value属性(在Java中使用value()或setValue())来访问累加器的值。<br>注意：工作节点上的任务不能访问累加器的值。从这些任务的角度来看，累加器是一个只写变量。<br>对于要在行动操作中使用的累加器，Spark只会把每个任务对各累加器的修改应用一次。因此，如果想要一个无论在失败还是重复计算时都绝对可靠的累加器，我们必须把它放在foreach()这样的行动操作中。转化操作中累加器可能会发生不止一次更新。</p><h2 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h2><p>自定义累加器类型的功能在1.X版本中就已经提供了，但是使用起来比较麻烦，在2.0版本后，累加器的易用性有了较大的改进，而且官方还提供了一个新的抽象类：AccumulatorV2来提供更加友好的自定义类型累加器的实现方式。实现自定义类型累加器需要继承AccumulatorV2并至少覆写下例中出现的方法，下面这个累加器可以用于在程序运行过程中收集一些文本类信息，最终以Set[String]的形式返回。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.moqi.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.util.<span class="type">AccumulatorV2</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogAccumulator</span> <span class="keyword">extends</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">util</span>.<span class="title">AccumulatorV2</span>[<span class="type">String</span>, java.util.<span class="type">Set</span>[<span class="type">String</span>]] </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> _logArray: java.util.<span class="type">Set</span>[<span class="type">String</span>] = <span class="keyword">new</span> java.util.<span class="type">HashSet</span>[<span class="type">String</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    _logArray.isEmpty</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    _logArray.clear()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    _logArray.add(v)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: org.apache.spark.util.<span class="type">AccumulatorV2</span>[<span class="type">String</span>, java.util.<span class="type">Set</span>[<span class="type">String</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    other <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> o: <span class="type">LogAccumulator</span> =&gt; _logArray.addAll(o.value)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: java.util.<span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    java.util.<span class="type">Collections</span>.unmodifiableSet(_logArray)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>():org.apache.spark.util.<span class="type">AccumulatorV2</span>[<span class="type">String</span>, java.util.<span class="type">Set</span>[<span class="type">String</span>]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> newAcc = <span class="keyword">new</span> <span class="type">LogAccumulator</span>()</span><br><span class="line">    _logArray.synchronized&#123;</span><br><span class="line">      newAcc._logArray.addAll(_logArray)</span><br><span class="line">    &#125;</span><br><span class="line">    newAcc</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 过滤掉带字母的</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LogAccumulator</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"LogAccumulator"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> accum = <span class="keyword">new</span> <span class="type">LogAccumulator</span></span><br><span class="line">    sc.register(accum, <span class="string">"logAccum"</span>)</span><br><span class="line">    <span class="keyword">val</span> sum = sc.parallelize(<span class="type">Array</span>(<span class="string">"1"</span>, <span class="string">"2a"</span>, <span class="string">"3"</span>, <span class="string">"4b"</span>, <span class="string">"5"</span>, <span class="string">"6"</span>, <span class="string">"7cd"</span>, <span class="string">"8"</span>, <span class="string">"9"</span>), <span class="number">2</span>).filter(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> pattern = <span class="string">""</span><span class="string">"^-?(\d+)"</span><span class="string">""</span></span><br><span class="line">      <span class="keyword">val</span> flag = line.matches(pattern)</span><br><span class="line">      <span class="keyword">if</span> (!flag) &#123;</span><br><span class="line">        accum.add(line)</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;).map(_.toInt).reduce(_ + _)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"sum: "</span> + sum)</span><br><span class="line">    <span class="keyword">for</span> (v &lt;- accum.value) print(v + <span class="string">""</span>)</span><br><span class="line">    println()</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，甚至是机器学习算法中的一个很大的特征向量，广播变量用起来都很顺手。<br>传统方式下，Spark会自动把闭包中所有引用到的变量发送到工作节点上。虽然这很方便，但也很低效。原因有二:首先，默认的任务发射机制是专门为小任务进行优化的；其次，事实上你可能会在多个并行操作中使用同一个变量，但是Spark会为每个任务分别发送。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> broadcastVar = sc.broadcast(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">broadcastVar: org.apache.spark.broadcast.<span class="type">Broadcast</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Broadcast</span>(<span class="number">35</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; broadcastVar.value</span><br><span class="line">res33: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>使用广播变量的过程如下：</p><ol><li>通过对一个类型 T 的对象调用 SparkContext.broadcast 创建出一个 Broadcast[T] 对象。 任何可序列化的类型都可以这么实现。 </li><li>通过 value 属性访问该对象的值(在 Java 中为 value() 方法)。 </li><li>变量只会被发到各个节点一次，应作为只读值处理(修改这个值不会影响到别的节点)。</li></ol><p>End. </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要解析部分 Spark Core 的技术点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="http://moqimoqidea.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>JVM GC 发展历程</title>
    <link href="http://moqimoqidea.github.io/2017/06/16/JVM-GC-Development-Path/"/>
    <id>http://moqimoqidea.github.io/2017/06/16/JVM-GC-Development-Path/</id>
    <published>2017-06-16T08:29:47.000Z</published>
    <updated>2018-11-23T05:59:40.630Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍 JVM GC 的发展。</p><a id="more"></a> <h1 id="较早的-From-To-阶段"><a href="#较早的-From-To-阶段" class="headerlink" title="较早的 From - To 阶段"></a>较早的 From - To 阶段</h1><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/The-Oldest-GC.png?raw=true" alt="The-Oldest-GC"></p><ul><li>最早的垃圾收集 From - To 架构的 GC，把整个堆内存分成大小差不多相等的两部分，中间有一个分配的指针（Free Point），对指针设定目标值（比如 From 区域的 80%）时，触发一次 GC。GC 触发时应用进入 Stop-The-World 状态，这时垃圾回收器检查 From 区域有哪些是可以回收的那些不是，将不可以回收的拷贝到 To 区域，其他回收。一次 GC 操作完成的时候完成区域交换（From 转换为 To 区域，To 转换为 From 区域），然后指针分配内存开始从新的 From 区域开始。</li><li>这种纯粹的拷贝垃圾回收方法最大的问题在于堆内存里面永远只可以用一半的内存，所以有一半的堆是浪费的。但在当时而言还是比较领先的，比如相对于引用计数的垃圾回收方法。引用计数垃圾回收的问题在于：引用计数在计数的时候需要维持一个锁的消耗，会降低分配内存的速度；另外一个是在循环引用中，这个消耗会更大。</li></ul><h1 id="回收分代的思想"><a href="#回收分代的思想" class="headerlink" title="回收分代的思想"></a>回收分代的思想</h1><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/object-age-based-on-GC-generation-generational-hypothesis.png?raw=true" alt="object-age-based-on-GC-generation-generational-hypothesis"></p><ul><li>将堆内存分代治理建立于这样一个假设之上：<a href="https://plumbr.io/handbook/garbage-collection-in-java/generational-hypothesis" target="_blank" rel="noopener">代际假设</a> , 核心论点有两个：<ul><li>大多数对象很快就会被闲置；</li><li>少部分活下来的对象会存在相当长一段时间。</li></ul></li></ul><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/Hotspot-Heap-Structure.PNG?raw=true" alt="Hotspot-Heap-Structure"></p><ul><li>因此，JVM GC 分代治理的核心基础是以下两个：（1）大多数对象都会在年轻代死亡。（2）年老代引用年轻代的对象只占很小的一部分。分代治理会发生不同区域的 GC。在年轻代发生的 GC 称为 “minor GC”，在年老代出现的 GC 称为 “major GC” 或者 “full GC”。</li><li>根据代际假设构建的堆内存首先避免了全盘扫描，这个时期的 JVM GC 发展为如上图所示结构，分为年轻代，年老代，永生代。年轻代分为 eden 区与两个 survivor 区，s0 和 s1 实现是最初的 From - To 架构，在这里我们假设 s0 为 From 区，s1 为 To 区。创建的对象首先进入 eden 区，如果发生 minor GC，eden 区中大部分对象被回收，小部分对象拷贝到 To 区，From 也拷贝到 To 区；如果在 eden 中的对象太大不能拷贝到 To 区，则会被直接移动到年老代。每次 minor GC 时 From 和 To 会交换，每交换一次区内的对象年龄会加一，当年龄到达一定值（比如15）（注：这一块在后面的 GC 实现了动态调整）的时候，这些大龄的对象也被移动到了年老代。</li></ul><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/young-to-old-generation.png?raw=true" alt="yount-to-old-generation"></p><ul><li>但是这里会发生一个问题：<strong>如果年老代的对象需要引用年轻代的对象怎么办？</strong>为了处理这些情况，年老代中有一种称为”CardTable” (卡表) 的东西，它是一个 512 字节的块。每当年老代中的对象引用年轻代中的对象时，它就会记录在此表中。当发生 minor GC 时，仅搜索该卡表以确定它是否是年轻代 GC 需要回收的对象，而不是检查旧代中的所有对象的引用。</li></ul><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/card-table-structure.png?raw=true" alt="card-table-structure"></p><ul><li>当数据已满时，触发年老代执行 GC 。执行程序因 GC 类型而异，根据JDK 7，有5种GC类型。<ul><li>Serial GC</li><li>Parallel GC</li><li>Parallel Old GC (Parallel Compacting GC)</li><li>Concurrent Mark &amp; Sweep GC  (or “CMS”)</li><li>Garbage First (G1) GC<h2 id="Serial-GC"><a href="#Serial-GC" class="headerlink" title="Serial GC"></a>Serial GC</h2>(-XX:+UseSerialGC)，即串行 GC，使用被称为 “mark-sweep-compant” 的算法。</li><li>第一步：标记年老代中幸存的对象。（标记 - mark）</li><li>第二步：从堆的最前面开始检查，只留下幸存的堆。（扫描 - sweep）</li><li>第三步：把对象从最前面开始填充，以便连续堆积对象，并将堆分为包含对象和不包含对象的两部分。（紧凑 - compant）</li><li>串行 GC 适用于小内存和少量 CPU 内核的 JVM。<h2 id="Parallel-GC"><a href="#Parallel-GC" class="headerlink" title="Parallel GC"></a>Parallel GC</h2>(-XX:+UseParallelGC)，即并行 GC，和串行 GC 最大的区别是用多个线程来处理 GC，因此更快，当有足够的内存和 CPU 资源时，此 GC 非常有用，它也被称为”吞吐量 GC“。</li></ul></li></ul><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/Difference-between-the-Serial-GC-and-Parallel-GC.png?raw=true" alt="Difference-between-the-Serial-GC-and-Parallel-GC"></p><h2 id="Parallel-Old-GC"><a href="#Parallel-Old-GC" class="headerlink" title="Parallel Old GC"></a>Parallel Old GC</h2><p>  (-XX:+UseParallelOldGC)，并行旧 GC，自 JDK 5 更新以来开始支持，与并行 GC 相比，唯一的区别是老年代的 GC 算法。它经历了三个步骤：mark – summary – compaction（标记 - 摘要 - 压缩）。”摘要“步骤经历了一些更复杂的步骤。</p><h2 id="CMS-GC"><a href="#CMS-GC" class="headerlink" title="CMS GC"></a>CMS GC</h2><p>  （-XX：+ UseConcMarkSweepGC），从下图中可以看出，CMS GC 相对于前三个复杂得多。第一步 “Initial Mark” (初始标记) 很简单，搜索最接近类加载器的对象中的幸存对象，因此暂停时间很短。在 “Concurrent Mark” (并发标记) 步骤中，跟踪并检查刚刚确认的幸存对象引用的对象。这一步的不同之处在于它在同时处理其他线程的同时继续进行。在 “Remark” (再次标记) 步骤中，将检查在并发标记步骤中新增加或停止引用的对象。最后，在 “Concurrent Sweep” (并发扫描) 步骤中进行垃圾回收，垃圾回收和其他线程同步进行。由于 CMS GC 独特的运行方式，因此 GC 的暂停时间非常短。CMS GC 也称为低延迟 GC。<strong>在应用程序的响应时间至关重要时使用。</strong>这种 GC 的主要缺点如下：（1）它比其他 GC 类型使用更多的内存和 CPU。（2）默认情况下不提供压缩步骤。因而如果由于许多内存碎片而需要执行压缩任务，那么 GC 需要的静止时间可能会比其他任何 GC 方式都要长。所以，如果在使用 CMS GC 的时候要尤其注意压缩任务执行的频率和持续时间。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/Serial-GC-and-CMS-GC.png?raw=true" alt="Serial-GC-and-CMS-GC"></p><h1 id="G1-GC"><a href="#G1-GC" class="headerlink" title="G1 GC"></a>G1 GC</h1><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/G1-GC.png?raw=true" alt="G1-GC"></p><ul><li>G1 是 Garbage First 的缩写，最开始的 G1 完全抛弃分代收集的思想，开始于 JDK 1.7 Update 4，可以视为上图中所有的 “E”, “O”, “S” 标志消失，只分成不同的 Region (块)。实际上现在的 G1 GC 也有了分代的逻辑。G1 的实现一直以来都在不断的变化之中。</li><li>由前文所知每种 GC 都有出现的历史背景，比如串行 GC 是在内存和 CPU 比较小的情况下出现的；并行 GC 是在吞吐量出现巨大需求的时候出现的；而 CMS GC 则是对低延迟有了更高的需求，尽量拖延 full GC 出现的时间。那么，出现 G1这种形式的垃圾收集器的原因是什么？在 JVM GC 不断发展的过程中，已经出现了自适应的堆，也就是不需要手动调节年轻代与年老代的比例，以及年轻代对象进入年老代的年龄。这些自适应堆对更灵活的堆块产生了强烈的需求：如果将堆空间划分为很多个 Region，G1 可以将某一块指定为各种不同的代（可以是年老代，Eden 或者 Survivor 区），而且各个块在空间上不需要连续的在一起，有一个 List 将它们组织在一起，这样 G1 就很容易调整各个代之间的比例。</li></ul><h2 id="为什么-G1-被称为-G1？"><a href="#为什么-G1-被称为-G1？" class="headerlink" title="为什么 G1 被称为 G1？"></a>为什么 G1 被称为 G1？</h2><ul><li><p>G1 会在内部维护一个优先列表，通过一个合理的模型，计算出每个 Region 的收集成本和收益期望并量化，这样每次进行 GC 时，G1 总是会选择最适合的 Region（通常垃圾比较多）进行回收，使 GC 时间满足设置的条件。</p></li><li><p>G1通过引入 Remembered Set 来避免全堆扫描（前面所说的 CardTable 是其的一种实现）。Remembered Set 用于跟踪对象引用。G1 中每个 Region 都有对应的 Remembered Set 。当 JVM 发现内部的一个引用关系需要更新（对 Reference 类型进行写操作），则立即产生一个 Write Barrier 中断这个写操作，并检查Reference 引用的对象是否处于不同的 Region 之间（用分代的思想，就是新生代和老年代之间的引用）。如果是，则通过 CardTable 通知 G1，G1 根据 CardTable 把相关引用信息记录到被引用对象所属的 Region 的Remembered Set 中，并将 Remembered Set 加入 GC Root 。这样，在 G1 进行根节点枚举时就可以扫描到该对象而不会出现遗漏。</p></li><li><p>通俗解释第二条：如果一个 Region 的 Reference 越少，JVM 倾向于认为这块 Region 里面活着的对象越少，这个 Region 块是可回收的垃圾块的百分比就越大，这样回收这个 Region 的收益就越大。所以称这种算法为 Garbage First.</p></li></ul><h2 id="G1-GC-年轻代的回收"><a href="#G1-GC-年轻代的回收" class="headerlink" title="G1 GC 年轻代的回收"></a>G1 GC 年轻代的回收</h2><ol><li>当 JVM 启动时基于启动参数，JVM 要求操作系统分配一个大的连续内存块来托管 JVM 的堆，被划分为 2048 个 Region (块)。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/how-young-gc-works-in-g1-1.png?raw=true" alt="how-young-gc-works-in-g1-1"></p><ol start="2"><li>年轻代的块发生 “minor GC”，将活着的对象拷贝到 survivor 块（依然是 From - To 算法）。如果对象过大或者对象的年龄足够，会拷贝到年老代。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/how-young-gc-works-in-g1-2.png?raw=true" alt="how-young-gc-works-in-g1-2"></p><ol start="3"><li>结果：图三有新增的 “Recently Copied” 两块。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/how-young-gc-works-in-g1-3.png?raw=true" alt="how-young-gc-works-in-g1-3"></p><h2 id="G1-GC-年老代的回收"><a href="#G1-GC-年老代的回收" class="headerlink" title="G1 GC 年老代的回收"></a>G1 GC 年老代的回收</h2><ol><li>初始标记阶段：初始标记的活着的对象在年轻代的垃圾收集上。在日志中，被标记为 GC pause (young) (inital-mark).</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/Initial-Marking-Phase.PNG?raw=true" alt="Initial-Marking-Phase"></p><ol start="2"><li>并行标记阶段，如果找到空区域（由”X”表示），则在 Remark 阶段立即将它们移除。此外，计算确定活跃度的信息。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/Concurrent-Marking-Phase.PNG?raw=true" alt="Concurrent-Marking-Phase"></p><ol start="3"><li>Remark 阶段，空区域被移除并回收，现在计算所有区域的区域活跃度。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/Remark-Phase.PNG?raw=true" alt="Remark-Phase"></p><ol start="4"><li>复制清理阶段，G1 选择具有最低”活跃度“的区域（比如引用其他 Region 最少的区域），那些可以最快收集的区域。这些区域与年轻代 GC 同时收集。这在日志中表示为 [GC pause (mixed)]。G1 实际上将 Stop-The-World 的操作放在一个时间区间，这样对应用性能和稳定性较好。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/Copying:Cleanup-Phase.PNG?raw=true" alt="Copying:Cleanup-Phase"></p><ol start="5"><li>复制清理阶段后，选择的区域已经被收集并压缩成图中所示的深蓝色区域和深绿色区域。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/After-Copying:Cleanup-Phase.PNG?raw=true" alt="After-Copying:Cleanup-Phase"></p><h2 id="JDK-8-中-JVM-的调整"><a href="#JDK-8-中-JVM-的调整" class="headerlink" title="JDK 8 中 JVM 的调整"></a>JDK 8 中 JVM 的调整</h2><p>JDK 8 的 JVM 去掉了永生代(PermGen)，用 Metaspace 来代替。Metaspace 使用系统的内存。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/Metaspace-And-PermGen.png?raw=true" alt="Metaspace-And-PermGen"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><blockquote class="blockquote-center"><a href="https://www.cubrid.org/blog/understanding-java-garbage-collection" target="_blank" rel="noopener">https://www.cubrid.org/blog/understanding-java-garbage-collection</a></blockquote> <blockquote class="blockquote-center"><a href="https://www.oracle.com/technetwork/tutorials/tutorials-1876574.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/tutorials/tutorials-1876574.html</a></blockquote> <blockquote class="blockquote-center"><a href="https://blog.idrsolutions.com/2017/05/g1gc-java-9-garbage-collector-explained-5-minutes/" target="_blank" rel="noopener">https://blog.idrsolutions.com/2017/05/g1gc-java-9-garbage-collector-explained-5-minutes/</a></blockquote> <blockquote class="blockquote-center"><a href="https://www.sczyh30.com/posts/Java/jvm-gc-hotspot-implements/" target="_blank" rel="noopener">https://www.sczyh30.com/posts/Java/jvm-gc-hotspot-implements/</a></blockquote><blockquote class="blockquote-center"><a href="https://software.intel.com/en-us/blogs/2014/06/18/part-1-tuning-java-garbage-collection-for-hbase" target="_blank" rel="noopener">https://software.intel.com/en-us/blogs/2014/06/18/part-1-tuning-java-garbage-collection-for-hbase</a></blockquote><blockquote class="blockquote-center"><a href="https://blog.csdn.net/elinespace/article/details/78852469" target="_blank" rel="noopener">https://blog.csdn.net/elinespace/article/details/78852469</a></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍 JVM GC 的发展。&lt;/p&gt;
    
    </summary>
    
    
      <category term="JVM" scheme="http://moqimoqidea.github.io/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>Linux 日志相关命令整理</title>
    <link href="http://moqimoqidea.github.io/2017/06/03/Linux-Check-Log-Commands/"/>
    <id>http://moqimoqidea.github.io/2017/06/03/Linux-Check-Log-Commands/</id>
    <published>2017-06-03T14:34:05.000Z</published>
    <updated>2018-11-23T01:30:07.208Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍开发中一些日志的常用操作。</p><a id="more"></a> <h2 id="tail"><a href="#tail" class="headerlink" title="tail"></a>tail</h2><p>n  是显示行号；相当于 nl 命令；例子如下：</p><ul><li>tail -100f test.log      实时监控 100 行日志</li><li>tail  -n  10  test.log   查询日志尾部最后 10 行的日志;</li><li>tail -n +10 test.log    查询 10 行之后的所有日志;</li></ul><h2 id="head"><a href="#head" class="headerlink" title="head"></a>head</h2><p>跟 tail 是相反的，tail 是看后多少行日志；例子如下：</p><ul><li>head -n 10  test.log   查询日志文件中的头 10 行日志;</li><li>head -n -10  test.log   查询日志文件除了最后 10 行的其他所有日志;</li></ul><h2 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h2><p>tac 是倒序查看，是 cat 单词反写；例子如下：</p><ul><li>cat -n test.log |grep “debug”   查询关键字的日志</li></ul><h2 id="vim"><a href="#vim" class="headerlink" title="vim"></a>vim</h2><p>进入编辑查找：vi(vim) </p><ul><li>进入vim编辑模式：<ul><li>vim  filename</li><li>vim +n filename  进入特定行号日志</li></ul></li><li>输入命令“set nu” 显示行号</li><li>输入“/关键字”,按enter键查找</li><li>查找下一个，按“n”即可</li><li>退出：按ESC键后，接着再输入:号时，vi会在屏幕的最下方等待我们输入命令<ul><li>wq! 保存退出；</li><li>q! 不保存退出；</li></ul></li><li>切换方向</li><li>/关键字  　　注：正向查找，按n键把光标移动到下一个符合条件的地方</li><li>?关键字  　　注：反向查找，按shift+n 键，把光标移动到下一个符合条件的</li></ul><h2 id="搜索关键字附近的日志"><a href="#搜索关键字附近的日志" class="headerlink" title="搜索关键字附近的日志"></a><strong>搜索关键字附近的日志</strong></h2><ul><li>最常用的：cat -n filename |grep “关键字”</li><li>其他情况：<ul><li>cat filename | grep -C 5 ‘关键字’ 　　(显示日志里匹配字串那行以及前后5行)</li><li>cat filename | grep -B 5 ‘关键字’ 　　(显示匹配字串及前5行)</li><li>cat filename | grep -A 5 ‘关键字’ 　　(显示匹配字串及后5行)</li></ul></li></ul><h2 id="按行号查看-过滤出关键字附近的日志"><a href="#按行号查看-过滤出关键字附近的日志" class="headerlink" title="按行号查看 - 过滤出关键字附近的日志"></a>按行号查看 - 过滤出关键字附近的日志</h2><ol><li>cat -n test.log |grep “debug”  得到关键日志的行号</li><li>cat -n test.log |tail -n +92|head -n 20  选择关键字所在的中间一行. 然后查看这个关键字后 20 行的日志:<ul><li>tail -n +92 表示查询第 92 行之后的日志</li><li>head -n 20 则表示在前面的查询结果里再查后 20 条记录</li></ul></li></ol><h2 id="根据日期查询日志"><a href="#根据日期查询日志" class="headerlink" title="根据日期查询日志"></a>根据日期查询日志</h2><p>sed -n ‘/2014-12-17 16:17:20/,/2014-12-17 16:17:36/p’  test.log</p><ul><li>特别说明:上面的两个日期必须是日志中打印出来的日志，否则无效；</li><li>先 grep ‘2014-12-17 16:17:20’ test.log 来确定日志中是否有该 时间点</li></ul><h2 id="日志内容特别多，打印在屏幕上不方便查看"><a href="#日志内容特别多，打印在屏幕上不方便查看" class="headerlink" title="日志内容特别多，打印在屏幕上不方便查看"></a>日志内容特别多，打印在屏幕上不方便查看</h2><ul><li>使用 more 和 less 命令，如： cat -n test.log |grep “debug” |more     这样就分页打印了,通过点击空格键翻页</li><li>使用 &gt;xxx.txt 将其保存到文件中，到时可以拉下这个文件分析，如：cat -n test.log |grep “debug”  &gt; debug.txt</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote class="blockquote-center"><a href="https://blog.csdn.net/yangkai_hudong/article/details/47783487" target="_blank" rel="noopener">https://blog.csdn.net/yangkai_hudong/article/details/47783487</a></blockquote> <blockquote class="blockquote-center"><a href="https://www.cnblogs.com/hunt/p/7064886.html" target="_blank" rel="noopener">https://www.cnblogs.com/hunt/p/7064886.html</a></blockquote> <blockquote class="blockquote-center"><a href="https://blog.csdn.net/dingnning/article/details/7189862" target="_blank" rel="noopener">https://blog.csdn.net/dingnning/article/details/7189862</a></blockquote> ]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍开发中一些日志的常用操作。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="http://moqimoqidea.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>IDEA - Debugger 经验总结</title>
    <link href="http://moqimoqidea.github.io/2017/06/02/IDEA-Debugger/"/>
    <id>http://moqimoqidea.github.io/2017/06/02/IDEA-Debugger/</id>
    <published>2017-06-02T06:38:42.000Z</published>
    <updated>2018-11-22T06:54:24.902Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍 IDEA - Debugger 的一些操作。每个都有场景和操作说明，动态 Gif 图体积较大，请耐心等待。</p><p><strong>觉得图比较小的单击查看大图。</strong></p><a id="more"></a> <h2 id="分析外部堆栈跟踪"><a href="#分析外部堆栈跟踪" class="headerlink" title="分析外部堆栈跟踪"></a>分析外部堆栈跟踪</h2><p>把报错信息复制到 Analyze -&gt; Analyze Stacktrace，快速进入程序块。开发中经常可以看到生产环境有错误日志，依照此方法快速将日志导入项目，定位问题。</p><p>场景：</p><p><img src="https://raw.githubusercontent.com/onefansofworld/hexo_images/master/201806/Analysis_Static_Stacktrace.png" alt="scene01"></p><p>操作：</p><p><img src="https://raw.githubusercontent.com/onefansofworld/hexo_images/master/201806/Analyze_Stacktrace.gif" alt="operate01"></p><h2 id="返回到前一个堆栈帧"><a href="#返回到前一个堆栈帧" class="headerlink" title="返回到前一个堆栈帧"></a>返回到前一个堆栈帧</h2><p>IDEA 可在程序的执行流程中回退到先前的堆栈帧。要求不是最上面入口方法，选择 Drop Frame 后，等于未进入调用的方法。请注意：已经对全局状态进行的更改不会被恢复，只有本地变量会被重置。</p><p><img src="https://raw.githubusercontent.com/onefansofworld/hexo_images/master/201806/drop_frame.gif" alt="drop_frame"></p><h2 id="强制从当前方法返回"><a href="#强制从当前方法返回" class="headerlink" title="强制从当前方法返回"></a>强制从当前方法返回</h2><p>在当前堆栈帧中右键单击选择 Force Return 然后根据需要的返回类型输入即可。</p><p><img src="https://raw.githubusercontent.com/onefansofworld/hexo_images/master/201806/force_return.gif" alt="force_return"></p><h2 id="抛出一个异常"><a href="#抛出一个异常" class="headerlink" title="抛出一个异常"></a>抛出一个异常</h2><p>在当前堆栈帧中右键单击选择 Throw Exception 然后手动输入异常即可，比如 new NullPointerException();</p><p><img src="https://raw.githubusercontent.com/onefansofworld/hexo_images/master/201806/throw_expection.gif" alt="throw_exception"></p><h2 id="重新加载修改的类"><a href="#重新加载修改的类" class="headerlink" title="重新加载修改的类"></a>重新加载修改的类</h2><p>一般而言应用于在 Debugger 时发现未调用的方法有需要改动的地方，这时候修改未调用的方法，然后选择 Run -&gt; Reload Changed Classes, 快捷键 Alt + U, 然后 A. 这时候 Debugger 继续进行调用，则执行的调用方法逻辑为重新编译之后。底层逻辑是用到 JVM 的 hotSwap.</p><p><img src="https://raw.githubusercontent.com/onefansofworld/hexo_images/master/201806/reload_change_class.gif" alt="reload_change_class"></p><h2 id="分析-Java-Stream-操作"><a href="#分析-Java-Stream-操作" class="headerlink" title="分析 Java Stream 操作"></a>分析 Java Stream 操作</h2><p>IDEA Debugger 时可以可视化 Java Stream 进行的操作和对值数据的影响，需要断点停留在 Stream 上点击 Trace Current Stream Chain 按钮。</p><p><img src="https://raw.githubusercontent.com/onefansofworld/hexo_images/master/201806/stream_trace.gif" alt="stream_trace"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote class="blockquote-center"><a href="https://www.jetbrains.com/help/idea/analyzing-external-stacktraces.html" target="_blank" rel="noopener">https://www.jetbrains.com/help/idea/analyzing-external-stacktraces.html</a></blockquote> <blockquote class="blockquote-center"><a href="https://www.jetbrains.com/help/idea/altering-the-program-s-execution-flow.html" target="_blank" rel="noopener">https://www.jetbrains.com/help/idea/altering-the-program-s-execution-flow.html</a></blockquote> <blockquote class="blockquote-center"><a href="https://www.jetbrains.com/help/idea/analyze-java-stream-operations.html" target="_blank" rel="noopener">https://www.jetbrains.com/help/idea/analyze-java-stream-operations.html</a></blockquote> ]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍 IDEA - Debugger 的一些操作。每个都有场景和操作说明，动态 Gif 图体积较大，请耐心等待。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;觉得图比较小的单击查看大图。&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="IDEA" scheme="http://moqimoqidea.github.io/tags/IDEA/"/>
    
  </entry>
  
  <entry>
    <title>Start</title>
    <link href="http://moqimoqidea.github.io/2017/06/01/Start/"/>
    <id>http://moqimoqidea.github.io/2017/06/01/Start/</id>
    <published>2017-06-01T14:02:59.000Z</published>
    <updated>2017-06-02T15:52:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文简要讨论了博客起源。</p><a id="more"></a> <ul><li>博客搭建于 20170601，儿童节做出的决定。</li><li>目前定位于分享技术和个人思考，狭义来讲技术在最近一段时间是开发工具 IntelliJ IDEA 的一些最佳实践，个人思考在最近一段时间是关于一些书的读后感。</li><li>目前对 Github + Hexo + NexT 刚开始熟悉，有建议欢迎联系邮箱。</li><li>Ernest Hemingway once wrote:”The world is a fine place，and worth fighting for.”I agree with the second part.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文简要讨论了博客起源。&lt;/p&gt;
    
    </summary>
    
    
      <category term="default" scheme="http://moqimoqidea.github.io/tags/default/"/>
    
  </entry>
  
</feed>
