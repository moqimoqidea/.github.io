<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Coding and Talking</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://moqimoqidea.github.io/"/>
  <updated>2018-11-25T05:58:31.562Z</updated>
  <id>http://moqimoqidea.github.io/</id>
  
  <author>
    <name>moqi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据仓库建设</title>
    <link href="http://moqimoqidea.github.io/2017/08/13/Data-Warehouse-Construction/"/>
    <id>http://moqimoqidea.github.io/2017/08/13/Data-Warehouse-Construction/</id>
    <published>2017-08-13T02:03:25.000Z</published>
    <updated>2018-11-25T05:58:31.562Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍数据仓库的起源和基本搭建。</p><a id="more"></a> <h1 id="什么是数据仓库"><a href="#什么是数据仓库" class="headerlink" title="什么是数据仓库"></a>什么是数据仓库</h1><p>数据仓库，英文名称为Data Warehouse，可简写为DW或DWH。数据仓库，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。它出于分析性报告和决策支持目的而创建。 为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制。<br>数据仓库能干什么？</p><ol><li>年度销售目标的指定，需要根据以往的历史报表进行决策，不能拍脑袋。</li><li>如何优化业务流程？<ol><li>案例1：某公司需要对APP进行推广，考核的主要目标是下载安装，有些第三方渠道会对这些数据造假，比如某个渠道在凌晨批量下载，点赞操作，操作步骤一致。通过数据分析，分析出应用的名称和安装时间，来判断一个渠道的是否优质、是否作假。</li><li>案例2：一个电商网站订单的完成包括：浏览、下单、支付、物流，其中物流环节可能和中通、申通、韵达等快递公司合作。快递公司每派送一个订单，都会有订单派送的确认时间，可以根据订单派送时间来分析哪个快递公司比较快捷高效，从而选择与哪些快递公司合作，剔除哪些快递公司，增加用户友好型。</li></ol></li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/01-Shopping-Website-Order.png?raw=true" alt="01-Shopping-Website-Order"></p><h1 id="数据仓库的特点"><a href="#数据仓库的特点" class="headerlink" title="数据仓库的特点"></a>数据仓库的特点</h1><h2 id="数据是面向主题的"><a href="#数据是面向主题的" class="headerlink" title="数据是面向主题的"></a>数据是面向主题的</h2><p>与传统数据库面向应用进行数据组织的特点相对应，数据仓库中的数据是面向主题进行组织的。什么是主题呢？首先，主题是一个抽象的概念，是较高层次上企业信息系统中的数据综合、归类并进行分析利用的抽象。在逻辑意义上，它是对应企业中某一宏观分析领域所涉及的分析对象。面向主题的数据组织方式，就是在较高层次上对分析对象的数据的一个完整、一致的描述，能完整、统一地刻划各个分析对象所涉及的企业的各项数据，以及数据之间的联系。所谓较高层次是相对面向应用的数据组织方式而言的，是指按照主题进行数据组织的方式具有更高的数据抽象级别。</p><h2 id="数据是集成的"><a href="#数据是集成的" class="headerlink" title="数据是集成的"></a>数据是集成的</h2><p>数据仓库的数据是从原有的分散的数据库数据抽取来的。操作型数据与DSS分析型数据之间差别甚大。第一，数据仓库的每一个主题所对应的源数据在原有的各分散数据库中有许多重复和不一致的地方，且来源于不同的联机系统的数据都和不同的应用逻辑捆绑在一起；第二，数据仓库中的综合数据不能从原有的数据库系统直接得到。因此在数据进入数据仓库之前，必然要经过统一与综合，这一步是数据仓库建设中最关键、最复杂的一步，所要完成的工作有：</p><ol><li>要统一源数据中所有矛盾之处，如字段的同名异义、异名同义、单位不统一、字长不一致，等等。</li><li><p>进行数据综合和计算。数据仓库中的数据综合工作可以在从原有数据库抽取 数据时生成，但许多是在数据仓库内部生成的，即进入数据仓库以后进行综合生成的。</p><h2 id="数据是不可更新的"><a href="#数据是不可更新的" class="headerlink" title="数据是不可更新的"></a>数据是不可更新的</h2><p>数据仓库的数据主要供企业决策分析之用，所涉及的数据操作主要是数据查询，一般情况下并不进行修改操作。数据仓库的数据反映的是一段相当长的时间内历史数据的内容，是不同时点的数据库快照的集合，以及基于这些快照进行统计、综合和重组的导出数据，而不是联机处理的数据。数据库中进行联机处理的数据经过集成输入到数据仓库中，一旦数据仓库存放的数据已经超过数据仓库的数据存储期限，这些数据将从当前的数据仓库中删去。因为数据仓库只进行数据查询操作，所以数据仓库管理系统相比数据库管理系统而言要简单得多。数据库管理系统中许多技术难点，如完整性保护、并发控制等等，在数据仓库的管理中几乎可以省去。但是由于数据仓库的查询数据量往往很大，所以就对数据查询提出了更高的要求，它要求采用各种复杂的索引技术；同时由于数据仓库面向的是商业企业的高层管理者，他们会对数据查询的界面友好性和数据表示提出更高的要求。</p><h2 id="数据是随时间不断变化的"><a href="#数据是随时间不断变化的" class="headerlink" title="数据是随时间不断变化的"></a>数据是随时间不断变化的</h2><p>数据仓库中的数据不可更新是针对应用来说的，也就是说，数据仓库的用户进行分析处理时是不进行数据更新操作的。但并不是说，在从数据集成输入数据仓库开始到最终被删除的整个数据生存周期中，所有的数据仓库数据都是永远不变的。<br>​    数据仓库的数据是随时间的变化而不断变化的，这是数据仓库数据的第四个特征。这一特征表现在以下3方面：</p></li><li><p>数据仓库随时间变化不断增加新的数据内容。数据仓库系统必须不断捕捉OLTP数据库中变化的数据，追加到数据仓库中去，也就是要不断地生成OLTP数据库的快照，经统一集成后增加到数据仓库中去；但对于确实不再变化的数据库快照，如果捕捉到新的变化数据，则只生成一个新的数据库快照增加进去，而不会对原有的数据库快照进行修改。</p></li><li>数据仓库随时间变化不断删去旧的数据内容。数据仓库的数据也有存储期限，一旦超过了这一期限，过期数据就要被删除。只是数据仓库内的数据时限要远远长于操作型环境中的数据时限。在操作型环境中一般只保存有60~90天的数据，而在数据仓库中则需要保存较长时限的数据（如5~10年），以适应DSS进行趋势分析的要求。</li><li>数据仓库中包含有大量的综合数据，这些综合数据中很多跟时间有关，如数据经常按照时间段进行综合，或隔一定的时间片进行抽样等等。这些数据要随着时间的变化不断地进行重新综合。因此，数据仓库的数据特征都包含时间项，以标明数据的历史时期。</li></ol><h1 id="数据仓库的发展历程"><a href="#数据仓库的发展历程" class="headerlink" title="数据仓库的发展历程"></a>数据仓库的发展历程</h1><p>数据仓库的发展大致经历了这样的三个过程：</p><h2 id="简单报表阶段"><a href="#简单报表阶段" class="headerlink" title="简单报表阶段"></a>简单报表阶段</h2><p>这个阶段，系统的主要目标是解决一些日常的工作中业务人员需要的报表，以及生成一些简单的能够帮助领导进行决策所需要的汇总数据。这个阶段的大部分表现形式为数据库和前端报表工具。</p><h2 id="数据集市阶段"><a href="#数据集市阶段" class="headerlink" title="数据集市阶段"></a>数据集市阶段</h2><p>这个阶段，主要是根据某个业务部门的需要，进行一定的数据的采集，整理，按照业务人员的需要，进行多维报表的展现，能够提供对特定业务指导的数据，并且能够提供特定的领导决策数据。</p><h2 id="数据仓库阶段"><a href="#数据仓库阶段" class="headerlink" title="数据仓库阶段"></a>数据仓库阶段</h2><p>这个阶段，主要是按照一定的数据模型，对整个企业的数据进行采集，整理，并且能够按照各个业务部门的需要，提供跨部门的，完全一致的业务报表数据，能够通过数据仓库生成对对业务具有指导性的数据，同时，为领导决策提供全面的数据支持。</p><p>通过数据仓库建设的发展阶段，我们能够看出，数据仓库的建设和数据集市的建设的重要区别就在于数据模型的支持。因此，数据模型的建设，对于我们数据仓库的建设，有着决定性的意义。</p><h1 id="数据仓库和数据库的区别"><a href="#数据仓库和数据库的区别" class="headerlink" title="数据仓库和数据库的区别"></a>数据仓库和数据库的区别</h1><p>了解数据库与数据仓库的区别之前，首先掌握三个概念。数据库软件、数据库、数据仓库。<br>数据库软件：是一种软件，可以看得见，可以操作。用来实现数据库逻辑功能。属于物理层。<br>数据库：是一种逻辑概念，用来存放数据的仓库。通过数据库软件来实现。数据库由很多表组成，表是二维的，一张表里可以有很多字段。字段一字排开，对应的数据就一行一行写入表中。数据库的表，在于能够用二维表现多维关系。目前市面上流行的数据库都是二维数据库。如：Oracle、DB2、MySQL、Sybase、MS SQL Server等。<br>数据仓库：是数据库概念的升级。从逻辑上理解，数据库和数据仓库没有区别，都是通过数据库软件实现的存放数据的地方，只不过从数据量来说，数据仓库要比数据库更庞大得多。数据仓库主要用于数据挖掘和数据分析，辅助领导做决策。<br>在IT的架构体系中，数据库是必须存在的。必须要有地方存放数据。比如现在的网购，淘宝，京东等等。物品的存货数量，货品的价格，用户的账户余额之类的。这些数据都是存放在后台数据库中。或者最简单理解，我们现在微博，QQ等账户的用户名和密码。在后台数据库必然有一张user表，字段起码有两个，即用户名和密码，然后我们的数据就一行一行的存在表上面。当我们登录的时候，我们填写了用户名和密码，这些数据就会被传回到后台去，去跟表上面的数据匹配，匹配成功了，你就能登录了。匹配不成功就会报错说密码错误或者没有此用户名等。这个就是数据库，数据库在生产环境就是用来干活的。凡是跟业务应用挂钩的，我们都使用数据库。<br>数据仓库则是BI下的其中一种技术。由于数据库是跟业务应用挂钩的，所以一个数据库不可能装下一家公司的所有数据。数据库的表设计往往是针对某一个应用进行设计的。比如刚才那个登录的功能，这张user表上就只有这两个字段，没有别的字段了。但是这张表符合应用，没有问题。但是这张表不符合分析。比如我想知道在哪个时间段，用户登录的量最多？哪个用户一年购物最多？诸如此类的指标。那就要重新设计数据库的表结构了。对于数据分析和数据挖掘，我们引入数据仓库概念。数据仓库的表结构是依照分析需求，分析维度，分析指标进行设计的。<br>数据库与数据仓库的区别实际讲的是OLTP与OLAP的区别。<br>操作型处理，叫联机事务处理OLTP（On-Line Transaction Processing），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。<br>分析型处理，叫联机分析处理OLAP（On-Line Analytical Processing）一般针对某些主题的历史数据进行分析，支持管理决策。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/02-OLTP-AND-OLAP.png?raw=true" alt="02-OLTP-AND-OLAP"></p><h1 id="数据仓库架构分层"><a href="#数据仓库架构分层" class="headerlink" title="数据仓库架构分层"></a>数据仓库架构分层</h1><p>数据仓库标准上可以分为四层：ODS（临时存储层）、PDW（数据仓库层）、DM（数据集市层）、APP（应用层）。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/03-DWBI.png?raw=true" alt="03-DWBI"></p><h2 id="ODS层"><a href="#ODS层" class="headerlink" title="ODS层"></a>ODS层</h2><p>为临时存储层，是接口数据的临时存储区域，为后一步的数据处理做准备。一般来说ODS层的数据和源系统的数据是同构的，主要目的是简化后续数据加工处理的工作。从数据粒度上来说ODS层的数据粒度是最细的。ODS层的表通常包括两类，一个用于存储当前需要加载的数据，一个用于存储处理完后的历史数据。历史数据一般保存3-6个月后需要清除，以节省空间。但不同的项目要区别对待，如果源系统的数据量不大，可以保留更长的时间，甚至全量保存；</p><h2 id="PDW层"><a href="#PDW层" class="headerlink" title="PDW层"></a>PDW层</h2><p>为数据仓库层，PDW层的数据应该是一致的、准确的、干净的数据，即对源系统数据进行了清洗（去除了杂质）后的数据。这一层的数据一般是遵循数据库第三范式的，其数据粒度通常和ODS的粒度相同。在PDW层会保存BI系统中所有的历史数据，例如保存10年的数据。</p><h2 id="DM层"><a href="#DM层" class="headerlink" title="DM层"></a>DM层</h2><p>为数据集市层，这层数据是面向主题来组织数据的，通常是星形或雪花结构的数据。从数据粒度来说，这层的数据是轻度汇总级的数据，已经不存在明细数据了。从数据的时间跨度来说，通常是PDW层的一部分，主要的目的是为了满足用户分析的需求，而从分析的角度来说，用户通常只需要分析近几年（如近三年的数据）的即可。从数据的广度来说，仍然覆盖了所有业务数据。</p><h2 id="APP层"><a href="#APP层" class="headerlink" title="APP层"></a>APP层</h2><p>为应用层，这层数据是完全为了满足具体的分析需求而构建的数据，也是星形或雪花结构的数据。从数据粒度来说是高度汇总的数据。从数据的广度来说，则并不一定会覆盖所有业务数据，而是DM层数据的一个真子集，从某种意义上来说是DM层数据的一个重复。从极端情况来说，可以为每一张报表在APP层构建一个模型来支持，达到以空间换时间的目的数据仓库的标准分层只是一个建议性质的标准，实际实施时需要根据实际情况确定数据仓库的分层，不同类型的数据也可能采取不同的分层方法。</p><h2 id="为什么要对数据仓库分层"><a href="#为什么要对数据仓库分层" class="headerlink" title="为什么要对数据仓库分层"></a>为什么要对数据仓库分层</h2><ol><li>用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据；</li><li>如果不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。</li><li>通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。</li></ol><h1 id="数据质量检查"><a href="#数据质量检查" class="headerlink" title="数据质量检查"></a>数据质量检查</h1><p>保证报表数据的正确性、稳定性，通过告警机制尽可能快的发现异常、尽可能快的解决问题。</p><p>检查方法：</p><ol><li>数据行数据的比较。</li><li>行数有变化，但是指标有变化。对重点指标进行筛选。</li><li>发现问题，及时通知相关模块负责人</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/04-Rule.png?raw=true" alt="04-Rule"></p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/05-Exception.png?raw=true" alt="05-Exception"></p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/06-Mail.png?raw=true" alt="06-Mail"></p><h1 id="元数据介绍"><a href="#元数据介绍" class="headerlink" title="元数据介绍"></a>元数据介绍</h1><p>当需要了解某地企业及其提供的服务时，电话黄页的重要性就体现出来了。元数据（Metadata）类似于这样的电话黄页。</p><h2 id="元数据的定义"><a href="#元数据的定义" class="headerlink" title="元数据的定义"></a>元数据的定义</h2><p>   数据仓库的元数据是关于数据仓库中数据的数据。它的作用类似于数据库管理系统的数据字典，保存了逻辑数据结构、文件、地址和索引等信息。广义上讲，在数据仓库中，元数据描述了数据仓库内数据的结构和建立方法的数据。</p><p>   <img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/07-Metadata.png?raw=true" alt="07-Metadata"></p><p>   元数据是数据仓库管理系统的重要组成部分，元数据管理器是企业级数据仓库中的关键组件，贯穿数据仓库构建的整个过程，直接影响着数据仓库的构建、使用和维护。</p><ol><li>构建数据仓库的主要步骤之一是ETL。这时元数据将发挥重要的作用，它定义了源数据系统到数据仓库的映射、数据转换的规则、数据仓库的逻辑结构、数据更新的规则、数据导入历史记录以及装载周期等相关内容。数据抽取和转换的专家以及数据仓库管理员正是通过元数据高效地构建数据仓库。</li><li>用户在使用数据仓库时，通过元数据访问数据，明确数据项的含义以及定制报表。</li><li><p>数据仓库的规模及其复杂性离不开正确的元数据管理，包括增加或移除外部数据源，改变数据清洗方法，控制出错的查询以及安排备份等。</p><p>元数据可分为技术元数据和业务元数据。技术元数据为开发和管理数据仓库的IT 人员使用，它描述了与数据仓库开发、管理和维护相关的数据，包括数据源信息、数据转换描述、数据仓库模型、数据清洗与更新规则、数据映射和访问权限等。而业务元数据为管理层和业务分析人员服务，从业务角度描述数据，包括商务术语、数据仓库中有什么数据、数据的位置和数据的可用性等，帮助业务人员更好地理解数据仓库中哪些数据是可用的以及如何使用。<br>由上可见，元数据不仅定义了数据仓库中数据的模式、来源、抽取和转换规则等，而且是整个数据仓库系统运行的基础，元数据把数据仓库系统中各个松散的组件联系起来，组成了一个有机的整体，如图所示：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/08-Metadata-And-Another.png?raw=true" alt="08-Metadata-And-Another"></p></li></ol><h2 id="元数据的存储方式"><a href="#元数据的存储方式" class="headerlink" title="元数据的存储方式"></a>元数据的存储方式</h2><p>   元数据有两种常见存储方式：一种是以数据集为基础，每一个数据集有对应的元数据文件，每一个元数据文件包含对应数据集的元数据内容；另一种存储方式是以数据库为基础，即元数据库。其中元数据文件由若干项组成，每一项表示元数据的一个要素，每条记录为数据集的元数据内容。上述存储方式各有优缺点，第一种存储方式的优点是调用数据时相应的元数据也作为一个独立的文件被传输，相对数据库有较强的独立性，在对元数据进行检索时可以利用数据库的功能实现，也可以把元数据文件调到其他数据库系统中操作；不足是如果每一数据集都对应一个元数据文档，在规模巨大的数据库中则会有大量的元数据文件，管理不方便。第二种存储方式下，元数据库中只有一个元数据文件，管理比较方便，添加或删除数据集，只要在该文件中添加或删除相应的记录项即可。在获取某数据集的元数据时，因为实际得到的只是关系表格数据的一条记录，所以要求用户系统可以接受这种特定形式的数据。因此推荐使用元数据库的方式。<br>   元数据库用于存储元数据，因此元数据库最好选用主流的关系数据库管理系统。元数据库还包含用于操作和查询元数据的机制。建立元数据库的主要好处是提供统一的数据结构和业务规则，易于把企业内部的多个数据集市有机地集成起来。目前，一些企业倾向建立多个数据集市，而不是一个集中的数据仓库，这时可以考虑在建立数据仓库（或数据集市）之前，先建立一个用于描述数据、服务应用集成的元数据库，做好数据仓库实施的初期支持工作，对后续开发和维护有很大的帮助。元数据库保证了数据仓库数据的一致性和准确性，为企业进行数据质量管理提供基础。</p><h2 id="元数据的作用"><a href="#元数据的作用" class="headerlink" title="元数据的作用"></a>元数据的作用</h2><ol><li>描述哪些数据在数据仓库中，帮助决策分析者对数据仓库的内容定位。</li><li>定义数据进入数据仓库的方式，作为数据汇总、映射和清洗的指南。</li><li>记录业务事件发生而随之进行的数据抽取工作时间安排。</li><li>记录并检测系统数据一致性的要求和执行情况。</li><li>评估数据质量。</li></ol><h1 id="什么是数据模型"><a href="#什么是数据模型" class="headerlink" title="什么是数据模型"></a>什么是数据模型</h1><p>数据模型是抽象描述现实世界的一种工具和方法，是通过抽象的实体及实体之间联系的形式，来表示现实世界中事务的相互关系的一种映射。在这里，数据模型表现的抽象的是实体和实体之间的关系，通过对实体和实体之间关系的定义和描述，来表达实际的业务中具体的业务关系。<br>数据仓库模型是数据模型中针对特定的数据仓库应用系统的一种特定的数据模型，一般的来说，我们数据仓库模型分为几下几个层次，如图所示：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/09-Data-Warehouse-Hierarchy.png?raw=true" alt="09-Data-Warehouse-Hierarchy"></p><p>通过上面的图形，我们能够很容易的看出在整个数据仓库得建模过程中，我们需要经历一般四个过程：</p><ul><li>业务建模，生成业务模型，主要解决业务层面的分解和程序化。</li><li>领域建模，生成领域模型，主要是对业务模型进行抽象处理，生成领域概念模型。</li><li>逻辑建模，生成逻辑模型，主要是将领域模型的概念实体以及实体之间的关系进行数据库层次的逻辑化。</li><li>物理建模，生成物理模型，主要解决，逻辑模型针对不同关系型数据库的物理化以及性能等一些具体的技术问题。</li></ul><p>因此，在整个数据仓库的模型的设计和架构中，既涉及到业务知识，也涉及到了具体的技术，我们既需要了解丰富的行业经验，同时，也需要一定的信息技术来帮助我们实现我们的数据模型，最重要的是，我们还需要一个非常适用的方法论，来指导我们自己针对我们的业务进行抽象，处理，生成各个阶段的模型。</p><h1 id="为什么需要数据模型"><a href="#为什么需要数据模型" class="headerlink" title="为什么需要数据模型"></a>为什么需要数据模型</h1><p>通过数据仓库建设的发展阶段，我们能够看出，数据仓库的建设和数据集市的建设的重要区别就在于数据模型的支持。因此，数据模型的建设，对于我们数据仓库的建设，有着决定性的意义。</p><p>一般来说，数据模型的建设主要能够帮助我们解决以下的一些问题：</p><ol><li>进行全面的业务梳理，改进业务流程。在业务模型建设的阶段，能够帮助我们的企业或者是管理机关对本单位的业务进行全面的梳理。通过业务模型的建设，我们应该能够全面了解该单位的业务架构图和整个业务的运行情况，能够将业务按照特定的规律进行分门别类和程序化，同时，帮助我们进一步的改进业务的流程，提高业务效率，指导我们的业务部门的生产。</li><li>建立全方位的数据视角，消灭信息孤岛和数据差异。通过数据仓库的模型建设，能够为企业提供一个整体的数据视角，不再是各个部门只是关注自己的数据，而且通过模型的建设，勾勒出了部门之间内在的联系，帮助消灭各个部门之间的信息孤岛的问题，更为重要的是，通过数据模型的建设，能够保证整个企业的数据的一致性，各个部门之间数据的差异将会得到有效解决。</li><li>解决业务的变动和数据仓库的灵活性。通过数据模型的建设，能够很好的分离出底层技术的实现和上层业务的展现。当上层业务发生变化时，通过数据模型，底层的技术实现可以非常轻松的完成业务的变动，从而达到整个数据仓库系统的灵活性。</li><li>帮助数据仓库系统本身的建设。通过数据仓库的模型建设，开发人员和业务人员能够很容易的达成系统建设范围的界定，以及长期目标的规划，从而能够使整个项目组明确当前的任务，加快整个系统建设的速度。</li></ol><h1 id="如何建设数据仓库模型"><a href="#如何建设数据仓库模型" class="headerlink" title="如何建设数据仓库模型"></a>如何建设数据仓库模型</h1><p>建设数据模型既然是整个数据仓库建设中一个非常重要的关键部分，那么，怎么建设我们的数据仓库模型就是我们需要解决的一个问题。这里我们将要详细介绍如何创建适合自己的数据模型。</p><h2 id="数据仓库数据模型架构"><a href="#数据仓库数据模型架构" class="headerlink" title="数据仓库数据模型架构"></a>数据仓库数据模型架构</h2><p>数据仓库的数据模型的架构和数据仓库的整体架构是紧密关联在一起的，我们首先来了解一下整个数据仓库的数据模型应该包含的几个部分。从下图我们可以很清楚地看到，整个数据模型的架构分成 5 大部分，每个部分其实都有其独特的功能。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/10-Data-Warehouse-Model-Architecture.png?raw=true" alt="10-Data-Warehouse-Model-Architecture"></p><p>从上图我们可以看出，整个数据仓库的数据模型可以分为大概 5 大部分：</p><ol><li>系统记录域（System of Record）：这部分是主要的数据仓库业务数据存储区，数据模型在这里保证了数据的一致性。</li><li>内部管理域（Housekeeping）：这部分主要存储数据仓库用于内部管理的元数据，数据模型在这里能够帮助进行统一的元数据的管理。</li><li>汇总域（Summary of Area）：这部分数据来自于系统记录域的汇总，数据模型在这里保证了分析域的主题分析的性能，满足了部分的报表查询。</li><li>分析域（Analysis Area）：这部分数据模型主要用于各个业务部分的具体的主题业务分析。这部分数据模型可以单独存储在相应的数据集市中。</li><li>反馈域（Feedback Area）：可选项，这部分数据模型主要用于相应前端的反馈数据，数据仓库可以视业务的需要设置这一区域。</li></ol><p>通过对整个数据仓库模型的数据区域的划分，我们可以了解到，一个好的数据模型，不仅仅是对业务进行抽象划分，而且对实现技术也进行具体的指导，它应该涵盖了从业务到实现技术的各个部分。</p><h2 id="数据仓库建模阶段划分"><a href="#数据仓库建模阶段划分" class="headerlink" title="数据仓库建模阶段划分"></a>数据仓库建模阶段划分</h2><p>我们前面介绍了数据仓库模型的几个层次，下面我们讲一下，针对这几个层次的不同阶段的数据建模的工作的主要内容：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/11-Data-Warehouse-Model-Hierarchy.png?raw=true" alt="11-Data-Warehouse-Model-Hierarchy"></p><h3 id="业务建模"><a href="#业务建模" class="headerlink" title="业务建模"></a>业务建模</h3><ol><li>划分整个单位的业务，一般按照业务部门的划分，进行各个部分之间业务工作的界定，理清各业务部门之间的关系。</li><li>深入了解各个业务部门的内具体业务流程并将其程序化。</li><li>提出修改和改进业务部门工作流程的方法并程序化。</li><li>数据建模的范围界定，整个数据仓库项目的目标和阶段划分。</li></ol><h3 id="领域概念建模"><a href="#领域概念建模" class="headerlink" title="领域概念建模"></a>领域概念建模</h3><ol><li>抽取关键业务概念，并将之抽象化。</li><li>将业务概念分组，按照业务主线聚合类似的分组概念。</li><li>细化分组概念，理清分组概念内的业务流程并抽象化。</li><li>理清分组概念之间的关联，形成完整的领域概念模型。</li></ol><h3 id="逻辑建模"><a href="#逻辑建模" class="headerlink" title="逻辑建模"></a>逻辑建模</h3><ol><li>业务概念实体化，并考虑其具体的属性。</li><li>事件实体化，并考虑其属性内容。</li><li>说明实体化，并考虑其属性内容。</li></ol><h3 id="物理建模"><a href="#物理建模" class="headerlink" title="物理建模"></a>物理建模</h3><ol><li>针对特定物理化平台，做出相应的技术调整。</li><li>针对模型的性能考虑，对特定平台作出相应的调整。</li><li>针对管理的需要，结合特定的平台，做出相应的调整。</li><li>生成最后的执行脚本，并完善之。</li></ol><p>从我们上面对数据仓库的数据建模阶段的各个阶段的划分，我们能够了解到整个数据仓库建模的主要工作和工作量，希望能够对我们在实际的项目建设能够有所帮助。</p><h2 id="数据仓库建模方法"><a href="#数据仓库建模方法" class="headerlink" title="数据仓库建模方法"></a>数据仓库建模方法</h2><p>大千世界，表面看五彩缤纷，实质上，万物都遵循其自有的法则。数据仓库的建模方法同样也有很多种，每一种建模方法其实代表了哲学上的一个观点，代表了一种归纳，概括世界的一种方法。目前业界较为流行的数据仓库的建模方法非常多，这里主要介绍范式建模法，维度建模法，实体建模法等几种方法，每种方法其实从本质上讲就是从不同的角度看我们业务中的问题，不管从技术层面还是业务层面，其实代表的是哲学上的一种世界观。我们下面给大家详细介绍一下这些建模方法。</p><h3 id="范式建模法"><a href="#范式建模法" class="headerlink" title="范式建模法"></a>范式建模法</h3><p>范式建模法（Third Normal Form，3NF）：范式建模法其实是我们在构建数据模型常用的一个方法，该方法的主要由 Inmon 所提倡，主要解决关系型数据库得数据存储，利用的一种技术层面上的方法。目前，我们在关系型数据库中的建模方法，大部分采用的是三范式建模法。<br>范式是数据库逻辑模型设计的基本理论，一个关系模型可以从第一范式到第五范式进行无损分解，这个过程也可称为规范化。在数据仓库的模型设计中目前一般采用第三范式，它有着严格的数学定义。从其表达的含义来看，一个符合第三范式的关系必须具有以下三个条件 :</p><ol><li>每个属性值唯一，不具有多义性 ;</li><li>每个非主属性必须完全依赖于整个主键，而非主键的一部分 ;</li><li>每个非主属性不能依赖于其他关系中的属性，因为这样的话，这种属性应该归到其他关系中去。</li></ol><p>由于范式是基于整个关系型数据库的理论基础之上发展而来的，因此，本人在这里不多做介绍，有兴趣的读者可以通过阅读相应的材料来获得这方面的知识。<br>根据 Inmon 的观点，数据仓库模型得建设方法和业务系统的企业数据模型类似。在业务系统中，企业数据模型决定了数据的来源，而企业数据模型也分为两个层次，即主题域模型和逻辑模型。同样，主题域模型可以看成是业务模型的概念模型，而逻辑模型则是域模型在关系型数据库上的实例。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/12-Business-And-DW.png?raw=true" alt="12-Business-And-DW"></p><p>从业务数据模型转向数据仓库模型时，同样也需要有数据仓库的域模型，即概念模型，同时也存在域模型的逻辑模型。这里，业务模型中的数据模型和数据仓库的模型稍微有一些不同。主要区别在于：</p><ul><li>数据仓库的域模型应该包含企业数据模型的域模型之间的关系，以及各主题域定义。数据仓库的域模型的概念应该比业务系统的主题域模型范围更加广。</li><li>在数据仓库的逻辑模型需要从业务系统的数据模型中的逻辑模型中抽象实体，实体的属性，实体的子类，以及实体的关系等。</li></ul><p>Inmon 的范式建模法的最大优点就是从关系型数据库的角度出发，结合了业务系统的数据模型，能够比较方便的实现数据仓库的建模。但其缺点也是明显的，由于建模方法限定在关系型数据库之上，在某些时候反而限制了整个数据仓库模型的灵活性，性能等，特别是考虑到数据仓库的底层数据向数据集市的数据进行汇总时，需要进行一定的变通才能满足相应的需求。因此，笔者建议读者们在实际的使用中，参考使用这一建模方式。</p><h3 id="维度建模法"><a href="#维度建模法" class="headerlink" title="维度建模法"></a>维度建模法</h3><p>维度建模法，Kimball 最先提出这一概念。其最简单的描述就是，按照事实表，维表来构建数据仓库，数据集市。这种方法的最被人广泛知晓的名字就是星型模式（Star-schema）。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/13-Dimension-Of-DW.png?raw=true" alt="13-Dimension-Of-DW"></p><p>上图的这个架构中是典型的星型架构。星型模式之所以广泛被使用，在于针对各个维作了大量的预处理，如按照维进行预先的统计、分类、排序等。通过这些预处理，能够极大的提升数据仓库的处理能力。特别是针对 3NF 的建模方法，星型模式在性能上占据明显的优势。<br>同时，维度建模法的另外一个优点是，维度建模非常直观，紧紧围绕着业务模型，可以直观的反映出业务模型中的业务问题。不需要经过特别的抽象处理，即可以完成维度建模。这一点也是维度建模的优势。<br>但是，维度建模法的缺点也是非常明显的，由于在构建星型模式之前需要进行大量的数据预处理，因此会导致大量的数据处理工作。而且，当业务发生变化，需要重新进行维度的定义时，往往需要重新进行维度数据的预处理。而在这些与处理过程中，往往会导致大量的数据冗余。<br>另外一个维度建模法的缺点就是，如果只是依靠单纯的维度建模，不能保证数据来源的一致性和准确性，而且在数据仓库的底层，不是特别适用于维度建模的方法。<br>因此以笔者的观点看，维度建模的领域主要适用与数据集市层，它的最大的作用其实是为了解决数据仓库建模中的性能问题。维度建模很难能够提供一个完整地描述真实业务实体之间的复杂关系的抽象方法。</p><h3 id="实体建模法"><a href="#实体建模法" class="headerlink" title="实体建模法"></a>实体建模法</h3><p>实体建模法并不是数据仓库建模中常见的一个方法，它来源于哲学的一个流派。从哲学的意义上说，客观世界应该是可以细分的，客观世界应该可以分成由一个个实体，以及实体与实体之间的关系组成。那么我们在数据仓库的建模过程中完全可以引入这个抽象的方法，将整个业务也可以划分成一个个的实体，而每个实体之间的关系，以及针对这些关系的说明就是我们数据建模需要做的工作。<br>虽然实体法粗看起来好像有一些抽象，其实理解起来很容易。即我们可以将任何一个业务过程划分成 3 个部分，实体，事件和说明，如下图所示：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/14-Entity-Of-Model.png?raw=true" alt="14-Entity-Of-Model"></p><p>上图表述的是一个抽象的含义，如果我们描述一个简单的事实：“小明开车去学校上学”。以这个业务事实为例，我们可以把“小明”，“学校”看成是一个实体，“上学”描述的是一个业务过程，我们在这里可以抽象为一个具体“事件”，而“开车去”则可以看成是事件“上学”的一个说明。<br>从上面的举例我们可以了解，我们使用的抽象归纳方法其实很简单，任何业务可以看成 3 个部分：</p><ul><li>实体，主要指领域模型中特定的概念主体，指发生业务关系的对象。</li><li>事件，主要指概念主体之间完成一次业务流程的过程，特指特定的业务过程。</li><li>说明，主要是针对实体和事件的特殊说明。</li></ul><p>由于实体建模法，能够很轻松的实现业务模型的划分，因此，在业务建模阶段和领域概念建模阶段，实体建模法有着广泛的应用。从笔者的经验来看，再没有现成的行业模型的情况下，我们可以采用实体建模的方法，和客户一起理清整个业务的模型，进行领域概念模型的划分，抽象出具体的业务概念，结合客户的使用特点，完全可以创建出一个符合自己需要的数据仓库模型来。<br>但是，实体建模法也有着自己先天的缺陷，由于实体说明法只是一种抽象客观世界的方法，因此，注定了该建模方法只能局限在业务建模和领域概念建模阶段。因此，到了逻辑建模阶段和物理建模阶段，则是范式建模和维度建模发挥长处的阶段。<br>因此，笔者建议读者在创建自己的数据仓库模型的时候，可以参考使用上述的三种数据仓库得建模方法，在各个不同阶段采用不同的方法，从而能够保证整个数据仓库建模的质量。</p><h1 id="维度建模"><a href="#维度建模" class="headerlink" title="维度建模"></a>维度建模</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在多维分析的商业智能解决方案中，根据事实表和维度表的关系，又可将常见的模型分为星型模型和雪花型模型。在设计逻辑型数据的模型的时候，就应考虑数据是按照星型模型还是雪花型模型进行组织。<br>当所有维表都直接连接到“ 事实表”上时，整个图解就像星星一样，故将该模型称为星型模型，如图 1 。<br>星型架构是一种非正规化的结构，多维数据集的每一个维度都直接与事实表相连接，不存在渐变维度，所以数据有一定的冗余，如在地域维度表中，存在国家 A 省 B 的城市 C 以及国家 A 省 B 的城市 D 两条记录，那么国家 A 和省 B 的信息分别存储了两次，即存在冗余。</p><p>销售数据仓库中的星型模型：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/15-Star-Model.png?raw=true" alt="15-Star-Model"></p><p>当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的 “ 层次 “ 区域，这些被分解的表都连接到主维度表而不是事实表。如图 2，将地域维表又分解为国家，省份，城市等维表。它的优点是 : 通过最大限度地减少数据存储量以及联合较小的维表来改善查询性能。雪花型结构去除了数据冗余。<br>销售数据仓库中的雪花型模型：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/16-Snowflake-Model.png?raw=true" alt="16-Snowflake-Model"></p><p>星型模型因为数据的冗余所以很多统计查询不需要做外部的连接，因此一般情况下效率比雪花型模型要高。星型结构不用考虑很多正规化的因素，设计与实现都比较简单。雪花型模型由于去除了冗余，有些统计就需要通过表的联接才能产生，所以效率不一定有星型模型高。正规化也是一种比较复杂的过程，相应的数据库结构设计、数据的 ETL、以及后期的维护都要复杂一些。因此在冗余可以接受的前提下，实际运用中星型模型使用更多，也更有效率。</p><h2 id="使用选择"><a href="#使用选择" class="headerlink" title="使用选择"></a>使用选择</h2><p>星形模型(Star Schema)和雪花模型(Snowflake Schema)是数据仓库中常用到的两种方式，而它们之间的对比要从四个角度来进行讨论。</p><h3 id="数据优化"><a href="#数据优化" class="headerlink" title="数据优化"></a>数据优化</h3><p>雪花模型使用的是规范化数据，也就是说数据在数据库内部是组织好的，以便消除冗余，因此它能够有效地减少数据量。通过引用完整性，其业务层级和维度都将存储在数据模型之中。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/17-Snowflake-Model.png?raw=true" alt="17-Snowflake-Model"></p><p>相比较而言，星形模型实用的是反规范化数据。在星形模型中，维度直接指的是事实表，业务层级不会通过维度之间的参照完整性来部署。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Data-Warehouse/18-Star-Model.png?raw=true" alt="18-Star-Model"></p><h3 id="业务模型"><a href="#业务模型" class="headerlink" title="业务模型"></a>业务模型</h3><p>主键是一个单独的唯一键(数据属性)，为特殊数据所选择。在上面的例子中，Advertiser_ID就将是一个主键。外键(参考属性)仅仅是一个表中的字段，用来匹配其他维度表中的主键。在我们所引用的例子中，Advertiser_ID将是Account_dimension的一个外键。<br>在雪花模型中，数据模型的业务层级是由一个不同维度表主键-外键的关系来代表的。而在星形模型中，所有必要的维度表在事实表中都只拥有外键。</p><h3 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h3><p>第三个区别在于性能的不同。雪花模型在维度表、事实表之间的连接很多，因此性能方面会比较低。举个例子，如果你想要知道Advertiser 的详细信息，雪花模型就会请求许多信息，比如Advertiser Name、ID以及那些广告主和客户表的地址需要连接起来，然后再与事实表连接。<br>而星形模型的连接就少的多，在这个模型中，如果你需要上述信息，你只要将Advertiser的维度表和事实表连接即可。</p><h3 id="ETL"><a href="#ETL" class="headerlink" title="ETL"></a>ETL</h3><p>雪花模型加载数据集市，因此ETL操作在设计上更加复杂，而且由于附属模型的限制，不能并行化。<br>星形模型加载维度表，不需要再维度之间添加附属模型，因此ETL就相对简单，而且可以实现高度的并行化。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>雪花模型使得维度分析更加容易，比如“针对特定的广告主，有哪些客户或者公司是在线的?”星形模型用来做指标分析更适合，比如“给定的一个客户他们的收入是多少?”</p><h2 id="缓慢变化维"><a href="#缓慢变化维" class="headerlink" title="缓慢变化维"></a>缓慢变化维</h2><p>维度建模的数据仓库中，有一个概念叫 Slowly Changing Dimensions, 中文一般翻译成”缓慢变化维“，经常被简写为 SCD. 缓慢变化维的提出是因为在现实世界中，维度的属性并不是静态的，它会随着时间的流失发生缓慢的变化。这种随着时间发生变化的维度我们一般称之为缓慢变化维，并且把处理温度表的历史变化信息的问题称为处理缓慢变化维问题，有时也简称为处理 SCD 的问题。</p><p>如何解决缓慢变化维带来的影响？</p><h3 id="第一种方法"><a href="#第一种方法" class="headerlink" title="第一种方法"></a>第一种方法</h3><p>直接在原来维度的基础上进行更新，不会产生新的记录。</p><p>更新前：</p><table><thead><tr><th style="text-align:center">emp_rid</th><th style="text-align:center">emp_id</th><th style="text-align:center">emp_name</th><th style="text-align:center">position</th></tr></thead><tbody><tr><td style="text-align:center">101212</td><td style="text-align:center">12345</td><td style="text-align:center">Jack</td><td style="text-align:center">Developer</td></tr></tbody></table><p>更新后：</p><table><thead><tr><th style="text-align:center">emp_rid</th><th style="text-align:center">emp_id</th><th style="text-align:center">emp_name</th><th style="text-align:center">position</th></tr></thead><tbody><tr><td style="text-align:center">101212</td><td style="text-align:center">12345</td><td style="text-align:center">Jack</td><td style="text-align:center">Manager</td></tr></tbody></table><p>position 发生变化。</p><h3 id="第二种方法"><a href="#第二种方法" class="headerlink" title="第二种方法"></a>第二种方法</h3><p>不修改原有的数据，重新产生一条新的记录，这样就可以追溯所有的历史记录。</p><p>更新前：</p><table><thead><tr><th style="text-align:center">emp_rid</th><th style="text-align:center">emp_id</th><th style="text-align:center">emp_name</th><th style="text-align:center">position</th><th style="text-align:center">start_date</th><th style="text-align:center">end_date</th></tr></thead><tbody><tr><td style="text-align:center">101212</td><td style="text-align:center">12345</td><td style="text-align:center">Jack</td><td style="text-align:center">Developer</td><td style="text-align:center">2010-02-05</td><td style="text-align:center">2012-06-12</td></tr></tbody></table><p>更新后：</p><table><thead><tr><th style="text-align:center">emp_rid</th><th style="text-align:center">emp_id</th><th style="text-align:center">emp_name</th><th style="text-align:center">position</th><th style="text-align:center">start_date</th><th style="text-align:center">end_date</th></tr></thead><tbody><tr><td style="text-align:center">101212</td><td style="text-align:center">12345</td><td style="text-align:center">Jack</td><td style="text-align:center">Manager</td><td style="text-align:center">2012-06-12</td></tr></tbody></table><p>新增了一条记录。</p><h3 id="第三种方法"><a href="#第三种方法" class="headerlink" title="第三种方法"></a>第三种方法</h3><p>直接在原来维度的基础上进行更新，不会产生新的记录但是会记录上一次的。</p><p>更新前：</p><table><thead><tr><th style="text-align:center">emp_rid</th><th style="text-align:center">emp_id</th><th style="text-align:center">emp_name</th><th style="text-align:center">position</th><th style="text-align:center">old_position</th></tr></thead><tbody><tr><td style="text-align:center">101212</td><td style="text-align:center">12345</td><td style="text-align:center">Jack</td><td style="text-align:center">Developer</td><td style="text-align:center">null</td></tr></tbody></table><p>更新后：</p><table><thead><tr><th style="text-align:center">emp_rid</th><th style="text-align:center">emp_id</th><th style="text-align:center">emp_name</th><th style="text-align:center">position</th><th style="text-align:center">old_position</th></tr></thead><tbody><tr><td style="text-align:center">101212</td><td style="text-align:center">12345</td><td style="text-align:center">Jack</td><td style="text-align:center">Manager</td><td style="text-align:center">Developer</td></tr></tbody></table><p>多一个字段，用来存放以前的 position</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍数据仓库的起源和基本搭建。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Data-Warehouse" scheme="http://moqimoqidea.github.io/tags/Data-Warehouse/"/>
    
  </entry>
  
  <entry>
    <title>Spark SQL 应用解析</title>
    <link href="http://moqimoqidea.github.io/2017/08/10/Spark-SQL-Analysis/"/>
    <id>http://moqimoqidea.github.io/2017/08/10/Spark-SQL-Analysis/</id>
    <published>2017-08-10T12:37:27.000Z</published>
    <updated>2018-11-25T06:26:45.897Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要解析部分 Spark SQL 的技术点。</p><a id="more"></a> <h1 id="Spark-SQL-概述"><a href="#Spark-SQL-概述" class="headerlink" title="Spark SQL 概述"></a>Spark SQL 概述</h1><h2 id="什么是-Spark-SQL"><a href="#什么是-Spark-SQL" class="headerlink" title="什么是 Spark SQL"></a>什么是 Spark SQL</h2><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/01-Spark-SQL-LOGO.png?raw=true" alt="01-Spark-SQL-LOGO"></p><p>Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。<br>我们已经学习了Hive，它是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！</p><p>Spark SQL 有四大特点：</p><ol><li>易整合。</li><li>统一的数据访问方式。</li><li>兼容 Hive.</li><li>标准的数据连接。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/02-Spark-SQL-Architecture.png?raw=true" alt="02-Spark-SQL-Architecture"></p><p>SparkSQL可以看做是一个转换层，向下对接各种不同的结构化数据源，向上提供不同的数据访问方式。</p><h2 id="RDD-vs-DataFrames-vs-DataSet"><a href="#RDD-vs-DataFrames-vs-DataSet" class="headerlink" title="RDD vs DataFrames vs DataSet"></a>RDD vs DataFrames vs DataSet</h2><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/03-RDD-DataFrames-DataSet.png?raw=true" alt="03-RDD-DataFrames-DataSet"></p><p>在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。他们和RDD有什么区别呢？首先从版本的产生上来看：<br>RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt; Dataset(Spark1.6)<br>如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。<br>在后期的Spark版本中，DataSet会逐步取代RDD和DataFrame成为唯一的API接口。</p><h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><ul><li>RDD是一个懒执行的不可变的可以支持Lambda表达式的并行数据集合。</li><li>RDD的最大好处就是简单，API的人性化程度很高。</li><li>RDD的劣势是性能限制，它是一个JVM驻内存对象，这也就决定了存在GC的限制和数据增加时Java序列化成本的升高。</li></ul><h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。由于与R和Pandas的DataFrame类似，Spark DataFrame很好地继承了传统单机数据分析的开发体验。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/04-RDD-Vs-DataFrame.png?raw=true" alt="04-RDD-Vs-DataFrame"></p><p>上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。<br>DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待。<br>DataFrame也是懒执行的。<br>性能上比RDD要高，主要有两方面原因： </p><ol><li>定制化内存管理：数据以二进制的方式存在于非堆内存，节省了大量空间之外，还摆脱了GC的限制。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/05-OnHeap-OffHeap-GC.png?raw=true" alt="05-OnHeap-OffHeap-GC"></p><ol start="2"><li>优化的执行计划：查询计划通过Spark catalyst optimiser进行优化。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/06-Spark-Catalyst.png?raw=true" alt="06-Spark-Catalyst"></p><p>比如下面这个例子：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">users.join(events, users(<span class="string">"id"</span>) === events(<span class="string">"uid"</span>))</span><br><span class="line"> .filter(events(<span class="string">"date"</span>) &gt; <span class="string">"2015-01-01"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/07-Spark-SQL-Optimization.png?raw=true" alt="07-Spark-SQL-Optimization"></p><p>为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。<br>得到的优化执行计划在转换成物 理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。<br>对于普通开发者而言，查询优化器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。<br>Dataframe的劣势在于在编译期缺少类型安全检查，导致运行时出错。</p><h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><ul><li>是Dataframe API的一个扩展，是Spark最新的数据抽象。</li><li>用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。</li><li>Dataset支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。</li><li>样例类被用来在Dataset中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称。</li><li>Dataframe是Dataset的特列，DataFrame=Dataset[Row] ，所以可以通过as方法将Dataframe转换为Dataset。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息我都用Row来表示。</li><li>DataSet是强类型的。比如可以有Dataset[Car]，Dataset[Person].</li></ul><p>DataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比。</p><p>RDD让我们能够决定怎么做，而DataFrame和DataSet让我们决定做什么，控制的粒度不一样。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/08-Comparisons-Them.png?raw=true" alt="08-Comparisons-Them"></p><h3 id="三者的共性"><a href="#三者的共性" class="headerlink" title="三者的共性"></a>三者的共性</h3><ol><li>RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利。</li><li>三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过。</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkconf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"test"</span>).set(<span class="string">"spark.port.maxRetries"</span>,<span class="string">"1000"</span>)</span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkconf).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> rdd=spark.sparkContext.parallelize(<span class="type">Seq</span>((<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">1</span>), (<span class="string">"a"</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="comment">// map不运行</span></span><br><span class="line">rdd.map&#123;line=&gt;</span><br><span class="line">  println(<span class="string">"运行"</span>)</span><br><span class="line">  line._1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="3"><li>三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出。</li><li>三者都有partition的概念。</li><li>三者有许多共同的函数，如filter，排序等。</li><li>在对DataFrame和Dataset进行操作许多操作都需要这个包进行支持。</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><ol start="7"><li>DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型。<br>DataFrame:</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">testDF.map&#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(col1:<span class="type">String</span>,col2:<span class="type">Int</span>)=&gt;</span><br><span class="line">        println(col1);println(col2)</span><br><span class="line">        col1</span><br><span class="line">      <span class="keyword">case</span> _=&gt;</span><br><span class="line">        <span class="string">""</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>   Dataset:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class">    <span class="title">testDS</span>.<span class="title">map</span></span>&#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Coltest</span>(col1:<span class="type">String</span>,col2:<span class="type">Int</span>)=&gt;</span><br><span class="line">        println(col1);println(col2)</span><br><span class="line">        col1</span><br><span class="line">      <span class="keyword">case</span> _=&gt;</span><br><span class="line">        <span class="string">""</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="三者的区别"><a href="#三者的区别" class="headerlink" title="三者的区别"></a>三者的区别</h3><p><strong>RDD:</strong></p><ol><li>RDD一般和spark mlib同时使用。</li><li>RDD不支持spark sql操作。</li></ol><p><strong>DataFrame:</strong></p><ol><li>与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值，如</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">testDF.foreach&#123;</span><br><span class="line">  line =&gt;</span><br><span class="line">    <span class="keyword">val</span> col1=line.getAs[<span class="type">String</span>](<span class="string">"col1"</span>)</span><br><span class="line">    <span class="keyword">val</span> col2=line.getAs[<span class="type">String</span>](<span class="string">"col2"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>   每一列的值没法直接访问</p><ol start="2"><li>DataFrame与Dataset一般不与spark ml同时使用。</li><li>DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作，如</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataDF.createOrReplaceTempView(<span class="string">"tmp"</span>)</span><br><span class="line">spark.sql(<span class="string">"select  ROW,DATE from tmp where DATE is not null order by DATE"</span>).show(<span class="number">100</span>,<span class="literal">false</span>)</span><br></pre></td></tr></table></figure><ol start="4"><li>DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然。</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//保存</span></span><br><span class="line"><span class="keyword">val</span> saveoptions = <span class="type">Map</span>(<span class="string">"header"</span> -&gt; <span class="string">"true"</span>, <span class="string">"delimiter"</span> -&gt; <span class="string">"\t"</span>, <span class="string">"path"</span> -&gt; <span class="string">"hdfs://master01:9000/test"</span>)</span><br><span class="line">datawDF.write.format(<span class="string">"com.atguigu.spark.csv"</span>).mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).options(saveoptions).save()</span><br><span class="line"><span class="comment">//读取</span></span><br><span class="line"><span class="keyword">val</span> options = <span class="type">Map</span>(<span class="string">"header"</span> -&gt; <span class="string">"true"</span>, <span class="string">"delimiter"</span> -&gt; <span class="string">"\t"</span>, <span class="string">"path"</span> -&gt; <span class="string">"hdfs://master01:9000/test"</span>)</span><br><span class="line"><span class="keyword">val</span> datarDF= spark.read.options(options).format(<span class="string">"com.atguigu.spark.csv"</span>).load()</span><br></pre></td></tr></table></figure><p>利用这样的保存方式，可以方便的获得字段名和列的对应，而且分隔符（delimiter）可以自由指定。</p><p><strong>Dataset:</strong></p><ol><li>Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同。</li><li>DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的getAS方法或者共性中的第七条提到的模式匹配拿出特定字段。</li><li>而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息。</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class"><span class="title">/**</span></span></span><br><span class="line"><span class="class"> <span class="title">rdd</span></span></span><br><span class="line"><span class="class"> (<span class="params">"a", 1</span>)</span></span><br><span class="line"><span class="class"> (<span class="params">"b", 1</span>)</span></span><br><span class="line"><span class="class"> (<span class="params">"a", 1</span>)</span></span><br><span class="line"><span class="class"><span class="title">**/</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">test</span></span>: <span class="type">Dataset</span>[<span class="type">Coltest</span>]=rdd.map&#123;line=&gt;</span><br><span class="line">      <span class="type">Coltest</span>(line._1,line._2)</span><br><span class="line">    &#125;.toDS</span><br><span class="line">test.map&#123;</span><br><span class="line">      line=&gt;</span><br><span class="line">        println(line.col1)</span><br><span class="line">        println(line.col2)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>可以看出，Dataset在需要访问列中的某个字段时是非常方便的，然而，如果要写一些适配性很强的函数时，如果使用Dataset，行的类型又不确定，可能是各种case class，无法实现适配，这时候用DataFrame即Dataset[Row]就能比较好的解决问题。</p><h1 id="执行-Spark-SQL-查询"><a href="#执行-Spark-SQL-查询" class="headerlink" title="执行 Spark SQL 查询"></a>执行 Spark SQL 查询</h1><h2 id="命令行查询流程"><a href="#命令行查询流程" class="headerlink" title="命令行查询流程"></a>命令行查询流程</h2><p>打开Spark shell<br>例子：查询大于30岁的用户<br>创建如下JSON文件，注意JSON的格式：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Michael"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Andy"</span>, <span class="attr">"age"</span>:<span class="number">30</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Justin"</span>, <span class="attr">"age"</span>:<span class="number">19</span>&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">"/opt/txt_data/json.txt"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">"persons"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">"SELECT * FROM persons"</span>).show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><h2 id="IDEA-创建-Spark-SQL-程序"><a href="#IDEA-创建-Spark-SQL-程序" class="headerlink" title="IDEA 创建 Spark SQL 程序"></a>IDEA 创建 Spark SQL 程序</h2><p>Maven 依赖：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>程序如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.moqi.sparksql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.<span class="type">LoggerFactory</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloWorld</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> logger = <span class="type">LoggerFactory</span>.getLogger(<span class="type">HelloWorld</span>.getClass)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">//创建SparkConf()并设置App名称</span></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"Spark SQL basic example"</span>)</span><br><span class="line">      .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// For implicit conversions like converting RDDs to DataFrames</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">    df.filter($<span class="string">"age"</span> &gt; <span class="number">21</span>).show()</span><br><span class="line"></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"persons"</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">"SELECT * FROM persons where age &gt; 21"</span>).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Spark-SQL-解析"><a href="#Spark-SQL-解析" class="headerlink" title="Spark SQL 解析"></a>Spark SQL 解析</h1><h2 id="新的起点：SparkSession"><a href="#新的起点：SparkSession" class="headerlink" title="新的起点：SparkSession"></a>新的起点：SparkSession</h2><p>在老的版本中，SparkSQL提供两种SQL查询起始点，一个叫SQLContext，用于Spark自己提供的SQL查询，一个叫HiveContext，用于连接Hive的查询，SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">.builder()</span><br><span class="line">.appName(<span class="string">"Spark SQL basic example"</span>)</span><br><span class="line">.config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">// For implicit conversions like converting RDDs to DataFrames</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><p>SparkSession.builder 用于创建一个SparkSession。<br>import spark.implicits._的引入是用于将DataFrames隐式转换成RDD，使df能够使用RDD中的方法。<br>如果需要Hive支持，则需要以下创建语句：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">.builder()</span><br><span class="line">.appName(<span class="string">"Spark SQL basic example"</span>)</span><br><span class="line">.config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">// For implicit conversions like converting RDDs to DataFrames</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><h2 id="创建-DataFrames"><a href="#创建-DataFrames" class="headerlink" title="创建 DataFrames"></a>创建 DataFrames</h2><p>在Spark SQL中SparkSession是创建DataFrames和执行SQL的入口，创建DataFrames有三种方式，一种是可以从一个存在的RDD进行转换，还可以从Hive Table进行查询返回，或者通过Spark的数据源进行创建。<br>从Spark数据源进行创建：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line"><span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><p>从 RDD 进行转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">Michael, 29</span></span><br><span class="line"><span class="comment">Andy, 30</span></span><br><span class="line"><span class="comment">Justin, 19</span></span><br><span class="line"><span class="comment">**/</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> peopleRdd = sc.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</span><br><span class="line">peopleRdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = examples/src/main/resources/people.txt <span class="type">MapPartitionsRDD</span>[<span class="number">18</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> peopleDF3 = peopleRdd.map(_.split(<span class="string">","</span>)).map(paras =&gt; (paras(<span class="number">0</span>),paras(<span class="number">1</span>).trim().toInt)).toDF(<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">peopleDF3: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.show()</span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">|<span class="type">Michael</span>| <span class="number">29</span>|</span><br><span class="line">|   <span class="type">Andy</span>| <span class="number">30</span>|</span><br><span class="line">| <span class="type">Justin</span>| <span class="number">19</span>|</span><br><span class="line">+-------+---+</span><br></pre></td></tr></table></figure><p>Hive 部分在后面数据源介绍。</p><h2 id="DataFrame-常用操作"><a href="#DataFrame-常用操作" class="headerlink" title="DataFrame 常用操作"></a>DataFrame 常用操作</h2><h3 id="DSL-风格语法"><a href="#DSL-风格语法" class="headerlink" title="DSL 风格语法"></a>DSL 风格语法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This import is needed to use the $-notation</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="comment">// Print the schema in a tree format</span></span><br><span class="line">df.printSchema()</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">// |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select only the "name" column</span></span><br><span class="line">df.select(<span class="string">"name"</span>).show()</span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |   name|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |Michael|</span></span><br><span class="line"><span class="comment">// |   Andy|</span></span><br><span class="line"><span class="comment">// | Justin|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select everybody, but increment the age by 1</span></span><br><span class="line">df.select($<span class="string">"name"</span>, $<span class="string">"age"</span> + <span class="number">1</span>).show()</span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |   name|(age + 1)|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |Michael|     null|</span></span><br><span class="line"><span class="comment">// |   Andy|       31|</span></span><br><span class="line"><span class="comment">// | Justin|       20|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select people older than 21</span></span><br><span class="line">df.filter($<span class="string">"age"</span> &gt; <span class="number">21</span>).show()</span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// |age|name|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// | 30|Andy|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Count people by age</span></span><br><span class="line">df.groupBy(<span class="string">"age"</span>).count().show()</span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// | age|count|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// |  19|    1|</span></span><br><span class="line"><span class="comment">// |null|    1|</span></span><br><span class="line"><span class="comment">// |  30|    1|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br></pre></td></tr></table></figure><h3 id="SQL-风格语法"><a href="#SQL-风格语法" class="headerlink" title="SQL 风格语法"></a>SQL 风格语法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Register the DataFrame as a SQL temporary view</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM people"</span>)</span><br><span class="line">sqlDF.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Register the DataFrame as a global temporary view</span></span><br><span class="line">df.createGlobalTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Global temporary view is tied to a system preserved database `global_temp`</span></span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Global temporary view is cross-session</span></span><br><span class="line">spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><p>临时表是Session范围内的，Session退出后，表就失效了。如果想应用范围内有效，可以使用全局表。注意使用全局表时需要全路径访问，如：global_temp.people.</p><h2 id="创建-DataSet"><a href="#创建-DataSet" class="headerlink" title="创建 DataSet"></a>创建 DataSet</h2><p>Dataset是具有强类型的数据集合，需要提供对应的类型信息。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,</span></span><br><span class="line"><span class="comment">// you can use custom classes that implement the Product interface</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">Encoders</span> <span class="title">are</span> <span class="title">created</span> <span class="title">for</span> <span class="title">case</span> <span class="title">classes</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">caseClassDS</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Andy"</span>, <span class="number">32</span>)).toDS()</span><br><span class="line">caseClassDS.show()</span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"><span class="comment">// |name|age|</span></span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"><span class="comment">// |Andy| 32|</span></span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Encoders for most common types are automatically provided by importing spark.implicits._</span></span><br><span class="line"><span class="keyword">val</span> primitiveDS = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).toDS()</span><br><span class="line">primitiveDS.map(_ + <span class="number">1</span>).collect() <span class="comment">// Returns: Array(2, 3, 4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name</span></span><br><span class="line"><span class="keyword">val</span> path = <span class="string">"examples/src/main/resources/people.json"</span></span><br><span class="line"><span class="keyword">val</span> peopleDS = spark.read.json(path).as[<span class="type">Person</span>]</span><br><span class="line">peopleDS.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><h2 id="DataSet-和-RDD-互操作"><a href="#DataSet-和-RDD-互操作" class="headerlink" title="DataSet 和 RDD 互操作"></a>DataSet 和 RDD 互操作</h2><p>Spark SQL支持通过两种方式将存在的RDD转换为Dataset，转换的过程中需要让Dataset获取RDD中的Schema信息，主要有两种方式，一种是通过反射来获取RDD中的Schema信息。这种方式适合于列名已知的情况下。第二种是通过编程接口的方式将Schema信息应用于RDD，这种方式可以处理那种在运行时才能知道列的方式。</p><h3 id="通过反射获取-Schema"><a href="#通过反射获取-Schema" class="headerlink" title="通过反射获取 Schema"></a>通过反射获取 Schema</h3><p>SparkSQL能够自动将包含有case类的RDD转换成DataFrame，case类定义了table的结构，case类属性通过反射变成了表的列名。Case类可以包含诸如Seqs或者Array等复杂的结构。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// For implicit conversions from RDDs to DataFrames</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD of Person objects from a text file, convert it to a Dataframe</span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.sparkContext</span><br><span class="line">.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</span><br><span class="line">.map(_.split(<span class="string">","</span>))</span><br><span class="line">.map(attributes =&gt; <span class="type">Person</span>(attributes(<span class="number">0</span>), attributes(<span class="number">1</span>).trim.toInt))</span><br><span class="line">.toDF()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Register the DataFrame as a temporary view</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by Spark</span></span><br><span class="line"><span class="keyword">val</span> teenagersDF = spark.sql(<span class="string">"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index  ROW object</span></span><br><span class="line">teenagersDF.map(teenager =&gt; <span class="string">"Name: "</span> + teenager(<span class="number">0</span>)).show()</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// or by field name</span></span><br><span class="line">teenagersDF.map(teenager =&gt; <span class="string">"Name: "</span> + teenager.getAs[<span class="type">String</span>](<span class="string">"name"</span>)).show()</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// No pre-defined encoders for Dataset[Map[K,V]], define explicitly</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> mapEncoder = org.apache.spark.sql.<span class="type">Encoders</span>.kryo[<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Any</span>]]</span><br><span class="line"><span class="comment">// Primitive types and case classes can be also defined as</span></span><br><span class="line"><span class="comment">// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span></span><br><span class="line">teenagersDF.map(teenager =&gt; teenager.getValuesMap[<span class="type">Any</span>](<span class="type">List</span>(<span class="string">"name"</span>, <span class="string">"age"</span>))).collect()</span><br><span class="line"><span class="comment">// Array(Map("name" -&gt; "Justin", "age" -&gt; 19))</span></span><br></pre></td></tr></table></figure><h3 id="通过编程设置-Schema"><a href="#通过编程设置-Schema" class="headerlink" title="通过编程设置 Schema"></a>通过编程设置 Schema</h3><p>如果case类不能够提前定义，可以通过下面三个步骤定义一个DataFrame<br>创建一个多行结构的RDD;<br>创建用StructType来表示的行结构信息。<br>通过SparkSession提供的createDataFrame方法来应用Schema.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD</span></span><br><span class="line"><span class="keyword">val</span> peopleRDD = spark.sparkContext.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The schema is encoded in a string,应该是动态通过程序生成的</span></span><br><span class="line"><span class="keyword">val</span> schemaString = <span class="string">"name age"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Generate the schema based on the string of schema   Array[StructFiled]</span></span><br><span class="line"><span class="keyword">val</span> fields = schemaString.split(<span class="string">" "</span>)</span><br><span class="line">.map(fieldName =&gt; <span class="type">StructField</span>(fieldName, <span class="type">StringType</span>, nullable = <span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// val filed = schemaString.split(" ").map(filename=&gt; filename match&#123; case "name"=&gt; StructField(filename,StringType,nullable = true); case "age"=&gt;StructField(filename, IntegerType,nullable = true)&#125; )</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(fields)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Convert records of the RDD (people) to Rows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"><span class="keyword">val</span> rowRDD = peopleRDD</span><br><span class="line">.map(_.split(<span class="string">","</span>))</span><br><span class="line">.map(attributes =&gt; <span class="type">Row</span>(attributes(<span class="number">0</span>), attributes(<span class="number">1</span>).trim))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Apply the schema to the RDD</span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL can be run over a temporary view created using DataFrames</span></span><br><span class="line"><span class="keyword">val</span> results = spark.sql(<span class="string">"SELECT name FROM people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations</span></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index or by field name</span></span><br><span class="line">results.map(attributes =&gt; <span class="string">"Name: "</span> + attributes(<span class="number">0</span>)).show()</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        value|</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |Name: Michael|</span></span><br><span class="line"><span class="comment">// |   Name: Andy|</span></span><br><span class="line"><span class="comment">// | Name: Justin|</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br></pre></td></tr></table></figure><h2 id="类型之间的转换总结"><a href="#类型之间的转换总结" class="headerlink" title="类型之间的转换总结"></a>类型之间的转换总结</h2><p>RDD、DataFrame、Dataset三者有许多共性，有各自适用的场景常常需要在三者之间转换<br><strong>DataFrame/Dataset转RDD：</strong><br>这个转换很简单：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1=testDF.rdd</span><br><span class="line"><span class="keyword">val</span> rdd2=testDS.rdd</span><br></pre></td></tr></table></figure><p><strong>RDD转DataFrame：</strong></p><p>一般用元组把一行的数据写在一起，然后在toDF中指定字段名。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> testDF = rdd.map &#123;line=&gt;</span><br><span class="line">      (line._1,line._2)</span><br><span class="line">    &#125;.toDF(<span class="string">"col1"</span>,<span class="string">"col2"</span>)</span><br></pre></td></tr></table></figure><p><strong>RDD转DataSet：</strong></p><p>可以注意到，定义每一行的类型（case class）时，已经给出了字段名和类型，后面只要往case class里面添加值即可。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">testDS</span> </span>= rdd.map &#123;line=&gt;</span><br><span class="line">      <span class="type">Coltest</span>(line._1,line._2)</span><br><span class="line">    &#125;.toDS</span><br></pre></td></tr></table></figure><p><strong>DataSet转DataFrame：</strong></p><p>这个也很简单，因为只是把case class封装成Row.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> testDF = testDS.toDF</span><br></pre></td></tr></table></figure><p><strong>DataFrame转DataSet：</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">testDS</span> </span>= testDF.as[<span class="type">Coltest</span>]</span><br></pre></td></tr></table></figure><p>这种方法就是在给出每一列的类型后，使用as方法，转成Dataset，这在数据类型是DataFrame又需要针对各个字段处理时极为方便。<br>在使用一些特殊的操作时，一定要加上 import spark.implicits._ 不然toDF、toDS无法使用。</p><h2 id="用户自定义函数"><a href="#用户自定义函数" class="headerlink" title="用户自定义函数"></a>用户自定义函数</h2><p>通过spark.udf功能用户可以自定义函数。</p><h3 id="用户自定义UDF函数"><a href="#用户自定义UDF函数" class="headerlink" title="用户自定义UDF函数"></a>用户自定义UDF函数</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; spark.udf.register(<span class="string">"addName"</span>, (x:<span class="type">String</span>)=&gt; <span class="string">"Name:"</span>+x)</span><br><span class="line">res5: org.apache.spark.sql.expressions.<span class="type">UserDefinedFunction</span> = <span class="type">UserDefinedFunction</span>(&lt;function1&gt;,<span class="type">StringType</span>,<span class="type">Some</span>(<span class="type">List</span>(<span class="type">StringType</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">"Select addName(name), age from people"</span>).show()</span><br><span class="line">+-----------------+----+</span><br><span class="line">|<span class="type">UDF</span>:addName(name)| age|</span><br><span class="line">+-----------------+----+</span><br><span class="line">|     <span class="type">Name</span>:<span class="type">Michael</span>|<span class="literal">null</span>|</span><br><span class="line">|        <span class="type">Name</span>:<span class="type">Andy</span>|  <span class="number">30</span>|</span><br><span class="line">|      <span class="type">Name</span>:<span class="type">Justin</span>|  <span class="number">19</span>|</span><br><span class="line">+-----------------+----+</span><br></pre></td></tr></table></figure><h3 id="用户自定义聚合函数"><a href="#用户自定义聚合函数" class="headerlink" title="用户自定义聚合函数"></a>用户自定义聚合函数</h3><p>强类型的Dataset和弱类型的DataFrame都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。</p><h4 id="弱类型"><a href="#弱类型" class="headerlink" title="弱类型"></a>弱类型</h4><p>通过继承UserDefinedAggregateFunction来实现用户自定义聚合函数。下面展示一个求平均工资的自定义聚合函数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">MutableAggregationBuffer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">UserDefinedAggregateFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line"><span class="comment">// 聚合函数输入参数的数据类型 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"inputColumn"</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span><br><span class="line"><span class="comment">// 聚合缓冲区中值得数据类型 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line"><span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"sum"</span>, <span class="type">LongType</span>) :: <span class="type">StructField</span>(<span class="string">"count"</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 返回值的数据类型 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"><span class="comment">// 对于相同的输入是否一直返回相同的输出。 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">// 初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="comment">// 存工资的总额</span></span><br><span class="line">buffer(<span class="number">0</span>) = <span class="number">0</span>L</span><br><span class="line"><span class="comment">// 存工资的个数</span></span><br><span class="line">buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 相同Execute间的数据合并。 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>)</span><br><span class="line">buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 不同Execute间的数据合并 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>)</span><br><span class="line">buffer1(<span class="number">1</span>) = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 计算最终结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Double</span> = buffer.getLong(<span class="number">0</span>).toDouble / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册函数</span></span><br><span class="line">spark.udf.register(<span class="string">"myAverage"</span>, <span class="type">MyAverage</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">"examples/src/main/resources/employees.json"</span>)</span><br><span class="line">df.createOrReplaceTempView(<span class="string">"employees"</span>)</span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |   name|salary|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |Michael|  3000|</span></span><br><span class="line"><span class="comment">// |   Andy|  4500|</span></span><br><span class="line"><span class="comment">// | Justin|  3500|</span></span><br><span class="line"><span class="comment">// |  Berta|  4000|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> result = spark.sql(<span class="string">"SELECT myAverage(salary) as average_salary FROM employees"</span>)</span><br><span class="line">result.show()</span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |average_salary|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |        3750.0|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br></pre></td></tr></table></figure><h4 id="强类型"><a href="#强类型" class="headerlink" title="强类型"></a>强类型</h4><p>通过继承Aggregator来实现强类型自定义聚合函数，同样是求平均工资。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Encoder</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Encoders</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">// 既然是强类型，可能有case类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span>(<span class="params">name: <span class="type">String</span>, salary: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Average</span>(<span class="params">var sum: <span class="type">Long</span>, var count: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Employee</span>, <span class="type">Average</span>, <span class="type">Double</span>] </span>&#123;</span><br><span class="line"><span class="comment">// 定义一个数据结构，保存工资总数和工资总个数，初始都为0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Average</span> = <span class="type">Average</span>(<span class="number">0</span>L, <span class="number">0</span>L)</span><br><span class="line"><span class="comment">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span></span><br><span class="line"><span class="comment">// and return it instead of constructing a new object</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buffer: <span class="type">Average</span>, employee: <span class="type">Employee</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">buffer.sum += employee.salary</span><br><span class="line">buffer.count += <span class="number">1</span></span><br><span class="line">buffer</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 聚合不同execute的结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">Average</span>, b2: <span class="type">Average</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">b1.sum += b2.sum</span><br><span class="line">b1.count += b2.count</span><br><span class="line">b1</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 计算输出</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">Average</span>): <span class="type">Double</span> = reduction.sum.toDouble / reduction.count</span><br><span class="line"><span class="comment">// 设定之间值类型的编码器，要转换成case类</span></span><br><span class="line"><span class="comment">// Encoders.product是进行scala元组和case类转换的编码器 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Average</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"><span class="comment">// 设定最终输出值的编码器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds = spark.read.json(<span class="string">"examples/src/main/resources/employees.json"</span>).as[<span class="type">Employee</span>]</span><br><span class="line">ds.show()</span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |   name|salary|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |Michael|  3000|</span></span><br><span class="line"><span class="comment">// |   Andy|  4500|</span></span><br><span class="line"><span class="comment">// | Justin|  3500|</span></span><br><span class="line"><span class="comment">// |  Berta|  4000|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Convert the function to a `TypedColumn` and give it a name</span></span><br><span class="line"><span class="keyword">val</span> averageSalary = <span class="type">MyAverage</span>.toColumn.name(<span class="string">"average_salary"</span>)</span><br><span class="line"><span class="keyword">val</span> result = ds.select(averageSalary)</span><br><span class="line">result.show()</span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |average_salary|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |        3750.0|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br></pre></td></tr></table></figure><h1 id="Spark-SQL-数据源"><a href="#Spark-SQL-数据源" class="headerlink" title="Spark SQL 数据源"></a>Spark SQL 数据源</h1><h2 id="通用加载-保存方法"><a href="#通用加载-保存方法" class="headerlink" title="通用加载 / 保存方法"></a>通用加载 / 保存方法</h2><h3 id="手动指定选项"><a href="#手动指定选项" class="headerlink" title="手动指定选项"></a>手动指定选项</h3><p>Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。<br>Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。修改配置项spark.sql.sources.default，可修改默认数据源格式。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.load(<span class="string">"examples/src/main/resources/users.parquet"</span>) df.select(<span class="string">"name"</span>, <span class="string">"favorite_color"</span>).write.save(<span class="string">"namesAndFavColors.parquet"</span>)</span><br></pre></td></tr></table></figure><p>当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：org.apache.spark.sql.parquet），如果数据源格式为内置格式，则只需要指定简称定json, parquet, jdbc, orc, libsvm, csv, text来指定数据的格式。<br>可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> peopleDF = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line">peopleDF.write.format(<span class="string">"parquet"</span>).save(<span class="string">"hdfs://master01:9000/namesAndAges.parquet"</span>)</span><br></pre></td></tr></table></figure><p>除此之外，可以直接运行SQL在文件上：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM parquet.`hdfs://master01:9000/namesAndAges.parquet`"</span>)</span><br><span class="line">sqlDF.show()</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> peopleDF = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line">peopleDF: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.write.format(<span class="string">"parquet"</span>).save(<span class="string">"hdfs://master01:9000/namesAndAges.parquet"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM parquet.`hdfs://master01:9000/namesAndAges.parquet`"</span>)</span><br><span class="line"><span class="number">17</span>/<span class="number">09</span>/<span class="number">05</span> <span class="number">04</span>:<span class="number">21</span>:<span class="number">11</span> <span class="type">WARN</span> <span class="type">ObjectStore</span>: <span class="type">Failed</span> to get database parquet, returning <span class="type">NoSuchObjectException</span></span><br><span class="line">sqlDF: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; sqlDF.show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><h3 id="文件保存选项"><a href="#文件保存选项" class="headerlink" title="文件保存选项"></a>文件保存选项</h3><p>可以采用SaveMode执行存储操作，SaveMode定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。SaveMode详细介绍如下表：</p><table><thead><tr><th><strong>Scala/Java</strong></th><th><strong>Any Language</strong></th><th><strong>Meaning</strong></th></tr></thead><tbody><tr><td><strong>SaveMode.ErrorIfExists(default)</strong></td><td>“error”(default)</td><td>如果文件存在，则报错</td></tr><tr><td><strong>SaveMode.Append</strong></td><td>“append”</td><td>追加</td></tr><tr><td><strong>SaveMode.Overwrite</strong></td><td>“overwrite”</td><td>覆写</td></tr><tr><td><strong>SaveMode.Ignore</strong></td><td>“ignore”</td><td>数据存在，则忽略</td></tr></tbody></table><h2 id="Parquet-文件"><a href="#Parquet-文件" class="headerlink" title="Parquet 文件"></a>Parquet 文件</h2><p>Parquet是一种流行的列式存储格式，可以高效地存储具有嵌套字段的记录。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/09-Parquet-File.png?raw=true" alt="09-Parquet-File"></p><h3 id="Parquet-读写"><a href="#Parquet-读写" class="headerlink" title="Parquet 读写"></a>Parquet 读写</h3><p>Parquet格式经常在Hadoop生态圈中被使用，它也支持Spark SQL的全部数据类型。Spark SQL 提供了直接读取和存储 Parquet 格式文件的方法。 </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Encoders for most common types are automatically provided by importing spark.implicits._</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrames can be saved as Parquet files, maintaining the schema information</span></span><br><span class="line">peopleDF.write.parquet(<span class="string">"hdfs://master01:9000/people.parquet"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read in the parquet file created above</span></span><br><span class="line"><span class="comment">// Parquet files are self-describing so the schema is preserved</span></span><br><span class="line"><span class="comment">// The result of loading a Parquet file is also a DataFrame</span></span><br><span class="line"><span class="keyword">val</span> parquetFileDF = spark.read.parquet(<span class="string">"hdfs://master01:9000/people.parquet"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Parquet files can also be used to create a temporary view and then used in SQL statements</span></span><br><span class="line">parquetFileDF.createOrReplaceTempView(<span class="string">"parquetFile"</span>)</span><br><span class="line"><span class="keyword">val</span> namesDF = spark.sql(<span class="string">"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19"</span>)</span><br><span class="line">namesDF.map(attributes =&gt; <span class="string">"Name: "</span> + attributes(<span class="number">0</span>)).show()</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br></pre></td></tr></table></figure><h3 id="解析分区信息"><a href="#解析分区信息" class="headerlink" title="解析分区信息"></a>解析分区信息</h3><p>对表进行分区是对数据进行优化的方式之一。在分区的表内，数据通过分区列将数据存储在不同的目录下。Parquet数据源现在能够自动发现并解析分区信息。例如，对人口数据进行分区存储，分区列为gender和country，使用下面的目录结构：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">path</span><br><span class="line">└── to</span><br><span class="line">    └── table</span><br><span class="line">        ├── gender=male</span><br><span class="line">        │   ├── ...</span><br><span class="line">        │   │</span><br><span class="line">        │   ├── country=US</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   ├── country=CN</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   └── ...</span><br><span class="line">        └── gender=female</span><br><span class="line">            ├── ...</span><br><span class="line">            │</span><br><span class="line">            ├── country=US</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            ├── country=CN</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            └── ...</span><br></pre></td></tr></table></figure><p>通过传递path/to/table给 SQLContext.read.parquet或SQLContext.read.load，Spark SQL将自动解析分区信息。返回的DataFrame的Schema如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- name: string (nullable = <span class="literal">true</span>)</span><br><span class="line">|-- age: long (nullable = <span class="literal">true</span>)</span><br><span class="line">|-- gender: string (nullable = <span class="literal">true</span>)</span><br><span class="line">|-- country: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure><p>需要注意的是，数据的分区列的数据类型是自动解析的。当前，支持数值类型和字符串类型。自动解析分区类型的参数为：spark.sql.sources.partitionColumnTypeInference.enabled，默认值为true。如果想关闭该功能，直接将该参数设置为disabled。此时，分区列数据格式将被默认设置为string类型，不再进行类型解析。</p><h3 id="Schema-合并"><a href="#Schema-合并" class="headerlink" title="Schema 合并"></a>Schema 合并</h3><p>像ProtocolBuffer、Avro和Thrift那样，Parquet也支持Schema evolution（Schema演变）。用户可以先定义一个简单的Schema，然后逐渐的向Schema中增加列描述。通过这种方式，用户可以获取多个有不同Schema但相互兼容的Parquet文件。现在Parquet数据源能自动检测这种情况，并合并这些文件的schemas。<br>因为Schema合并是一个高消耗的操作，在大多数情况下并不需要，所以Spark SQL从1.5.0开始默认关闭了该功能。可以通过下面两种方式开启该功能：<br>当数据源为Parquet文件时，将数据源选项mergeSchema设置为true<br>设置全局SQL选项spark.sql.parquet.mergeSchema为true<br>示例如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sqlContext from the previous example is used in this example.</span></span><br><span class="line"><span class="comment">// This is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a simple DataFrame, stored into a partition directory</span></span><br><span class="line"><span class="keyword">val</span> df1 = sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>).map(i =&gt; (i, i * <span class="number">2</span>)).toDF(<span class="string">"single"</span>, <span class="string">"double"</span>)</span><br><span class="line">df1.write.parquet(<span class="string">"hdfs://master01:9000/data/test_table/key=1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create another DataFrame in a new partition directory,</span></span><br><span class="line"><span class="comment">// adding a new column and dropping an existing column</span></span><br><span class="line"><span class="keyword">val</span> df2 = sc.makeRDD(<span class="number">6</span> to <span class="number">10</span>).map(i =&gt; (i, i * <span class="number">3</span>)).toDF(<span class="string">"single"</span>, <span class="string">"triple"</span>)</span><br><span class="line">df2.write.parquet(<span class="string">"hdfs://master01:9000/data/test_table/key=2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read the partitioned table</span></span><br><span class="line"><span class="keyword">val</span> df3 = spark.read.option(<span class="string">"mergeSchema"</span>, <span class="string">"true"</span>).parquet(<span class="string">"hdfs://master01:9000/data/test_table"</span>)</span><br><span class="line">df3.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment">// The final schema consists of all 3 columns in the Parquet files together</span></span><br><span class="line"><span class="comment">// with the partitioning column appeared in the partition directory paths.</span></span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">// |-- single: int (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- double: int (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- triple: int (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- key : int (nullable = true)</span></span><br></pre></td></tr></table></figure><h2 id="Hive-数据库"><a href="#Hive-数据库" class="headerlink" title="Hive 数据库"></a>Hive 数据库</h2><p>Apache Hive是Hadoop上的SQL引擎，Spark SQL编译时可以包含Hive支持，也可以不包含。包含Hive支持的Spark SQL可以支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的 一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。<br>若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好Hive，Spark SQL也可以运行。 需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span>(<span class="params">key: <span class="type">Int</span>, value: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">warehouseLocation</span> <span class="title">points</span> <span class="title">to</span> <span class="title">the</span> <span class="title">default</span> <span class="title">location</span> <span class="title">for</span> <span class="title">managed</span> <span class="title">databases</span> <span class="title">and</span> <span class="title">tables</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">warehouseLocation</span> </span>= <span class="keyword">new</span> <span class="type">File</span>(<span class="string">"spark-warehouse"</span>).getAbsolutePath</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">.builder()</span><br><span class="line">.appName(<span class="string">"Spark Hive Example"</span>)</span><br><span class="line">.config(<span class="string">"spark.sql.warehouse.dir"</span>, warehouseLocation)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">import</span> spark.sql</span><br><span class="line"></span><br><span class="line">sql(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)"</span>)</span><br><span class="line">sql(<span class="string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries are expressed in HiveQL</span></span><br><span class="line">sql(<span class="string">"SELECT * FROM src"</span>).show()</span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |key|  value|</span></span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |238|val_238|</span></span><br><span class="line"><span class="comment">// | 86| val_86|</span></span><br><span class="line"><span class="comment">// |311|val_311|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Aggregation queries are also supported.</span></span><br><span class="line">sql(<span class="string">"SELECT COUNT(*) FROM src"</span>).show()</span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |count(1)|</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |    500 |</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are themselves DataFrames and support all normal functions.</span></span><br><span class="line"><span class="keyword">val</span> sqlDF = sql(<span class="string">"SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span></span><br><span class="line"><span class="keyword">val</span> stringsDS = sqlDF.map &#123;</span><br><span class="line"><span class="keyword">case</span> <span class="type">Row</span>(key: <span class="type">Int</span>, value: <span class="type">String</span>) =&gt; <span class="string">s"Key: <span class="subst">$key</span>, Value: <span class="subst">$value</span>"</span></span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |               value|</span></span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// You can also use DataFrames to create temporary views within a SparkSession.</span></span><br><span class="line"><span class="keyword">val</span> recordsDF = spark.createDataFrame((<span class="number">1</span> to <span class="number">100</span>).map(i =&gt; <span class="type">Record</span>(i, <span class="string">s"val_<span class="subst">$i</span>"</span>)))</span><br><span class="line">recordsDF.createOrReplaceTempView(<span class="string">"records"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries can then join DataFrame data with data stored in Hive.</span></span><br><span class="line">sql(<span class="string">"SELECT * FROM records r JOIN src s ON r.key = s.key"</span>).show()</span><br><span class="line"><span class="comment">// +---+------+---+------+</span></span><br><span class="line"><span class="comment">// |key| value|key| value|</span></span><br><span class="line"><span class="comment">// +---+------+---+------+</span></span><br><span class="line"><span class="comment">// |  2| val_2|  2| val_2|</span></span><br><span class="line"><span class="comment">// |  4| val_4|  4| val_4|</span></span><br><span class="line"><span class="comment">// |  5| val_5|  5| val_5|</span></span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></table></figure><h3 id="内联-Hive-应用"><a href="#内联-Hive-应用" class="headerlink" title="内联 Hive 应用"></a>内联 Hive 应用</h3><p>如果要使用内嵌的Hive，什么都不用做，直接用就可以了。 –conf : spark.sql.warehouse.dir=</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">"show tables"</span>).show</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|        |  persons|       <span class="literal">true</span>|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">"SELECT * FROM persons"</span>).show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure><p>注意：如果你使用的是内部的Hive，在Spark2.0之后，spark.sql.warehouse.dir用于指定数据仓库的地址，如果你需要是用HDFS作为路径，那么需要将core-site.xml和hdfs-site.xml 加入到Spark conf目录，否则只会创建master节点上的warehouse目录，查询时会出现文件找不到的问题。所以如果需要使用HDFS，则需要将metastore删除，重启集群。</p><h3 id="外部-Hive-应用"><a href="#外部-Hive-应用" class="headerlink" title="外部 Hive 应用"></a>外部 Hive 应用</h3><p>如果想连接外部已经部署好的Hive，需要通过以下几个步骤。</p><ol><li>将Hive中的hive-site.xml拷贝或者软连接到Spark安装目录下的conf目录下。</li><li>打开spark shell，注意带上访问Hive元数据库的JDBC客户端。</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master spark://master01:7077 --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure><h2 id="JSON-数据集"><a href="#JSON-数据集" class="headerlink" title="JSON 数据集"></a>JSON 数据集</h2><p>Spark SQL 能够自动推测 JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载一个 Dataset[String]或者一个JSON 文件.注意，这个JSON文件不是一个传统的JSON文件，每一行都得是一个JSON串。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Michael"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Andy"</span>, <span class="attr">"age"</span>:<span class="number">30</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Justin"</span>, <span class="attr">"age"</span>:<span class="number">19</span>&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Primitive types (Int, String, etc) and Product types (case classes) encoders are</span></span><br><span class="line"><span class="comment">// supported by importing this when creating a Dataset.</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// A JSON dataset is pointed to by path.</span></span><br><span class="line"><span class="comment">// The path can be either a single text file or a directory storing text files</span></span><br><span class="line"><span class="keyword">val</span> path = <span class="string">"examples/src/main/resources/people.json"</span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.read.json(path)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The inferred schema can be visualized using the printSchema() method</span></span><br><span class="line">peopleDF.printSchema()</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by spark</span></span><br><span class="line"><span class="keyword">val</span> teenagerNamesDF = spark.sql(<span class="string">"SELECT name FROM people WHERE age BETWEEN 13 AND 19"</span>)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line"><span class="comment">// +------+</span></span><br><span class="line"><span class="comment">// |  name|</span></span><br><span class="line"><span class="comment">// +------+</span></span><br><span class="line"><span class="comment">// |Justin|</span></span><br><span class="line"><span class="comment">// +------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span></span><br><span class="line"><span class="comment">// a Dataset[String] storing one JSON object per string</span></span><br><span class="line"><span class="keyword">val</span> otherPeopleDataset = spark.createDataset(</span><br><span class="line"><span class="string">""</span><span class="string">"&#123;"</span><span class="string">name":"</span><span class="type">Yin</span><span class="string">","</span><span class="string">address":&#123;"</span><span class="string">city":"</span><span class="type">Columbus</span><span class="string">","</span><span class="string">state":"</span><span class="type">Ohio</span><span class="string">"&#125;&#125;"</span><span class="string">""</span> :: <span class="type">Nil</span>)</span><br><span class="line"><span class="keyword">val</span> otherPeople = spark.read.json(otherPeopleDataset)</span><br><span class="line">otherPeople.show()</span><br><span class="line"><span class="comment">// +---------------+----+</span></span><br><span class="line"><span class="comment">// |        address|name|</span></span><br><span class="line"><span class="comment">// +---------------+----+</span></span><br><span class="line"><span class="comment">// |[Columbus,Ohio]| Yin|</span></span><br><span class="line"><span class="comment">// +---------------+----+</span></span><br></pre></td></tr></table></figure><h2 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h2><p>Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。<br>注意，需要将相关的数据库驱动放到spark的类路径下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master spark://master01:7077 --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span></span><br><span class="line"><span class="comment">// Loading data from a JDBC source</span></span><br><span class="line"><span class="keyword">val</span> jdbcDF = spark.read.format(<span class="string">"jdbc"</span>).option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://master01:3306/rdd"</span>).option(<span class="string">"dbtable"</span>, <span class="string">" rddtable"</span>).option(<span class="string">"user"</span>, <span class="string">"root"</span>).option(<span class="string">"password"</span>, <span class="string">"hive"</span>).load()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> connectionProperties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">connectionProperties.put(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">connectionProperties.put(<span class="string">"password"</span>, <span class="string">"hive"</span>)</span><br><span class="line"><span class="keyword">val</span> jdbcDF2 = spark.read</span><br><span class="line">.jdbc(<span class="string">"jdbc:mysql://master01:3306/rdd"</span>, <span class="string">"rddtable"</span>, connectionProperties)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Saving data to a JDBC source</span></span><br><span class="line">jdbcDF.write</span><br><span class="line">.format(<span class="string">"jdbc"</span>)</span><br><span class="line">.option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://master01:3306/rdd"</span>)</span><br><span class="line">.option(<span class="string">"dbtable"</span>, <span class="string">"rddtable2"</span>)</span><br><span class="line">.option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">.option(<span class="string">"password"</span>, <span class="string">"hive"</span>)</span><br><span class="line">.save()</span><br><span class="line"></span><br><span class="line">jdbcDF2.write</span><br><span class="line">.jdbc(<span class="string">"jdbc:mysql://master01:3306/mysql"</span>, <span class="string">"db"</span>, connectionProperties)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Specifying create table column data types on write</span></span><br><span class="line">jdbcDF.write</span><br><span class="line">.option(<span class="string">"createTableColumnTypes"</span>, <span class="string">"name CHAR(64), comments VARCHAR(1024)"</span>)</span><br><span class="line">.jdbc(<span class="string">"jdbc:mysql://master01:3306/mysql"</span>, <span class="string">"db"</span>, connectionProperties)</span><br></pre></td></tr></table></figure><h1 id="JDBC-ODBC-服务器"><a href="#JDBC-ODBC-服务器" class="headerlink" title="JDBC / ODBC 服务器"></a>JDBC / ODBC 服务器</h1><p>Spark SQL也提供JDBC连接支持，这对于让商业智能(BI)工具连接到Spark集群上以 及在多用户间共享一个集群的场景都非常有用。JDBC 服务器作为一个独立的 Spark 驱动 器程序运行，可以在多用户之间共享。任意一个客户端都可以在内存中缓存数据表，对表 进行查询。集群的资源以及缓存数据都在所有用户之间共享。<br>Spark SQL的JDBC服务器与Hive中的HiveServer2相一致。由于使用了Thrift通信协议，它也被称为“Thrift server”。<br>服务器可以通过 Spark 目录中的 sbin/start-thriftserver.sh 启动。这个 脚本接受的参数选项大多与 spark-submit 相同。默认情况下，服务器会在 localhost:10000 上进行监听，我们可以通过环境变量(HIVE_SERVER2_THRIFT_PORT 和 HIVE_SERVER2_THRIFT_BIND_HOST)修改这些设置，也可以通过 Hive配置选项(hive. server2.thrift.port 和 hive.server2.thrift.bind.host)来修改。你也可以通过命令行参 数–hiveconf property=value来设置Hive选项。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-thriftserver.sh \</span><br><span class="line">--hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \</span><br><span class="line">--hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \</span><br><span class="line">--master &lt;master-uri&gt;</span><br><span class="line">...</span><br><span class="line">./bin/beeline</span><br><span class="line">beeline&gt; !connect jdbc:hive2://master01:10000</span><br></pre></td></tr></table></figure><p>在 Beeline 客户端中，你可以使用标准的 HiveQL 命令来创建、列举以及查询数据表。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[bigdata<span class="meta">@master</span>01 spark<span class="number">-2.1</span><span class="number">.1</span>-bin-hadoop2<span class="number">.7</span>]$ ./sbin/start-thriftserver.sh</span><br><span class="line">starting org.apache.spark.sql.hive.thriftserver.<span class="type">HiveThriftServer2</span>, logging to /home/bigdata/hadoop/spark<span class="number">-2.1</span><span class="number">.1</span>-bin-hadoop2<span class="number">.7</span>/logs/spark-bigdata-org.apache.spark.sql.hive.thriftserver.<span class="type">HiveThriftServer2</span><span class="number">-1</span>-master01.out</span><br><span class="line"></span><br><span class="line">[bigdata<span class="meta">@master</span>01 spark<span class="number">-2.1</span><span class="number">.1</span>-bin-hadoop2<span class="number">.7</span>]$ ./bin/beeline</span><br><span class="line"><span class="type">Beeline</span> version <span class="number">1.2</span><span class="number">.1</span>.spark2 by <span class="type">Apache</span> <span class="type">Hive</span></span><br><span class="line"></span><br><span class="line">beeline&gt; !connect jdbc:hive2:<span class="comment">//master01:10000</span></span><br><span class="line"><span class="type">Connecting</span> to jdbc:hive2:<span class="comment">//master01:10000</span></span><br><span class="line"><span class="type">Enter</span> username <span class="keyword">for</span> jdbc:hive2:<span class="comment">//master01:10000: bigdata</span></span><br><span class="line"><span class="type">Enter</span> password <span class="keyword">for</span> jdbc:hive2:<span class="comment">//master01:10000: *******</span></span><br><span class="line">log4j:<span class="type">WARN</span> <span class="type">No</span> appenders could be found <span class="keyword">for</span> logger (org.apache.hive.jdbc.<span class="type">Utils</span>).</span><br><span class="line">log4j:<span class="type">WARN</span> <span class="type">Please</span> initialize the log4j system properly.</span><br><span class="line">log4j:<span class="type">WARN</span> <span class="type">See</span> http:<span class="comment">//logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span></span><br><span class="line"><span class="type">Connected</span> to: <span class="type">Spark</span> <span class="type">SQL</span> (version <span class="number">2.1</span><span class="number">.1</span>)</span><br><span class="line"><span class="type">Driver</span>: <span class="type">Hive</span> <span class="type">JDBC</span> (version <span class="number">1.2</span><span class="number">.1</span>.spark2)</span><br><span class="line"><span class="type">Transaction</span> isolation: <span class="type">TRANSACTION_REPEATABLE_READ</span></span><br><span class="line"></span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="comment">//master01:10000&gt; show tables;</span></span><br><span class="line">+-----------+------------+--------------+--+</span><br><span class="line">| database  | tableName  | isTemporary  |</span><br><span class="line">+-----------+------------+--------------+--+</span><br><span class="line">| <span class="keyword">default</span>   | src        | <span class="literal">false</span>        |</span><br><span class="line">+-----------+------------+--------------+--+</span><br><span class="line"><span class="number">1</span> row selected (<span class="number">0.726</span> seconds)</span><br><span class="line"></span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="comment">//master01:10000&gt;</span></span><br></pre></td></tr></table></figure><h1 id="Spark-SQL-CLI"><a href="#Spark-SQL-CLI" class="headerlink" title="Spark SQL CLI"></a>Spark SQL CLI</h1><p>Spark SQL CLI可以很方便的在本地运行Hive元数据服务以及从命令行执行查询任务。需要注意的是，Spark SQL CLI不能与Thrift JDBC服务交互。在Spark目录下执行如下命令启动Spark SQL CLI：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-sql</span><br></pre></td></tr></table></figure><p>配置Hive需要替换 conf/ 下的 hive-site.xml 。</p><h1 id="Spark-SQL-的运行原理"><a href="#Spark-SQL-的运行原理" class="headerlink" title="Spark SQL 的运行原理"></a>Spark SQL 的运行原理</h1><h2 id="Spark-SQL-运行架构"><a href="#Spark-SQL-运行架构" class="headerlink" title="Spark SQL 运行架构"></a>Spark SQL 运行架构</h2><p>Spark SQL对SQL语句的处理和关系型数据库类似，即词法/语法解析、绑定、优化、执行。Spark SQL会先将SQL语句解析成一棵树，然后使用规则(Rule)对Tree进行绑定、优化等处理过程。Spark SQL由Core、Catalyst、Hive、Hive-ThriftServer四部分构成：<br>Core: 负责处理数据的输入和输出，如获取数据，查询结果输出成DataFrame等<br>Catalyst: 负责处理整个查询过程，包括解析、绑定、优化等<br>Hive: 负责对Hive数据进行处理<br>Hive-ThriftServer: 主要用于对hive的访问</p><h3 id="TreeNode"><a href="#TreeNode" class="headerlink" title="TreeNode"></a>TreeNode</h3><p>逻辑计划、表达式等都可以用tree来表示，它只是在内存中维护，并不会进行磁盘的持久化，分析器和优化器对树的修改只是替换已有节点。<br>TreeNode有2个直接子类，QueryPlan和Expression。QueryPlam下又有LogicalPlan和SparkPlan. Expression是表达式体系，不需要执行引擎计算而是可以直接处理或者计算的节点，包括投影操作，操作符运算等。</p><h3 id="Rule-amp-RuleExecutor"><a href="#Rule-amp-RuleExecutor" class="headerlink" title="Rule &amp; RuleExecutor"></a>Rule &amp; RuleExecutor</h3><p>Rule就是指对逻辑计划要应用的规则，以到达绑定和优化。他的实现类就是RuleExecutor。优化器和分析器都需要继承RuleExecutor。每一个子类中都会定义Batch、Once、FixPoint. 其中每一个Batch代表着一套规则，Once表示对树进行一次操作，FixPoint表示对树进行多次的迭代操作。RuleExecutor内部提供一个Seq[Batch]属性，里面定义的是RuleExecutor的处理逻辑，具体的处理逻辑由具体的Rule子类实现。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/10-Rule-And-RuleExecutor.png?raw=true" alt="10-Rule-And-RuleExecutor"></p><p>整个流程架构图：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/11-Spark-SQL-Run-Architecture.png?raw=true" alt="11-Spark-SQL-Run-Architecture"></p><h2 id="Spark-SQL-运行原理"><a href="#Spark-SQL-运行原理" class="headerlink" title="Spark SQL 运行原理"></a>Spark SQL 运行原理</h2><ol><li><strong>使用SessionCatalog保存元数据</strong>：在解析SQL语句之前，会创建SparkSession，或者如果是2.0之前的版本初始化SQLContext，SparkSession只是封装了SparkContext和SQLContext的创建而已。会把元数据保存在SessionCatalog中，涉及到表名，字段名称和字段类型。创建临时表或者视图，其实就会往SessionCatalog注册。</li><li><strong>解析SQL,使用ANTLR生成未绑定的逻辑计划</strong>：当调用SparkSession的sql或者SQLContext的sql方法，我们以2.0为准，就会使用SparkSqlParser进行解析SQL. 使用的ANTLR进行词法解析和语法解析。它分为2个步骤来生成Unresolved LogicalPlan：(1)词法分析：Lexical Analysis，负责将token分组成符号类； (2)构建一个分析树或者语法树AST。</li><li><strong>使用分析器Analyzer绑定逻辑计划</strong>：在该阶段，Analyzer会使用Analyzer Rules，并结合SessionCatalog，对未绑定的逻辑计划进行解析，生成已绑定的逻辑计划。</li><li><strong>使用优化器Optimizer优化逻辑计划</strong>：优化器也是会定义一套Rules，利用这些Rule对逻辑计划和Exepression进行迭代处理，从而使得树的节点进行和并和优化。</li><li><strong>使用SparkPlanner生成物理计划</strong>：SparkSpanner使用Planning Strategies，对优化后的逻辑计划进行转换，生成可以执行的物理计划SparkPlan.</li><li><strong>使用QueryExecution执行物理计划</strong>：此时调用SparkPlan的execute方法，底层其实已经再触发JOB了，然后返回RDD.</li></ol><p>End. </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要解析部分 Spark SQL 的技术点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="http://moqimoqidea.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark Core 应用解析</title>
    <link href="http://moqimoqidea.github.io/2017/08/04/Spark-Core-Analysis/"/>
    <id>http://moqimoqidea.github.io/2017/08/04/Spark-Core-Analysis/</id>
    <published>2017-08-04T05:29:46.000Z</published>
    <updated>2018-11-24T12:21:42.396Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要解析部分 Spark Core 的技术点。</p><a id="more"></a> <h1 id="RDD-概念"><a href="#RDD-概念" class="headerlink" title="RDD 概念"></a>RDD 概念</h1><h2 id="RDD-为什么会产生"><a href="#RDD-为什么会产生" class="headerlink" title="RDD 为什么会产生"></a>RDD 为什么会产生</h2><p>RDD是Spark的基石，是实现Spark数据处理的核心抽象。那么RDD为什么会产生呢？</p><p>Hadoop的MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后写入物理存储设备。数据更多面临的是一次性处理。</p><p>MR的这种方式对数据领域两种常见的操作不是很高效。第一种是迭代式的算法。比如机器学习中ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR这种模式不太合适，即使多MR串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。</p><p>MR中的迭代：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/01-Iteration-In-MapReduce.png?raw=true" alt="01-Iteration-In-MapReduce"></p><p>Spark中的迭代：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/02-Iteration-In-Spark.png?raw=true" alt="02-Iteration-In-Spark"></p><p>我们需要一个效率非常快，且能够支持迭代计算和有效数据共享的模型，Spark应运而生。RDD是基于工作集的工作模式，更多的是面向工作流。</p><p>但是无论是MR还是RDD都应该具有类似位置感知、容错和负载均衡等特性。</p><h2 id="RDD-概述"><a href="#RDD-概述" class="headerlink" title="RDD 概述"></a>RDD 概述</h2><h3 id="什么是-RDD"><a href="#什么是-RDD" class="headerlink" title="什么是 RDD"></a>什么是 RDD</h3><p>RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。在 Spark 中，对数据的所有操作不外乎创建 RDD、转化已有RDD 以及调用 RDD 操作进行求值。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。RDD 可以包含 Python、Java、Scala 中任意类型的对象， 甚至可以包含用户自定义的对象。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。<br>RDD支持两种操作:转化操作和行动操作。RDD 的转化操作是返回一个新的 RDD的操作，比如 map()和 filter()，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作。比如 count() 和 first()。<br>Spark采用惰性计算模式，RDD只有第一次在一个行动操作中用到时，才会真正计算。Spark可以优化整个计算过程。默认情况下，Spark 的 RDD 会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个 RDD，可以使用 RDD.persist() 让 Spark 把这个 RDD 缓存下来。</p><h3 id="RDD-的属性"><a href="#RDD-的属性" class="headerlink" title="RDD 的属性"></a>RDD 的属性</h3><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/03-RDD-Attributes.png?raw=true" alt="03-RDD-Attributes"></p><ol><li>一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</li><li>一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</li><li>RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</li><li>一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</li><li>一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</li></ol><p>RDD是一个应用层面的逻辑概念。一个RDD多个分片。RDD就是一个元数据记录集，记录了RDD内存所有的关系数据。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/04-Native-Data-Space-And-Spark-RDD.png?raw=true" alt="04-Native-Data-Space-And-Spark-RDD"></p><h2 id="RDD-弹性"><a href="#RDD-弹性" class="headerlink" title="RDD 弹性"></a>RDD 弹性</h2><ol><li>自动进行内存和磁盘数据存储的切换：Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换</li><li>基于血统的高效容错机制：在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。</li><li>Task如果失败会自动进行特定次数的重试：RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。</li><li>Stage如果失败会自动进行特定次数的重试：如果Job的某个Stage阶段计算失败，框架也会自动进行任务的重新计算，默认次数也是4次。</li><li>Checkpoint和Persist可主动或被动触发：RDD可以通过Persist持久化将RDD缓存到内存或者磁盘，当再次用到该RDD时直接读取就行。也可以将RDD进行检查点，检查点会将数据存储在HDFS中，该RDD的所有父RD都会被移除。</li><li>数据调度弹性：Spark把这个JOB执行模型抽象为通用的有向无环图DAG，可以将多Stage的任务串联或并行执行，调度引擎自动处理Stage的失败以及Task的失败。</li><li>数据分片的高度弹性：可以根据业务的特征，动态调整数据分片的个数，提升整体的应用执行效率。</li></ol><p>RDD全称叫做弹性分布式数据集(Resilient Distributed Datasets)一种分布式的内存抽象，表示一个只读的记录分区的集合，它只能通过其他RDD转换而创建，为此，RDD支持丰富的转换操作(如map, join, filter, groupBy等过这种转换操作，新的RDD则包含了如何从其他RDDs衍生所必需的信息，所以说RDDs之间是有依赖关系的。基于RDDs之间的依赖，RDDs会形成一个有向无环图DAG，G描述了整个流式计算的流程，实际执行的时候，RDD是通过血缘关系(Lineage)成的，即使出现数据分区丢失，也可以通过血缘关系重建分区，总结起来，基于RDD的流式计算任务可描述为：从稳定的物理存储(如分布式文件系统)中加载记录，记入由一组确定性操作构成的DAG，然后写回稳定存储。另外RDD还可以将数据集缓存到内存中，使得在多个操作之间可以重用数据集，基于这个特点可以很方便地构建应用(图计算、机器学习等)或者交互式数据分析应用。可以说Spark最初也就是实现RDD的一个分布式系统，后面通过不断发展壮大成为现在较为完善的大数据生态系单来讲，Spark-RDD的关系类似于Hadoop-MapReduce关系。</p><h2 id="RDD-特点"><a href="#RDD-特点" class="headerlink" title="RDD 特点"></a>RDD 特点</h2><p>RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。</p><h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><p>RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/05-RDD-Partition.png?raw=true" alt="05-RDD-Partition"></p><h3 id="只读"><a href="#只读" class="headerlink" title="只读"></a>只读</h3><p>如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/06-RDD-Read-Only.png?raw=true" alt="06-RDD-Read-Only"></p><p>由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/07-Variety-Of-Operators.png?raw=true" alt="07-Variety-Of-Operators"></p><p>RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/08-Transformation-Operations-And-Actions-Operations.png?raw=true" alt="08-Transformation-Operations-And-Actions-Operations"></p><h3 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h3><p>RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是一一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/09-Dependencies-Between-RDD.png?raw=true" alt="09-Dependencies-Between-RDD"></p><p>通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，如下图所示，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/10-DAG.png?raw=true" alt="10-DAG"></p><h3 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h3><p>如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/11-RDD-Cache.png?raw=true" alt="11-RDD-Cache"></p><h3 id="CheckPoint"><a href="#CheckPoint" class="headerlink" title="CheckPoint"></a>CheckPoint</h3><p>虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。</p><p>给定一个RDD我们至少可以知道如下几点信息：1、分区数以及分区方式；2、由父RDDs衍生而来的相关依赖信息；3、计算每个分区的数据，计算步骤为：1）如果被缓存，则从缓存中取的分区的数据；2）如果被checkpoint，则从checkpoint处恢复数据；3）根据血缘关系计算分区的数据。</p><h1 id="RDD-编程"><a href="#RDD-编程" class="headerlink" title="RDD 编程"></a>RDD 编程</h1><h2 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h2><p>在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。<br>要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/12-Driver-And-Worker.png?raw=true" alt="12-Driver-And-Worker"></p><p>那么这其中的 Dirver、SparkContext、Executor、Master、Worker分别是什么？</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/13-MasterNode-And-WorkerNode.png?raw=true" alt="13-MasterNode-And-WorkerNode"></p><h2 id="创建-RDD"><a href="#创建-RDD" class="headerlink" title="创建 RDD"></a>创建 RDD</h2><p>在Spark中创建RDD的创建方式大概可以分为三种：（1）、从集合中创建RDD；（2）、从外部存储创建RDD；（3）、从其他RDD创建。</p><ol><li>由一个已经存在的Scala集合创建，集合并行化。val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))；而从集合中创建RDD，Spark主要提供了两种函数：parallelize和makeRDD。我们可以先看看这两个函数的声明：</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[(<span class="type">T</span>, <span class="type">Seq</span>[<span class="type">String</span>])]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure><p>我们可以从上面看出makeRDD有两种实现，而且第一个makeRDD函数接收的参数和parallelize完全一致。其实第一种makeRDD函数实现是依赖了parallelize函数的实现，来看看Spark中是怎么实现这个makeRDD函数的：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  parallelize(seq, numSlices)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们可以看出，这个makeRDD函数完全和parallelize函数一致。但是我们得看看第二种makeRDD函数函数实现了，它接收的参数类型是Seq[(T, Seq[String])]，Spark文档的说明是：<br>Distribute a local Scala collection to form an RDD, with one or more location preferences (hostnames of Spark nodes) for each object. Create a new partition for each collection item.<br>原来，这个函数还为数据提供了位置信息，来看看我们怎么使用：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> guigu1= sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">guigu1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> guigu2 = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">guigu2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">11</span>] at makeRDD at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> seq = <span class="type">List</span>((<span class="number">1</span>, <span class="type">List</span>(<span class="string">"slave01"</span>)),</span><br><span class="line">     | (<span class="number">2</span>, <span class="type">List</span>(<span class="string">"slave02"</span>)))</span><br><span class="line">seq: <span class="type">List</span>[(<span class="type">Int</span>, <span class="type">List</span>[<span class="type">String</span>])] = <span class="type">List</span>((<span class="number">1</span>,<span class="type">List</span>(slave01)),</span><br><span class="line"> (<span class="number">2</span>,<span class="type">List</span>(slave02)))</span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> guigu3 = sc.makeRDD(seq)</span><br><span class="line">guigu3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at makeRDD at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"> </span><br><span class="line">scala&gt; guigu3.preferredLocations(guigu3.partitions(<span class="number">1</span>))</span><br><span class="line">res26: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>(slave02)</span><br><span class="line"> </span><br><span class="line">scala&gt; guigu3.preferredLocations(guigu3.partitions(<span class="number">0</span>))</span><br><span class="line">res27: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>(slave01)</span><br><span class="line"> </span><br><span class="line">scala&gt; guigu1.preferredLocations(guigu1.partitions(<span class="number">0</span>))</span><br><span class="line">res28: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>()</span><br></pre></td></tr></table></figure><p>我们可以看到，makeRDD函数有两种实现，第一种实现其实完全和parallelize一致；而第二种实现可以为数据提供位置信息，而除此之外的实现和parallelize函数也是一致的，如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq, numSlices, <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">String</span>]]())</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[(<span class="type">T</span>, <span class="type">Seq</span>[<span class="type">String</span>])]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">val</span> indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq.map(_._1), seq.size, indexToPrefs)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>都是返回ParallelCollectionRDD，而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。</p><ol start="2"><li>由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等。</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://slave1:9000/txtFile"</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = hdfs:<span class="comment">//slave1:9000/RELEASE MapPartitionsRDD[4] at textFile at &lt;console&gt;:24</span></span><br></pre></td></tr></table></figure><h2 id="RDD-编程-1"><a href="#RDD-编程-1" class="headerlink" title="RDD 编程"></a>RDD 编程</h2><p>RDD一般分为数值RDD和键值对RDD，本章不进行具体区分，先统一来看，下一章会对键值对RDD做专门说明。</p><h3 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h3><p>RDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。常用的 Transformation：</p><ul><li>map(func): 返回通过函数func传递源的每个元素形成的新分布式数据集。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> source  = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">source: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">8</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; source.collect()</span><br><span class="line">res7: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> mapadd = source.map(_ * <span class="number">2</span>)</span><br><span class="line">mapadd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">9</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; mapadd.collect()</span><br><span class="line">res8: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">18</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure><ul><li>filter(func): 返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> sourceFilter = sc.parallelize(<span class="type">Array</span>(<span class="string">"xiaoming"</span>,<span class="string">"xiaojiang"</span>,<span class="string">"xiaohe"</span>,<span class="string">"dazhi"</span>))</span><br><span class="line">sourceFilter: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> filter = sourceFilter.filter(_.contains(<span class="string">"xiao"</span>))</span><br><span class="line">filter: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">11</span>] at filter at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; sourceFilter.collect()</span><br><span class="line">res9: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(xiaoming, xiaojiang, xiaohe, dazhi)</span><br><span class="line"></span><br><span class="line">scala&gt; filter.collect()</span><br><span class="line">res10: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(xiaoming, xiaojiang, xiaohe)</span><br></pre></td></tr></table></figure><ul><li>flatMap(func): 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> sourceFlat = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">sourceFlat: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; sourceFlat.collect()</span><br><span class="line">res11: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> flatMap = sourceFlat.flatMap(<span class="number">1</span> to _)</span><br><span class="line">flatMap: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">13</span>] at flatMap at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; flatMap.collect()</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure><ul><li>mapPartitions(func): 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"kpop"</span>,<span class="string">"female"</span>),(<span class="string">"zorro"</span>,<span class="string">"male"</span>),(<span class="string">"mobin"</span>,<span class="string">"male"</span>),(<span class="string">"lucy"</span>,<span class="string">"female"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">16</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionsFun</span></span>(iter : <span class="type">Iterator</span>[(<span class="type">String</span>,<span class="type">String</span>)]) : <span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">  <span class="keyword">var</span> woman = <span class="type">List</span>[<span class="type">String</span>]()</span><br><span class="line">  <span class="keyword">while</span> (iter.hasNext)&#123;</span><br><span class="line">    <span class="keyword">val</span> next = iter.next()</span><br><span class="line">    next <span class="keyword">match</span> &#123;</span><br><span class="line">       <span class="keyword">case</span> (_,<span class="string">"female"</span>) =&gt; woman = next._1 :: woman</span><br><span class="line">       <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  woman.iterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line"></span><br><span class="line">partitionsFun: (iter: <span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">String</span>)])<span class="type">Iterator</span>[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> result = rdd.mapPartitions(partitionsFun)</span><br><span class="line">result: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">17</span>] at mapPartitions at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; result.collect()</span><br><span class="line">res13: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(kpop, lucy)</span><br></pre></td></tr></table></figure><ul><li>mapPartitionsWithIndex(func): 类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"kpop"</span>,<span class="string">"female"</span>),(<span class="string">"zorro"</span>,<span class="string">"male"</span>),(<span class="string">"mobin"</span>,<span class="string">"male"</span>),(<span class="string">"lucy"</span>,<span class="string">"female"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">18</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionsFun</span></span>(index : <span class="type">Int</span>, iter : <span class="type">Iterator</span>[(<span class="type">String</span>,<span class="type">String</span>)]) : <span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">  <span class="keyword">var</span> woman = <span class="type">List</span>[<span class="type">String</span>]()</span><br><span class="line">  <span class="keyword">while</span> (iter.hasNext)&#123;</span><br><span class="line">    <span class="keyword">val</span> next = iter.next()</span><br><span class="line">    next <span class="keyword">match</span> &#123;</span><br><span class="line">       <span class="keyword">case</span> (_,<span class="string">"female"</span>) =&gt; woman = <span class="string">"["</span>+index+<span class="string">"]"</span>+next._1 :: woman</span><br><span class="line">       <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  woman.iterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line"></span><br><span class="line">partitionsFun: (index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">String</span>)])<span class="type">Iterator</span>[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> result = rdd.mapPartitionsWithIndex(partitionsFun)</span><br><span class="line">result: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">19</span>] at mapPartitionsWithIndex at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; result.collect()</span><br><span class="line">res14: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>([<span class="number">0</span>]kpop, [<span class="number">3</span>]lucy)</span><br></pre></td></tr></table></figure><ul><li>sample(withReplacement, fraction, seed): 以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。例子从RDD中随机且有放回的抽出50%的数据，随机种子值为3（即可能以1 2 3的其中一个起始值）。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">20</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect()</span><br><span class="line">res15: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> sample1 = rdd.sample(<span class="literal">true</span>,<span class="number">0.4</span>,<span class="number">2</span>)</span><br><span class="line">sample1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">PartitionwiseSampledRDD</span>[<span class="number">21</span>] at sample at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; sample1.collect()</span><br><span class="line">res16: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> sample2 = rdd.sample(<span class="literal">false</span>,<span class="number">0.2</span>,<span class="number">3</span>)</span><br><span class="line">sample2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">PartitionwiseSampledRDD</span>[<span class="number">22</span>] at sample at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; sample2.collect()</span><br><span class="line">res17: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">9</span>)</span><br></pre></td></tr></table></figure><ul><li>union(otherDataset): 对源RDD和参数RDD求并集后返回一个新的RDD。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">23</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">5</span> to <span class="number">10</span>)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">24</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = rdd1.union(rdd2)</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">UnionRDD</span>[<span class="number">25</span>] at union at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.collect()</span><br><span class="line">res18: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure><ul><li>intersection(otherDataset): 对源RDD和参数RDD求交集后返回一个新的RDD。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">7</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">26</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">5</span> to <span class="number">10</span>)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">27</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = rdd1.intersection(rdd2)</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">33</span>] at intersection at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.collect()</span><br><span class="line">res19: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure><ul><li>distinct([numTasks])): 对源RDD进行去重后返回一个新的RDD. 默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> distinctRdd = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">9</span>,<span class="number">6</span>,<span class="number">1</span>))</span><br><span class="line">distinctRdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">34</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> unionRDD = distinctRdd.distinct()</span><br><span class="line">unionRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">37</span>] at distinct at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; unionRDD.collect()</span><br><span class="line">res20: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> unionRDD = distinctRdd.distinct(<span class="number">2</span>)</span><br><span class="line">unionRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">40</span>] at distinct at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; unionRDD.collect()</span><br><span class="line">res21: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">6</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure><ul><li>partitionBy(partitioner): 对RDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区， 否则会生成ShuffleRDD。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"aaa"</span>),(<span class="number">2</span>,<span class="string">"bbb"</span>),(<span class="number">3</span>,<span class="string">"ccc"</span>),(<span class="number">4</span>,<span class="string">"ddd"</span>)),<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">44</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res24: <span class="type">Int</span> = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> rdd2 = rdd.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">45</span>] at partitionBy at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.partitions.size</span><br><span class="line">res25: <span class="type">Int</span> = <span class="number">2</span></span><br></pre></td></tr></table></figure><ul><li>reduceByKey(func, [numTasks]): 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"female"</span>,<span class="number">1</span>),(<span class="string">"male"</span>,<span class="number">5</span>),(<span class="string">"female"</span>,<span class="number">5</span>),(<span class="string">"male"</span>,<span class="number">2</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">46</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> reduce = rdd.reduceByKey((x,y) =&gt; x+y)</span><br><span class="line">reduce: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">47</span>] at reduceByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; reduce.collect()</span><br><span class="line">res29: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((female,<span class="number">6</span>), (male,<span class="number">7</span>))</span><br></pre></td></tr></table></figure><ul><li>groupByKey(): groupByKey也是对每个key进行操作，但只生成一个sequence。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> words = <span class="type">Array</span>(<span class="string">"one"</span>, <span class="string">"two"</span>, <span class="string">"two"</span>, <span class="string">"three"</span>, <span class="string">"three"</span>, <span class="string">"three"</span>)</span><br><span class="line">words: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(one, two, two, three, three, three)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> wordPairsRDD = sc.parallelize(words).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">wordPairsRDD: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">4</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> group = wordPairsRDD.groupByKey()</span><br><span class="line">group: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">ShuffledRDD</span>[<span class="number">5</span>] at groupByKey at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; group.collect()</span><br><span class="line">res1: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">Array</span>((two,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">1</span>)), (one,<span class="type">CompactBuffer</span>(<span class="number">1</span>)), (three,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; group.map(t =&gt; (t._1, t._2.sum))</span><br><span class="line">res2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">6</span>] at map at &lt;console&gt;:<span class="number">31</span></span><br><span class="line"></span><br><span class="line">scala&gt; res2.collect()</span><br><span class="line">res3: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((two,<span class="number">2</span>), (one,<span class="number">1</span>), (three,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> map = group.map(t =&gt; (t._1, t._2.sum))</span><br><span class="line">map: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">7</span>] at map at &lt;console&gt;:<span class="number">30</span></span><br><span class="line"></span><br><span class="line">scala&gt; map.collect()</span><br><span class="line">res4: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((two,<span class="number">2</span>), (one,<span class="number">1</span>), (three,<span class="number">3</span>))</span><br></pre></td></tr></table></figure><ul><li>combineByKey[C](  createCombiner: V =&gt; C,  mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): 对相同K，把V合并成一个集合.<ul><li>createCombiner: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。如果这是一个新的元素,combineByKey() 会使用一个叫作 createCombiner() 的函数来创建 那个键对应的累加器的初始值。</li><li>mergeValue: 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并。</li><li>mergeCombiners: 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。</li></ul></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> scores = <span class="type">Array</span>((<span class="string">"Fred"</span>, <span class="number">88</span>), (<span class="string">"Fred"</span>, <span class="number">95</span>), (<span class="string">"Fred"</span>, <span class="number">91</span>), (<span class="string">"Wilma"</span>, <span class="number">93</span>), (<span class="string">"Wilma"</span>, <span class="number">95</span>), (<span class="string">"Wilma"</span>, <span class="number">98</span>))</span><br><span class="line">scores: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="type">Fred</span>,<span class="number">88</span>), (<span class="type">Fred</span>,<span class="number">95</span>), (<span class="type">Fred</span>,<span class="number">91</span>), (<span class="type">Wilma</span>,<span class="number">93</span>), (<span class="type">Wilma</span>,<span class="number">95</span>), (<span class="type">Wilma</span>,<span class="number">98</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> input = sc.parallelize(scores)</span><br><span class="line">input: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">52</span>] at parallelize at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> combine = input.combineByKey(</span><br><span class="line">     |     (v)=&gt;(v,<span class="number">1</span>),</span><br><span class="line">     |     (acc:(<span class="type">Int</span>,<span class="type">Int</span>),v)=&gt;(acc._1+v,acc._2+<span class="number">1</span>),</span><br><span class="line">     |     (acc1:(<span class="type">Int</span>,<span class="type">Int</span>),acc2:(<span class="type">Int</span>,<span class="type">Int</span>))=&gt;(acc1._1+acc2._1,acc1._2+acc2._2))</span><br><span class="line">combine: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = <span class="type">ShuffledRDD</span>[<span class="number">53</span>] at combineByKey at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> result = combine.map&#123;</span><br><span class="line">     |     <span class="keyword">case</span> (key,value) =&gt; (key,value._1/value._2.toDouble)&#125;</span><br><span class="line">result: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">54</span>] at map at &lt;console&gt;:<span class="number">30</span></span><br><span class="line"></span><br><span class="line">scala&gt; result.collect()</span><br><span class="line">res33: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = <span class="type">Array</span>((<span class="type">Wilma</span>,<span class="number">95.33333333333333</span>), (<span class="type">Fred</span>,<span class="number">91.33333333333333</span>))</span><br></pre></td></tr></table></figure><ul><li>aggregateByKey(zeroValue:U,[partitioner: Partitioner]) (seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U): 在kv对的RDD中，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。seqOp函数用于在每一个分区中用初始值逐步迭代value，combOp函数用于合并每个分区中的结果。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">3</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> agg = rdd.aggregateByKey(<span class="number">0</span>)(math.max(_,_),_+_)</span><br><span class="line">agg: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">13</span>] at aggregateByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; agg.collect()</span><br><span class="line">res7: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">3</span>,<span class="number">8</span>), (<span class="number">1</span>,<span class="number">7</span>), (<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; agg.partitions.size</span><br><span class="line">res8: <span class="type">Int</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">1</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> agg = rdd.aggregateByKey(<span class="number">0</span>)(math.max(_,_),_+_).collect()</span><br><span class="line">agg: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>), (<span class="number">3</span>,<span class="number">8</span>), (<span class="number">2</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure><ul><li>foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]: aggregateByKey的简化操作，seqop和combop相同。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">3</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">91</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> agg = rdd.foldByKey(<span class="number">0</span>)(_+_)</span><br><span class="line">agg: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">92</span>] at foldByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; agg.collect()</span><br><span class="line">res61: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">3</span>,<span class="number">14</span>), (<span class="number">1</span>,<span class="number">9</span>), (<span class="number">2</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure><ul><li>sortByKey([ascending], [numTasks]): 在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">3</span>,<span class="string">"aa"</span>),(<span class="number">6</span>,<span class="string">"cc"</span>),(<span class="number">2</span>,<span class="string">"bb"</span>),(<span class="number">1</span>,<span class="string">"dd"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">14</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortByKey(<span class="literal">true</span>).collect()</span><br><span class="line">res9: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">1</span>,dd), (<span class="number">2</span>,bb), (<span class="number">3</span>,aa), (<span class="number">6</span>,cc))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortByKey(<span class="literal">false</span>).collect()</span><br><span class="line">res10: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">6</span>,cc), (<span class="number">3</span>,aa), (<span class="number">2</span>,bb), (<span class="number">1</span>,dd))</span><br></pre></td></tr></table></figure><ul><li>sortBy(func,[ascending], [numTasks]): 与sortByKey类似，但是更灵活,可以用func先对数据进行处理，按照处理后的数据比较结果排序。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">21</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortBy(x =&gt; x).collect()</span><br><span class="line">res11: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortBy(x =&gt; x%<span class="number">3</span>).collect()</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><ul><li>join(otherDataset, [numTasks]): 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">32</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">33</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.join(rdd1).collect()</span><br><span class="line">res13: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">String</span>, <span class="type">Int</span>))] = <span class="type">Array</span>((<span class="number">1</span>,(a,<span class="number">4</span>)), (<span class="number">2</span>,(b,<span class="number">5</span>)), (<span class="number">3</span>,(c,<span class="number">6</span>)))</span><br></pre></td></tr></table></figure><ul><li>cogroup(otherDataset, [numTasks]): 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable\&lt;V>,Iterable\&lt;W>))类型的RDD。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">37</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">38</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.cogroup(rdd1).collect()</span><br><span class="line">res14: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = <span class="type">Array</span>((<span class="number">1</span>,(<span class="type">CompactBuffer</span>(a),<span class="type">CompactBuffer</span>(<span class="number">4</span>))), (<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="number">5</span>))), (<span class="number">3</span>,(<span class="type">CompactBuffer</span>(c),<span class="type">CompactBuffer</span>(<span class="number">6</span>))))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">Array</span>((<span class="number">4</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">41</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.cogroup(rdd2).collect()</span><br><span class="line">res15: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = <span class="type">Array</span>((<span class="number">4</span>,(<span class="type">CompactBuffer</span>(),<span class="type">CompactBuffer</span>(<span class="number">4</span>))), (<span class="number">1</span>,(<span class="type">CompactBuffer</span>(a),<span class="type">CompactBuffer</span>())), (<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="number">5</span>))), (<span class="number">3</span>,(<span class="type">CompactBuffer</span>(c),<span class="type">CompactBuffer</span>(<span class="number">6</span>))))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">1</span>,<span class="string">"d"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">44</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.cogroup(rdd2).collect()</span><br><span class="line">res16: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = <span class="type">Array</span>((<span class="number">4</span>,(<span class="type">CompactBuffer</span>(),<span class="type">CompactBuffer</span>(<span class="number">4</span>))), (<span class="number">1</span>,(<span class="type">CompactBuffer</span>(d, a),<span class="type">CompactBuffer</span>())), (<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="number">5</span>))), (<span class="number">3</span>,(<span class="type">CompactBuffer</span>(c),<span class="type">CompactBuffer</span>(<span class="number">6</span>))))</span><br></pre></td></tr></table></figure><ul><li>cartesian(otherDataset): 笛卡尔积。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">3</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">47</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">2</span> to <span class="number">5</span>)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">48</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.cartesian(rdd2).collect()</span><br><span class="line">res17: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="number">2</span>), (<span class="number">1</span>,<span class="number">3</span>), (<span class="number">1</span>,<span class="number">4</span>), (<span class="number">1</span>,<span class="number">5</span>), (<span class="number">2</span>,<span class="number">2</span>), (<span class="number">2</span>,<span class="number">3</span>), (<span class="number">2</span>,<span class="number">4</span>), (<span class="number">2</span>,<span class="number">5</span>), (<span class="number">3</span>,<span class="number">2</span>), (<span class="number">3</span>,<span class="number">3</span>), (<span class="number">3</span>,<span class="number">4</span>), (<span class="number">3</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure><ul><li>pipe(command, [envVars]): 对于每个分区，都执行一个perl或者shell脚本，返回输出的RDD。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="string">"hi"</span>,<span class="string">"Hello"</span>,<span class="string">"how"</span>,<span class="string">"are"</span>,<span class="string">"you"</span>),<span class="number">1</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">50</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.pipe(<span class="string">"/home/spark_shell/pipe.sh"</span>).collect()</span><br><span class="line">res18: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="type">AA</span>, &gt;&gt;&gt;hi, &gt;&gt;&gt;<span class="type">Hello</span>, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="string">"hi"</span>,<span class="string">"Hello"</span>,<span class="string">"how"</span>,<span class="string">"are"</span>,<span class="string">"you"</span>),<span class="number">2</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">52</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.pipe(<span class="string">"/home/spark_shell/pipe.sh"</span>).collect()</span><br><span class="line">res19: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="type">AA</span>, &gt;&gt;&gt;hi, &gt;&gt;&gt;<span class="type">Hello</span>, <span class="type">AA</span>, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pipe.sh:</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">echo "AA"</span><br><span class="line">while read LINE; do</span><br><span class="line">   echo "&gt;&gt;&gt;"$&#123;LINE&#125;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><ul><li>coalesce(numPartitions): 缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">16</span>,<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">54</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res20: <span class="type">Int</span> = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> coalesceRDD = rdd.coalesce(<span class="number">3</span>)</span><br><span class="line">coalesceRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">CoalescedRDD</span>[<span class="number">55</span>] at coalesce at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; coalesceRDD.partitions.size</span><br><span class="line">res21: <span class="type">Int</span> = <span class="number">3</span></span><br></pre></td></tr></table></figure><ul><li>repartition(numPartitions): 根据分区数，从新通过网络随机洗牌所有数据。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">16</span>,<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">56</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res22: <span class="type">Int</span> = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rerdd = rdd.repartition(<span class="number">2</span>)</span><br><span class="line">rerdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">60</span>] at repartition at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rerdd.partitions.size</span><br><span class="line">res23: <span class="type">Int</span> = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rerdd = rdd.repartition(<span class="number">4</span>)</span><br><span class="line">rerdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">64</span>] at repartition at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rerdd.partitions.size</span><br><span class="line">res24: <span class="type">Int</span> = <span class="number">4</span></span><br></pre></td></tr></table></figure><ul><li>glom(): 将每一个分区形成一个数组，形成新的RDD类型时RDD[Array[T]]。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">16</span>,<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">65</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.glom().collect()</span><br><span class="line">res25: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="type">Array</span>(<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>), <span class="type">Array</span>(<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>), <span class="type">Array</span>(<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>))</span><br></pre></td></tr></table></figure><ul><li>mapValues(func):    针对于(K,V)形式的类型只对V进行操作。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">1</span>,<span class="string">"d"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">67</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.mapValues(_+<span class="string">"|||"</span>).collect()</span><br><span class="line">res26: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">1</span>,a|||), (<span class="number">1</span>,d|||), (<span class="number">2</span>,b|||), (<span class="number">3</span>,c|||))</span><br></pre></td></tr></table></figure><ul><li>subtract(other:RDD): 计算差的一种函数去除两个RDD中相同的元素，不同的RDD将保留下来 。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">3</span> to <span class="number">8</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">70</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">71</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.subtract(rdd1).collect()</span><br><span class="line">res27: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">8</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure><h3 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h3><p>常用的 Action 如下：</p><ul><li>reduce(func): 通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">85</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.reduce(_+_)</span><br><span class="line">res50: <span class="type">Int</span> = <span class="number">55</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="type">Array</span>((<span class="string">"a"</span>,<span class="number">1</span>),(<span class="string">"a"</span>,<span class="number">3</span>),(<span class="string">"c"</span>,<span class="number">3</span>),(<span class="string">"d"</span>,<span class="number">5</span>)))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">86</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.reduce((x,y)=&gt;(x._1 + y._1,x._2 + y._2))</span><br><span class="line">res51: (<span class="type">String</span>, <span class="type">Int</span>) = (adca,<span class="number">12</span>)</span><br></pre></td></tr></table></figure><ul><li>collect(): 在驱动程序中，以数组的形式返回数据集的所有元素。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">17</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect()</span><br><span class="line">res7: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure><ul><li>count(): 返回RDD的元素个数。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">18</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count()</span><br><span class="line">res8: <span class="type">Long</span> = <span class="number">10</span></span><br></pre></td></tr></table></figure><ul><li>first(): 返回RDD的第一个元素（类似于take(1)）。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">19</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.first()</span><br><span class="line">res9: <span class="type">Int</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure><ul><li>take(n): 返回一个由数据集的前n个元素组成的数组。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">20</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.take(<span class="number">7</span>)</span><br><span class="line">res10: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure><ul><li>takeSample(withReplacement,num, [seed]): 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">21</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.takeSample(<span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">res11: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">6</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><ul><li>takeOrdered(n): 返回前几个的排序。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Seq</span>(<span class="number">10</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">19</span>, <span class="number">4</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">23</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.top(<span class="number">2</span>)</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">19</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.takeOrdered(<span class="number">2</span>)</span><br><span class="line">res13: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure><ul><li>aggregate (zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U): aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">88</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(<span class="number">1</span>)(</span><br><span class="line">     | &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x + y&#125;,</span><br><span class="line">     | &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">     | )</span><br><span class="line">res56: <span class="type">Int</span> = <span class="number">58</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(<span class="number">1</span>)(</span><br><span class="line">     | &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x * y&#125;,</span><br><span class="line">     | &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">     | )</span><br><span class="line">res57: <span class="type">Int</span> = <span class="number">30361</span></span><br></pre></td></tr></table></figure><ul><li>fold(num)(func): 折叠操作，aggregate的简化操作，seqop和combop一样。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">90</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(<span class="number">1</span>)(</span><br><span class="line">     | &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x + y&#125;,</span><br><span class="line">     | &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">     | )</span><br><span class="line">res59: <span class="type">Int</span> = <span class="number">13</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.fold(<span class="number">1</span>)(_+_)</span><br><span class="line">res60: <span class="type">Int</span> = <span class="number">13</span></span><br></pre></td></tr></table></figure><ul><li>countByKey(): 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">3</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">95</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.countByKey()</span><br><span class="line">res63: scala.collection.<span class="type">Map</span>[<span class="type">Int</span>,<span class="type">Long</span>] = <span class="type">Map</span>(<span class="number">3</span> -&gt; <span class="number">2</span>, <span class="number">1</span> -&gt; <span class="number">3</span>, <span class="number">2</span> -&gt; <span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li>foreach(func): 在数据集的每一个元素上，运行函数func进行更新。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">30</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.collect().foreach(println)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure><ul><li>saveAsTextFile(path): 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本。</li><li>saveAsSequenceFile(path): 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。</li><li>saveAsObjectFile(path): 用于将RDD中的元素序列化成对象，存储到文件中。</li></ul><h3 id="数值-RDD-的统计操作"><a href="#数值-RDD-的统计操作" class="headerlink" title="数值 RDD 的统计操作"></a>数值 RDD 的统计操作</h3><p>Spark对包含数值数据的RDD提供了一些描述性的统计操作。Spark的数值操作是通过流式算法实现的，允许以每次一个元素的方式构建出模型。这些统计数据都会在调用stats()时通过一次遍历数据计算出来，并以StatsCounter对象返回。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/14-RDD-Statistical-Operation.png?raw=true" alt="14-RDD-Statistical-Operation"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">42</span>] at makeRDD at &lt;console&gt;:<span class="number">32</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.sum()</span><br><span class="line">res34: <span class="type">Double</span> = <span class="number">5050.0</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.max()</span><br><span class="line">res35: <span class="type">Int</span> = <span class="number">100</span></span><br></pre></td></tr></table></figure><h3 id="向RDD操作传递函数注意"><a href="#向RDD操作传递函数注意" class="headerlink" title="向RDD操作传递函数注意"></a>向RDD操作传递函数注意</h3><p>Spark的大部分转化操作和一部分行动操作，都需要依赖用户传递的函数来计算。在Scala中，我们可以把定义的内联函数、方法的引用或静态方法传递给Spark，就像Scala的其他函数式API一样。我们还要考虑其他一些细节，比如所传递的函数及其引用的数据需要是可序列化的(实现了Java的Serializable接口)。传递一个对象的方法或者字段时，会包含对整个对象的引用。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SearchFunctions</span>(<span class="params">val query: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    s.contains(query)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatchesFunctionReference</span></span>(rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>]): org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="comment">// 问题:"isMatch"表示"this.isMatch"，因此我们要传递整个"this" </span></span><br><span class="line">    rdd.filter(isMatch)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatchesFieldReference</span></span>(rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>]): org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = &#123; </span><br><span class="line"><span class="comment">// 问题:"query"表示"this.query"，因此我们要传递整个"this" </span></span><br><span class="line">rdd.filter(x =&gt; x.contains(query)) </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatchesNoReference</span></span>(rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>]): org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="comment">// 安全:只把我们需要的字段拿出来放入局部变量中 </span></span><br><span class="line"><span class="keyword">val</span> query_ = <span class="keyword">this</span>.query</span><br><span class="line">    rdd.filter(x =&gt; x.contains(query_))</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果在Scala中出现了NotSerializableException，通常问题就在于我们传递了一个不可序列化的类中的函数或字段。</p><h3 id="在不同-RDD-类型间转换"><a href="#在不同-RDD-类型间转换" class="headerlink" title="在不同 RDD 类型间转换"></a>在不同 RDD 类型间转换</h3><p>有些函数只能用于特定类型的RDD，比如mean()和variance()只能用在数值RDD上，而join()只能用在键值对RDD上。在Scala和Java中，这些函数都没有定义在标准的RDD类中，所以要访问这些附加功能，必须要确保获得了正确的专用RDD类。<br>在Scala中，将RDD转为有特定函数的RDD(比如在RDD[Double]上进行数值操作)是由隐式转换来自动处理的。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/15-RDD-In-IDEA.png?raw=true" alt="15-RDD-In-IDEA"></p><h2 id="RDD-持久化"><a href="#RDD-持久化" class="headerlink" title="RDD 持久化"></a>RDD 持久化</h2><h3 id="RDD-的缓存"><a href="#RDD-的缓存" class="headerlink" title="RDD 的缓存"></a>RDD 的缓存</h3><p>Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存个数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。如果一个有持久化数据的节点发生故障，Spark会在需要用到缓存的数据时重算丢失的数据分区。如果希望节点故障的情况不会拖累我们的执行速度，也可以把数据备份到多个节点上。</p><h3 id="RDD-的缓存方式"><a href="#RDD-的缓存方式" class="headerlink" title="RDD 的缓存方式"></a>RDD 的缓存方式</h3><p>RDD通过persist方法或cache方法可以将前面的计算结果缓存，默认情况下persist()会把数据以序列化的形式缓存在JVM的堆空间中。<br>但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/16-RDD-Persist.png?raw=true" alt="16-RDD-Persist"></p><p>通过查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在objectStorageLevel中定义的。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">19</span>] at makeRDD at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> nocache = rdd.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">nocache: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">20</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> cache =  rdd.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">cache: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">21</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; cache.cache</span><br><span class="line">res24: cache.<span class="keyword">type</span> = <span class="type">MapPartitionsRDD</span>[<span class="number">21</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; nocache.collect</span><br><span class="line">res25: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479375155</span>], <span class="number">2</span>[<span class="number">1505479374674</span>], <span class="number">3</span>[<span class="number">1505479374674</span>], <span class="number">4</span>[<span class="number">1505479375153</span>], <span class="number">5</span>[<span class="number">1505479375153</span>], <span class="number">6</span>[<span class="number">1505479374675</span>], <span class="number">7</span>[<span class="number">1505479375154</span>], <span class="number">8</span>[<span class="number">1505479375154</span>], <span class="number">9</span>[<span class="number">1505479374676</span>], <span class="number">10</span>[<span class="number">1505479374676</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; nocache.collect</span><br><span class="line">res26: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479375679</span>], <span class="number">2</span>[<span class="number">1505479376157</span>], <span class="number">3</span>[<span class="number">1505479376157</span>], <span class="number">4</span>[<span class="number">1505479375680</span>], <span class="number">5</span>[<span class="number">1505479375680</span>], <span class="number">6</span>[<span class="number">1505479376159</span>], <span class="number">7</span>[<span class="number">1505479375680</span>], <span class="number">8</span>[<span class="number">1505479375680</span>], <span class="number">9</span>[<span class="number">1505479376158</span>], <span class="number">10</span>[<span class="number">1505479376158</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; nocache.collect</span><br><span class="line">res27: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479376743</span>], <span class="number">2</span>[<span class="number">1505479377218</span>], <span class="number">3</span>[<span class="number">1505479377218</span>], <span class="number">4</span>[<span class="number">1505479376745</span>], <span class="number">5</span>[<span class="number">1505479376745</span>], <span class="number">6</span>[<span class="number">1505479377219</span>], <span class="number">7</span>[<span class="number">1505479376747</span>], <span class="number">8</span>[<span class="number">1505479376747</span>], <span class="number">9</span>[<span class="number">1505479377218</span>], <span class="number">10</span>[<span class="number">1505479377218</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; cache.collect</span><br><span class="line">res28: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479382745</span>], <span class="number">2</span>[<span class="number">1505479382253</span>], <span class="number">3</span>[<span class="number">1505479382253</span>], <span class="number">4</span>[<span class="number">1505479382748</span>], <span class="number">5</span>[<span class="number">1505479382748</span>], <span class="number">6</span>[<span class="number">1505479382257</span>], <span class="number">7</span>[<span class="number">1505479382747</span>], <span class="number">8</span>[<span class="number">1505479382747</span>], <span class="number">9</span>[<span class="number">1505479382253</span>], <span class="number">10</span>[<span class="number">1505479382253</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; cache.collect</span><br><span class="line">res29: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479382745</span>], <span class="number">2</span>[<span class="number">1505479382253</span>], <span class="number">3</span>[<span class="number">1505479382253</span>], <span class="number">4</span>[<span class="number">1505479382748</span>], <span class="number">5</span>[<span class="number">1505479382748</span>], <span class="number">6</span>[<span class="number">1505479382257</span>], <span class="number">7</span>[<span class="number">1505479382747</span>], <span class="number">8</span>[<span class="number">1505479382747</span>], <span class="number">9</span>[<span class="number">1505479382253</span>], <span class="number">10</span>[<span class="number">1505479382253</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; cache.collect</span><br><span class="line">res30: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479382745</span>], <span class="number">2</span>[<span class="number">1505479382253</span>], <span class="number">3</span>[<span class="number">1505479382253</span>], <span class="number">4</span>[<span class="number">1505479382748</span>], <span class="number">5</span>[<span class="number">1505479382748</span>], <span class="number">6</span>[<span class="number">1505479382257</span>], <span class="number">7</span>[<span class="number">1505479382747</span>], <span class="number">8</span>[<span class="number">1505479382747</span>], <span class="number">9</span>[<span class="number">1505479382253</span>], <span class="number">10</span>[<span class="number">1505479382253</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; cache.persist(org.apache.spark.storage.<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br></pre></td></tr></table></figure><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/17-RDD-StorageLevel.png?raw=true" alt="17-RDD-StorageLevel"></p><p>在存储级别的末尾加上“_2”来把持久化数据存为两份</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/18-RDD-StorageLevel-Description.png?raw=true" alt="18-RDD-StorageLevel-Description"></p><p>缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。</p><p>注意：使用Tachyon可以实现堆外缓存。</p><h2 id="RDD-CheckPoint-机制"><a href="#RDD-CheckPoint-机制" class="headerlink" title="RDD CheckPoint 机制"></a>RDD CheckPoint 机制</h2><p>Spark中对于数据的保存除了持久化操作之外，还提供了一种CheckPoint的机制，CheckPoint（本质是通过将RDD写入Disk做CheckPoint）是为了通过lineage做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做CheckPoint容错，如果之后有节点出现问题而丢失分区，从做CheckPoint的RDD开始重做Lineage，就会减少开销。CheckPoint通过将数据写入到HDFS文件系统实现了RDD的CheckPoint功能。<br>cache和CheckPoint是有显著区别的，缓存把RDD计算出来然后放在内存中，但是RDD的依赖链（相当于数据库中的redo日志），也不能丢掉，当某个点某个executor宕了，上面cache的RDD就会丢掉，需要通过依赖链重放计算出来，不同的是，CheckPoint是把RDD保存在HDFS中，是多副本可靠存储，所以依赖链就可以丢掉了，就斩断了依赖链，是通过复制实现的高容错。<br>如果存在以下场景，则比较适合使用CheckPoint机制：</p><ol><li>DAG中的Lineage过长，如果重算，则开销太大（如在PageRank中）。</li><li>在宽依赖上做Checkpoint获得的收益更大。</li></ol><p>为当前RDD设置 CheckPoint。该函数将会创建一个二进制的文件，并存储到checkpoint目录中，该目录是用SparkContext.setCheckpointDir()设置的。在checkpoint的过程中，该RDD的所有依赖于父RDD中的信息将全部被移出。对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">100</span> , <span class="number">5</span>)</span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] =</span><br><span class="line">　　<span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at parallelize at &lt;console&gt;:<span class="number">12</span></span><br><span class="line"> </span><br><span class="line">scala&gt; sc.setCheckpointDir(<span class="string">"hdfs://slave1:9000/checkpoint"</span>)</span><br><span class="line"> </span><br><span class="line">scala&gt; data.checkpoint</span><br><span class="line"> </span><br><span class="line">scala&gt; data.count</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ch1 = sc.parallelize(<span class="number">1</span> to <span class="number">2</span>)</span><br><span class="line">ch1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">33</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ch2 = ch1.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">ch2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">36</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ch3 = ch1.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">ch3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">37</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; ch3.checkpoint</span><br><span class="line"></span><br><span class="line">scala&gt; ch2.collect</span><br><span class="line">res62: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480940726</span>], <span class="number">2</span>[<span class="number">1505480940243</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; ch2.collect</span><br><span class="line">res63: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480941957</span>], <span class="number">2</span>[<span class="number">1505480941480</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; ch2.collect</span><br><span class="line">res64: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480942736</span>], <span class="number">2</span>[<span class="number">1505480942257</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; ch3.collect</span><br><span class="line">res65: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480949080</span>], <span class="number">2</span>[<span class="number">1505480948603</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; ch3.collect</span><br><span class="line">res66: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480948683</span>], <span class="number">2</span>[<span class="number">1505480949161</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; ch3.collect</span><br><span class="line">res67: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480948683</span>], <span class="number">2</span>[<span class="number">1505480949161</span>])</span><br></pre></td></tr></table></figure><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/19-RDD-CheckPoint.png?raw=true" alt="19-RDD-CheckPoint"></p><h3 id="CheckPoint-写流程"><a href="#CheckPoint-写流程" class="headerlink" title="CheckPoint 写流程"></a>CheckPoint 写流程</h3><p>RDD checkpoint 过程中会经过以下几个状态：[ Initialized → marked for checkpointing → checkpointing in progress → checkpointed ]，转换流程如下：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/20-RDD-CheckPoint-Transforme.png?raw=true" alt="20-RDD-CheckPoint-Transforme"></p><ol><li>data.CheckPoint这个函数调用中，设置的目录中，所有依赖的RDD都会被删除，函数必须在job运行之前调用执行，强烈建议RDD缓存在内存中（又提到一次，千万要注意哟），否则保存到文件的时候需要从头计算。初始化RDD的CheckPointData变量为ReliableRDDCheckpointData。这时候标记为Initialized状态。</li><li>在所有jobaction的时候，runJob方法中都会调用rdd.doCheckpoint,这个会向前递归调用所有的依赖的RDD，看看需不需要CheckPoint。需要需要CheckPoint，然后调用CheckPointData.get.CheckPoint()，里面标记状态为CheckpointingInProgress，里面调用具体实现类的ReliableRDDCheckpointData的doCheckpoint方法。</li><li>doCheckpoint-&gt;writeRDDToCheckpointDirectory，注意这里会把job再运行一次，如果已经cache了，就可以直接使用缓存中的RDD了，就不需要重头计算一遍了（怎么又说了一遍），这时候直接把RDD，输出到hdfs，每个分区一个文件，会先写到一个临时文件，如果全部输出完，进行rename，如果输出失败，就回滚delete。</li><li>标记状态为Checkpointed，markCheckpointed方法中清除所有的依赖，怎么清除依赖的呢，就是把RDD变量的强引用设置为null，垃圾回收了，会触发ContextCleaner里面监听清除实际BlockManager缓存中的数据。</li></ol><h3 id="CheckPoint-读流程"><a href="#CheckPoint-读流程" class="headerlink" title="CheckPoint 读流程"></a>CheckPoint 读流程</h3><p>如果一个RDD我们已经CheckPoint了那么是什么时候用呢，CheckPoint将RDD持久化到HDFS或本地文件夹，如果不被手动remove掉，是一直存在的，也就是说可以被下一个driverprogram使用。比如sparkstreaming挂掉了，重启后就可以使用之前CheckPoint的数据进行recover,当然在同一个driverprogram也可以使用。我们讲下在同一个driverprogram中是怎么使用CheckPoint数据的。<br>如果一个RDD被CheckPoint了，当这个RDD上有action操作时候，或者回溯的这个RDD的时候，触发这个RDD进行计算，里面判断是否CheckPoint过，对分区和依赖的处理都是使用的RDD内部的CheckPointRDD变量。<br>具体细节如下：如果一个RDD被CheckPoint了，那么这个RDD中对分区和依赖的处理都是使用的RDD内部的CheckPointRDD变量，具体实现是ReliableCheckpointRDD类型。这个是在CheckPoint写流程中创建的。依赖和获取分区方法中先判断是否已经CheckPoint，如果已经CheckPoint了，就斩断依赖，使用ReliableCheckpointRDD，来处理依赖和获取分区。如果没有，才往前回溯依赖。依赖就是没有依赖，因为已经斩断了依赖，获取分区数据就是读取CheckPoint到HDFS目录中不同分区保存下来的文件。</p><h2 id="RDD-的依赖关系"><a href="#RDD-的依赖关系" class="headerlink" title="RDD 的依赖关系"></a>RDD 的依赖关系</h2><p>RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/21-RDD-Narrow-And-Wide-Dependencies.png?raw=true" alt="21-RDD-Narrow-And-Wide-Dependencies"></p><ol><li>窄依赖：窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用。总结：窄依赖我们形象的比喻为独生子女。</li><li>宽依赖：宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition，会引起shuffle。总结：宽依赖我们形象的比喻为超生。</li><li>Lineage：RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/22-RDD-Lineage.png?raw=true" alt="22-RDD-Lineage"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> text = sc.textFile(<span class="string">"README.md"</span>)</span><br><span class="line">text: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">README</span>.md <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> words = text.flatMap(_.split)</span><br><span class="line">split   splitAt</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> words = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">words: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at flatMap at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; words.map((_,<span class="number">1</span>))</span><br><span class="line">res0: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at &lt;console&gt;:<span class="number">29</span></span><br><span class="line"></span><br><span class="line">scala&gt; res0.reduceByKey</span><br><span class="line">reduceByKey   reduceByKeyLocally</span><br><span class="line"></span><br><span class="line">scala&gt; res0.reduceByKey(_+_)</span><br><span class="line">res1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">4</span>] at reduceByKey at &lt;console&gt;:<span class="number">31</span></span><br><span class="line"></span><br><span class="line">scala&gt; res1.dependencies</span><br><span class="line">res2: <span class="type">Seq</span>[org.apache.spark.<span class="type">Dependency</span>[_]] = <span class="type">List</span>(org.apache.spark.<span class="type">ShuffleDependency</span>@<span class="number">6</span>cfe48a4)</span><br><span class="line"></span><br><span class="line">scala&gt; res0.dependencies</span><br><span class="line">res3: <span class="type">Seq</span>[org.apache.spark.<span class="type">Dependency</span>[_]] = <span class="type">List</span>(org.apache.spark.<span class="type">OneToOneDependency</span>@<span class="number">6</span>c9e24c4)</span><br></pre></td></tr></table></figure><h2 id="DAG-的生成"><a href="#DAG-的生成" class="headerlink" title="DAG 的生成"></a>DAG 的生成</h2><p>DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/23-DAG.png?raw=true" alt="23-DAG"></p><h2 id="RDD-相关概念关系"><a href="#RDD-相关概念关系" class="headerlink" title="RDD 相关概念关系"></a>RDD 相关概念关系</h2><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/24-RDD-Partition.png?raw=true" alt="24-RDD-Partition"></p><p>输入可能以多个文件的形式存储在HDFS上，每个File都包含了很多块，称为Block。<br>当Spark读取这些文件作为输入时，会根据具体数据格式对应的InputFormat进行解析，一般是将若干个Block合并成一个输入分片，称为InputSplit，注意InputSplit不能跨越文件。<br>随后将为这些输入分片生成具体的Task。InputSplit与Task是一一对应的关系。<br>随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。</p><ol><li>每个节点可以起一个或多个Executor。</li><li>每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。</li><li>每个Task执行的结果就是生成了目标RDD的一个partiton。</li></ol><p>注意: 这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。<br>而Task被执行的并发度 = Executor数目 * 每个Executor核数。<br>至于partition的数目：</p><ol><li>对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。</li><li>在Map阶段partition数目保持不变。</li><li>在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。</li></ol><p>RDD在计算的时候，每个分区都会起一个task，所以rdd的分区数目决定了总的的task数目。<br>申请的计算节点（Executor）数目和每个计算节点核数，决定了你同一时刻可以并行执行的task。<br>比如的RDD有100个分区，那么计算的时候就会生成100个task，你的资源配置为10个计算节点，每个两2个核，同一时刻可以并行的task数目为20，计算这个RDD就需要5个轮次。<br>如果计算资源不变，你有101个task的话，就需要6个轮次，在最后一轮中，只有一个task在执行，其余核都在空转。<br>如果资源不变，你的RDD只有2个分区，那么同一时刻只有2个task运行，其余18个核空转，造成资源浪费。这就是在spark调优中，增大RDD分区数目，增大任务并行度的做法。</p><h1 id="键值对-RDD"><a href="#键值对-RDD" class="headerlink" title="键值对 RDD"></a>键值对 RDD</h1><p>键值对RDD是Spark中许多操作所需要的常见数据类型。本章做特别讲解。除了在基础RDD类中定义的操作之外，Spark为包含键值对类型的RDD提供了一些专有的操作在PairRDDFunctions专门进行了定义。这些RDD被称为pairRDD。<br>有很多种方式创建pairRDD，在输入输出章节会讲解。一般如果从一个普通的RDD转为pairRDD时，可以调用map()函数来实现，传递的函数需要返回键值对。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pairs = lines.map(x =&gt; (x.split(<span class="string">" "</span>)(<span class="number">0</span>), x))</span><br></pre></td></tr></table></figure><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/25-PairRDDFunctions-In-IDEA.png?raw=true" alt="25-PairRDDFunctions-In-IDEA"></p><h2 id="Pair-RDD-的-Transformation-操作"><a href="#Pair-RDD-的-Transformation-操作" class="headerlink" title="Pair RDD 的 Transformation 操作"></a>Pair RDD 的 Transformation 操作</h2><h3 id="转化操作"><a href="#转化操作" class="headerlink" title="转化操作"></a>转化操作</h3><p>上一章进行了练习，这一章会重点讲解。针对一个Pair RDD的转化操作：</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/26-PairRDD-Transformation-1.png?raw=true" alt="26-PairRDD-Transformation-1"></p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/27-PairRDD-Transformation-2.png?raw=true" alt="27-PairRDD-Transformation-2"></p><h3 id="聚合操作"><a href="#聚合操作" class="headerlink" title="聚合操作"></a>聚合操作</h3><p>当数据集以键值对形式组织的时候，聚合具有相同键的元素进行一些统计是很常见的操作。之前讲解过基础RDD上的fold()、combine()、reduce()等行动操作，pairRDD上则有相应的针对键的转化操作。Spark有一组类似的操作，可以组合具有相同键的值。这些操作返回RDD，因此它们是转化操作而不是行动操作。<br>reduceByKey()与reduce()相当类似;它们都接收一个函数，并使用该函数对值进行合并。reduceByKey()会为数据集中的每个键进行并行的归约操作，每个归约操作会将键相同的值合并起来。因为数据集中可能有大量的键，所以reduceByKey()没有被实现为向用户程序返回一个值的行动操作。实际上，它会返回一个由各键和对应键归约出来的结果值组成的新的RDD。<br>foldByKey()则与fold()相当类似;它们都使用一个与RDD和合并函数中的数据类型相同的零值作为初始值。与fold()一样，foldByKey()操作所使用的合并函数对零值与另一个元素进行合并，结果仍为该元素。<br>求均值操作：版本一</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input.mapValues(x =&gt; (x, <span class="number">1</span>)).reduceByKey((x, y) =&gt; (x._1 + y._1, x._2 + y._2)).map&#123; <span class="keyword">case</span> (key, value) =&gt; (key, value._1 / value._2.toFloat) &#125;</span><br></pre></td></tr></table></figure><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/28-PairRDD-MapReduce.png?raw=true" alt="28-PairRDD-MapReduce"></p><p>combineByKey()是最为常用的基于键进行聚合的函数。大多数基于键聚合的函数都是用它实现的。和aggregate()一样，combineByKey()可以让用户返回与输入数据的类型不同的返回值。<br>要理解combineByKey()，要先理解它在处理数据时是如何处理每个元素的。由于combineByKey()会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。<br>如果这是一个新的元素，combineByKey()会使用一个叫作createCombiner()的函数来创建那个键对应的累加器的初始值。需要注意的是，这一过程会在每个分区中第一次出现各个键时发生，而不是在整个RDD中第一次出现一个键时发生。<br>如果这是一个在处理当前分区之前已经遇到的键，它会使用mergeValue()方法将该键的累加器对应的当前值与这个新的值进行合并。<br>由于每个分区都是独立处理的，因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器，就需要使用用户提供的mergeCombiners()方法将各个分区的结果进行合并。</p><p>求均值操作：版本二</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> result = input.combineByKey(</span><br><span class="line">  (v) =&gt; (v, <span class="number">1</span>),</span><br><span class="line">  (acc: (<span class="type">Int</span>, <span class="type">Int</span>), v) =&gt; (acc._1 + v, acc._2 + <span class="number">1</span>),</span><br><span class="line">  (acc1: (<span class="type">Int</span>, <span class="type">Int</span>), acc2: (<span class="type">Int</span>, <span class="type">Int</span>)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)</span><br><span class="line">).map&#123; <span class="keyword">case</span> (key, value) =&gt; (key, value._1 / value._2.toFloat) &#125;</span><br><span class="line"></span><br><span class="line">result.collectAsMap().map(println(_))</span><br></pre></td></tr></table></figure><h3 id="数据分组"><a href="#数据分组" class="headerlink" title="数据分组"></a>数据分组</h3><p>如果数据已经以预期的方式提取了键，groupByKey()就会使用RDD中的键来对数据进行分组。对于一个由类型K的键和类型V的值组成的RDD，所得到的结果RDD类型会是[K,Iterable[V]]。<br>groupBy()可以用于未成对的数据上，也可以根据除键相同以外的条件进行分组。它可以接收一个函数，对源RDD中的每个元素使用该函数，将返回结果作为键再进行分组。<br>多个RDD分组，可以使用cogroup函数，cogroup()的函数对多个共享同一个键的RDD进行分组。对两个键的类型均为K而值的类型分别为V和W的RDD进行cogroup()时，得到的结果RDD类型为[(K,(Iterable[V],Iterable[W]))]。如果其中的一个RDD对于另一个RDD中存在的某个键没有对应的记录，那么对应的迭代器则为空。cogroup()提供了为多个RDD进行数据分组的方法。</p><h3 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h3><p>连接主要用于多个PairRDD的操作，连接方式多种多样:右外连接、左外连接、交叉连接以及内连接。<br>普通的join操作符表示内连接2。只有在两个pairRDD中都存在的键才叫输出。当一个输入对应的某个键有多个值时，生成的pairRDD会包括来自两个输入RDD的每一组相对应的记录。<br>leftOuterJoin()产生的pairRDD中，源RDD的每一个键都有对应的记录。每个键相应的值是由一个源RDD中的值与一个包含第二个RDD的值的Option(在Java中为Optional)对象组成的二元组。<br>rightOuterJoin()几乎与leftOuterJoin()完全一样，只不过预期结果中的键必须出现在第二个RDD中，而二元组中的可缺失的部分则来自于源RDD而非第二个RDD。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/29-PairRDD-Connection.png?raw=true" alt="29-PairRDD-Connection"></p><h3 id="数据排序"><a href="#数据排序" class="headerlink" title="数据排序"></a>数据排序</h3><p>sortByKey()函数接收一个叫作ascending的参数，表示我们是否想要让结果按升序排序(默认值为true)。</p><h2 id="Pair-RDD-的-Action-操作"><a href="#Pair-RDD-的-Action-操作" class="headerlink" title="Pair RDD 的 Action 操作"></a>Pair RDD 的 Action 操作</h2><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/28-PairRDD-Actions.png?raw=true" alt="28-PairRDD-Actions"></p><h2 id="Pair-RDD-的数据分区"><a href="#Pair-RDD-的数据分区" class="headerlink" title="Pair RDD 的数据分区"></a>Pair RDD 的数据分区</h2><p>Spark目前支持Hash分区和Range分区，用户也可以自定义分区，Hash分区为当前的默认分区，Spark中分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle过程属于哪个分区和Reduce的个数。注意：</p><ol><li>只有Key-Value类型的RDD才有分区的，非Key-Value类型的RDD分区的值是None.</li><li>每个RDD的分区ID范围：0~numPartitions-1，决定这个值是属于那个分区的。</li></ol><h3 id="获取-RDD-的分区方式"><a href="#获取-RDD-的分区方式" class="headerlink" title="获取 RDD 的分区方式"></a>获取 RDD 的分区方式</h3><p>可以通过使用RDD的partitioner属性来获取RDD的分区方式。它会返回一个scala.Option对象，通过get方法获取其中的值。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> pairs = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">3</span>)))</span><br><span class="line">pairs: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">33</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; pairs.partitioner</span><br><span class="line">res26: <span class="type">Option</span>[org.apache.spark.<span class="type">Partitioner</span>] = <span class="type">None</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> partitioned = pairs.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">partitioned: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">34</span>] at partitionBy at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; partitioned.partitioner</span><br><span class="line">res27: <span class="type">Option</span>[org.apache.spark.<span class="type">Partitioner</span>] = <span class="type">Some</span>(org.apache.spark.<span class="type">HashPartitioner</span>@<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h3 id="Hash-分区方式"><a href="#Hash-分区方式" class="headerlink" title="Hash 分区方式"></a>Hash 分区方式</h3><p>HashPartitioner分区的原理：对于给定的key，计算其hashCode，并除于分区的个数取余，如果余数小于0，则用余数+分区的个数，最后返回的值就是这个key所属的分区ID。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> nopar = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">2</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">8</span>)</span><br><span class="line">nopar: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; nopar.partitioner</span><br><span class="line">res20: <span class="type">Option</span>[org.apache.spark.<span class="type">Partitioner</span>] = <span class="type">None</span></span><br><span class="line"></span><br><span class="line">scala&gt;nopar.mapPartitionsWithIndex((index,iter)=&gt;&#123; <span class="type">Iterator</span>(index.toString+<span class="string">" : "</span>+iter.mkString(<span class="string">"|"</span>)) &#125;).collect</span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="string">"0 : "</span>, <span class="number">1</span> : (<span class="number">1</span>,<span class="number">3</span>), <span class="number">2</span> : (<span class="number">1</span>,<span class="number">2</span>), <span class="number">3</span> : (<span class="number">2</span>,<span class="number">4</span>), <span class="string">"4 : "</span>, <span class="number">5</span> : (<span class="number">2</span>,<span class="number">3</span>), <span class="number">6</span> : (<span class="number">3</span>,<span class="number">6</span>), <span class="number">7</span> : (<span class="number">3</span>,<span class="number">8</span>)) </span><br><span class="line">scala&gt; <span class="keyword">val</span> hashpar = nopar.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">7</span>))</span><br><span class="line">hashpar: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">12</span>] at partitionBy at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; hashpar.count</span><br><span class="line">res18: <span class="type">Long</span> = <span class="number">6</span></span><br><span class="line"></span><br><span class="line">scala&gt; hashpar.partitioner</span><br><span class="line">res21: <span class="type">Option</span>[org.apache.spark.<span class="type">Partitioner</span>] = <span class="type">Some</span>(org.apache.spark.<span class="type">HashPartitioner</span>@<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; hashpar.mapPartitions(iter =&gt; <span class="type">Iterator</span>(iter.length)).collect()</span><br><span class="line">res19: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/30-PairRDD-HashPartition.png?raw=true" alt="30-PairRDD-HashPartition"></p><h3 id="Range-分区方式"><a href="#Range-分区方式" class="headerlink" title="Range 分区方式"></a>Range 分区方式</h3><p>HashPartitioner分区弊端：可能导致每个分区中数据量的不均匀，极端情况下会导致某些分区拥有RDD的全部数据。<br>RangePartitioner分区优势：尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大；<br>但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。<br>RangePartitioner作用：将一定范围内的数映射到某一个分区内，在实现中，分界的算法尤为重要。用到了水塘抽样算法。</p><h3 id="自定义分区方式"><a href="#自定义分区方式" class="headerlink" title="自定义分区方式"></a>自定义分区方式</h3><p>要实现自定义的分区器，你需要继承org.apache.spark.Partitioner类并实现下面三个方法。</p><ul><li>numPartitions:Int:返回创建出来的分区数。 </li><li>getPartition(key:Any):Int:返回给定键的分区编号(0到numPartitions-1)。</li><li>equals():Java判断相等性的标准方法。这个方法的实现非常重要，Spark需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样Spark才可以判断两个RDD的分区方式是否相同。 </li></ul><p>假设我们需要将相同后缀的数据写入相同的文件，我们通过将相同后缀的数据分区到相同的分区并保存输出来实现。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.moqi.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">Partitioner</span>, <span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomerPartitioner</span>(<span class="params">numParts:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//覆盖分区数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = numParts</span><br><span class="line"></span><br><span class="line">  <span class="comment">//覆盖分区号获取函数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> ckey: <span class="type">String</span> = key.toString</span><br><span class="line">    ckey.substring(ckey.length<span class="number">-1</span>).toInt%numParts</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CustomerPartitioner</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"partitioner"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> data=sc.parallelize(<span class="type">List</span>(<span class="string">"aa.2"</span>,<span class="string">"bb.2"</span>,<span class="string">"cc.3"</span>,<span class="string">"dd.3"</span>,<span class="string">"ee.5"</span>))</span><br><span class="line"></span><br><span class="line">    data.map((_,<span class="number">1</span>)).partitionBy(<span class="keyword">new</span> <span class="type">CustomerPartitioner</span>(<span class="number">5</span>)).keys.saveAsTextFile(<span class="string">"hdfs://slave1:9000/partitioner"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> data=sc.parallelize(<span class="type">List</span>(<span class="string">"aa.2"</span>,<span class="string">"bb.2"</span>,<span class="string">"cc.3"</span>,<span class="string">"dd.3"</span>,<span class="string">"ee.5"</span>).zipWithIndex,<span class="number">2</span>)</span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">7</span>] at parallelize at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; data.collect</span><br><span class="line">res4: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((aa<span class="number">.2</span>,<span class="number">0</span>), (bb<span class="number">.2</span>,<span class="number">1</span>), (cc<span class="number">.3</span>,<span class="number">2</span>), (dd<span class="number">.3</span>,<span class="number">3</span>), (ee<span class="number">.5</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; data.mapPartitionsWithIndex((index,iter)=&gt;<span class="type">Iterator</span>(index.toString +<span class="string">" : "</span>+ iter.mkString(<span class="string">"|"</span>))).collect</span><br><span class="line">res5: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">0</span> : (aa<span class="number">.2</span>,<span class="number">0</span>)|(bb<span class="number">.2</span>,<span class="number">1</span>), <span class="number">1</span> : (cc<span class="number">.3</span>,<span class="number">2</span>)|(dd<span class="number">.3</span>,<span class="number">3</span>)|(ee<span class="number">.5</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomerPartitioner</span>(<span class="params">numParts:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">Partitioner</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//覆盖分区数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = numParts</span><br><span class="line"></span><br><span class="line">  <span class="comment">//覆盖分区号获取函数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> ckey: <span class="type">String</span> = key.toString</span><br><span class="line">    ckey.substring(ckey.length<span class="number">-1</span>).toInt%numParts</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line"></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">CustomerPartitioner</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">data</span>.<span class="title">partitionBy</span>(<span class="params">new <span class="type">CustomerPartitioner</span>(4</span>))</span></span><br><span class="line"><span class="class"><span class="title">res7</span></span>: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">9</span>] at partitionBy at &lt;console&gt;:<span class="number">31</span></span><br><span class="line"></span><br><span class="line">scala&gt; res7.mapPartitionsWithIndex((index,iter)=&gt;<span class="type">Iterator</span>(index.toString +<span class="string">" : "</span>+ iter.mkString(<span class="string">"|"</span>))).collect</span><br><span class="line">res8: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="string">"0 : "</span>, <span class="number">1</span> : (ee<span class="number">.5</span>,<span class="number">4</span>), <span class="number">2</span> : (aa<span class="number">.2</span>,<span class="number">0</span>)|(bb<span class="number">.2</span>,<span class="number">1</span>), <span class="number">3</span> : (cc<span class="number">.3</span>,<span class="number">2</span>)|(dd<span class="number">.3</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>使用自定义的Partitioner是很容易的:只要把它传给partitionBy()方法即可。Spark中有许多依赖于数据混洗的方法，比如join()和groupByKey()，它们也可以接收一个可选的Partitioner对象来控制输出数据的分区方式。</p><h3 id="分区-Shuffle-优化"><a href="#分区-Shuffle-优化" class="headerlink" title="分区 Shuffle 优化"></a>分区 Shuffle 优化</h3><p>在分布式程序中，通信的代价是很大的，因此控制数据分布以获得最少的网络传输可以极大地提升整体性能。<br>Spark中所有的键值对RDD都可以进行分区。系统会根据一个针对键的函数对元素进行分组。主要有哈希分区和范围分区，当然用户也可以自定义分区函数。<br>通过分区可以有效提升程序性能。如下例子：<br>分析这样一个应用，它在内存中保存着一张很大的用户信息表——也就是一个由(UserID,UserInfo)对组成的RDD，其中UserInfo包含一个该用户所订阅的主题的列表。该应用会周期性地将这张表与一个小文件进行组合，这个小文件中存着过去五分钟内发生的事件——其实就是一个由(UserID,LinkInfo)对组成的表，存放着过去五分钟内某网站各用户的访问情况。例如，我们可能需要对用户访问其未订阅主题的页面的情况进行统计。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始化代码，从 HDFS 的一个 Hadoop SequenceFile 读取用户信息</span></span><br><span class="line"><span class="comment">// userData 中的元素会根据它们被读取时的来源，即 HDFS 块所在的节点来分布</span></span><br><span class="line"><span class="comment">// Spark 此时无法获知某个特定的 User ID 对应的记录位于那个节点上</span></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(...)</span><br><span class="line"><span class="keyword">val</span> userData = sc.sequenceFile[<span class="type">UserId</span>, <span class="type">UserInfo</span>](<span class="string">"hdfs://..."</span>).persist()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 周期性调用函数来处理过去五分钟产生的事件日志</span></span><br><span class="line"><span class="comment">// 假设这是一个包含(UserID, LinkInfo)键值对的 SequenceFile</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">processNewLogs</span></span>(logFileName: <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> events = sc.sequenceFile[<span class="type">UserID</span>, <span class="type">LinkInfo</span>](logFileName)</span><br><span class="line">    <span class="keyword">val</span> joined = userData.join(events) <span class="comment">// RDD of (UserID, (UserInfo, LinkInfo)) pairs</span></span><br><span class="line">    <span class="keyword">val</span> offTopicVisits = joined.filter &#123;</span><br><span class="line">        <span class="comment">// Expand the tuple into tis components</span></span><br><span class="line">        <span class="keyword">case</span> (userId, (userInfo, linkInfo)) =&gt; !userInfo.topics.contains(linkInfo.topic)</span><br><span class="line">    &#125;.count()</span><br><span class="line">    println(<span class="string">"Number of visits to non-subscribed topics: "</span> + offTopicVisits)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/31-PairRDD-Partition-Shuffle-Optimization-1.png?raw=true" alt="31-PairRDD-Partition-Shuffle-Optimization-1"></p><p>这段代码可以正确运行，但是不够高效。这是因为在每次调用processNewLogs()时都会用到join()操作，而我们对数据集是如何分区的却一无所知。默认情况下，连接操作会将两个数据集中的所有键的哈希值都求出来，将该哈希值相同的记录通过网络传到同一台机器上，然后在那台机器上对所有键相同的记录进行连接操作。因为userData表比每五分钟出现的访问日志表events要大得多，所以要浪费时间做很多额外工作:在每次调用时都对userData表进行哈希值计算和跨节点数据混洗，降低了程序的执行效率。<br>优化方法：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(...)</span><br><span class="line"><span class="keyword">val</span> userData = sc.sequenceFile[<span class="type">UserID</span>, <span class="type">UserInfo</span>](<span class="string">"hdfs://..."</span>)</span><br><span class="line"> .partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">100</span>)) <span class="comment">// 构造 100 个分区</span></span><br><span class="line"> .persist()</span><br></pre></td></tr></table></figure><p>我们在构建userData时调用了partitionBy()，Spark就知道了该RDD是根据键的哈希值来分区的，这样在调用join()时，Spark就会利用到这一点。具体来说，当调用userData.join(events)时，Spark只会对events进行数据混洗操作，将events中特定UserID的记录发送到userData的对应分区所在的那台机器上。这样，需要通过网络传输的数据就大大减少了，程序运行速度也可以显著提升了。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/32-PairRDD-Partition-Shuffle-Optimization-2.png?raw=true" alt="32-PairRDD-Partition-Shuffle-Optimization-2"></p><h3 id="基于分区进行操作"><a href="#基于分区进行操作" class="headerlink" title="基于分区进行操作"></a>基于分区进行操作</h3><p>基于分区对数据进行操作可以让我们避免为每个数据元素进行重复的配置工作。诸如打开数据库连接或创建随机数生成器等操作，都是我们应当尽量避免为每个元素都配置一次的工作。Spark提供基于分区的mapPartition和foreachPartition，让你的部分代码只对RDD的每个分区运行一次，这样可以帮助降低这些操作的代价。</p><h3 id="从分区中获益的操作"><a href="#从分区中获益的操作" class="headerlink" title="从分区中获益的操作"></a>从分区中获益的操作</h3><p>能够从数据分区中获得性能提升的操作有cogroup()、groupWith()、join()、leftOuterJoin()、rightOuterJoin()、groupByKey()、reduceByKey()、combineByKey()以及lookup()等。</p><h1 id="数据读取与保存主要方式"><a href="#数据读取与保存主要方式" class="headerlink" title="数据读取与保存主要方式"></a>数据读取与保存主要方式</h1><h2 id="文本文件"><a href="#文本文件" class="headerlink" title="文本文件"></a>文本文件</h2><p>当我们将一个文本文件读取为RDD时，输入的每一行都会成为RDD的一个元素。也可以将多个完整的文本文件一次性读取为一个pairRDD，其中键是文件名，值是文件内容。val input = sc.textFile(“./README.md”). 如果传递目录，则将目录下的所有文件读取作为RDD。文件路径支持通配符。通过wholeTextFiles()对于大量的小文件读取效率比较高，大文件效果没有那么高。<br>Spark通过saveAsTextFile()进行文本文件的输出，该方法接收一个路径，并将RDD中的内容都输入到路径对应的文件中。Spark将传入的路径作为目录对待，会在那个目录下输出多个文件。这样，Spark就可以从多个节点上并行输出了。result.saveAsTextFile(outputFile).</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.textFile(<span class="string">"./README.md"</span>)</span><br><span class="line">res6: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = ./<span class="type">README</span>.md <span class="type">MapPartitionsRDD</span>[<span class="number">7</span>] at textFile at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> readme = sc.textFile(<span class="string">"./README.md"</span>)</span><br><span class="line">readme: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = ./<span class="type">README</span>.md <span class="type">MapPartitionsRDD</span>[<span class="number">9</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; readme.collect()</span><br><span class="line">res7: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(# <span class="type">Apache</span> <span class="type">Spark</span>, <span class="string">""</span>, <span class="type">Spark</span> is a fast and general cluster...</span><br><span class="line">scala&gt; readme.saveAsTextFile(<span class="string">"hdfs://slave1:9000/test"</span>)</span><br></pre></td></tr></table></figure><h2 id="JSON-文件"><a href="#JSON-文件" class="headerlink" title="JSON 文件"></a>JSON 文件</h2><p>如果JSON文件中每一行就是一个JSON记录，那么可以通过将JSON文件当做文本文件来读取，然后利用相关的JSON库对每一条数据进行JSON解析。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> org.json4s._  </span><br><span class="line"><span class="keyword">import</span> org.json4s._</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">import</span> org.json4s.jackson.<span class="type">JsonMethods</span>._  </span><br><span class="line"><span class="keyword">import</span> org.json4s.jackson.<span class="type">JsonMethods</span>._</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">import</span> org.json4s.jackson.<span class="type">Serialization</span>  </span><br><span class="line"><span class="keyword">import</span> org.json4s.jackson.<span class="type">Serialization</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> result = sc.textFile(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line">result: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = examples/src/main/resources/people.json <span class="type">MapPartitionsRDD</span>[<span class="number">7</span>] at textFile at &lt;console&gt;:<span class="number">47</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">implicit</span> <span class="keyword">val</span> formats = <span class="type">Serialization</span>.formats(<span class="type">ShortTypeHints</span>(<span class="type">List</span>())) </span><br><span class="line">formats: org.json4s.<span class="type">Formats</span>&#123;<span class="keyword">val</span> dateFormat: org.json4s.<span class="type">DateFormat</span>; <span class="keyword">val</span> typeHints: org.json4s.<span class="type">TypeHints</span>&#125; = org.json4s.<span class="type">Serialization</span>$$anon$<span class="number">1</span>@<span class="number">61</span>f2c1da</span><br><span class="line"></span><br><span class="line">scala&gt; result.collect().foreach(x =&gt; &#123;<span class="keyword">var</span> c = parse(x).extract[<span class="type">Person</span>];println(c.name + <span class="string">","</span> + c.age)&#125;)  </span><br><span class="line"><span class="type">Michael</span>,<span class="number">30</span></span><br><span class="line"><span class="type">Andy</span>,<span class="number">30</span></span><br><span class="line"><span class="type">Justin</span>,<span class="number">19</span></span><br></pre></td></tr></table></figure><p>如果JSON数据是跨行的，那么只能读入整个文件，然后对每个文件进行解析。JSON数据的输出主要是通过在输出之前将由结构化数据组成的RDD转为字符串RDD，然后使用Spark的文本文件API写出去。说白了还是以文本文件的形式存，只是文本的格式已经在程序中转换为JSON。</p><h2 id="CSV-文件"><a href="#CSV-文件" class="headerlink" title="CSV 文件"></a>CSV 文件</h2><p>读取CSV/TSV数据和读取JSON数据相似，都需要先把文件当作普通文本文件来读取数据，然后通过将每一行进行解析实现对CSV的读取。CSV/TSV数据的输出也是需要将结构化RDD通过相关的库转换成字符串RDD，然后使用Spark的文本文件API写出去。</p><h2 id="SequenceFile-文件"><a href="#SequenceFile-文件" class="headerlink" title="SequenceFile 文件"></a>SequenceFile 文件</h2><p>SequenceFile文件是Hadoop用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。Spark有专门用来读取SequenceFile的接口。在SparkContext中，可以调用sequenceFile[keyClass, valueClass](path).</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/33-Type-In-Scala-Java-Hadoop.png?raw=true" alt="33-Type-In-Scala-Java-Hadoop"></p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/34-SequenceFile-Structures.png?raw=true" alt="34-SequenceFile-Structures"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> data=sc.parallelize(<span class="type">List</span>((<span class="number">2</span>,<span class="string">"aa"</span>),(<span class="number">3</span>,<span class="string">"bb"</span>),(<span class="number">4</span>,<span class="string">"cc"</span>),(<span class="number">5</span>,<span class="string">"dd"</span>),(<span class="number">6</span>,<span class="string">"ee"</span>)))</span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">16</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; data.saveAsSequenceFile(<span class="string">"hdfs://slave1:9000/sequdata"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> sdata = sc.sequenceFile[<span class="type">Int</span>,<span class="type">String</span>](<span class="string">"hdfs://slave1:9000/sdata/p*"</span>)</span><br><span class="line">sdata: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">19</span>] at sequenceFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; sdata.collect()</span><br><span class="line">res14: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">2</span>,aa), (<span class="number">3</span>,bb), (<span class="number">4</span>,cc), (<span class="number">5</span>,dd), (<span class="number">6</span>,ee))</span><br></pre></td></tr></table></figure><p>可以直接调用saveAsSequenceFile(path)保存你的PairRDD，它会帮你写出数据。需要键和值能够自动转为Writable类型。</p><h2 id="对象文件"><a href="#对象文件" class="headerlink" title="对象文件"></a>对象文件</h2><p>对象文件是将对象序列化后保存的文件，采用Java的序列化机制。可以通过objectFile<a href="path">k,v</a>函数接收一个路径，读取对象文件，返回对应的RDD，也可以通过调用saveAsObjectFile()实现对对象文件的输出。因为是序列化所以要指定类型。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> data=sc.parallelize(<span class="type">List</span>((<span class="number">2</span>,<span class="string">"aa"</span>),(<span class="number">3</span>,<span class="string">"bb"</span>),(<span class="number">4</span>,<span class="string">"cc"</span>),(<span class="number">5</span>,<span class="string">"dd"</span>),(<span class="number">6</span>,<span class="string">"ee"</span>)))</span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">20</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; data.saveAsObjectFile(<span class="string">"hdfs://slave1:9000/objfile"</span>)</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> objrdd:<span class="type">RDD</span>[(<span class="type">Int</span>,<span class="type">String</span>)] = sc.objectFile[(<span class="type">Int</span>,<span class="type">String</span>)](<span class="string">"hdfs://slave1:9000/objfile/p*"</span>)</span><br><span class="line">objrdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">28</span>] at objectFile at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; objrdd.collect()</span><br><span class="line">res20: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">2</span>,aa), (<span class="number">3</span>,bb), (<span class="number">4</span>,cc), (<span class="number">5</span>,dd), (<span class="number">6</span>,ee))</span><br></pre></td></tr></table></figure><h2 id="Hadoop-文件"><a href="#Hadoop-文件" class="headerlink" title="Hadoop 文件"></a>Hadoop 文件</h2><p>Spark的整个生态系统与Hadoop是完全兼容的,所以对于Hadoop所支持的文件类型或者数据库类型,Spark也同样支持.另外,由于Hadoop的API有新旧两个版本,所以Spark为了能够兼容Hadoop所有的版本,也提供了两套创建操作接口.对于外部存储创建操作而言,hadoopRDD和newHadoopRDD是最为抽象的两个函数接口,主要包含以下四个参数。</p><ol><li>输入格式(InputFormat): 制定数据输入的类型,如TextInputFormat等,新旧两个版本所引用的版本分别是org.apache.hadoop.mapred.InputFormat和org.apache.hadoop.mapreduce.InputFormat(NewInputFormat)</li><li>键类型: 指定[K,V]键值对中K的类型</li><li>值类型: 指定[K,V]键值对中V的类型</li><li>分区值: 指定由外部存储生成的RDD的partition数量的最小值,如果没有指定,系统会使用默认值defaultMinSplits</li></ol><p>其他创建操作的API接口都是为了方便最终的Spark程序开发者而设置的,是这两个接口的高效实现版本.例如,对于textFile而言,只有path这个指定文件路径的参数,其他参数在系统内部指定了默认值。</p><p>注意:</p><ol><li>在Hadoop中以压缩形式存储的数据,不需要指定解压方式就能够进行读取,因为Hadoop本身有一个解压器会根据压缩文件的后缀推断解压算法进行解压。</li><li>如果用Spark从Hadoop中读取某种类型的数据不知道怎么读取的时候,上网查找一个使用map-reduce的时候是怎么读取这种这种数据的,然后再将对应的读取方式改写成兼容版本的的hadoopRDD和newAPIHadoopRDD两个类就行了。</li></ol><p>读取示例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> data = sc.parallelize(<span class="type">Array</span>((<span class="number">30</span>,<span class="string">"hadoop"</span>), (<span class="number">71</span>,<span class="string">"hive"</span>), (<span class="number">11</span>,<span class="string">"cat"</span>)))</span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">47</span>] at parallelize at &lt;console&gt;:<span class="number">35</span></span><br><span class="line"></span><br><span class="line">scala&gt; data.saveAsNewAPIHadoopFile(<span class="string">"hdfs://slave1:9000/output4/"</span>,classOf[<span class="type">LongWritable</span>] ,classOf[<span class="type">Text</span>] ,classOf[org.apache.hadoop.mapreduce.lib.output.<span class="type">TextOutputFormat</span>[<span class="type">LongWritable</span>, <span class="type">Text</span>]])</span><br></pre></td></tr></table></figure><p>对于RDD最后的归宿除了返回为集合和标量，也可以将RDD存储到外部文件系统或者数据库中，Spark系统与Hadoop是完全兼容的，所以MapReduce所支持的读写文件或者数据库类型，Spark也同样支持。另外，由于Hadoop的API有新旧两个版本，所以Spark为了能够兼容Hadoop所有的版本，也提供了两套API。将RDD保存到HDFS中在通常情况下需要关注或者设置五个参数，即文件保存的路径，key值的class类型，Value值的class类型，RDD的输出格式(OutputFormat，如TextOutputFormat/SequenceFileOutputFormat)，以及最后一个相关的参数codec(这个参数表示压缩存储的压缩形式，如DefaultCodec，Gzip，Codec等等)。</p><table><thead><tr><th>兼容旧版API</th></tr></thead><tbody><tr><td>saveAsObjectFile(path:   String): Unit</td></tr><tr><td>saveAsTextFile(path:   String, codec: Class[_ &lt;: CompressionCodec]): Unit</td></tr><tr><td>saveAsTextFile(path:   String): Unit</td></tr><tr><td>saveAsHadoopFile[F   &lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unit</td></tr><tr><td>saveAsHadoopFile[F   &lt;: OutputFormat[K, V]](path: String, codec: Class[_ &lt;:   CompressionCodec])(implicit fm: ClassTag[F]): Unit</td></tr><tr><td>saveAsHadoopFile(path:   String, keyClass: Class[<em>], valueClass: Class[</em>], outputFormatClass: Class[<em>&lt;: OutputFormat[</em>, <em>]], codec: Class[</em> &lt;: CompressionCodec]): Unit</td></tr><tr><td>saveAsHadoopDataset(conf:   JobConf): Unit</td></tr></tbody></table><p>这里列出的API，前面6个都是saveAsHadoopDataset的简易实现版本，仅仅支持将RDD存储到HDFS中，而saveAsHadoopDataset的参数类型是JobConf，所以其不仅能够将RDD存储到HDFS中，也可以将RDD存储到其他数据库中，如Hbase，MangoDB，Cassandra等。</p><table><thead><tr><th>兼容新版API</th></tr></thead><tbody><tr><td>saveAsNewAPIHadoopFile(path:   String, keyClass: Class[<em>], valueClass: Class[</em>], outputFormatClass: Class[_   &lt;: OutputFormat[_, _]], conf: Configuration =   self.context.hadoopConfiguration): Unit</td></tr><tr><td>saveAsNewAPIHadoopFile[F   &lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unit</td></tr><tr><td>saveAsNewAPIHadoopDataset(conf:   Configuration): Unit</td></tr></tbody></table><p>同样的，前2个API是saveAsNewAPIHadoopDataset的简易实现，只能将RDD存到HDFS中，而saveAsNewAPIHadoopDataset比较灵活.新版的API没有codec的参数，所以要压缩存储文件到HDFS中每需要使用hadoopConfiguration参数，设置对应mapreduce.map.output.compress.codec参数和mapreduce.map.output.compress参数。<br>注意：如果不知道怎么将RDD存储到Hadoop生态的系统中，主要上网搜索一下对应的map-reduce是怎么将数据存储进去的，然后改写成对应的saveAsHadoopDataset或saveAsNewAPIHadoopDataset就可以了。</p><p>写入示例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> read =  sc.newAPIHadoopFile[<span class="type">LongWritable</span>, <span class="type">Text</span>, org.apache.hadoop.mapreduce.lib.input.<span class="type">TextInputFormat</span>](<span class="string">"hdfs://slave1:9000/output3/part*"</span>, classOf[org.apache.hadoop.mapreduce.lib.input.<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>])</span><br><span class="line">read: org.apache.spark.rdd.<span class="type">RDD</span>[(org.apache.hadoop.io.<span class="type">LongWritable</span>, org.apache.hadoop.io.<span class="type">Text</span>)] = hdfs:<span class="comment">//slave1:9000/output3/part* NewHadoopRDD[48] at newAPIHadoopFile at &lt;console&gt;:35</span></span><br><span class="line"></span><br><span class="line">scala&gt; read.map&#123;<span class="keyword">case</span> (k, v) =&gt; v.toString&#125;.collect</span><br><span class="line">res44: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">30</span> hadoop, <span class="number">71</span> hive, <span class="number">11</span> cat)</span><br></pre></td></tr></table></figure><h2 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h2><p>Spark支持读写很多种文件系统，像本地文件系统、Amazon S3、HDFS 等。</p><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><h3 id="关系型数据库连接"><a href="#关系型数据库连接" class="headerlink" title="关系型数据库连接"></a>关系型数据库连接</h3><p>支持通过Java JDBC访问关系型数据库。需要通过Jdbc RDD进行，示例如下:<br>MySQL 读取：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span> </span>(args: <span class="type">Array</span>[<span class="type">String</span>] ) &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span> ().setMaster (<span class="string">"local[2]"</span>).setAppName (<span class="string">"JdbcApp"</span>)</span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span> (sparkConf)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> rdd = <span class="keyword">new</span> org.apache.spark.rdd.<span class="type">JdbcRDD</span> (</span><br><span class="line">    sc,</span><br><span class="line">    () =&gt; &#123;</span><br><span class="line">      <span class="type">Class</span>.forName (<span class="string">"com.mysql.jdbc.Driver"</span>).newInstance()</span><br><span class="line">      java.sql.<span class="type">DriverManager</span>.getConnection (<span class="string">"jdbc:mysql://slave1:3306/rdd"</span>, <span class="string">"root"</span>, <span class="string">"hive"</span>)</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"select * from rddtable where id &gt;= ? and id &lt;= ?;"</span>,</span><br><span class="line">    <span class="number">1</span>,</span><br><span class="line">    <span class="number">10</span>,</span><br><span class="line">    <span class="number">1</span>,</span><br><span class="line">    r =&gt; (r.getInt(<span class="number">1</span>), r.getString(<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">  println (rdd.count () )</span><br><span class="line">  rdd.foreach (println (_) )</span><br><span class="line">  sc.stop ()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>MySQL 写入：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"HBaseApp"</span>)</span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">  <span class="keyword">val</span> data = sc.parallelize(<span class="type">List</span>(<span class="string">"Female"</span>, <span class="string">"Male"</span>,<span class="string">"Female"</span>))</span><br><span class="line"></span><br><span class="line">  data.foreachPartition(insertData)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertData</span></span>(iterator: <span class="type">Iterator</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="type">Class</span>.forName (<span class="string">"com.mysql.jdbc.Driver"</span>).newInstance()</span><br><span class="line">  <span class="keyword">val</span> conn = java.sql.<span class="type">DriverManager</span>.getConnection(<span class="string">"jdbc:mysql://slave1:3306/rdd"</span>, <span class="string">"root"</span>, <span class="string">"hive"</span>)</span><br><span class="line">  iterator.foreach(data =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> ps = conn.prepareStatement(<span class="string">"insert into rddtable(name) values (?)"</span>)</span><br><span class="line">    ps.setString(<span class="number">1</span>, data) </span><br><span class="line">    ps.executeUpdate()</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>JdbcRDD 接收这样几个参数：</p><ul><li>首先，要提供一个用于对数据库创建连接的函数。这个函数让每个节点在连接必要的配 置后创建自己读取数据的连接。 </li><li>接下来，要提供一个可以读取一定范围内数据的查询，以及查询参数中lowerBound和 upperBound 的值。这些参数可以让 Spark 在不同机器上查询不同范围的数据，这样就不 会因尝试在一个节点上读取所有数据而遭遇性能瓶颈。</li><li>这个函数的最后一个参数是一个可以将输出结果从转为对操作数据有用的格式的函数。如果这个参数空缺，Spark会自动将每行结果转为一个对象数组。 </li></ul><h3 id="HBase-数据库"><a href="#HBase-数据库" class="headerlink" title="HBase 数据库"></a>HBase 数据库</h3><p>由于 org.apache.hadoop.hbase.mapreduce.TableInputFormat 类的实现，Spark 可以通过Hadoop输入格式访问HBase。这个输入格式会返回键值对数据，其中键的类型为org.apache.hadoop.hbase.io.ImmutableBytesWritable，而值的类型为org.apache.hadoop.hbase.client.Result.<br>HBase 读取：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"HBaseApp"</span>)</span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">  <span class="comment">//HBase中的表名</span></span><br><span class="line">  conf.set(<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>, <span class="string">"fruit"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> hBaseRDD = sc.newAPIHadoopRDD(conf, classOf[<span class="type">TableInputFormat</span>],</span><br><span class="line">    classOf[org.apache.hadoop.hbase.io.<span class="type">ImmutableBytesWritable</span>],</span><br><span class="line">    classOf[org.apache.hadoop.hbase.client.<span class="type">Result</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> count = hBaseRDD.count()</span><br><span class="line">  println(<span class="string">"hBaseRDD RDD Count:"</span>+ count)</span><br><span class="line">  hBaseRDD.cache()</span><br><span class="line">  hBaseRDD.foreach &#123;</span><br><span class="line">    <span class="keyword">case</span> (_, result) =&gt;</span><br><span class="line">      <span class="keyword">val</span> key = <span class="type">Bytes</span>.toString(result.getRow)</span><br><span class="line">      <span class="keyword">val</span> name = <span class="type">Bytes</span>.toString(result.getValue(<span class="string">"info"</span>.getBytes, <span class="string">"name"</span>.getBytes))</span><br><span class="line">      <span class="keyword">val</span> color = <span class="type">Bytes</span>.toString(result.getValue(<span class="string">"info"</span>.getBytes, <span class="string">"color"</span>.getBytes))</span><br><span class="line">      println(<span class="string">"Row key:"</span> + key + <span class="string">" Name:"</span> + name + <span class="string">" Color:"</span> + color)</span><br><span class="line">  &#125;</span><br><span class="line">  sc.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>HBase 写入：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"HBaseApp"</span>)</span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">  <span class="keyword">val</span> jobConf = <span class="keyword">new</span> <span class="type">JobConf</span>(conf)</span><br><span class="line">  jobConf.setOutputFormat(classOf[<span class="type">TableOutputFormat</span>])</span><br><span class="line">  jobConf.set(<span class="type">TableOutputFormat</span>.<span class="type">OUTPUT_TABLE</span>, <span class="string">"fruit_spark"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fruitTable = <span class="type">TableName</span>.valueOf(<span class="string">"fruit_spark"</span>)</span><br><span class="line">  <span class="keyword">val</span> tableDescr = <span class="keyword">new</span> <span class="type">HTableDescriptor</span>(fruitTable)</span><br><span class="line">  tableDescr.addFamily(<span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(<span class="string">"info"</span>.getBytes))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> admin = <span class="keyword">new</span> <span class="type">HBaseAdmin</span>(conf)</span><br><span class="line">  <span class="keyword">if</span> (admin.tableExists(fruitTable)) &#123;</span><br><span class="line">    admin.disableTable(fruitTable)</span><br><span class="line">    admin.deleteTable(fruitTable)</span><br><span class="line">  &#125;</span><br><span class="line">  admin.createTable(tableDescr)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">convert</span></span>(triple: (<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>)) = &#123;</span><br><span class="line">    <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(<span class="type">Bytes</span>.toBytes(triple._1))</span><br><span class="line">    put.addImmutable(<span class="type">Bytes</span>.toBytes(<span class="string">"info"</span>), <span class="type">Bytes</span>.toBytes(<span class="string">"name"</span>), <span class="type">Bytes</span>.toBytes(triple._2))</span><br><span class="line">    put.addImmutable(<span class="type">Bytes</span>.toBytes(<span class="string">"info"</span>), <span class="type">Bytes</span>.toBytes(<span class="string">"price"</span>), <span class="type">Bytes</span>.toBytes(triple._3))</span><br><span class="line">    (<span class="keyword">new</span> <span class="type">ImmutableBytesWritable</span>, put)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> initialRDD = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="string">"apple"</span>,<span class="number">11</span>), (<span class="number">2</span>,<span class="string">"banana"</span>,<span class="number">12</span>), (<span class="number">3</span>,<span class="string">"pear"</span>,<span class="number">13</span>)))</span><br><span class="line">  <span class="keyword">val</span> localData = initialRDD.map(convert)</span><br><span class="line"></span><br><span class="line">  localData.saveAsHadoopDataset(jobConf)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Cassandra-和-ElasticSearch"><a href="#Cassandra-和-ElasticSearch" class="headerlink" title="Cassandra 和 ElasticSearch"></a>Cassandra 和 ElasticSearch</h3><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/35-Cassandra-And-ElasticSearch.png?raw=true" alt="35-Cassandra-And-ElasticSearch"></p><h1 id="RDD-编程进阶"><a href="#RDD-编程进阶" class="headerlink" title="RDD 编程进阶"></a>RDD 编程进阶</h1><h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><p>累加器用来对信息进行聚合，通常在向Spark传递函数时，比如使用map()函数或者用filter()传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，更新这些副本的值也不会影响驱动器中的对应变量。如果我们想实现所有分片处理时更新共享变量的功能，那么累加器可以实现我们想要的效果。<br>针对一个输入的日志文件，如果我们想计算文件中所有空行的数量，我们可以编写以下程序：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> notice = sc.textFile(<span class="string">"./NOTICE"</span>)</span><br><span class="line">notice: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = ./<span class="type">NOTICE</span> <span class="type">MapPartitionsRDD</span>[<span class="number">40</span>] at textFile at &lt;console&gt;:<span class="number">32</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> blanklines = sc.accumulator(<span class="number">0</span>)</span><br><span class="line">warning: there were two deprecation warnings; re-run <span class="keyword">with</span> -deprecation <span class="keyword">for</span> details</span><br><span class="line">blanklines: org.apache.spark.<span class="type">Accumulator</span>[<span class="type">Int</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> tmp = notice.flatMap(line =&gt; &#123;</span><br><span class="line">     |    <span class="keyword">if</span> (line == <span class="string">""</span>) &#123;</span><br><span class="line">     |       blanklines += <span class="number">1</span></span><br><span class="line">     |    &#125;</span><br><span class="line">     |    line.split(<span class="string">" "</span>)</span><br><span class="line">     | &#125;)</span><br><span class="line">tmp: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">41</span>] at flatMap at &lt;console&gt;:<span class="number">36</span></span><br><span class="line"></span><br><span class="line">scala&gt; tmp.count()</span><br><span class="line">res31: <span class="type">Long</span> = <span class="number">3213</span></span><br><span class="line"></span><br><span class="line">scala&gt; blanklines.value</span><br><span class="line">res32: <span class="type">Int</span> = <span class="number">171</span></span><br></pre></td></tr></table></figure><p>累加器的用法如下所示。<br>通过在驱动器中调用SparkContext.accumulator(initialValue)方法，创建出存有初始值的累加器。返回值为org.apache.spark.Accumulator[T]对象，其中T是初始值initialValue的类型。<br>Spark闭包里的执行器代码可以使用累加器的+=方法(在Java中是add)增加累加器的值。 <br>驱动器程序可以调用累加器的value属性(在Java中使用value()或setValue())来访问累加器的值。<br>注意：工作节点上的任务不能访问累加器的值。从这些任务的角度来看，累加器是一个只写变量。<br>对于要在行动操作中使用的累加器，Spark只会把每个任务对各累加器的修改应用一次。因此，如果想要一个无论在失败还是重复计算时都绝对可靠的累加器，我们必须把它放在foreach()这样的行动操作中。转化操作中累加器可能会发生不止一次更新。</p><h2 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h2><p>自定义累加器类型的功能在1.X版本中就已经提供了，但是使用起来比较麻烦，在2.0版本后，累加器的易用性有了较大的改进，而且官方还提供了一个新的抽象类：AccumulatorV2来提供更加友好的自定义类型累加器的实现方式。实现自定义类型累加器需要继承AccumulatorV2并至少覆写下例中出现的方法，下面这个累加器可以用于在程序运行过程中收集一些文本类信息，最终以Set[String]的形式返回。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.moqi.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.util.<span class="type">AccumulatorV2</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogAccumulator</span> <span class="keyword">extends</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">util</span>.<span class="title">AccumulatorV2</span>[<span class="type">String</span>, java.util.<span class="type">Set</span>[<span class="type">String</span>]] </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> _logArray: java.util.<span class="type">Set</span>[<span class="type">String</span>] = <span class="keyword">new</span> java.util.<span class="type">HashSet</span>[<span class="type">String</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    _logArray.isEmpty</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    _logArray.clear()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    _logArray.add(v)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: org.apache.spark.util.<span class="type">AccumulatorV2</span>[<span class="type">String</span>, java.util.<span class="type">Set</span>[<span class="type">String</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    other <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> o: <span class="type">LogAccumulator</span> =&gt; _logArray.addAll(o.value)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: java.util.<span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    java.util.<span class="type">Collections</span>.unmodifiableSet(_logArray)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>():org.apache.spark.util.<span class="type">AccumulatorV2</span>[<span class="type">String</span>, java.util.<span class="type">Set</span>[<span class="type">String</span>]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> newAcc = <span class="keyword">new</span> <span class="type">LogAccumulator</span>()</span><br><span class="line">    _logArray.synchronized&#123;</span><br><span class="line">      newAcc._logArray.addAll(_logArray)</span><br><span class="line">    &#125;</span><br><span class="line">    newAcc</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 过滤掉带字母的</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LogAccumulator</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"LogAccumulator"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> accum = <span class="keyword">new</span> <span class="type">LogAccumulator</span></span><br><span class="line">    sc.register(accum, <span class="string">"logAccum"</span>)</span><br><span class="line">    <span class="keyword">val</span> sum = sc.parallelize(<span class="type">Array</span>(<span class="string">"1"</span>, <span class="string">"2a"</span>, <span class="string">"3"</span>, <span class="string">"4b"</span>, <span class="string">"5"</span>, <span class="string">"6"</span>, <span class="string">"7cd"</span>, <span class="string">"8"</span>, <span class="string">"9"</span>), <span class="number">2</span>).filter(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> pattern = <span class="string">""</span><span class="string">"^-?(\d+)"</span><span class="string">""</span></span><br><span class="line">      <span class="keyword">val</span> flag = line.matches(pattern)</span><br><span class="line">      <span class="keyword">if</span> (!flag) &#123;</span><br><span class="line">        accum.add(line)</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;).map(_.toInt).reduce(_ + _)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"sum: "</span> + sum)</span><br><span class="line">    <span class="keyword">for</span> (v &lt;- accum.value) print(v + <span class="string">""</span>)</span><br><span class="line">    println()</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，甚至是机器学习算法中的一个很大的特征向量，广播变量用起来都很顺手。<br>传统方式下，Spark会自动把闭包中所有引用到的变量发送到工作节点上。虽然这很方便，但也很低效。原因有二:首先，默认的任务发射机制是专门为小任务进行优化的；其次，事实上你可能会在多个并行操作中使用同一个变量，但是Spark会为每个任务分别发送。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> broadcastVar = sc.broadcast(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">broadcastVar: org.apache.spark.broadcast.<span class="type">Broadcast</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Broadcast</span>(<span class="number">35</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; broadcastVar.value</span><br><span class="line">res33: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>使用广播变量的过程如下：</p><ol><li>通过对一个类型 T 的对象调用 SparkContext.broadcast 创建出一个 Broadcast[T] 对象。 任何可序列化的类型都可以这么实现。 </li><li>通过 value 属性访问该对象的值(在 Java 中为 value() 方法)。 </li><li>变量只会被发到各个节点一次，应作为只读值处理(修改这个值不会影响到别的节点)。</li></ol><p>End. </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要解析部分 Spark Core 的技术点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="http://moqimoqidea.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>JVM GC 发展历程</title>
    <link href="http://moqimoqidea.github.io/2017/06/16/JVM-GC-Development-Path/"/>
    <id>http://moqimoqidea.github.io/2017/06/16/JVM-GC-Development-Path/</id>
    <published>2017-06-16T08:29:47.000Z</published>
    <updated>2018-11-23T05:59:40.630Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍 JVM GC 的发展。</p><a id="more"></a> <h1 id="较早的-From-To-阶段"><a href="#较早的-From-To-阶段" class="headerlink" title="较早的 From - To 阶段"></a>较早的 From - To 阶段</h1><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/The-Oldest-GC.png?raw=true" alt="The-Oldest-GC"></p><ul><li>最早的垃圾收集 From - To 架构的 GC，把整个堆内存分成大小差不多相等的两部分，中间有一个分配的指针（Free Point），对指针设定目标值（比如 From 区域的 80%）时，触发一次 GC。GC 触发时应用进入 Stop-The-World 状态，这时垃圾回收器检查 From 区域有哪些是可以回收的那些不是，将不可以回收的拷贝到 To 区域，其他回收。一次 GC 操作完成的时候完成区域交换（From 转换为 To 区域，To 转换为 From 区域），然后指针分配内存开始从新的 From 区域开始。</li><li>这种纯粹的拷贝垃圾回收方法最大的问题在于堆内存里面永远只可以用一半的内存，所以有一半的堆是浪费的。但在当时而言还是比较领先的，比如相对于引用计数的垃圾回收方法。引用计数垃圾回收的问题在于：引用计数在计数的时候需要维持一个锁的消耗，会降低分配内存的速度；另外一个是在循环引用中，这个消耗会更大。</li></ul><h1 id="回收分代的思想"><a href="#回收分代的思想" class="headerlink" title="回收分代的思想"></a>回收分代的思想</h1><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/object-age-based-on-GC-generation-generational-hypothesis.png?raw=true" alt="object-age-based-on-GC-generation-generational-hypothesis"></p><ul><li>将堆内存分代治理建立于这样一个假设之上：<a href="https://plumbr.io/handbook/garbage-collection-in-java/generational-hypothesis" target="_blank" rel="noopener">代际假设</a> , 核心论点有两个：<ul><li>大多数对象很快就会被闲置；</li><li>少部分活下来的对象会存在相当长一段时间。</li></ul></li></ul><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/Hotspot-Heap-Structure.PNG?raw=true" alt="Hotspot-Heap-Structure"></p><ul><li>因此，JVM GC 分代治理的核心基础是以下两个：（1）大多数对象都会在年轻代死亡。（2）年老代引用年轻代的对象只占很小的一部分。分代治理会发生不同区域的 GC。在年轻代发生的 GC 称为 “minor GC”，在年老代出现的 GC 称为 “major GC” 或者 “full GC”。</li><li>根据代际假设构建的堆内存首先避免了全盘扫描，这个时期的 JVM GC 发展为如上图所示结构，分为年轻代，年老代，永生代。年轻代分为 eden 区与两个 survivor 区，s0 和 s1 实现是最初的 From - To 架构，在这里我们假设 s0 为 From 区，s1 为 To 区。创建的对象首先进入 eden 区，如果发生 minor GC，eden 区中大部分对象被回收，小部分对象拷贝到 To 区，From 也拷贝到 To 区；如果在 eden 中的对象太大不能拷贝到 To 区，则会被直接移动到年老代。每次 minor GC 时 From 和 To 会交换，每交换一次区内的对象年龄会加一，当年龄到达一定值（比如15）（注：这一块在后面的 GC 实现了动态调整）的时候，这些大龄的对象也被移动到了年老代。</li></ul><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/young-to-old-generation.png?raw=true" alt="yount-to-old-generation"></p><ul><li>但是这里会发生一个问题：<strong>如果年老代的对象需要引用年轻代的对象怎么办？</strong>为了处理这些情况，年老代中有一种称为”CardTable” (卡表) 的东西，它是一个 512 字节的块。每当年老代中的对象引用年轻代中的对象时，它就会记录在此表中。当发生 minor GC 时，仅搜索该卡表以确定它是否是年轻代 GC 需要回收的对象，而不是检查旧代中的所有对象的引用。</li></ul><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/card-table-structure.png?raw=true" alt="card-table-structure"></p><ul><li>当数据已满时，触发年老代执行 GC 。执行程序因 GC 类型而异，根据JDK 7，有5种GC类型。<ul><li>Serial GC</li><li>Parallel GC</li><li>Parallel Old GC (Parallel Compacting GC)</li><li>Concurrent Mark &amp; Sweep GC  (or “CMS”)</li><li>Garbage First (G1) GC<h2 id="Serial-GC"><a href="#Serial-GC" class="headerlink" title="Serial GC"></a>Serial GC</h2>(-XX:+UseSerialGC)，即串行 GC，使用被称为 “mark-sweep-compant” 的算法。</li><li>第一步：标记年老代中幸存的对象。（标记 - mark）</li><li>第二步：从堆的最前面开始检查，只留下幸存的堆。（扫描 - sweep）</li><li>第三步：把对象从最前面开始填充，以便连续堆积对象，并将堆分为包含对象和不包含对象的两部分。（紧凑 - compant）</li><li>串行 GC 适用于小内存和少量 CPU 内核的 JVM。<h2 id="Parallel-GC"><a href="#Parallel-GC" class="headerlink" title="Parallel GC"></a>Parallel GC</h2>(-XX:+UseParallelGC)，即并行 GC，和串行 GC 最大的区别是用多个线程来处理 GC，因此更快，当有足够的内存和 CPU 资源时，此 GC 非常有用，它也被称为”吞吐量 GC“。</li></ul></li></ul><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/Difference-between-the-Serial-GC-and-Parallel-GC.png?raw=true" alt="Difference-between-the-Serial-GC-and-Parallel-GC"></p><h2 id="Parallel-Old-GC"><a href="#Parallel-Old-GC" class="headerlink" title="Parallel Old GC"></a>Parallel Old GC</h2><p>  (-XX:+UseParallelOldGC)，并行旧 GC，自 JDK 5 更新以来开始支持，与并行 GC 相比，唯一的区别是老年代的 GC 算法。它经历了三个步骤：mark – summary – compaction（标记 - 摘要 - 压缩）。”摘要“步骤经历了一些更复杂的步骤。</p><h2 id="CMS-GC"><a href="#CMS-GC" class="headerlink" title="CMS GC"></a>CMS GC</h2><p>  （-XX：+ UseConcMarkSweepGC），从下图中可以看出，CMS GC 相对于前三个复杂得多。第一步 “Initial Mark” (初始标记) 很简单，搜索最接近类加载器的对象中的幸存对象，因此暂停时间很短。在 “Concurrent Mark” (并发标记) 步骤中，跟踪并检查刚刚确认的幸存对象引用的对象。这一步的不同之处在于它在同时处理其他线程的同时继续进行。在 “Remark” (再次标记) 步骤中，将检查在并发标记步骤中新增加或停止引用的对象。最后，在 “Concurrent Sweep” (并发扫描) 步骤中进行垃圾回收，垃圾回收和其他线程同步进行。由于 CMS GC 独特的运行方式，因此 GC 的暂停时间非常短。CMS GC 也称为低延迟 GC。<strong>在应用程序的响应时间至关重要时使用。</strong>这种 GC 的主要缺点如下：（1）它比其他 GC 类型使用更多的内存和 CPU。（2）默认情况下不提供压缩步骤。因而如果由于许多内存碎片而需要执行压缩任务，那么 GC 需要的静止时间可能会比其他任何 GC 方式都要长。所以，如果在使用 CMS GC 的时候要尤其注意压缩任务执行的频率和持续时间。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/Serial-GC-and-CMS-GC.png?raw=true" alt="Serial-GC-and-CMS-GC"></p><h1 id="G1-GC"><a href="#G1-GC" class="headerlink" title="G1 GC"></a>G1 GC</h1><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/G1-GC.png?raw=true" alt="G1-GC"></p><ul><li>G1 是 Garbage First 的缩写，最开始的 G1 完全抛弃分代收集的思想，开始于 JDK 1.7 Update 4，可以视为上图中所有的 “E”, “O”, “S” 标志消失，只分成不同的 Region (块)。实际上现在的 G1 GC 也有了分代的逻辑。G1 的实现一直以来都在不断的变化之中。</li><li>由前文所知每种 GC 都有出现的历史背景，比如串行 GC 是在内存和 CPU 比较小的情况下出现的；并行 GC 是在吞吐量出现巨大需求的时候出现的；而 CMS GC 则是对低延迟有了更高的需求，尽量拖延 full GC 出现的时间。那么，出现 G1这种形式的垃圾收集器的原因是什么？在 JVM GC 不断发展的过程中，已经出现了自适应的堆，也就是不需要手动调节年轻代与年老代的比例，以及年轻代对象进入年老代的年龄。这些自适应堆对更灵活的堆块产生了强烈的需求：如果将堆空间划分为很多个 Region，G1 可以将某一块指定为各种不同的代（可以是年老代，Eden 或者 Survivor 区），而且各个块在空间上不需要连续的在一起，有一个 List 将它们组织在一起，这样 G1 就很容易调整各个代之间的比例。</li></ul><h2 id="为什么-G1-被称为-G1？"><a href="#为什么-G1-被称为-G1？" class="headerlink" title="为什么 G1 被称为 G1？"></a>为什么 G1 被称为 G1？</h2><ul><li><p>G1 会在内部维护一个优先列表，通过一个合理的模型，计算出每个 Region 的收集成本和收益期望并量化，这样每次进行 GC 时，G1 总是会选择最适合的 Region（通常垃圾比较多）进行回收，使 GC 时间满足设置的条件。</p></li><li><p>G1通过引入 Remembered Set 来避免全堆扫描（前面所说的 CardTable 是其的一种实现）。Remembered Set 用于跟踪对象引用。G1 中每个 Region 都有对应的 Remembered Set 。当 JVM 发现内部的一个引用关系需要更新（对 Reference 类型进行写操作），则立即产生一个 Write Barrier 中断这个写操作，并检查Reference 引用的对象是否处于不同的 Region 之间（用分代的思想，就是新生代和老年代之间的引用）。如果是，则通过 CardTable 通知 G1，G1 根据 CardTable 把相关引用信息记录到被引用对象所属的 Region 的Remembered Set 中，并将 Remembered Set 加入 GC Root 。这样，在 G1 进行根节点枚举时就可以扫描到该对象而不会出现遗漏。</p></li><li><p>通俗解释第二条：如果一个 Region 的 Reference 越少，JVM 倾向于认为这块 Region 里面活着的对象越少，这个 Region 块是可回收的垃圾块的百分比就越大，这样回收这个 Region 的收益就越大。所以称这种算法为 Garbage First.</p></li></ul><h2 id="G1-GC-年轻代的回收"><a href="#G1-GC-年轻代的回收" class="headerlink" title="G1 GC 年轻代的回收"></a>G1 GC 年轻代的回收</h2><ol><li>当 JVM 启动时基于启动参数，JVM 要求操作系统分配一个大的连续内存块来托管 JVM 的堆，被划分为 2048 个 Region (块)。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/how-young-gc-works-in-g1-1.png?raw=true" alt="how-young-gc-works-in-g1-1"></p><ol start="2"><li>年轻代的块发生 “minor GC”，将活着的对象拷贝到 survivor 块（依然是 From - To 算法）。如果对象过大或者对象的年龄足够，会拷贝到年老代。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/how-young-gc-works-in-g1-2.png?raw=true" alt="how-young-gc-works-in-g1-2"></p><ol start="3"><li>结果：图三有新增的 “Recently Copied” 两块。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/how-young-gc-works-in-g1-3.png?raw=true" alt="how-young-gc-works-in-g1-3"></p><h2 id="G1-GC-年老代的回收"><a href="#G1-GC-年老代的回收" class="headerlink" title="G1 GC 年老代的回收"></a>G1 GC 年老代的回收</h2><ol><li>初始标记阶段：初始标记的活着的对象在年轻代的垃圾收集上。在日志中，被标记为 GC pause (young) (inital-mark).</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/Initial-Marking-Phase.PNG?raw=true" alt="Initial-Marking-Phase"></p><ol start="2"><li>并行标记阶段，如果找到空区域（由”X”表示），则在 Remark 阶段立即将它们移除。此外，计算确定活跃度的信息。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/Concurrent-Marking-Phase.PNG?raw=true" alt="Concurrent-Marking-Phase"></p><ol start="3"><li>Remark 阶段，空区域被移除并回收，现在计算所有区域的区域活跃度。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/Remark-Phase.PNG?raw=true" alt="Remark-Phase"></p><ol start="4"><li>复制清理阶段，G1 选择具有最低”活跃度“的区域（比如引用其他 Region 最少的区域），那些可以最快收集的区域。这些区域与年轻代 GC 同时收集。这在日志中表示为 [GC pause (mixed)]。G1 实际上将 Stop-The-World 的操作放在一个时间区间，这样对应用性能和稳定性较好。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/Copying:Cleanup-Phase.PNG?raw=true" alt="Copying:Cleanup-Phase"></p><ol start="5"><li>复制清理阶段后，选择的区域已经被收集并压缩成图中所示的深蓝色区域和深绿色区域。</li></ol><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/After-Copying:Cleanup-Phase.PNG?raw=true" alt="After-Copying:Cleanup-Phase"></p><h2 id="JDK-8-中-JVM-的调整"><a href="#JDK-8-中-JVM-的调整" class="headerlink" title="JDK 8 中 JVM 的调整"></a>JDK 8 中 JVM 的调整</h2><p>JDK 8 的 JVM 去掉了永生代(PermGen)，用 Metaspace 来代替。Metaspace 使用系统的内存。</p><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/JVM-GC-Development-Path/Metaspace-And-PermGen.png?raw=true" alt="Metaspace-And-PermGen"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><blockquote class="blockquote-center"><a href="https://www.cubrid.org/blog/understanding-java-garbage-collection" target="_blank" rel="noopener">https://www.cubrid.org/blog/understanding-java-garbage-collection</a></blockquote> <blockquote class="blockquote-center"><a href="https://www.oracle.com/technetwork/tutorials/tutorials-1876574.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/tutorials/tutorials-1876574.html</a></blockquote> <blockquote class="blockquote-center"><a href="https://blog.idrsolutions.com/2017/05/g1gc-java-9-garbage-collector-explained-5-minutes/" target="_blank" rel="noopener">https://blog.idrsolutions.com/2017/05/g1gc-java-9-garbage-collector-explained-5-minutes/</a></blockquote> <blockquote class="blockquote-center"><a href="https://www.sczyh30.com/posts/Java/jvm-gc-hotspot-implements/" target="_blank" rel="noopener">https://www.sczyh30.com/posts/Java/jvm-gc-hotspot-implements/</a></blockquote><blockquote class="blockquote-center"><a href="https://software.intel.com/en-us/blogs/2014/06/18/part-1-tuning-java-garbage-collection-for-hbase" target="_blank" rel="noopener">https://software.intel.com/en-us/blogs/2014/06/18/part-1-tuning-java-garbage-collection-for-hbase</a></blockquote><blockquote class="blockquote-center"><a href="https://blog.csdn.net/elinespace/article/details/78852469" target="_blank" rel="noopener">https://blog.csdn.net/elinespace/article/details/78852469</a></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍 JVM GC 的发展。&lt;/p&gt;
    
    </summary>
    
    
      <category term="JVM" scheme="http://moqimoqidea.github.io/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>Linux 日志相关命令整理</title>
    <link href="http://moqimoqidea.github.io/2017/06/03/Linux-Check-Log-Commands/"/>
    <id>http://moqimoqidea.github.io/2017/06/03/Linux-Check-Log-Commands/</id>
    <published>2017-06-03T14:34:05.000Z</published>
    <updated>2018-11-23T01:30:07.208Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍开发中一些日志的常用操作。</p><a id="more"></a> <h2 id="tail"><a href="#tail" class="headerlink" title="tail"></a>tail</h2><p>n  是显示行号；相当于 nl 命令；例子如下：</p><ul><li>tail -100f test.log      实时监控 100 行日志</li><li>tail  -n  10  test.log   查询日志尾部最后 10 行的日志;</li><li>tail -n +10 test.log    查询 10 行之后的所有日志;</li></ul><h2 id="head"><a href="#head" class="headerlink" title="head"></a>head</h2><p>跟 tail 是相反的，tail 是看后多少行日志；例子如下：</p><ul><li>head -n 10  test.log   查询日志文件中的头 10 行日志;</li><li>head -n -10  test.log   查询日志文件除了最后 10 行的其他所有日志;</li></ul><h2 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h2><p>tac 是倒序查看，是 cat 单词反写；例子如下：</p><ul><li>cat -n test.log |grep “debug”   查询关键字的日志</li></ul><h2 id="vim"><a href="#vim" class="headerlink" title="vim"></a>vim</h2><p>进入编辑查找：vi(vim) </p><ul><li>进入vim编辑模式：<ul><li>vim  filename</li><li>vim +n filename  进入特定行号日志</li></ul></li><li>输入命令“set nu” 显示行号</li><li>输入“/关键字”,按enter键查找</li><li>查找下一个，按“n”即可</li><li>退出：按ESC键后，接着再输入:号时，vi会在屏幕的最下方等待我们输入命令<ul><li>wq! 保存退出；</li><li>q! 不保存退出；</li></ul></li><li>切换方向</li><li>/关键字  　　注：正向查找，按n键把光标移动到下一个符合条件的地方</li><li>?关键字  　　注：反向查找，按shift+n 键，把光标移动到下一个符合条件的</li></ul><h2 id="搜索关键字附近的日志"><a href="#搜索关键字附近的日志" class="headerlink" title="搜索关键字附近的日志"></a><strong>搜索关键字附近的日志</strong></h2><ul><li>最常用的：cat -n filename |grep “关键字”</li><li>其他情况：<ul><li>cat filename | grep -C 5 ‘关键字’ 　　(显示日志里匹配字串那行以及前后5行)</li><li>cat filename | grep -B 5 ‘关键字’ 　　(显示匹配字串及前5行)</li><li>cat filename | grep -A 5 ‘关键字’ 　　(显示匹配字串及后5行)</li></ul></li></ul><h2 id="按行号查看-过滤出关键字附近的日志"><a href="#按行号查看-过滤出关键字附近的日志" class="headerlink" title="按行号查看 - 过滤出关键字附近的日志"></a>按行号查看 - 过滤出关键字附近的日志</h2><ol><li>cat -n test.log |grep “debug”  得到关键日志的行号</li><li>cat -n test.log |tail -n +92|head -n 20  选择关键字所在的中间一行. 然后查看这个关键字后 20 行的日志:<ul><li>tail -n +92 表示查询第 92 行之后的日志</li><li>head -n 20 则表示在前面的查询结果里再查后 20 条记录</li></ul></li></ol><h2 id="根据日期查询日志"><a href="#根据日期查询日志" class="headerlink" title="根据日期查询日志"></a>根据日期查询日志</h2><p>sed -n ‘/2014-12-17 16:17:20/,/2014-12-17 16:17:36/p’  test.log</p><ul><li>特别说明:上面的两个日期必须是日志中打印出来的日志，否则无效；</li><li>先 grep ‘2014-12-17 16:17:20’ test.log 来确定日志中是否有该 时间点</li></ul><h2 id="日志内容特别多，打印在屏幕上不方便查看"><a href="#日志内容特别多，打印在屏幕上不方便查看" class="headerlink" title="日志内容特别多，打印在屏幕上不方便查看"></a>日志内容特别多，打印在屏幕上不方便查看</h2><ul><li>使用 more 和 less 命令，如： cat -n test.log |grep “debug” |more     这样就分页打印了,通过点击空格键翻页</li><li>使用 &gt;xxx.txt 将其保存到文件中，到时可以拉下这个文件分析，如：cat -n test.log |grep “debug”  &gt; debug.txt</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote class="blockquote-center"><a href="https://blog.csdn.net/yangkai_hudong/article/details/47783487" target="_blank" rel="noopener">https://blog.csdn.net/yangkai_hudong/article/details/47783487</a></blockquote> <blockquote class="blockquote-center"><a href="https://www.cnblogs.com/hunt/p/7064886.html" target="_blank" rel="noopener">https://www.cnblogs.com/hunt/p/7064886.html</a></blockquote> <blockquote class="blockquote-center"><a href="https://blog.csdn.net/dingnning/article/details/7189862" target="_blank" rel="noopener">https://blog.csdn.net/dingnning/article/details/7189862</a></blockquote> ]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍开发中一些日志的常用操作。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="http://moqimoqidea.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>IDEA - Debugger 经验总结</title>
    <link href="http://moqimoqidea.github.io/2017/06/02/IDEA-Debugger/"/>
    <id>http://moqimoqidea.github.io/2017/06/02/IDEA-Debugger/</id>
    <published>2017-06-02T06:38:42.000Z</published>
    <updated>2018-11-22T06:54:24.902Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍 IDEA - Debugger 的一些操作。每个都有场景和操作说明，动态 Gif 图体积较大，请耐心等待。</p><p><strong>觉得图比较小的单击查看大图。</strong></p><a id="more"></a> <h2 id="分析外部堆栈跟踪"><a href="#分析外部堆栈跟踪" class="headerlink" title="分析外部堆栈跟踪"></a>分析外部堆栈跟踪</h2><p>把报错信息复制到 Analyze -&gt; Analyze Stacktrace，快速进入程序块。开发中经常可以看到生产环境有错误日志，依照此方法快速将日志导入项目，定位问题。</p><p>场景：</p><p><img src="https://raw.githubusercontent.com/onefansofworld/hexo_images/master/201806/Analysis_Static_Stacktrace.png" alt="scene01"></p><p>操作：</p><p><img src="https://raw.githubusercontent.com/onefansofworld/hexo_images/master/201806/Analyze_Stacktrace.gif" alt="operate01"></p><h2 id="返回到前一个堆栈帧"><a href="#返回到前一个堆栈帧" class="headerlink" title="返回到前一个堆栈帧"></a>返回到前一个堆栈帧</h2><p>IDEA 可在程序的执行流程中回退到先前的堆栈帧。要求不是最上面入口方法，选择 Drop Frame 后，等于未进入调用的方法。请注意：已经对全局状态进行的更改不会被恢复，只有本地变量会被重置。</p><p><img src="https://raw.githubusercontent.com/onefansofworld/hexo_images/master/201806/drop_frame.gif" alt="drop_frame"></p><h2 id="强制从当前方法返回"><a href="#强制从当前方法返回" class="headerlink" title="强制从当前方法返回"></a>强制从当前方法返回</h2><p>在当前堆栈帧中右键单击选择 Force Return 然后根据需要的返回类型输入即可。</p><p><img src="https://raw.githubusercontent.com/onefansofworld/hexo_images/master/201806/force_return.gif" alt="force_return"></p><h2 id="抛出一个异常"><a href="#抛出一个异常" class="headerlink" title="抛出一个异常"></a>抛出一个异常</h2><p>在当前堆栈帧中右键单击选择 Throw Exception 然后手动输入异常即可，比如 new NullPointerException();</p><p><img src="https://raw.githubusercontent.com/onefansofworld/hexo_images/master/201806/throw_expection.gif" alt="throw_exception"></p><h2 id="重新加载修改的类"><a href="#重新加载修改的类" class="headerlink" title="重新加载修改的类"></a>重新加载修改的类</h2><p>一般而言应用于在 Debugger 时发现未调用的方法有需要改动的地方，这时候修改未调用的方法，然后选择 Run -&gt; Reload Changed Classes, 快捷键 Alt + U, 然后 A. 这时候 Debugger 继续进行调用，则执行的调用方法逻辑为重新编译之后。底层逻辑是用到 JVM 的 hotSwap.</p><p><img src="https://raw.githubusercontent.com/onefansofworld/hexo_images/master/201806/reload_change_class.gif" alt="reload_change_class"></p><h2 id="分析-Java-Stream-操作"><a href="#分析-Java-Stream-操作" class="headerlink" title="分析 Java Stream 操作"></a>分析 Java Stream 操作</h2><p>IDEA Debugger 时可以可视化 Java Stream 进行的操作和对值数据的影响，需要断点停留在 Stream 上点击 Trace Current Stream Chain 按钮。</p><p><img src="https://raw.githubusercontent.com/onefansofworld/hexo_images/master/201806/stream_trace.gif" alt="stream_trace"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote class="blockquote-center"><a href="https://www.jetbrains.com/help/idea/analyzing-external-stacktraces.html" target="_blank" rel="noopener">https://www.jetbrains.com/help/idea/analyzing-external-stacktraces.html</a></blockquote> <blockquote class="blockquote-center"><a href="https://www.jetbrains.com/help/idea/altering-the-program-s-execution-flow.html" target="_blank" rel="noopener">https://www.jetbrains.com/help/idea/altering-the-program-s-execution-flow.html</a></blockquote> <blockquote class="blockquote-center"><a href="https://www.jetbrains.com/help/idea/analyze-java-stream-operations.html" target="_blank" rel="noopener">https://www.jetbrains.com/help/idea/analyze-java-stream-operations.html</a></blockquote> ]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍 IDEA - Debugger 的一些操作。每个都有场景和操作说明，动态 Gif 图体积较大，请耐心等待。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;觉得图比较小的单击查看大图。&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="IDEA" scheme="http://moqimoqidea.github.io/tags/IDEA/"/>
    
  </entry>
  
  <entry>
    <title>Start</title>
    <link href="http://moqimoqidea.github.io/2017/06/01/Start/"/>
    <id>http://moqimoqidea.github.io/2017/06/01/Start/</id>
    <published>2017-06-01T14:02:59.000Z</published>
    <updated>2017-06-02T15:52:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文简要讨论了博客起源。</p><a id="more"></a> <ul><li>博客搭建于 20170601，儿童节做出的决定。</li><li>目前定位于分享技术和个人思考，狭义来讲技术在最近一段时间是开发工具 IntelliJ IDEA 的一些最佳实践，个人思考在最近一段时间是关于一些书的读后感。</li><li>目前对 Github + Hexo + NexT 刚开始熟悉，有建议欢迎联系邮箱。</li><li>Ernest Hemingway once wrote:”The world is a fine place，and worth fighting for.”I agree with the second part.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文简要讨论了博客起源。&lt;/p&gt;
    
    </summary>
    
    
      <category term="default" scheme="http://moqimoqidea.github.io/tags/default/"/>
    
  </entry>
  
</feed>
