<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="CgWpgpEd7fSixhMFnnAhPIXT3nCIz9VbArCcXf9ZC0c" />














  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Spark," />





  <link rel="alternate" href="/atom.xml" title="Coding and Talking" type="application/atom+xml" />






<meta name="description" content="本文主要解析部分 Spark Core 的技术点。">
<meta name="keywords" content="Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark Core 应用解析">
<meta property="og:url" content="http://moqimoqidea.github.io/2017/08/04/Spark-Core-Analysis/index.html">
<meta property="og:site_name" content="Coding and Talking">
<meta property="og:description" content="本文主要解析部分 Spark Core 的技术点。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/01-Iteration-In-MapReduce.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/02-Iteration-In-Spark.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/03-RDD-Attributes.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/04-Native-Data-Space-And-Spark-RDD.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/05-RDD-Partition.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/06-RDD-Read-Only.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/07-Variety-Of-Operators.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/08-Transformation-Operations-And-Actions-Operations.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/09-Dependencies-Between-RDD.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/10-DAG.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/11-RDD-Cache.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/12-Driver-And-Worker.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/13-MasterNode-And-WorkerNode.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/14-RDD-Statistical-Operation.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/15-RDD-In-IDEA.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/16-RDD-Persist.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/17-RDD-StorageLevel.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/18-RDD-StorageLevel-Description.png?raw=true">
<meta property="og:updated_time" content="2018-11-24T03:15:57.565Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark Core 应用解析">
<meta name="twitter:description" content="本文主要解析部分 Spark Core 的技术点。">
<meta name="twitter:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/01-Iteration-In-MapReduce.png?raw=true">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://moqimoqidea.github.io/2017/08/04/Spark-Core-Analysis/"/>





  <title>Spark Core 应用解析 | Coding and Talking</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-120200111-1', 'auto');
  ga('send', 'pageview');
</script>





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Coding and Talking</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://moqimoqidea.github.io/2017/08/04/Spark-Core-Analysis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="moqi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars3.githubusercontent.com/u/39821951?s=400&u=65c6d8145d7b591ca2051e7082fd842b56e62567&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Coding and Talking">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Spark Core 应用解析</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-04T13:29:46+08:00">
                2017-08-04
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/04/Spark-Core-Analysis/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/08/04/Spark-Core-Analysis/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  10,515
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  49
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文主要解析部分 Spark Core 的技术点。</p>
<a id="more"></a> 
<h1 id="RDD-概念"><a href="#RDD-概念" class="headerlink" title="RDD 概念"></a>RDD 概念</h1><h2 id="RDD-为什么会产生"><a href="#RDD-为什么会产生" class="headerlink" title="RDD 为什么会产生"></a>RDD 为什么会产生</h2><p>RDD是Spark的基石，是实现Spark数据处理的核心抽象。那么RDD为什么会产生呢？</p>
<p>Hadoop的MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后写入物理存储设备。数据更多面临的是一次性处理。</p>
<p>MR的这种方式对数据领域两种常见的操作不是很高效。第一种是迭代式的算法。比如机器学习中ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR这种模式不太合适，即使多MR串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。</p>
<p>MR中的迭代：</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/01-Iteration-In-MapReduce.png?raw=true" alt="01-Iteration-In-MapReduce"></p>
<p>Spark中的迭代：</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/02-Iteration-In-Spark.png?raw=true" alt="02-Iteration-In-Spark"></p>
<p>我们需要一个效率非常快，且能够支持迭代计算和有效数据共享的模型，Spark应运而生。RDD是基于工作集的工作模式，更多的是面向工作流。</p>
<p>但是无论是MR还是RDD都应该具有类似位置感知、容错和负载均衡等特性。</p>
<h2 id="RDD-概述"><a href="#RDD-概述" class="headerlink" title="RDD 概述"></a>RDD 概述</h2><h3 id="什么是-RDD"><a href="#什么是-RDD" class="headerlink" title="什么是 RDD"></a>什么是 RDD</h3><p>RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。在 Spark 中，对数据的所有操作不外乎创建 RDD、转化已有RDD 以及调用 RDD 操作进行求值。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。RDD 可以包含 Python、Java、Scala 中任意类型的对象， 甚至可以包含用户自定义的对象。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。<br>RDD支持两种操作:转化操作和行动操作。RDD 的转化操作是返回一个新的 RDD的操作，比如 map()和 filter()，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作。比如 count() 和 first()。<br>Spark采用惰性计算模式，RDD只有第一次在一个行动操作中用到时，才会真正计算。Spark可以优化整个计算过程。默认情况下，Spark 的 RDD 会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个 RDD，可以使用 RDD.persist() 让 Spark 把这个 RDD 缓存下来。</p>
<h3 id="RDD-的属性"><a href="#RDD-的属性" class="headerlink" title="RDD 的属性"></a>RDD 的属性</h3><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/03-RDD-Attributes.png?raw=true" alt="03-RDD-Attributes"></p>
<ol>
<li>一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</li>
<li>一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</li>
<li>RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</li>
<li>一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</li>
<li>一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</li>
</ol>
<p>RDD是一个应用层面的逻辑概念。一个RDD多个分片。RDD就是一个元数据记录集，记录了RDD内存所有的关系数据。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/04-Native-Data-Space-And-Spark-RDD.png?raw=true" alt="04-Native-Data-Space-And-Spark-RDD"></p>
<h2 id="RDD-弹性"><a href="#RDD-弹性" class="headerlink" title="RDD 弹性"></a>RDD 弹性</h2><ol>
<li>自动进行内存和磁盘数据存储的切换：Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换</li>
<li>基于血统的高效容错机制：在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。</li>
<li>Task如果失败会自动进行特定次数的重试：RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。</li>
<li>Stage如果失败会自动进行特定次数的重试：如果Job的某个Stage阶段计算失败，框架也会自动进行任务的重新计算，默认次数也是4次。</li>
<li>Checkpoint和Persist可主动或被动触发：RDD可以通过Persist持久化将RDD缓存到内存或者磁盘，当再次用到该RDD时直接读取就行。也可以将RDD进行检查点，检查点会将数据存储在HDFS中，该RDD的所有父RD都会被移除。</li>
<li>数据调度弹性：Spark把这个JOB执行模型抽象为通用的有向无环图DAG，可以将多Stage的任务串联或并行执行，调度引擎自动处理Stage的失败以及Task的失败。</li>
<li>数据分片的高度弹性：可以根据业务的特征，动态调整数据分片的个数，提升整体的应用执行效率。</li>
</ol>
<p>RDD全称叫做弹性分布式数据集(Resilient Distributed Datasets)一种分布式的内存抽象，表示一个只读的记录分区的集合，它只能通过其他RDD转换而创建，为此，RDD支持丰富的转换操作(如map, join, filter, groupBy等过这种转换操作，新的RDD则包含了如何从其他RDDs衍生所必需的信息，所以说RDDs之间是有依赖关系的。基于RDDs之间的依赖，RDDs会形成一个有向无环图DAG，G描述了整个流式计算的流程，实际执行的时候，RDD是通过血缘关系(Lineage)成的，即使出现数据分区丢失，也可以通过血缘关系重建分区，总结起来，基于RDD的流式计算任务可描述为：从稳定的物理存储(如分布式文件系统)中加载记录，记入由一组确定性操作构成的DAG，然后写回稳定存储。另外RDD还可以将数据集缓存到内存中，使得在多个操作之间可以重用数据集，基于这个特点可以很方便地构建应用(图计算、机器学习等)或者交互式数据分析应用。可以说Spark最初也就是实现RDD的一个分布式系统，后面通过不断发展壮大成为现在较为完善的大数据生态系单来讲，Spark-RDD的关系类似于Hadoop-MapReduce关系。</p>
<h2 id="RDD-特点"><a href="#RDD-特点" class="headerlink" title="RDD 特点"></a>RDD 特点</h2><p>RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。</p>
<h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><p>RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/05-RDD-Partition.png?raw=true" alt="05-RDD-Partition"></p>
<h3 id="只读"><a href="#只读" class="headerlink" title="只读"></a>只读</h3><p>如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/06-RDD-Read-Only.png?raw=true" alt="06-RDD-Read-Only"></p>
<p>由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/07-Variety-Of-Operators.png?raw=true" alt="07-Variety-Of-Operators"></p>
<p>RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/08-Transformation-Operations-And-Actions-Operations.png?raw=true" alt="08-Transformation-Operations-And-Actions-Operations"></p>
<h3 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h3><p>RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是一一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/09-Dependencies-Between-RDD.png?raw=true" alt="09-Dependencies-Between-RDD"></p>
<p>通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，如下图所示，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/10-DAG.png?raw=true" alt="10-DAG"></p>
<h3 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h3><p>如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/11-RDD-Cache.png?raw=true" alt="11-RDD-Cache"></p>
<h3 id="CheckPoint"><a href="#CheckPoint" class="headerlink" title="CheckPoint"></a>CheckPoint</h3><p>虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。</p>
<p>给定一个RDD我们至少可以知道如下几点信息：1、分区数以及分区方式；2、由父RDDs衍生而来的相关依赖信息；3、计算每个分区的数据，计算步骤为：1）如果被缓存，则从缓存中取的分区的数据；2）如果被checkpoint，则从checkpoint处恢复数据；3）根据血缘关系计算分区的数据。</p>
<h1 id="RDD-编程"><a href="#RDD-编程" class="headerlink" title="RDD 编程"></a>RDD 编程</h1><h2 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h2><p>在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。<br>要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/12-Driver-And-Worker.png?raw=true" alt="12-Driver-And-Worker"></p>
<p>那么这其中的 Dirver、SparkContext、Executor、Master、Worker分别是什么？</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/13-MasterNode-And-WorkerNode.png?raw=true" alt="13-MasterNode-And-WorkerNode"></p>
<h2 id="创建-RDD"><a href="#创建-RDD" class="headerlink" title="创建 RDD"></a>创建 RDD</h2><p>在Spark中创建RDD的创建方式大概可以分为三种：（1）、从集合中创建RDD；（2）、从外部存储创建RDD；（3）、从其他RDD创建。</p>
<ol>
<li>由一个已经存在的Scala集合创建，集合并行化。val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))；而从集合中创建RDD，Spark主要提供了两种函数：parallelize和makeRDD。我们可以先看看这两个函数的声明：</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[(<span class="type">T</span>, <span class="type">Seq</span>[<span class="type">String</span>])]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>
<p>我们可以从上面看出makeRDD有两种实现，而且第一个makeRDD函数接收的参数和parallelize完全一致。其实第一种makeRDD函数实现是依赖了parallelize函数的实现，来看看Spark中是怎么实现这个makeRDD函数的：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  parallelize(seq, numSlices)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们可以看出，这个makeRDD函数完全和parallelize函数一致。但是我们得看看第二种makeRDD函数函数实现了，它接收的参数类型是Seq[(T, Seq[String])]，Spark文档的说明是：<br>Distribute a local Scala collection to form an RDD, with one or more location preferences (hostnames of Spark nodes) for each object. Create a new partition for each collection item.<br>原来，这个函数还为数据提供了位置信息，来看看我们怎么使用：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> guigu1= sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">guigu1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> guigu2 = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">guigu2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">11</span>] at makeRDD at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> seq = <span class="type">List</span>((<span class="number">1</span>, <span class="type">List</span>(<span class="string">"slave01"</span>)),</span><br><span class="line">     | (<span class="number">2</span>, <span class="type">List</span>(<span class="string">"slave02"</span>)))</span><br><span class="line">seq: <span class="type">List</span>[(<span class="type">Int</span>, <span class="type">List</span>[<span class="type">String</span>])] = <span class="type">List</span>((<span class="number">1</span>,<span class="type">List</span>(slave01)),</span><br><span class="line"> (<span class="number">2</span>,<span class="type">List</span>(slave02)))</span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> guigu3 = sc.makeRDD(seq)</span><br><span class="line">guigu3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at makeRDD at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"> </span><br><span class="line">scala&gt; guigu3.preferredLocations(guigu3.partitions(<span class="number">1</span>))</span><br><span class="line">res26: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>(slave02)</span><br><span class="line"> </span><br><span class="line">scala&gt; guigu3.preferredLocations(guigu3.partitions(<span class="number">0</span>))</span><br><span class="line">res27: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>(slave01)</span><br><span class="line"> </span><br><span class="line">scala&gt; guigu1.preferredLocations(guigu1.partitions(<span class="number">0</span>))</span><br><span class="line">res28: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>()</span><br></pre></td></tr></table></figure>
<p>我们可以看到，makeRDD函数有两种实现，第一种实现其实完全和parallelize一致；而第二种实现可以为数据提供位置信息，而除此之外的实现和parallelize函数也是一致的，如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq, numSlices, <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">String</span>]]())</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[(<span class="type">T</span>, <span class="type">Seq</span>[<span class="type">String</span>])]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">val</span> indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq.map(_._1), seq.size, indexToPrefs)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>都是返回ParallelCollectionRDD，而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。</p>
<ol start="2">
<li>由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://slave1:9000/txtFile"</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = hdfs:<span class="comment">//slave1:9000/RELEASE MapPartitionsRDD[4] at textFile at &lt;console&gt;:24</span></span><br></pre></td></tr></table></figure>
<h2 id="RDD-编程-1"><a href="#RDD-编程-1" class="headerlink" title="RDD 编程"></a>RDD 编程</h2><p>RDD一般分为数值RDD和键值对RDD，本章不进行具体区分，先统一来看，下一章会对键值对RDD做专门说明。</p>
<h3 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h3><p>RDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。常用的 Transformation：</p>
<ul>
<li>map(func): 返回通过函数func传递源的每个元素形成的新分布式数据集。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> source  = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">source: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">8</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; source.collect()</span><br><span class="line">res7: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> mapadd = source.map(_ * <span class="number">2</span>)</span><br><span class="line">mapadd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">9</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; mapadd.collect()</span><br><span class="line">res8: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">18</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>filter(func): 返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> sourceFilter = sc.parallelize(<span class="type">Array</span>(<span class="string">"xiaoming"</span>,<span class="string">"xiaojiang"</span>,<span class="string">"xiaohe"</span>,<span class="string">"dazhi"</span>))</span><br><span class="line">sourceFilter: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> filter = sourceFilter.filter(_.contains(<span class="string">"xiao"</span>))</span><br><span class="line">filter: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">11</span>] at filter at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; sourceFilter.collect()</span><br><span class="line">res9: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(xiaoming, xiaojiang, xiaohe, dazhi)</span><br><span class="line"></span><br><span class="line">scala&gt; filter.collect()</span><br><span class="line">res10: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(xiaoming, xiaojiang, xiaohe)</span><br></pre></td></tr></table></figure>
<ul>
<li>flatMap(func): 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> sourceFlat = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">sourceFlat: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; sourceFlat.collect()</span><br><span class="line">res11: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> flatMap = sourceFlat.flatMap(<span class="number">1</span> to _)</span><br><span class="line">flatMap: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">13</span>] at flatMap at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; flatMap.collect()</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>mapPartitions(func): 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"kpop"</span>,<span class="string">"female"</span>),(<span class="string">"zorro"</span>,<span class="string">"male"</span>),(<span class="string">"mobin"</span>,<span class="string">"male"</span>),(<span class="string">"lucy"</span>,<span class="string">"female"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">16</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionsFun</span></span>(iter : <span class="type">Iterator</span>[(<span class="type">String</span>,<span class="type">String</span>)]) : <span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">  <span class="keyword">var</span> woman = <span class="type">List</span>[<span class="type">String</span>]()</span><br><span class="line">  <span class="keyword">while</span> (iter.hasNext)&#123;</span><br><span class="line">    <span class="keyword">val</span> next = iter.next()</span><br><span class="line">    next <span class="keyword">match</span> &#123;</span><br><span class="line">       <span class="keyword">case</span> (_,<span class="string">"female"</span>) =&gt; woman = next._1 :: woman</span><br><span class="line">       <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  woman.iterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line"></span><br><span class="line">partitionsFun: (iter: <span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">String</span>)])<span class="type">Iterator</span>[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> result = rdd.mapPartitions(partitionsFun)</span><br><span class="line">result: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">17</span>] at mapPartitions at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; result.collect()</span><br><span class="line">res13: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(kpop, lucy)</span><br></pre></td></tr></table></figure>
<ul>
<li>mapPartitionsWithIndex(func): 类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"kpop"</span>,<span class="string">"female"</span>),(<span class="string">"zorro"</span>,<span class="string">"male"</span>),(<span class="string">"mobin"</span>,<span class="string">"male"</span>),(<span class="string">"lucy"</span>,<span class="string">"female"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">18</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionsFun</span></span>(index : <span class="type">Int</span>, iter : <span class="type">Iterator</span>[(<span class="type">String</span>,<span class="type">String</span>)]) : <span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">  <span class="keyword">var</span> woman = <span class="type">List</span>[<span class="type">String</span>]()</span><br><span class="line">  <span class="keyword">while</span> (iter.hasNext)&#123;</span><br><span class="line">    <span class="keyword">val</span> next = iter.next()</span><br><span class="line">    next <span class="keyword">match</span> &#123;</span><br><span class="line">       <span class="keyword">case</span> (_,<span class="string">"female"</span>) =&gt; woman = <span class="string">"["</span>+index+<span class="string">"]"</span>+next._1 :: woman</span><br><span class="line">       <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  woman.iterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line"></span><br><span class="line">partitionsFun: (index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">String</span>)])<span class="type">Iterator</span>[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> result = rdd.mapPartitionsWithIndex(partitionsFun)</span><br><span class="line">result: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">19</span>] at mapPartitionsWithIndex at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; result.collect()</span><br><span class="line">res14: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>([<span class="number">0</span>]kpop, [<span class="number">3</span>]lucy)</span><br></pre></td></tr></table></figure>
<ul>
<li>sample(withReplacement, fraction, seed): 以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。例子从RDD中随机且有放回的抽出50%的数据，随机种子值为3（即可能以1 2 3的其中一个起始值）。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">20</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect()</span><br><span class="line">res15: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> sample1 = rdd.sample(<span class="literal">true</span>,<span class="number">0.4</span>,<span class="number">2</span>)</span><br><span class="line">sample1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">PartitionwiseSampledRDD</span>[<span class="number">21</span>] at sample at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; sample1.collect()</span><br><span class="line">res16: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> sample2 = rdd.sample(<span class="literal">false</span>,<span class="number">0.2</span>,<span class="number">3</span>)</span><br><span class="line">sample2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">PartitionwiseSampledRDD</span>[<span class="number">22</span>] at sample at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; sample2.collect()</span><br><span class="line">res17: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">9</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>union(otherDataset): 对源RDD和参数RDD求并集后返回一个新的RDD。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">23</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">5</span> to <span class="number">10</span>)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">24</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = rdd1.union(rdd2)</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">UnionRDD</span>[<span class="number">25</span>] at union at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.collect()</span><br><span class="line">res18: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>intersection(otherDataset): 对源RDD和参数RDD求交集后返回一个新的RDD。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">7</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">26</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">5</span> to <span class="number">10</span>)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">27</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = rdd1.intersection(rdd2)</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">33</span>] at intersection at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.collect()</span><br><span class="line">res19: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>distinct([numTasks])): 对源RDD进行去重后返回一个新的RDD. 默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> distinctRdd = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">9</span>,<span class="number">6</span>,<span class="number">1</span>))</span><br><span class="line">distinctRdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">34</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> unionRDD = distinctRdd.distinct()</span><br><span class="line">unionRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">37</span>] at distinct at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; unionRDD.collect()</span><br><span class="line">res20: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> unionRDD = distinctRdd.distinct(<span class="number">2</span>)</span><br><span class="line">unionRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">40</span>] at distinct at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; unionRDD.collect()</span><br><span class="line">res21: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">6</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>partitionBy(partitioner): 对RDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区， 否则会生成ShuffleRDD。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"aaa"</span>),(<span class="number">2</span>,<span class="string">"bbb"</span>),(<span class="number">3</span>,<span class="string">"ccc"</span>),(<span class="number">4</span>,<span class="string">"ddd"</span>)),<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">44</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res24: <span class="type">Int</span> = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> rdd2 = rdd.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">45</span>] at partitionBy at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.partitions.size</span><br><span class="line">res25: <span class="type">Int</span> = <span class="number">2</span></span><br></pre></td></tr></table></figure>
<ul>
<li>reduceByKey(func, [numTasks]): 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"female"</span>,<span class="number">1</span>),(<span class="string">"male"</span>,<span class="number">5</span>),(<span class="string">"female"</span>,<span class="number">5</span>),(<span class="string">"male"</span>,<span class="number">2</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">46</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> reduce = rdd.reduceByKey((x,y) =&gt; x+y)</span><br><span class="line">reduce: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">47</span>] at reduceByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; reduce.collect()</span><br><span class="line">res29: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((female,<span class="number">6</span>), (male,<span class="number">7</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>groupByKey(): groupByKey也是对每个key进行操作，但只生成一个sequence。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> words = <span class="type">Array</span>(<span class="string">"one"</span>, <span class="string">"two"</span>, <span class="string">"two"</span>, <span class="string">"three"</span>, <span class="string">"three"</span>, <span class="string">"three"</span>)</span><br><span class="line">words: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(one, two, two, three, three, three)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> wordPairsRDD = sc.parallelize(words).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">wordPairsRDD: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">4</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> group = wordPairsRDD.groupByKey()</span><br><span class="line">group: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">ShuffledRDD</span>[<span class="number">5</span>] at groupByKey at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; group.collect()</span><br><span class="line">res1: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">Array</span>((two,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">1</span>)), (one,<span class="type">CompactBuffer</span>(<span class="number">1</span>)), (three,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; group.map(t =&gt; (t._1, t._2.sum))</span><br><span class="line">res2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">6</span>] at map at &lt;console&gt;:<span class="number">31</span></span><br><span class="line"></span><br><span class="line">scala&gt; res2.collect()</span><br><span class="line">res3: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((two,<span class="number">2</span>), (one,<span class="number">1</span>), (three,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> map = group.map(t =&gt; (t._1, t._2.sum))</span><br><span class="line">map: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">7</span>] at map at &lt;console&gt;:<span class="number">30</span></span><br><span class="line"></span><br><span class="line">scala&gt; map.collect()</span><br><span class="line">res4: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((two,<span class="number">2</span>), (one,<span class="number">1</span>), (three,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>combineByKey[C](  createCombiner: V =&gt; C,  mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): 对相同K，把V合并成一个集合.<ul>
<li>createCombiner: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。如果这是一个新的元素,combineByKey() 会使用一个叫作 createCombiner() 的函数来创建 那个键对应的累加器的初始值。</li>
<li>mergeValue: 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并。</li>
<li>mergeCombiners: 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。</li>
</ul>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> scores = <span class="type">Array</span>((<span class="string">"Fred"</span>, <span class="number">88</span>), (<span class="string">"Fred"</span>, <span class="number">95</span>), (<span class="string">"Fred"</span>, <span class="number">91</span>), (<span class="string">"Wilma"</span>, <span class="number">93</span>), (<span class="string">"Wilma"</span>, <span class="number">95</span>), (<span class="string">"Wilma"</span>, <span class="number">98</span>))</span><br><span class="line">scores: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="type">Fred</span>,<span class="number">88</span>), (<span class="type">Fred</span>,<span class="number">95</span>), (<span class="type">Fred</span>,<span class="number">91</span>), (<span class="type">Wilma</span>,<span class="number">93</span>), (<span class="type">Wilma</span>,<span class="number">95</span>), (<span class="type">Wilma</span>,<span class="number">98</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> input = sc.parallelize(scores)</span><br><span class="line">input: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">52</span>] at parallelize at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> combine = input.combineByKey(</span><br><span class="line">     |     (v)=&gt;(v,<span class="number">1</span>),</span><br><span class="line">     |     (acc:(<span class="type">Int</span>,<span class="type">Int</span>),v)=&gt;(acc._1+v,acc._2+<span class="number">1</span>),</span><br><span class="line">     |     (acc1:(<span class="type">Int</span>,<span class="type">Int</span>),acc2:(<span class="type">Int</span>,<span class="type">Int</span>))=&gt;(acc1._1+acc2._1,acc1._2+acc2._2))</span><br><span class="line">combine: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = <span class="type">ShuffledRDD</span>[<span class="number">53</span>] at combineByKey at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> result = combine.map&#123;</span><br><span class="line">     |     <span class="keyword">case</span> (key,value) =&gt; (key,value._1/value._2.toDouble)&#125;</span><br><span class="line">result: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">54</span>] at map at &lt;console&gt;:<span class="number">30</span></span><br><span class="line"></span><br><span class="line">scala&gt; result.collect()</span><br><span class="line">res33: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = <span class="type">Array</span>((<span class="type">Wilma</span>,<span class="number">95.33333333333333</span>), (<span class="type">Fred</span>,<span class="number">91.33333333333333</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>aggregateByKey(zeroValue:U,[partitioner: Partitioner]) (seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U): 在kv对的RDD中，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。seqOp函数用于在每一个分区中用初始值逐步迭代value，combOp函数用于合并每个分区中的结果。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">3</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> agg = rdd.aggregateByKey(<span class="number">0</span>)(math.max(_,_),_+_)</span><br><span class="line">agg: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">13</span>] at aggregateByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; agg.collect()</span><br><span class="line">res7: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">3</span>,<span class="number">8</span>), (<span class="number">1</span>,<span class="number">7</span>), (<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; agg.partitions.size</span><br><span class="line">res8: <span class="type">Int</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">1</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> agg = rdd.aggregateByKey(<span class="number">0</span>)(math.max(_,_),_+_).collect()</span><br><span class="line">agg: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>), (<span class="number">3</span>,<span class="number">8</span>), (<span class="number">2</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]: aggregateByKey的简化操作，seqop和combop相同。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">3</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">91</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> agg = rdd.foldByKey(<span class="number">0</span>)(_+_)</span><br><span class="line">agg: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">92</span>] at foldByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; agg.collect()</span><br><span class="line">res61: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">3</span>,<span class="number">14</span>), (<span class="number">1</span>,<span class="number">9</span>), (<span class="number">2</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>sortByKey([ascending], [numTasks]): 在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">3</span>,<span class="string">"aa"</span>),(<span class="number">6</span>,<span class="string">"cc"</span>),(<span class="number">2</span>,<span class="string">"bb"</span>),(<span class="number">1</span>,<span class="string">"dd"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">14</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortByKey(<span class="literal">true</span>).collect()</span><br><span class="line">res9: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">1</span>,dd), (<span class="number">2</span>,bb), (<span class="number">3</span>,aa), (<span class="number">6</span>,cc))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortByKey(<span class="literal">false</span>).collect()</span><br><span class="line">res10: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">6</span>,cc), (<span class="number">3</span>,aa), (<span class="number">2</span>,bb), (<span class="number">1</span>,dd))</span><br></pre></td></tr></table></figure>
<ul>
<li>sortBy(func,[ascending], [numTasks]): 与sortByKey类似，但是更灵活,可以用func先对数据进行处理，按照处理后的数据比较结果排序。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">21</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortBy(x =&gt; x).collect()</span><br><span class="line">res11: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortBy(x =&gt; x%<span class="number">3</span>).collect()</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>join(otherDataset, [numTasks]): 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">32</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">33</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.join(rdd1).collect()</span><br><span class="line">res13: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">String</span>, <span class="type">Int</span>))] = <span class="type">Array</span>((<span class="number">1</span>,(a,<span class="number">4</span>)), (<span class="number">2</span>,(b,<span class="number">5</span>)), (<span class="number">3</span>,(c,<span class="number">6</span>)))</span><br></pre></td></tr></table></figure>
<ul>
<li>cogroup(otherDataset, [numTasks]): 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable\&lt;V>,Iterable\&lt;W>))类型的RDD。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">37</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">38</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.cogroup(rdd1).collect()</span><br><span class="line">res14: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = <span class="type">Array</span>((<span class="number">1</span>,(<span class="type">CompactBuffer</span>(a),<span class="type">CompactBuffer</span>(<span class="number">4</span>))), (<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="number">5</span>))), (<span class="number">3</span>,(<span class="type">CompactBuffer</span>(c),<span class="type">CompactBuffer</span>(<span class="number">6</span>))))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">Array</span>((<span class="number">4</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">41</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.cogroup(rdd2).collect()</span><br><span class="line">res15: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = <span class="type">Array</span>((<span class="number">4</span>,(<span class="type">CompactBuffer</span>(),<span class="type">CompactBuffer</span>(<span class="number">4</span>))), (<span class="number">1</span>,(<span class="type">CompactBuffer</span>(a),<span class="type">CompactBuffer</span>())), (<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="number">5</span>))), (<span class="number">3</span>,(<span class="type">CompactBuffer</span>(c),<span class="type">CompactBuffer</span>(<span class="number">6</span>))))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">1</span>,<span class="string">"d"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">44</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.cogroup(rdd2).collect()</span><br><span class="line">res16: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = <span class="type">Array</span>((<span class="number">4</span>,(<span class="type">CompactBuffer</span>(),<span class="type">CompactBuffer</span>(<span class="number">4</span>))), (<span class="number">1</span>,(<span class="type">CompactBuffer</span>(d, a),<span class="type">CompactBuffer</span>())), (<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="number">5</span>))), (<span class="number">3</span>,(<span class="type">CompactBuffer</span>(c),<span class="type">CompactBuffer</span>(<span class="number">6</span>))))</span><br></pre></td></tr></table></figure>
<ul>
<li>cartesian(otherDataset): 笛卡尔积。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">3</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">47</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">2</span> to <span class="number">5</span>)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">48</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.cartesian(rdd2).collect()</span><br><span class="line">res17: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="number">2</span>), (<span class="number">1</span>,<span class="number">3</span>), (<span class="number">1</span>,<span class="number">4</span>), (<span class="number">1</span>,<span class="number">5</span>), (<span class="number">2</span>,<span class="number">2</span>), (<span class="number">2</span>,<span class="number">3</span>), (<span class="number">2</span>,<span class="number">4</span>), (<span class="number">2</span>,<span class="number">5</span>), (<span class="number">3</span>,<span class="number">2</span>), (<span class="number">3</span>,<span class="number">3</span>), (<span class="number">3</span>,<span class="number">4</span>), (<span class="number">3</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>pipe(command, [envVars]): 对于每个分区，都执行一个perl或者shell脚本，返回输出的RDD。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="string">"hi"</span>,<span class="string">"Hello"</span>,<span class="string">"how"</span>,<span class="string">"are"</span>,<span class="string">"you"</span>),<span class="number">1</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">50</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.pipe(<span class="string">"/home/spark_shell/pipe.sh"</span>).collect()</span><br><span class="line">res18: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="type">AA</span>, &gt;&gt;&gt;hi, &gt;&gt;&gt;<span class="type">Hello</span>, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="string">"hi"</span>,<span class="string">"Hello"</span>,<span class="string">"how"</span>,<span class="string">"are"</span>,<span class="string">"you"</span>),<span class="number">2</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">52</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.pipe(<span class="string">"/home/spark_shell/pipe.sh"</span>).collect()</span><br><span class="line">res19: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="type">AA</span>, &gt;&gt;&gt;hi, &gt;&gt;&gt;<span class="type">Hello</span>, <span class="type">AA</span>, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pipe.sh:</span><br><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line">echo "AA"</span><br><span class="line">while read LINE; do</span><br><span class="line">   echo "&gt;&gt;&gt;"$&#123;LINE&#125;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<ul>
<li>coalesce(numPartitions): 缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">16</span>,<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">54</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res20: <span class="type">Int</span> = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> coalesceRDD = rdd.coalesce(<span class="number">3</span>)</span><br><span class="line">coalesceRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">CoalescedRDD</span>[<span class="number">55</span>] at coalesce at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; coalesceRDD.partitions.size</span><br><span class="line">res21: <span class="type">Int</span> = <span class="number">3</span></span><br></pre></td></tr></table></figure>
<ul>
<li>repartition(numPartitions): 根据分区数，从新通过网络随机洗牌所有数据。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">16</span>,<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">56</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res22: <span class="type">Int</span> = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rerdd = rdd.repartition(<span class="number">2</span>)</span><br><span class="line">rerdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">60</span>] at repartition at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rerdd.partitions.size</span><br><span class="line">res23: <span class="type">Int</span> = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rerdd = rdd.repartition(<span class="number">4</span>)</span><br><span class="line">rerdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">64</span>] at repartition at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rerdd.partitions.size</span><br><span class="line">res24: <span class="type">Int</span> = <span class="number">4</span></span><br></pre></td></tr></table></figure>
<ul>
<li>glom(): 将每一个分区形成一个数组，形成新的RDD类型时RDD[Array[T]]。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">16</span>,<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">65</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.glom().collect()</span><br><span class="line">res25: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="type">Array</span>(<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>), <span class="type">Array</span>(<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>), <span class="type">Array</span>(<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>mapValues(func):    针对于(K,V)形式的类型只对V进行操作。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">1</span>,<span class="string">"d"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">67</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.mapValues(_+<span class="string">"|||"</span>).collect()</span><br><span class="line">res26: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">1</span>,a|||), (<span class="number">1</span>,d|||), (<span class="number">2</span>,b|||), (<span class="number">3</span>,c|||))</span><br></pre></td></tr></table></figure>
<ul>
<li>subtract(other:RDD): 计算差的一种函数去除两个RDD中相同的元素，不同的RDD将保留下来 。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">3</span> to <span class="number">8</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">70</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">71</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.subtract(rdd1).collect()</span><br><span class="line">res27: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">8</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h3><p>常用的 Action 如下：</p>
<ul>
<li>reduce(func): 通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">85</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.reduce(_+_)</span><br><span class="line">res50: <span class="type">Int</span> = <span class="number">55</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="type">Array</span>((<span class="string">"a"</span>,<span class="number">1</span>),(<span class="string">"a"</span>,<span class="number">3</span>),(<span class="string">"c"</span>,<span class="number">3</span>),(<span class="string">"d"</span>,<span class="number">5</span>)))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">86</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.reduce((x,y)=&gt;(x._1 + y._1,x._2 + y._2))</span><br><span class="line">res51: (<span class="type">String</span>, <span class="type">Int</span>) = (adca,<span class="number">12</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>collect(): 在驱动程序中，以数组的形式返回数据集的所有元素。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">17</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect()</span><br><span class="line">res7: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>count(): 返回RDD的元素个数。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">18</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count()</span><br><span class="line">res8: <span class="type">Long</span> = <span class="number">10</span></span><br></pre></td></tr></table></figure>
<ul>
<li>first(): 返回RDD的第一个元素（类似于take(1)）。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">19</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.first()</span><br><span class="line">res9: <span class="type">Int</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<ul>
<li>take(n): 返回一个由数据集的前n个元素组成的数组。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">20</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.take(<span class="number">7</span>)</span><br><span class="line">res10: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>takeSample(withReplacement,num, [seed]): 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">21</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.takeSample(<span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">res11: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">6</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>takeOrdered(n): 返回前几个的排序。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Seq</span>(<span class="number">10</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">19</span>, <span class="number">4</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">23</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.top(<span class="number">2</span>)</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">19</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.takeOrdered(<span class="number">2</span>)</span><br><span class="line">res13: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>aggregate (zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U): aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">88</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(<span class="number">1</span>)(</span><br><span class="line">     | &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x + y&#125;,</span><br><span class="line">     | &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">     | )</span><br><span class="line">res56: <span class="type">Int</span> = <span class="number">58</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(<span class="number">1</span>)(</span><br><span class="line">     | &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x * y&#125;,</span><br><span class="line">     | &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">     | )</span><br><span class="line">res57: <span class="type">Int</span> = <span class="number">30361</span></span><br></pre></td></tr></table></figure>
<ul>
<li>fold(num)(func): 折叠操作，aggregate的简化操作，seqop和combop一样。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">90</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(<span class="number">1</span>)(</span><br><span class="line">     | &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x + y&#125;,</span><br><span class="line">     | &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">     | )</span><br><span class="line">res59: <span class="type">Int</span> = <span class="number">13</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.fold(<span class="number">1</span>)(_+_)</span><br><span class="line">res60: <span class="type">Int</span> = <span class="number">13</span></span><br></pre></td></tr></table></figure>
<ul>
<li>countByKey(): 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">3</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">95</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.countByKey()</span><br><span class="line">res63: scala.collection.<span class="type">Map</span>[<span class="type">Int</span>,<span class="type">Long</span>] = <span class="type">Map</span>(<span class="number">3</span> -&gt; <span class="number">2</span>, <span class="number">1</span> -&gt; <span class="number">3</span>, <span class="number">2</span> -&gt; <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>foreach(func): 在数据集的每一个元素上，运行函数func进行更新。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">30</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.collect().foreach(println)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure>
<ul>
<li>saveAsTextFile(path): 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本。</li>
<li>saveAsSequenceFile(path): 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。</li>
<li>saveAsObjectFile(path): 用于将RDD中的元素序列化成对象，存储到文件中。</li>
</ul>
<h3 id="数值-RDD-的统计操作"><a href="#数值-RDD-的统计操作" class="headerlink" title="数值 RDD 的统计操作"></a>数值 RDD 的统计操作</h3><p>Spark 对包含数值数据的 RDD 提供了一些描述性的统计操作。 Spark 的数值操作是通过流式算法实现的，允许以每次一个元素的方式构建出模型。这些 统计数据都会在调用 stats() 时通过一次遍历数据计算出来，并以 StatsCounter 对象返回。 </p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/14-RDD-Statistical-Operation.png?raw=true" alt="14-RDD-Statistical-Operation"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">42</span>] at makeRDD at &lt;console&gt;:<span class="number">32</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.sum()</span><br><span class="line">res34: <span class="type">Double</span> = <span class="number">5050.0</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.max()</span><br><span class="line">res35: <span class="type">Int</span> = <span class="number">100</span></span><br></pre></td></tr></table></figure>
<h3 id="向RDD操作传递函数注意"><a href="#向RDD操作传递函数注意" class="headerlink" title="向RDD操作传递函数注意"></a>向RDD操作传递函数注意</h3><p>Spark 的大部分转化操作和一部分行动操作，都需要依赖用户传递的函数来计算。 在 Scala 中，我们可以把定义的内联函数、方法的引用或静态方法传递给 Spark，就像 Scala 的其他函数式 API 一样。我们还要考虑其他一些细节，比如所传递的函数及其引用 的数据需要是可序列化的(实现了 Java 的 Serializable 接口)。 传递一个对象的方法或者字段时，会包含对整个对象的引用。 </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SearchFunctions</span>(<span class="params">val query: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    s.contains(query)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatchesFunctionReference</span></span>(rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>]): org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="comment">// 问题:"isMatch"表示"this.isMatch"，因此我们要传递整个"this" </span></span><br><span class="line">    rdd.filter(isMatch)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatchesFieldReference</span></span>(rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>]): org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = &#123; </span><br><span class="line"><span class="comment">// 问题:"query"表示"this.query"，因此我们要传递整个"this" </span></span><br><span class="line">rdd.filter(x =&gt; x.contains(query)) </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatchesNoReference</span></span>(rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>]): org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="comment">// 安全:只把我们需要的字段拿出来放入局部变量中 </span></span><br><span class="line"><span class="keyword">val</span> query_ = <span class="keyword">this</span>.query</span><br><span class="line">    rdd.filter(x =&gt; x.contains(query_))</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果在 Scala 中出现了 NotSerializableException，通常问题就在于我们传递了一个不可序列 化的类中的函数或字段。</p>
<h3 id="在不同-RDD-类型间转换"><a href="#在不同-RDD-类型间转换" class="headerlink" title="在不同 RDD 类型间转换"></a>在不同 RDD 类型间转换</h3><p>有些函数只能用于特定类型的 RDD，比如 mean() 和 variance() 只能用在数值 RDD 上， 而 join() 只能用在键值对 RDD 上。在 Scala 和 Java 中，这些函数都没有定义在标准的 RDD 类中，所以要访问这些附加功能，必须要确保获得了正确的专用 RDD 类。<br>在 Scala 中，将 RDD 转为有特定函数的 RDD(比如在 RDD[Double] 上进行数值操作)是由隐式转换来自动处理的。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/15-RDD-In-IDEA.png?raw=true" alt="15-RDD-In-IDEA"></p>
<h2 id="RDD-持久化"><a href="#RDD-持久化" class="headerlink" title="RDD 持久化"></a>RDD 持久化</h2><h3 id="RDD-的缓存"><a href="#RDD-的缓存" class="headerlink" title="RDD 的缓存"></a>RDD 的缓存</h3><p>Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存个数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。如果一个有持久化数据的节点发生故障，Spark 会在需要用到缓存的数据时重算丢失的数据分区。如果 希望节点故障的情况不会拖累我们的执行速度，也可以把数据备份到多个节点上。</p>
<h3 id="RDD-的缓存方式"><a href="#RDD-的缓存方式" class="headerlink" title="RDD 的缓存方式"></a>RDD 的缓存方式</h3><p>RDD通过persist方法或cache方法可以将前面的计算结果缓存，默认情况下 persist() 会把数据以序列化的形式缓存在 JVM 的堆空间中。<br>但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/16-RDD-Persist.png?raw=true" alt="16-RDD-Persist"></p>
<p>通过查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在object StorageLevel中定义的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">19</span>] at makeRDD at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> nocache = rdd.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">nocache: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">20</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> cache =  rdd.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">cache: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">21</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; cache.cache</span><br><span class="line">res24: cache.<span class="keyword">type</span> = <span class="type">MapPartitionsRDD</span>[<span class="number">21</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; nocache.collect</span><br><span class="line">res25: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479375155</span>], <span class="number">2</span>[<span class="number">1505479374674</span>], <span class="number">3</span>[<span class="number">1505479374674</span>], <span class="number">4</span>[<span class="number">1505479375153</span>], <span class="number">5</span>[<span class="number">1505479375153</span>], <span class="number">6</span>[<span class="number">1505479374675</span>], <span class="number">7</span>[<span class="number">1505479375154</span>], <span class="number">8</span>[<span class="number">1505479375154</span>], <span class="number">9</span>[<span class="number">1505479374676</span>], <span class="number">10</span>[<span class="number">1505479374676</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; nocache.collect</span><br><span class="line">res26: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479375679</span>], <span class="number">2</span>[<span class="number">1505479376157</span>], <span class="number">3</span>[<span class="number">1505479376157</span>], <span class="number">4</span>[<span class="number">1505479375680</span>], <span class="number">5</span>[<span class="number">1505479375680</span>], <span class="number">6</span>[<span class="number">1505479376159</span>], <span class="number">7</span>[<span class="number">1505479375680</span>], <span class="number">8</span>[<span class="number">1505479375680</span>], <span class="number">9</span>[<span class="number">1505479376158</span>], <span class="number">10</span>[<span class="number">1505479376158</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; nocache.collect</span><br><span class="line">res27: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479376743</span>], <span class="number">2</span>[<span class="number">1505479377218</span>], <span class="number">3</span>[<span class="number">1505479377218</span>], <span class="number">4</span>[<span class="number">1505479376745</span>], <span class="number">5</span>[<span class="number">1505479376745</span>], <span class="number">6</span>[<span class="number">1505479377219</span>], <span class="number">7</span>[<span class="number">1505479376747</span>], <span class="number">8</span>[<span class="number">1505479376747</span>], <span class="number">9</span>[<span class="number">1505479377218</span>], <span class="number">10</span>[<span class="number">1505479377218</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; cache.collect</span><br><span class="line">res28: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479382745</span>], <span class="number">2</span>[<span class="number">1505479382253</span>], <span class="number">3</span>[<span class="number">1505479382253</span>], <span class="number">4</span>[<span class="number">1505479382748</span>], <span class="number">5</span>[<span class="number">1505479382748</span>], <span class="number">6</span>[<span class="number">1505479382257</span>], <span class="number">7</span>[<span class="number">1505479382747</span>], <span class="number">8</span>[<span class="number">1505479382747</span>], <span class="number">9</span>[<span class="number">1505479382253</span>], <span class="number">10</span>[<span class="number">1505479382253</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; cache.collect</span><br><span class="line">res29: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479382745</span>], <span class="number">2</span>[<span class="number">1505479382253</span>], <span class="number">3</span>[<span class="number">1505479382253</span>], <span class="number">4</span>[<span class="number">1505479382748</span>], <span class="number">5</span>[<span class="number">1505479382748</span>], <span class="number">6</span>[<span class="number">1505479382257</span>], <span class="number">7</span>[<span class="number">1505479382747</span>], <span class="number">8</span>[<span class="number">1505479382747</span>], <span class="number">9</span>[<span class="number">1505479382253</span>], <span class="number">10</span>[<span class="number">1505479382253</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; cache.collect</span><br><span class="line">res30: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479382745</span>], <span class="number">2</span>[<span class="number">1505479382253</span>], <span class="number">3</span>[<span class="number">1505479382253</span>], <span class="number">4</span>[<span class="number">1505479382748</span>], <span class="number">5</span>[<span class="number">1505479382748</span>], <span class="number">6</span>[<span class="number">1505479382257</span>], <span class="number">7</span>[<span class="number">1505479382747</span>], <span class="number">8</span>[<span class="number">1505479382747</span>], <span class="number">9</span>[<span class="number">1505479382253</span>], <span class="number">10</span>[<span class="number">1505479382253</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; cache.persist(org.apache.spark.storage.<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/17-RDD-StorageLevel.png?raw=true" alt="17-RDD-StorageLevel"></p>
<p>在存储级别的末尾加上“_2”来把持久化数据存为两份</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/18-RDD-StorageLevel-Description.png?raw=true" alt="18-RDD-StorageLevel-Description"></p>
<p>缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。</p>
<p>注意：使用 Tachyon可以实现堆外缓存。</p>
<h2 id="RDD-CheckPoint-机制"><a href="#RDD-CheckPoint-机制" class="headerlink" title="RDD CheckPoint 机制"></a>RDD CheckPoint 机制</h2>
      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>您的支持将鼓励我继续创作</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="moqi 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="moqi 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Spark/" rel="tag"># Spark</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/06/16/JVM-GC-Development-Path/" rel="next" title="JVM GC 发展历程">
                <i class="fa fa-chevron-left"></i> JVM GC 发展历程
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://avatars3.githubusercontent.com/u/39821951?s=400&u=65c6d8145d7b591ca2051e7082fd842b56e62567&v=4"
                alt="moqi" />
            
              <p class="site-author-name" itemprop="name">moqi</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="mailto:moqimoqidea@gmail.com" target="_blank" title="E-mail">
                      
                        <i class="fa fa-fw fa-meh-o"></i>E-mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#RDD-概念"><span class="nav-number">1.</span> <span class="nav-text">RDD 概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-为什么会产生"><span class="nav-number">1.1.</span> <span class="nav-text">RDD 为什么会产生</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-概述"><span class="nav-number">1.2.</span> <span class="nav-text">RDD 概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是-RDD"><span class="nav-number">1.2.1.</span> <span class="nav-text">什么是 RDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD-的属性"><span class="nav-number">1.2.2.</span> <span class="nav-text">RDD 的属性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-弹性"><span class="nav-number">1.3.</span> <span class="nav-text">RDD 弹性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-特点"><span class="nav-number">1.4.</span> <span class="nav-text">RDD 特点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#分区"><span class="nav-number">1.4.1.</span> <span class="nav-text">分区</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#只读"><span class="nav-number">1.4.2.</span> <span class="nav-text">只读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#依赖"><span class="nav-number">1.4.3.</span> <span class="nav-text">依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#缓存"><span class="nav-number">1.4.4.</span> <span class="nav-text">缓存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CheckPoint"><span class="nav-number">1.4.5.</span> <span class="nav-text">CheckPoint</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RDD-编程"><span class="nav-number">2.</span> <span class="nav-text">RDD 编程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#编程模型"><span class="nav-number">2.1.</span> <span class="nav-text">编程模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建-RDD"><span class="nav-number">2.2.</span> <span class="nav-text">创建 RDD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-编程-1"><span class="nav-number">2.3.</span> <span class="nav-text">RDD 编程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Transformation"><span class="nav-number">2.3.1.</span> <span class="nav-text">Transformation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Action"><span class="nav-number">2.3.2.</span> <span class="nav-text">Action</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数值-RDD-的统计操作"><span class="nav-number">2.3.3.</span> <span class="nav-text">数值 RDD 的统计操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#向RDD操作传递函数注意"><span class="nav-number">2.3.4.</span> <span class="nav-text">向RDD操作传递函数注意</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#在不同-RDD-类型间转换"><span class="nav-number">2.3.5.</span> <span class="nav-text">在不同 RDD 类型间转换</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-持久化"><span class="nav-number">2.4.</span> <span class="nav-text">RDD 持久化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD-的缓存"><span class="nav-number">2.4.1.</span> <span class="nav-text">RDD 的缓存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD-的缓存方式"><span class="nav-number">2.4.2.</span> <span class="nav-text">RDD 的缓存方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-CheckPoint-机制"><span class="nav-number">2.5.</span> <span class="nav-text">RDD CheckPoint 机制</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">moqi</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">14.5k</span>
  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://moqimoqidea.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://moqimoqidea.github.io/2017/08/04/Spark-Core-Analysis/';
          this.page.identifier = '2017/08/04/Spark-Core-Analysis/';
          this.page.title = 'Spark Core 应用解析';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://moqimoqidea.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('3');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
