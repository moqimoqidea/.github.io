<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="CgWpgpEd7fSixhMFnnAhPIXT3nCIz9VbArCcXf9ZC0c" />














  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Spark," />





  <link rel="alternate" href="/atom.xml" title="Coding and Talking" type="application/atom+xml" />






<meta name="description" content="本文主要解析部分 Spark Core 的技术点。">
<meta name="keywords" content="Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark Core 应用解析">
<meta property="og:url" content="http://moqimoqidea.github.io/2017/08/04/Spark-Core-Analysis/index.html">
<meta property="og:site_name" content="Coding and Talking">
<meta property="og:description" content="本文主要解析部分 Spark Core 的技术点。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/01-Iteration-In-MapReduce.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/02-Iteration-In-Spark.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/03-RDD-Attributes.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/04-Native-Data-Space-And-Spark-RDD.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/05-RDD-Partition.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/06-RDD-Read-Only.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/07-Variety-Of-Operators.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/08-Transformation-Operations-And-Actions-Operations.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/09-Dependencies-Between-RDD.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/10-DAG.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/11-RDD-Cache.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/12-Driver-And-Worker.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/13-MasterNode-And-WorkerNode.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/14-RDD-Statistical-Operation.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/15-RDD-In-IDEA.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/16-RDD-Persist.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/17-RDD-StorageLevel.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/18-RDD-StorageLevel-Description.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/19-RDD-CheckPoint.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/20-RDD-CheckPoint-Transforme.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/21-RDD-Narrow-And-Wide-Dependencies.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/22-RDD-Lineage.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/23-DAG.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/24-RDD-Partition.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/25-PairRDDFunctions-In-IDEA.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/26-PairRDD-Transformation-1.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/27-PairRDD-Transformation-2.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/28-PairRDD-MapReduce.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/29-PairRDD-Connection.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/28-PairRDD-Actions.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/30-PairRDD-HashPartition.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/31-PairRDD-Partition-Shuffle-Optimization-1.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/32-PairRDD-Partition-Shuffle-Optimization-2.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/33-Type-In-Scala-Java-Hadoop.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/34-SequenceFile-Structures.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/35-Cassandra-And-ElasticSearch.png?raw=true">
<meta property="og:updated_time" content="2018-11-24T12:21:42.396Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark Core 应用解析">
<meta name="twitter:description" content="本文主要解析部分 Spark Core 的技术点。">
<meta name="twitter:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/01-Iteration-In-MapReduce.png?raw=true">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://moqimoqidea.github.io/2017/08/04/Spark-Core-Analysis/"/>





  <title>Spark Core 应用解析 | Coding and Talking</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-120200111-1', 'auto');
  ga('send', 'pageview');
</script>





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Coding and Talking</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://moqimoqidea.github.io/2017/08/04/Spark-Core-Analysis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="moqi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars3.githubusercontent.com/u/39821951?s=400&u=65c6d8145d7b591ca2051e7082fd842b56e62567&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Coding and Talking">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Spark Core 应用解析</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-04T13:29:46+08:00">
                2017-08-04
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/04/Spark-Core-Analysis/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/08/04/Spark-Core-Analysis/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  20,974
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  93
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文主要解析部分 Spark Core 的技术点。</p>
<a id="more"></a> 
<h1 id="RDD-概念"><a href="#RDD-概念" class="headerlink" title="RDD 概念"></a>RDD 概念</h1><h2 id="RDD-为什么会产生"><a href="#RDD-为什么会产生" class="headerlink" title="RDD 为什么会产生"></a>RDD 为什么会产生</h2><p>RDD是Spark的基石，是实现Spark数据处理的核心抽象。那么RDD为什么会产生呢？</p>
<p>Hadoop的MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后写入物理存储设备。数据更多面临的是一次性处理。</p>
<p>MR的这种方式对数据领域两种常见的操作不是很高效。第一种是迭代式的算法。比如机器学习中ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR这种模式不太合适，即使多MR串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。</p>
<p>MR中的迭代：</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/01-Iteration-In-MapReduce.png?raw=true" alt="01-Iteration-In-MapReduce"></p>
<p>Spark中的迭代：</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/02-Iteration-In-Spark.png?raw=true" alt="02-Iteration-In-Spark"></p>
<p>我们需要一个效率非常快，且能够支持迭代计算和有效数据共享的模型，Spark应运而生。RDD是基于工作集的工作模式，更多的是面向工作流。</p>
<p>但是无论是MR还是RDD都应该具有类似位置感知、容错和负载均衡等特性。</p>
<h2 id="RDD-概述"><a href="#RDD-概述" class="headerlink" title="RDD 概述"></a>RDD 概述</h2><h3 id="什么是-RDD"><a href="#什么是-RDD" class="headerlink" title="什么是 RDD"></a>什么是 RDD</h3><p>RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。在 Spark 中，对数据的所有操作不外乎创建 RDD、转化已有RDD 以及调用 RDD 操作进行求值。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。RDD 可以包含 Python、Java、Scala 中任意类型的对象， 甚至可以包含用户自定义的对象。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。<br>RDD支持两种操作:转化操作和行动操作。RDD 的转化操作是返回一个新的 RDD的操作，比如 map()和 filter()，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作。比如 count() 和 first()。<br>Spark采用惰性计算模式，RDD只有第一次在一个行动操作中用到时，才会真正计算。Spark可以优化整个计算过程。默认情况下，Spark 的 RDD 会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个 RDD，可以使用 RDD.persist() 让 Spark 把这个 RDD 缓存下来。</p>
<h3 id="RDD-的属性"><a href="#RDD-的属性" class="headerlink" title="RDD 的属性"></a>RDD 的属性</h3><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/03-RDD-Attributes.png?raw=true" alt="03-RDD-Attributes"></p>
<ol>
<li>一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</li>
<li>一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</li>
<li>RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</li>
<li>一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</li>
<li>一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</li>
</ol>
<p>RDD是一个应用层面的逻辑概念。一个RDD多个分片。RDD就是一个元数据记录集，记录了RDD内存所有的关系数据。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/04-Native-Data-Space-And-Spark-RDD.png?raw=true" alt="04-Native-Data-Space-And-Spark-RDD"></p>
<h2 id="RDD-弹性"><a href="#RDD-弹性" class="headerlink" title="RDD 弹性"></a>RDD 弹性</h2><ol>
<li>自动进行内存和磁盘数据存储的切换：Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换</li>
<li>基于血统的高效容错机制：在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。</li>
<li>Task如果失败会自动进行特定次数的重试：RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。</li>
<li>Stage如果失败会自动进行特定次数的重试：如果Job的某个Stage阶段计算失败，框架也会自动进行任务的重新计算，默认次数也是4次。</li>
<li>Checkpoint和Persist可主动或被动触发：RDD可以通过Persist持久化将RDD缓存到内存或者磁盘，当再次用到该RDD时直接读取就行。也可以将RDD进行检查点，检查点会将数据存储在HDFS中，该RDD的所有父RD都会被移除。</li>
<li>数据调度弹性：Spark把这个JOB执行模型抽象为通用的有向无环图DAG，可以将多Stage的任务串联或并行执行，调度引擎自动处理Stage的失败以及Task的失败。</li>
<li>数据分片的高度弹性：可以根据业务的特征，动态调整数据分片的个数，提升整体的应用执行效率。</li>
</ol>
<p>RDD全称叫做弹性分布式数据集(Resilient Distributed Datasets)一种分布式的内存抽象，表示一个只读的记录分区的集合，它只能通过其他RDD转换而创建，为此，RDD支持丰富的转换操作(如map, join, filter, groupBy等过这种转换操作，新的RDD则包含了如何从其他RDDs衍生所必需的信息，所以说RDDs之间是有依赖关系的。基于RDDs之间的依赖，RDDs会形成一个有向无环图DAG，G描述了整个流式计算的流程，实际执行的时候，RDD是通过血缘关系(Lineage)成的，即使出现数据分区丢失，也可以通过血缘关系重建分区，总结起来，基于RDD的流式计算任务可描述为：从稳定的物理存储(如分布式文件系统)中加载记录，记入由一组确定性操作构成的DAG，然后写回稳定存储。另外RDD还可以将数据集缓存到内存中，使得在多个操作之间可以重用数据集，基于这个特点可以很方便地构建应用(图计算、机器学习等)或者交互式数据分析应用。可以说Spark最初也就是实现RDD的一个分布式系统，后面通过不断发展壮大成为现在较为完善的大数据生态系单来讲，Spark-RDD的关系类似于Hadoop-MapReduce关系。</p>
<h2 id="RDD-特点"><a href="#RDD-特点" class="headerlink" title="RDD 特点"></a>RDD 特点</h2><p>RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。</p>
<h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><p>RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/05-RDD-Partition.png?raw=true" alt="05-RDD-Partition"></p>
<h3 id="只读"><a href="#只读" class="headerlink" title="只读"></a>只读</h3><p>如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/06-RDD-Read-Only.png?raw=true" alt="06-RDD-Read-Only"></p>
<p>由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/07-Variety-Of-Operators.png?raw=true" alt="07-Variety-Of-Operators"></p>
<p>RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/08-Transformation-Operations-And-Actions-Operations.png?raw=true" alt="08-Transformation-Operations-And-Actions-Operations"></p>
<h3 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h3><p>RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是一一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/09-Dependencies-Between-RDD.png?raw=true" alt="09-Dependencies-Between-RDD"></p>
<p>通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，如下图所示，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/10-DAG.png?raw=true" alt="10-DAG"></p>
<h3 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h3><p>如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/11-RDD-Cache.png?raw=true" alt="11-RDD-Cache"></p>
<h3 id="CheckPoint"><a href="#CheckPoint" class="headerlink" title="CheckPoint"></a>CheckPoint</h3><p>虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。</p>
<p>给定一个RDD我们至少可以知道如下几点信息：1、分区数以及分区方式；2、由父RDDs衍生而来的相关依赖信息；3、计算每个分区的数据，计算步骤为：1）如果被缓存，则从缓存中取的分区的数据；2）如果被checkpoint，则从checkpoint处恢复数据；3）根据血缘关系计算分区的数据。</p>
<h1 id="RDD-编程"><a href="#RDD-编程" class="headerlink" title="RDD 编程"></a>RDD 编程</h1><h2 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h2><p>在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。<br>要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/12-Driver-And-Worker.png?raw=true" alt="12-Driver-And-Worker"></p>
<p>那么这其中的 Dirver、SparkContext、Executor、Master、Worker分别是什么？</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/13-MasterNode-And-WorkerNode.png?raw=true" alt="13-MasterNode-And-WorkerNode"></p>
<h2 id="创建-RDD"><a href="#创建-RDD" class="headerlink" title="创建 RDD"></a>创建 RDD</h2><p>在Spark中创建RDD的创建方式大概可以分为三种：（1）、从集合中创建RDD；（2）、从外部存储创建RDD；（3）、从其他RDD创建。</p>
<ol>
<li>由一个已经存在的Scala集合创建，集合并行化。val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))；而从集合中创建RDD，Spark主要提供了两种函数：parallelize和makeRDD。我们可以先看看这两个函数的声明：</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[(<span class="type">T</span>, <span class="type">Seq</span>[<span class="type">String</span>])]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>
<p>我们可以从上面看出makeRDD有两种实现，而且第一个makeRDD函数接收的参数和parallelize完全一致。其实第一种makeRDD函数实现是依赖了parallelize函数的实现，来看看Spark中是怎么实现这个makeRDD函数的：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  parallelize(seq, numSlices)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们可以看出，这个makeRDD函数完全和parallelize函数一致。但是我们得看看第二种makeRDD函数函数实现了，它接收的参数类型是Seq[(T, Seq[String])]，Spark文档的说明是：<br>Distribute a local Scala collection to form an RDD, with one or more location preferences (hostnames of Spark nodes) for each object. Create a new partition for each collection item.<br>原来，这个函数还为数据提供了位置信息，来看看我们怎么使用：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> guigu1= sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">guigu1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> guigu2 = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">guigu2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">11</span>] at makeRDD at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> seq = <span class="type">List</span>((<span class="number">1</span>, <span class="type">List</span>(<span class="string">"slave01"</span>)),</span><br><span class="line">     | (<span class="number">2</span>, <span class="type">List</span>(<span class="string">"slave02"</span>)))</span><br><span class="line">seq: <span class="type">List</span>[(<span class="type">Int</span>, <span class="type">List</span>[<span class="type">String</span>])] = <span class="type">List</span>((<span class="number">1</span>,<span class="type">List</span>(slave01)),</span><br><span class="line"> (<span class="number">2</span>,<span class="type">List</span>(slave02)))</span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> guigu3 = sc.makeRDD(seq)</span><br><span class="line">guigu3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at makeRDD at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"> </span><br><span class="line">scala&gt; guigu3.preferredLocations(guigu3.partitions(<span class="number">1</span>))</span><br><span class="line">res26: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>(slave02)</span><br><span class="line"> </span><br><span class="line">scala&gt; guigu3.preferredLocations(guigu3.partitions(<span class="number">0</span>))</span><br><span class="line">res27: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>(slave01)</span><br><span class="line"> </span><br><span class="line">scala&gt; guigu1.preferredLocations(guigu1.partitions(<span class="number">0</span>))</span><br><span class="line">res28: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">List</span>()</span><br></pre></td></tr></table></figure>
<p>我们可以看到，makeRDD函数有两种实现，第一种实现其实完全和parallelize一致；而第二种实现可以为数据提供位置信息，而除此之外的实现和parallelize函数也是一致的，如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq, numSlices, <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">String</span>]]())</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[(<span class="type">T</span>, <span class="type">Seq</span>[<span class="type">String</span>])]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">val</span> indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq.map(_._1), seq.size, indexToPrefs)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>都是返回ParallelCollectionRDD，而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。</p>
<ol start="2">
<li>由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://slave1:9000/txtFile"</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = hdfs:<span class="comment">//slave1:9000/RELEASE MapPartitionsRDD[4] at textFile at &lt;console&gt;:24</span></span><br></pre></td></tr></table></figure>
<h2 id="RDD-编程-1"><a href="#RDD-编程-1" class="headerlink" title="RDD 编程"></a>RDD 编程</h2><p>RDD一般分为数值RDD和键值对RDD，本章不进行具体区分，先统一来看，下一章会对键值对RDD做专门说明。</p>
<h3 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h3><p>RDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。常用的 Transformation：</p>
<ul>
<li>map(func): 返回通过函数func传递源的每个元素形成的新分布式数据集。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> source  = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">source: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">8</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; source.collect()</span><br><span class="line">res7: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> mapadd = source.map(_ * <span class="number">2</span>)</span><br><span class="line">mapadd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">9</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; mapadd.collect()</span><br><span class="line">res8: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">18</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>filter(func): 返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> sourceFilter = sc.parallelize(<span class="type">Array</span>(<span class="string">"xiaoming"</span>,<span class="string">"xiaojiang"</span>,<span class="string">"xiaohe"</span>,<span class="string">"dazhi"</span>))</span><br><span class="line">sourceFilter: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> filter = sourceFilter.filter(_.contains(<span class="string">"xiao"</span>))</span><br><span class="line">filter: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">11</span>] at filter at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; sourceFilter.collect()</span><br><span class="line">res9: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(xiaoming, xiaojiang, xiaohe, dazhi)</span><br><span class="line"></span><br><span class="line">scala&gt; filter.collect()</span><br><span class="line">res10: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(xiaoming, xiaojiang, xiaohe)</span><br></pre></td></tr></table></figure>
<ul>
<li>flatMap(func): 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> sourceFlat = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">sourceFlat: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; sourceFlat.collect()</span><br><span class="line">res11: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> flatMap = sourceFlat.flatMap(<span class="number">1</span> to _)</span><br><span class="line">flatMap: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">13</span>] at flatMap at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; flatMap.collect()</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>mapPartitions(func): 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"kpop"</span>,<span class="string">"female"</span>),(<span class="string">"zorro"</span>,<span class="string">"male"</span>),(<span class="string">"mobin"</span>,<span class="string">"male"</span>),(<span class="string">"lucy"</span>,<span class="string">"female"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">16</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionsFun</span></span>(iter : <span class="type">Iterator</span>[(<span class="type">String</span>,<span class="type">String</span>)]) : <span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">  <span class="keyword">var</span> woman = <span class="type">List</span>[<span class="type">String</span>]()</span><br><span class="line">  <span class="keyword">while</span> (iter.hasNext)&#123;</span><br><span class="line">    <span class="keyword">val</span> next = iter.next()</span><br><span class="line">    next <span class="keyword">match</span> &#123;</span><br><span class="line">       <span class="keyword">case</span> (_,<span class="string">"female"</span>) =&gt; woman = next._1 :: woman</span><br><span class="line">       <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  woman.iterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line"></span><br><span class="line">partitionsFun: (iter: <span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">String</span>)])<span class="type">Iterator</span>[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> result = rdd.mapPartitions(partitionsFun)</span><br><span class="line">result: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">17</span>] at mapPartitions at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; result.collect()</span><br><span class="line">res13: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(kpop, lucy)</span><br></pre></td></tr></table></figure>
<ul>
<li>mapPartitionsWithIndex(func): 类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"kpop"</span>,<span class="string">"female"</span>),(<span class="string">"zorro"</span>,<span class="string">"male"</span>),(<span class="string">"mobin"</span>,<span class="string">"male"</span>),(<span class="string">"lucy"</span>,<span class="string">"female"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">18</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionsFun</span></span>(index : <span class="type">Int</span>, iter : <span class="type">Iterator</span>[(<span class="type">String</span>,<span class="type">String</span>)]) : <span class="type">Iterator</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">  <span class="keyword">var</span> woman = <span class="type">List</span>[<span class="type">String</span>]()</span><br><span class="line">  <span class="keyword">while</span> (iter.hasNext)&#123;</span><br><span class="line">    <span class="keyword">val</span> next = iter.next()</span><br><span class="line">    next <span class="keyword">match</span> &#123;</span><br><span class="line">       <span class="keyword">case</span> (_,<span class="string">"female"</span>) =&gt; woman = <span class="string">"["</span>+index+<span class="string">"]"</span>+next._1 :: woman</span><br><span class="line">       <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  woman.iterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line"></span><br><span class="line">partitionsFun: (index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">String</span>)])<span class="type">Iterator</span>[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> result = rdd.mapPartitionsWithIndex(partitionsFun)</span><br><span class="line">result: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">19</span>] at mapPartitionsWithIndex at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; result.collect()</span><br><span class="line">res14: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>([<span class="number">0</span>]kpop, [<span class="number">3</span>]lucy)</span><br></pre></td></tr></table></figure>
<ul>
<li>sample(withReplacement, fraction, seed): 以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。例子从RDD中随机且有放回的抽出50%的数据，随机种子值为3（即可能以1 2 3的其中一个起始值）。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">20</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect()</span><br><span class="line">res15: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> sample1 = rdd.sample(<span class="literal">true</span>,<span class="number">0.4</span>,<span class="number">2</span>)</span><br><span class="line">sample1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">PartitionwiseSampledRDD</span>[<span class="number">21</span>] at sample at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; sample1.collect()</span><br><span class="line">res16: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> sample2 = rdd.sample(<span class="literal">false</span>,<span class="number">0.2</span>,<span class="number">3</span>)</span><br><span class="line">sample2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">PartitionwiseSampledRDD</span>[<span class="number">22</span>] at sample at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; sample2.collect()</span><br><span class="line">res17: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">9</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>union(otherDataset): 对源RDD和参数RDD求并集后返回一个新的RDD。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">23</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">5</span> to <span class="number">10</span>)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">24</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = rdd1.union(rdd2)</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">UnionRDD</span>[<span class="number">25</span>] at union at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.collect()</span><br><span class="line">res18: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>intersection(otherDataset): 对源RDD和参数RDD求交集后返回一个新的RDD。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">7</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">26</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">5</span> to <span class="number">10</span>)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">27</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = rdd1.intersection(rdd2)</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">33</span>] at intersection at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.collect()</span><br><span class="line">res19: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>distinct([numTasks])): 对源RDD进行去重后返回一个新的RDD. 默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> distinctRdd = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">9</span>,<span class="number">6</span>,<span class="number">1</span>))</span><br><span class="line">distinctRdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">34</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> unionRDD = distinctRdd.distinct()</span><br><span class="line">unionRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">37</span>] at distinct at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; unionRDD.collect()</span><br><span class="line">res20: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> unionRDD = distinctRdd.distinct(<span class="number">2</span>)</span><br><span class="line">unionRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">40</span>] at distinct at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; unionRDD.collect()</span><br><span class="line">res21: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">6</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>partitionBy(partitioner): 对RDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区， 否则会生成ShuffleRDD。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"aaa"</span>),(<span class="number">2</span>,<span class="string">"bbb"</span>),(<span class="number">3</span>,<span class="string">"ccc"</span>),(<span class="number">4</span>,<span class="string">"ddd"</span>)),<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">44</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res24: <span class="type">Int</span> = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> rdd2 = rdd.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">45</span>] at partitionBy at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.partitions.size</span><br><span class="line">res25: <span class="type">Int</span> = <span class="number">2</span></span><br></pre></td></tr></table></figure>
<ul>
<li>reduceByKey(func, [numTasks]): 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"female"</span>,<span class="number">1</span>),(<span class="string">"male"</span>,<span class="number">5</span>),(<span class="string">"female"</span>,<span class="number">5</span>),(<span class="string">"male"</span>,<span class="number">2</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">46</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> reduce = rdd.reduceByKey((x,y) =&gt; x+y)</span><br><span class="line">reduce: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">47</span>] at reduceByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; reduce.collect()</span><br><span class="line">res29: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((female,<span class="number">6</span>), (male,<span class="number">7</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>groupByKey(): groupByKey也是对每个key进行操作，但只生成一个sequence。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> words = <span class="type">Array</span>(<span class="string">"one"</span>, <span class="string">"two"</span>, <span class="string">"two"</span>, <span class="string">"three"</span>, <span class="string">"three"</span>, <span class="string">"three"</span>)</span><br><span class="line">words: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(one, two, two, three, three, three)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> wordPairsRDD = sc.parallelize(words).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">wordPairsRDD: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">4</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> group = wordPairsRDD.groupByKey()</span><br><span class="line">group: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">ShuffledRDD</span>[<span class="number">5</span>] at groupByKey at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; group.collect()</span><br><span class="line">res1: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">Array</span>((two,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">1</span>)), (one,<span class="type">CompactBuffer</span>(<span class="number">1</span>)), (three,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; group.map(t =&gt; (t._1, t._2.sum))</span><br><span class="line">res2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">6</span>] at map at &lt;console&gt;:<span class="number">31</span></span><br><span class="line"></span><br><span class="line">scala&gt; res2.collect()</span><br><span class="line">res3: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((two,<span class="number">2</span>), (one,<span class="number">1</span>), (three,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> map = group.map(t =&gt; (t._1, t._2.sum))</span><br><span class="line">map: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">7</span>] at map at &lt;console&gt;:<span class="number">30</span></span><br><span class="line"></span><br><span class="line">scala&gt; map.collect()</span><br><span class="line">res4: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((two,<span class="number">2</span>), (one,<span class="number">1</span>), (three,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>combineByKey[C](  createCombiner: V =&gt; C,  mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): 对相同K，把V合并成一个集合.<ul>
<li>createCombiner: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。如果这是一个新的元素,combineByKey() 会使用一个叫作 createCombiner() 的函数来创建 那个键对应的累加器的初始值。</li>
<li>mergeValue: 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并。</li>
<li>mergeCombiners: 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。</li>
</ul>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> scores = <span class="type">Array</span>((<span class="string">"Fred"</span>, <span class="number">88</span>), (<span class="string">"Fred"</span>, <span class="number">95</span>), (<span class="string">"Fred"</span>, <span class="number">91</span>), (<span class="string">"Wilma"</span>, <span class="number">93</span>), (<span class="string">"Wilma"</span>, <span class="number">95</span>), (<span class="string">"Wilma"</span>, <span class="number">98</span>))</span><br><span class="line">scores: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="type">Fred</span>,<span class="number">88</span>), (<span class="type">Fred</span>,<span class="number">95</span>), (<span class="type">Fred</span>,<span class="number">91</span>), (<span class="type">Wilma</span>,<span class="number">93</span>), (<span class="type">Wilma</span>,<span class="number">95</span>), (<span class="type">Wilma</span>,<span class="number">98</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> input = sc.parallelize(scores)</span><br><span class="line">input: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">52</span>] at parallelize at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> combine = input.combineByKey(</span><br><span class="line">     |     (v)=&gt;(v,<span class="number">1</span>),</span><br><span class="line">     |     (acc:(<span class="type">Int</span>,<span class="type">Int</span>),v)=&gt;(acc._1+v,acc._2+<span class="number">1</span>),</span><br><span class="line">     |     (acc1:(<span class="type">Int</span>,<span class="type">Int</span>),acc2:(<span class="type">Int</span>,<span class="type">Int</span>))=&gt;(acc1._1+acc2._1,acc1._2+acc2._2))</span><br><span class="line">combine: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = <span class="type">ShuffledRDD</span>[<span class="number">53</span>] at combineByKey at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> result = combine.map&#123;</span><br><span class="line">     |     <span class="keyword">case</span> (key,value) =&gt; (key,value._1/value._2.toDouble)&#125;</span><br><span class="line">result: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">54</span>] at map at &lt;console&gt;:<span class="number">30</span></span><br><span class="line"></span><br><span class="line">scala&gt; result.collect()</span><br><span class="line">res33: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Double</span>)] = <span class="type">Array</span>((<span class="type">Wilma</span>,<span class="number">95.33333333333333</span>), (<span class="type">Fred</span>,<span class="number">91.33333333333333</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>aggregateByKey(zeroValue:U,[partitioner: Partitioner]) (seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U): 在kv对的RDD中，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。seqOp函数用于在每一个分区中用初始值逐步迭代value，combOp函数用于合并每个分区中的结果。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">3</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> agg = rdd.aggregateByKey(<span class="number">0</span>)(math.max(_,_),_+_)</span><br><span class="line">agg: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">13</span>] at aggregateByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; agg.collect()</span><br><span class="line">res7: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">3</span>,<span class="number">8</span>), (<span class="number">1</span>,<span class="number">7</span>), (<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; agg.partitions.size</span><br><span class="line">res8: <span class="type">Int</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">1</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> agg = rdd.aggregateByKey(<span class="number">0</span>)(math.max(_,_),_+_).collect()</span><br><span class="line">agg: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>), (<span class="number">3</span>,<span class="number">8</span>), (<span class="number">2</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]: aggregateByKey的简化操作，seqop和combop相同。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">3</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">91</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> agg = rdd.foldByKey(<span class="number">0</span>)(_+_)</span><br><span class="line">agg: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">92</span>] at foldByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; agg.collect()</span><br><span class="line">res61: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">3</span>,<span class="number">14</span>), (<span class="number">1</span>,<span class="number">9</span>), (<span class="number">2</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>sortByKey([ascending], [numTasks]): 在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">3</span>,<span class="string">"aa"</span>),(<span class="number">6</span>,<span class="string">"cc"</span>),(<span class="number">2</span>,<span class="string">"bb"</span>),(<span class="number">1</span>,<span class="string">"dd"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">14</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortByKey(<span class="literal">true</span>).collect()</span><br><span class="line">res9: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">1</span>,dd), (<span class="number">2</span>,bb), (<span class="number">3</span>,aa), (<span class="number">6</span>,cc))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortByKey(<span class="literal">false</span>).collect()</span><br><span class="line">res10: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">6</span>,cc), (<span class="number">3</span>,aa), (<span class="number">2</span>,bb), (<span class="number">1</span>,dd))</span><br></pre></td></tr></table></figure>
<ul>
<li>sortBy(func,[ascending], [numTasks]): 与sortByKey类似，但是更灵活,可以用func先对数据进行处理，按照处理后的数据比较结果排序。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">21</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortBy(x =&gt; x).collect()</span><br><span class="line">res11: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortBy(x =&gt; x%<span class="number">3</span>).collect()</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>join(otherDataset, [numTasks]): 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">32</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">33</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.join(rdd1).collect()</span><br><span class="line">res13: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">String</span>, <span class="type">Int</span>))] = <span class="type">Array</span>((<span class="number">1</span>,(a,<span class="number">4</span>)), (<span class="number">2</span>,(b,<span class="number">5</span>)), (<span class="number">3</span>,(c,<span class="number">6</span>)))</span><br></pre></td></tr></table></figure>
<ul>
<li>cogroup(otherDataset, [numTasks]): 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable\&lt;V>,Iterable\&lt;W>))类型的RDD。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">37</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">38</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.cogroup(rdd1).collect()</span><br><span class="line">res14: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = <span class="type">Array</span>((<span class="number">1</span>,(<span class="type">CompactBuffer</span>(a),<span class="type">CompactBuffer</span>(<span class="number">4</span>))), (<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="number">5</span>))), (<span class="number">3</span>,(<span class="type">CompactBuffer</span>(c),<span class="type">CompactBuffer</span>(<span class="number">6</span>))))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">Array</span>((<span class="number">4</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">41</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.cogroup(rdd2).collect()</span><br><span class="line">res15: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = <span class="type">Array</span>((<span class="number">4</span>,(<span class="type">CompactBuffer</span>(),<span class="type">CompactBuffer</span>(<span class="number">4</span>))), (<span class="number">1</span>,(<span class="type">CompactBuffer</span>(a),<span class="type">CompactBuffer</span>())), (<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="number">5</span>))), (<span class="number">3</span>,(<span class="type">CompactBuffer</span>(c),<span class="type">CompactBuffer</span>(<span class="number">6</span>))))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">1</span>,<span class="string">"d"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">44</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.cogroup(rdd2).collect()</span><br><span class="line">res16: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = <span class="type">Array</span>((<span class="number">4</span>,(<span class="type">CompactBuffer</span>(),<span class="type">CompactBuffer</span>(<span class="number">4</span>))), (<span class="number">1</span>,(<span class="type">CompactBuffer</span>(d, a),<span class="type">CompactBuffer</span>())), (<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="number">5</span>))), (<span class="number">3</span>,(<span class="type">CompactBuffer</span>(c),<span class="type">CompactBuffer</span>(<span class="number">6</span>))))</span><br></pre></td></tr></table></figure>
<ul>
<li>cartesian(otherDataset): 笛卡尔积。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">3</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">47</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">2</span> to <span class="number">5</span>)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">48</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.cartesian(rdd2).collect()</span><br><span class="line">res17: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="number">2</span>), (<span class="number">1</span>,<span class="number">3</span>), (<span class="number">1</span>,<span class="number">4</span>), (<span class="number">1</span>,<span class="number">5</span>), (<span class="number">2</span>,<span class="number">2</span>), (<span class="number">2</span>,<span class="number">3</span>), (<span class="number">2</span>,<span class="number">4</span>), (<span class="number">2</span>,<span class="number">5</span>), (<span class="number">3</span>,<span class="number">2</span>), (<span class="number">3</span>,<span class="number">3</span>), (<span class="number">3</span>,<span class="number">4</span>), (<span class="number">3</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>pipe(command, [envVars]): 对于每个分区，都执行一个perl或者shell脚本，返回输出的RDD。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="string">"hi"</span>,<span class="string">"Hello"</span>,<span class="string">"how"</span>,<span class="string">"are"</span>,<span class="string">"you"</span>),<span class="number">1</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">50</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.pipe(<span class="string">"/home/spark_shell/pipe.sh"</span>).collect()</span><br><span class="line">res18: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="type">AA</span>, &gt;&gt;&gt;hi, &gt;&gt;&gt;<span class="type">Hello</span>, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="string">"hi"</span>,<span class="string">"Hello"</span>,<span class="string">"how"</span>,<span class="string">"are"</span>,<span class="string">"you"</span>),<span class="number">2</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">52</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.pipe(<span class="string">"/home/spark_shell/pipe.sh"</span>).collect()</span><br><span class="line">res19: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="type">AA</span>, &gt;&gt;&gt;hi, &gt;&gt;&gt;<span class="type">Hello</span>, <span class="type">AA</span>, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pipe.sh:</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">echo "AA"</span><br><span class="line">while read LINE; do</span><br><span class="line">   echo "&gt;&gt;&gt;"$&#123;LINE&#125;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<ul>
<li>coalesce(numPartitions): 缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">16</span>,<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">54</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res20: <span class="type">Int</span> = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> coalesceRDD = rdd.coalesce(<span class="number">3</span>)</span><br><span class="line">coalesceRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">CoalescedRDD</span>[<span class="number">55</span>] at coalesce at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; coalesceRDD.partitions.size</span><br><span class="line">res21: <span class="type">Int</span> = <span class="number">3</span></span><br></pre></td></tr></table></figure>
<ul>
<li>repartition(numPartitions): 根据分区数，从新通过网络随机洗牌所有数据。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">16</span>,<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">56</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res22: <span class="type">Int</span> = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rerdd = rdd.repartition(<span class="number">2</span>)</span><br><span class="line">rerdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">60</span>] at repartition at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rerdd.partitions.size</span><br><span class="line">res23: <span class="type">Int</span> = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rerdd = rdd.repartition(<span class="number">4</span>)</span><br><span class="line">rerdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">64</span>] at repartition at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rerdd.partitions.size</span><br><span class="line">res24: <span class="type">Int</span> = <span class="number">4</span></span><br></pre></td></tr></table></figure>
<ul>
<li>glom(): 将每一个分区形成一个数组，形成新的RDD类型时RDD[Array[T]]。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">16</span>,<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">65</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.glom().collect()</span><br><span class="line">res25: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="type">Array</span>(<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>), <span class="type">Array</span>(<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>), <span class="type">Array</span>(<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>mapValues(func):    针对于(K,V)形式的类型只对V进行操作。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">1</span>,<span class="string">"d"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">67</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.mapValues(_+<span class="string">"|||"</span>).collect()</span><br><span class="line">res26: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">1</span>,a|||), (<span class="number">1</span>,d|||), (<span class="number">2</span>,b|||), (<span class="number">3</span>,c|||))</span><br></pre></td></tr></table></figure>
<ul>
<li>subtract(other:RDD): 计算差的一种函数去除两个RDD中相同的元素，不同的RDD将保留下来 。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">3</span> to <span class="number">8</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">70</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">71</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.subtract(rdd1).collect()</span><br><span class="line">res27: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">8</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h3><p>常用的 Action 如下：</p>
<ul>
<li>reduce(func): 通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">85</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.reduce(_+_)</span><br><span class="line">res50: <span class="type">Int</span> = <span class="number">55</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="type">Array</span>((<span class="string">"a"</span>,<span class="number">1</span>),(<span class="string">"a"</span>,<span class="number">3</span>),(<span class="string">"c"</span>,<span class="number">3</span>),(<span class="string">"d"</span>,<span class="number">5</span>)))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">86</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.reduce((x,y)=&gt;(x._1 + y._1,x._2 + y._2))</span><br><span class="line">res51: (<span class="type">String</span>, <span class="type">Int</span>) = (adca,<span class="number">12</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>collect(): 在驱动程序中，以数组的形式返回数据集的所有元素。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">17</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect()</span><br><span class="line">res7: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>count(): 返回RDD的元素个数。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">18</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count()</span><br><span class="line">res8: <span class="type">Long</span> = <span class="number">10</span></span><br></pre></td></tr></table></figure>
<ul>
<li>first(): 返回RDD的第一个元素（类似于take(1)）。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">19</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.first()</span><br><span class="line">res9: <span class="type">Int</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<ul>
<li>take(n): 返回一个由数据集的前n个元素组成的数组。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">20</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.take(<span class="number">7</span>)</span><br><span class="line">res10: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>takeSample(withReplacement,num, [seed]): 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">21</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.takeSample(<span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">res11: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">6</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>takeOrdered(n): 返回前几个的排序。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Seq</span>(<span class="number">10</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">19</span>, <span class="number">4</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">23</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.top(<span class="number">2</span>)</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">19</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.takeOrdered(<span class="number">2</span>)</span><br><span class="line">res13: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>aggregate (zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U): aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">88</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(<span class="number">1</span>)(</span><br><span class="line">     | &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x + y&#125;,</span><br><span class="line">     | &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">     | )</span><br><span class="line">res56: <span class="type">Int</span> = <span class="number">58</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(<span class="number">1</span>)(</span><br><span class="line">     | &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x * y&#125;,</span><br><span class="line">     | &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">     | )</span><br><span class="line">res57: <span class="type">Int</span> = <span class="number">30361</span></span><br></pre></td></tr></table></figure>
<ul>
<li>fold(num)(func): 折叠操作，aggregate的简化操作，seqop和combop一样。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">90</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(<span class="number">1</span>)(</span><br><span class="line">     | &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x + y&#125;,</span><br><span class="line">     | &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">     | )</span><br><span class="line">res59: <span class="type">Int</span> = <span class="number">13</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.fold(<span class="number">1</span>)(_+_)</span><br><span class="line">res60: <span class="type">Int</span> = <span class="number">13</span></span><br></pre></td></tr></table></figure>
<ul>
<li>countByKey(): 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">3</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">95</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.countByKey()</span><br><span class="line">res63: scala.collection.<span class="type">Map</span>[<span class="type">Int</span>,<span class="type">Long</span>] = <span class="type">Map</span>(<span class="number">3</span> -&gt; <span class="number">2</span>, <span class="number">1</span> -&gt; <span class="number">3</span>, <span class="number">2</span> -&gt; <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>foreach(func): 在数据集的每一个元素上，运行函数func进行更新。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">30</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.collect().foreach(println)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure>
<ul>
<li>saveAsTextFile(path): 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本。</li>
<li>saveAsSequenceFile(path): 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。</li>
<li>saveAsObjectFile(path): 用于将RDD中的元素序列化成对象，存储到文件中。</li>
</ul>
<h3 id="数值-RDD-的统计操作"><a href="#数值-RDD-的统计操作" class="headerlink" title="数值 RDD 的统计操作"></a>数值 RDD 的统计操作</h3><p>Spark对包含数值数据的RDD提供了一些描述性的统计操作。Spark的数值操作是通过流式算法实现的，允许以每次一个元素的方式构建出模型。这些统计数据都会在调用stats()时通过一次遍历数据计算出来，并以StatsCounter对象返回。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/14-RDD-Statistical-Operation.png?raw=true" alt="14-RDD-Statistical-Operation"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">42</span>] at makeRDD at &lt;console&gt;:<span class="number">32</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.sum()</span><br><span class="line">res34: <span class="type">Double</span> = <span class="number">5050.0</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.max()</span><br><span class="line">res35: <span class="type">Int</span> = <span class="number">100</span></span><br></pre></td></tr></table></figure>
<h3 id="向RDD操作传递函数注意"><a href="#向RDD操作传递函数注意" class="headerlink" title="向RDD操作传递函数注意"></a>向RDD操作传递函数注意</h3><p>Spark的大部分转化操作和一部分行动操作，都需要依赖用户传递的函数来计算。在Scala中，我们可以把定义的内联函数、方法的引用或静态方法传递给Spark，就像Scala的其他函数式API一样。我们还要考虑其他一些细节，比如所传递的函数及其引用的数据需要是可序列化的(实现了Java的Serializable接口)。传递一个对象的方法或者字段时，会包含对整个对象的引用。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SearchFunctions</span>(<span class="params">val query: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    s.contains(query)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatchesFunctionReference</span></span>(rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>]): org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="comment">// 问题:"isMatch"表示"this.isMatch"，因此我们要传递整个"this" </span></span><br><span class="line">    rdd.filter(isMatch)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatchesFieldReference</span></span>(rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>]): org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = &#123; </span><br><span class="line"><span class="comment">// 问题:"query"表示"this.query"，因此我们要传递整个"this" </span></span><br><span class="line">rdd.filter(x =&gt; x.contains(query)) </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatchesNoReference</span></span>(rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>]): org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="comment">// 安全:只把我们需要的字段拿出来放入局部变量中 </span></span><br><span class="line"><span class="keyword">val</span> query_ = <span class="keyword">this</span>.query</span><br><span class="line">    rdd.filter(x =&gt; x.contains(query_))</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果在Scala中出现了NotSerializableException，通常问题就在于我们传递了一个不可序列化的类中的函数或字段。</p>
<h3 id="在不同-RDD-类型间转换"><a href="#在不同-RDD-类型间转换" class="headerlink" title="在不同 RDD 类型间转换"></a>在不同 RDD 类型间转换</h3><p>有些函数只能用于特定类型的RDD，比如mean()和variance()只能用在数值RDD上，而join()只能用在键值对RDD上。在Scala和Java中，这些函数都没有定义在标准的RDD类中，所以要访问这些附加功能，必须要确保获得了正确的专用RDD类。<br>在Scala中，将RDD转为有特定函数的RDD(比如在RDD[Double]上进行数值操作)是由隐式转换来自动处理的。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/15-RDD-In-IDEA.png?raw=true" alt="15-RDD-In-IDEA"></p>
<h2 id="RDD-持久化"><a href="#RDD-持久化" class="headerlink" title="RDD 持久化"></a>RDD 持久化</h2><h3 id="RDD-的缓存"><a href="#RDD-的缓存" class="headerlink" title="RDD 的缓存"></a>RDD 的缓存</h3><p>Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存个数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。如果一个有持久化数据的节点发生故障，Spark会在需要用到缓存的数据时重算丢失的数据分区。如果希望节点故障的情况不会拖累我们的执行速度，也可以把数据备份到多个节点上。</p>
<h3 id="RDD-的缓存方式"><a href="#RDD-的缓存方式" class="headerlink" title="RDD 的缓存方式"></a>RDD 的缓存方式</h3><p>RDD通过persist方法或cache方法可以将前面的计算结果缓存，默认情况下persist()会把数据以序列化的形式缓存在JVM的堆空间中。<br>但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/16-RDD-Persist.png?raw=true" alt="16-RDD-Persist"></p>
<p>通过查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在objectStorageLevel中定义的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">19</span>] at makeRDD at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> nocache = rdd.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">nocache: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">20</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> cache =  rdd.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">cache: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">21</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; cache.cache</span><br><span class="line">res24: cache.<span class="keyword">type</span> = <span class="type">MapPartitionsRDD</span>[<span class="number">21</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; nocache.collect</span><br><span class="line">res25: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479375155</span>], <span class="number">2</span>[<span class="number">1505479374674</span>], <span class="number">3</span>[<span class="number">1505479374674</span>], <span class="number">4</span>[<span class="number">1505479375153</span>], <span class="number">5</span>[<span class="number">1505479375153</span>], <span class="number">6</span>[<span class="number">1505479374675</span>], <span class="number">7</span>[<span class="number">1505479375154</span>], <span class="number">8</span>[<span class="number">1505479375154</span>], <span class="number">9</span>[<span class="number">1505479374676</span>], <span class="number">10</span>[<span class="number">1505479374676</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; nocache.collect</span><br><span class="line">res26: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479375679</span>], <span class="number">2</span>[<span class="number">1505479376157</span>], <span class="number">3</span>[<span class="number">1505479376157</span>], <span class="number">4</span>[<span class="number">1505479375680</span>], <span class="number">5</span>[<span class="number">1505479375680</span>], <span class="number">6</span>[<span class="number">1505479376159</span>], <span class="number">7</span>[<span class="number">1505479375680</span>], <span class="number">8</span>[<span class="number">1505479375680</span>], <span class="number">9</span>[<span class="number">1505479376158</span>], <span class="number">10</span>[<span class="number">1505479376158</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; nocache.collect</span><br><span class="line">res27: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479376743</span>], <span class="number">2</span>[<span class="number">1505479377218</span>], <span class="number">3</span>[<span class="number">1505479377218</span>], <span class="number">4</span>[<span class="number">1505479376745</span>], <span class="number">5</span>[<span class="number">1505479376745</span>], <span class="number">6</span>[<span class="number">1505479377219</span>], <span class="number">7</span>[<span class="number">1505479376747</span>], <span class="number">8</span>[<span class="number">1505479376747</span>], <span class="number">9</span>[<span class="number">1505479377218</span>], <span class="number">10</span>[<span class="number">1505479377218</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; cache.collect</span><br><span class="line">res28: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479382745</span>], <span class="number">2</span>[<span class="number">1505479382253</span>], <span class="number">3</span>[<span class="number">1505479382253</span>], <span class="number">4</span>[<span class="number">1505479382748</span>], <span class="number">5</span>[<span class="number">1505479382748</span>], <span class="number">6</span>[<span class="number">1505479382257</span>], <span class="number">7</span>[<span class="number">1505479382747</span>], <span class="number">8</span>[<span class="number">1505479382747</span>], <span class="number">9</span>[<span class="number">1505479382253</span>], <span class="number">10</span>[<span class="number">1505479382253</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; cache.collect</span><br><span class="line">res29: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479382745</span>], <span class="number">2</span>[<span class="number">1505479382253</span>], <span class="number">3</span>[<span class="number">1505479382253</span>], <span class="number">4</span>[<span class="number">1505479382748</span>], <span class="number">5</span>[<span class="number">1505479382748</span>], <span class="number">6</span>[<span class="number">1505479382257</span>], <span class="number">7</span>[<span class="number">1505479382747</span>], <span class="number">8</span>[<span class="number">1505479382747</span>], <span class="number">9</span>[<span class="number">1505479382253</span>], <span class="number">10</span>[<span class="number">1505479382253</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; cache.collect</span><br><span class="line">res30: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505479382745</span>], <span class="number">2</span>[<span class="number">1505479382253</span>], <span class="number">3</span>[<span class="number">1505479382253</span>], <span class="number">4</span>[<span class="number">1505479382748</span>], <span class="number">5</span>[<span class="number">1505479382748</span>], <span class="number">6</span>[<span class="number">1505479382257</span>], <span class="number">7</span>[<span class="number">1505479382747</span>], <span class="number">8</span>[<span class="number">1505479382747</span>], <span class="number">9</span>[<span class="number">1505479382253</span>], <span class="number">10</span>[<span class="number">1505479382253</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; cache.persist(org.apache.spark.storage.<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/17-RDD-StorageLevel.png?raw=true" alt="17-RDD-StorageLevel"></p>
<p>在存储级别的末尾加上“_2”来把持久化数据存为两份</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/18-RDD-StorageLevel-Description.png?raw=true" alt="18-RDD-StorageLevel-Description"></p>
<p>缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。</p>
<p>注意：使用Tachyon可以实现堆外缓存。</p>
<h2 id="RDD-CheckPoint-机制"><a href="#RDD-CheckPoint-机制" class="headerlink" title="RDD CheckPoint 机制"></a>RDD CheckPoint 机制</h2><p>Spark中对于数据的保存除了持久化操作之外，还提供了一种CheckPoint的机制，CheckPoint（本质是通过将RDD写入Disk做CheckPoint）是为了通过lineage做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做CheckPoint容错，如果之后有节点出现问题而丢失分区，从做CheckPoint的RDD开始重做Lineage，就会减少开销。CheckPoint通过将数据写入到HDFS文件系统实现了RDD的CheckPoint功能。<br>cache和CheckPoint是有显著区别的，缓存把RDD计算出来然后放在内存中，但是RDD的依赖链（相当于数据库中的redo日志），也不能丢掉，当某个点某个executor宕了，上面cache的RDD就会丢掉，需要通过依赖链重放计算出来，不同的是，CheckPoint是把RDD保存在HDFS中，是多副本可靠存储，所以依赖链就可以丢掉了，就斩断了依赖链，是通过复制实现的高容错。<br>如果存在以下场景，则比较适合使用CheckPoint机制：</p>
<ol>
<li>DAG中的Lineage过长，如果重算，则开销太大（如在PageRank中）。</li>
<li>在宽依赖上做Checkpoint获得的收益更大。</li>
</ol>
<p>为当前RDD设置 CheckPoint。该函数将会创建一个二进制的文件，并存储到checkpoint目录中，该目录是用SparkContext.setCheckpointDir()设置的。在checkpoint的过程中，该RDD的所有依赖于父RDD中的信息将全部被移出。对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">100</span> , <span class="number">5</span>)</span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] =</span><br><span class="line">　　<span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at parallelize at &lt;console&gt;:<span class="number">12</span></span><br><span class="line"> </span><br><span class="line">scala&gt; sc.setCheckpointDir(<span class="string">"hdfs://slave1:9000/checkpoint"</span>)</span><br><span class="line"> </span><br><span class="line">scala&gt; data.checkpoint</span><br><span class="line"> </span><br><span class="line">scala&gt; data.count</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ch1 = sc.parallelize(<span class="number">1</span> to <span class="number">2</span>)</span><br><span class="line">ch1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">33</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ch2 = ch1.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">ch2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">36</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ch3 = ch1.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">ch3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">37</span>] at map at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; ch3.checkpoint</span><br><span class="line"></span><br><span class="line">scala&gt; ch2.collect</span><br><span class="line">res62: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480940726</span>], <span class="number">2</span>[<span class="number">1505480940243</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; ch2.collect</span><br><span class="line">res63: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480941957</span>], <span class="number">2</span>[<span class="number">1505480941480</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; ch2.collect</span><br><span class="line">res64: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480942736</span>], <span class="number">2</span>[<span class="number">1505480942257</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; ch3.collect</span><br><span class="line">res65: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480949080</span>], <span class="number">2</span>[<span class="number">1505480948603</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; ch3.collect</span><br><span class="line">res66: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480948683</span>], <span class="number">2</span>[<span class="number">1505480949161</span>])</span><br><span class="line"></span><br><span class="line">scala&gt; ch3.collect</span><br><span class="line">res67: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">1</span>[<span class="number">1505480948683</span>], <span class="number">2</span>[<span class="number">1505480949161</span>])</span><br></pre></td></tr></table></figure>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/19-RDD-CheckPoint.png?raw=true" alt="19-RDD-CheckPoint"></p>
<h3 id="CheckPoint-写流程"><a href="#CheckPoint-写流程" class="headerlink" title="CheckPoint 写流程"></a>CheckPoint 写流程</h3><p>RDD checkpoint 过程中会经过以下几个状态：[ Initialized → marked for checkpointing → checkpointing in progress → checkpointed ]，转换流程如下：</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/20-RDD-CheckPoint-Transforme.png?raw=true" alt="20-RDD-CheckPoint-Transforme"></p>
<ol>
<li>data.CheckPoint这个函数调用中，设置的目录中，所有依赖的RDD都会被删除，函数必须在job运行之前调用执行，强烈建议RDD缓存在内存中（又提到一次，千万要注意哟），否则保存到文件的时候需要从头计算。初始化RDD的CheckPointData变量为ReliableRDDCheckpointData。这时候标记为Initialized状态。</li>
<li>在所有jobaction的时候，runJob方法中都会调用rdd.doCheckpoint,这个会向前递归调用所有的依赖的RDD，看看需不需要CheckPoint。需要需要CheckPoint，然后调用CheckPointData.get.CheckPoint()，里面标记状态为CheckpointingInProgress，里面调用具体实现类的ReliableRDDCheckpointData的doCheckpoint方法。</li>
<li>doCheckpoint-&gt;writeRDDToCheckpointDirectory，注意这里会把job再运行一次，如果已经cache了，就可以直接使用缓存中的RDD了，就不需要重头计算一遍了（怎么又说了一遍），这时候直接把RDD，输出到hdfs，每个分区一个文件，会先写到一个临时文件，如果全部输出完，进行rename，如果输出失败，就回滚delete。</li>
<li>标记状态为Checkpointed，markCheckpointed方法中清除所有的依赖，怎么清除依赖的呢，就是把RDD变量的强引用设置为null，垃圾回收了，会触发ContextCleaner里面监听清除实际BlockManager缓存中的数据。</li>
</ol>
<h3 id="CheckPoint-读流程"><a href="#CheckPoint-读流程" class="headerlink" title="CheckPoint 读流程"></a>CheckPoint 读流程</h3><p>如果一个RDD我们已经CheckPoint了那么是什么时候用呢，CheckPoint将RDD持久化到HDFS或本地文件夹，如果不被手动remove掉，是一直存在的，也就是说可以被下一个driverprogram使用。比如sparkstreaming挂掉了，重启后就可以使用之前CheckPoint的数据进行recover,当然在同一个driverprogram也可以使用。我们讲下在同一个driverprogram中是怎么使用CheckPoint数据的。<br>如果一个RDD被CheckPoint了，当这个RDD上有action操作时候，或者回溯的这个RDD的时候，触发这个RDD进行计算，里面判断是否CheckPoint过，对分区和依赖的处理都是使用的RDD内部的CheckPointRDD变量。<br>具体细节如下：如果一个RDD被CheckPoint了，那么这个RDD中对分区和依赖的处理都是使用的RDD内部的CheckPointRDD变量，具体实现是ReliableCheckpointRDD类型。这个是在CheckPoint写流程中创建的。依赖和获取分区方法中先判断是否已经CheckPoint，如果已经CheckPoint了，就斩断依赖，使用ReliableCheckpointRDD，来处理依赖和获取分区。如果没有，才往前回溯依赖。依赖就是没有依赖，因为已经斩断了依赖，获取分区数据就是读取CheckPoint到HDFS目录中不同分区保存下来的文件。</p>
<h2 id="RDD-的依赖关系"><a href="#RDD-的依赖关系" class="headerlink" title="RDD 的依赖关系"></a>RDD 的依赖关系</h2><p>RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/21-RDD-Narrow-And-Wide-Dependencies.png?raw=true" alt="21-RDD-Narrow-And-Wide-Dependencies"></p>
<ol>
<li>窄依赖：窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用。总结：窄依赖我们形象的比喻为独生子女。</li>
<li>宽依赖：宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition，会引起shuffle。总结：宽依赖我们形象的比喻为超生。</li>
<li>Lineage：RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</li>
</ol>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/22-RDD-Lineage.png?raw=true" alt="22-RDD-Lineage"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> text = sc.textFile(<span class="string">"README.md"</span>)</span><br><span class="line">text: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">README</span>.md <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> words = text.flatMap(_.split)</span><br><span class="line">split   splitAt</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> words = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">words: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at flatMap at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; words.map((_,<span class="number">1</span>))</span><br><span class="line">res0: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at &lt;console&gt;:<span class="number">29</span></span><br><span class="line"></span><br><span class="line">scala&gt; res0.reduceByKey</span><br><span class="line">reduceByKey   reduceByKeyLocally</span><br><span class="line"></span><br><span class="line">scala&gt; res0.reduceByKey(_+_)</span><br><span class="line">res1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">4</span>] at reduceByKey at &lt;console&gt;:<span class="number">31</span></span><br><span class="line"></span><br><span class="line">scala&gt; res1.dependencies</span><br><span class="line">res2: <span class="type">Seq</span>[org.apache.spark.<span class="type">Dependency</span>[_]] = <span class="type">List</span>(org.apache.spark.<span class="type">ShuffleDependency</span>@<span class="number">6</span>cfe48a4)</span><br><span class="line"></span><br><span class="line">scala&gt; res0.dependencies</span><br><span class="line">res3: <span class="type">Seq</span>[org.apache.spark.<span class="type">Dependency</span>[_]] = <span class="type">List</span>(org.apache.spark.<span class="type">OneToOneDependency</span>@<span class="number">6</span>c9e24c4)</span><br></pre></td></tr></table></figure>
<h2 id="DAG-的生成"><a href="#DAG-的生成" class="headerlink" title="DAG 的生成"></a>DAG 的生成</h2><p>DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/23-DAG.png?raw=true" alt="23-DAG"></p>
<h2 id="RDD-相关概念关系"><a href="#RDD-相关概念关系" class="headerlink" title="RDD 相关概念关系"></a>RDD 相关概念关系</h2><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/24-RDD-Partition.png?raw=true" alt="24-RDD-Partition"></p>
<p>输入可能以多个文件的形式存储在HDFS上，每个File都包含了很多块，称为Block。<br>当Spark读取这些文件作为输入时，会根据具体数据格式对应的InputFormat进行解析，一般是将若干个Block合并成一个输入分片，称为InputSplit，注意InputSplit不能跨越文件。<br>随后将为这些输入分片生成具体的Task。InputSplit与Task是一一对应的关系。<br>随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。</p>
<ol>
<li>每个节点可以起一个或多个Executor。</li>
<li>每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。</li>
<li>每个Task执行的结果就是生成了目标RDD的一个partiton。</li>
</ol>
<p>注意: 这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。<br>而Task被执行的并发度 = Executor数目 * 每个Executor核数。<br>至于partition的数目：</p>
<ol>
<li>对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。</li>
<li>在Map阶段partition数目保持不变。</li>
<li>在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。</li>
</ol>
<p>RDD在计算的时候，每个分区都会起一个task，所以rdd的分区数目决定了总的的task数目。<br>申请的计算节点（Executor）数目和每个计算节点核数，决定了你同一时刻可以并行执行的task。<br>比如的RDD有100个分区，那么计算的时候就会生成100个task，你的资源配置为10个计算节点，每个两2个核，同一时刻可以并行的task数目为20，计算这个RDD就需要5个轮次。<br>如果计算资源不变，你有101个task的话，就需要6个轮次，在最后一轮中，只有一个task在执行，其余核都在空转。<br>如果资源不变，你的RDD只有2个分区，那么同一时刻只有2个task运行，其余18个核空转，造成资源浪费。这就是在spark调优中，增大RDD分区数目，增大任务并行度的做法。</p>
<h1 id="键值对-RDD"><a href="#键值对-RDD" class="headerlink" title="键值对 RDD"></a>键值对 RDD</h1><p>键值对RDD是Spark中许多操作所需要的常见数据类型。本章做特别讲解。除了在基础RDD类中定义的操作之外，Spark为包含键值对类型的RDD提供了一些专有的操作在PairRDDFunctions专门进行了定义。这些RDD被称为pairRDD。<br>有很多种方式创建pairRDD，在输入输出章节会讲解。一般如果从一个普通的RDD转为pairRDD时，可以调用map()函数来实现，传递的函数需要返回键值对。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pairs = lines.map(x =&gt; (x.split(<span class="string">" "</span>)(<span class="number">0</span>), x))</span><br></pre></td></tr></table></figure>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/25-PairRDDFunctions-In-IDEA.png?raw=true" alt="25-PairRDDFunctions-In-IDEA"></p>
<h2 id="Pair-RDD-的-Transformation-操作"><a href="#Pair-RDD-的-Transformation-操作" class="headerlink" title="Pair RDD 的 Transformation 操作"></a>Pair RDD 的 Transformation 操作</h2><h3 id="转化操作"><a href="#转化操作" class="headerlink" title="转化操作"></a>转化操作</h3><p>上一章进行了练习，这一章会重点讲解。针对一个Pair RDD的转化操作：</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/26-PairRDD-Transformation-1.png?raw=true" alt="26-PairRDD-Transformation-1"></p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/27-PairRDD-Transformation-2.png?raw=true" alt="27-PairRDD-Transformation-2"></p>
<h3 id="聚合操作"><a href="#聚合操作" class="headerlink" title="聚合操作"></a>聚合操作</h3><p>当数据集以键值对形式组织的时候，聚合具有相同键的元素进行一些统计是很常见的操作。之前讲解过基础RDD上的fold()、combine()、reduce()等行动操作，pairRDD上则有相应的针对键的转化操作。Spark有一组类似的操作，可以组合具有相同键的值。这些操作返回RDD，因此它们是转化操作而不是行动操作。<br>reduceByKey()与reduce()相当类似;它们都接收一个函数，并使用该函数对值进行合并。reduceByKey()会为数据集中的每个键进行并行的归约操作，每个归约操作会将键相同的值合并起来。因为数据集中可能有大量的键，所以reduceByKey()没有被实现为向用户程序返回一个值的行动操作。实际上，它会返回一个由各键和对应键归约出来的结果值组成的新的RDD。<br>foldByKey()则与fold()相当类似;它们都使用一个与RDD和合并函数中的数据类型相同的零值作为初始值。与fold()一样，foldByKey()操作所使用的合并函数对零值与另一个元素进行合并，结果仍为该元素。<br>求均值操作：版本一</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input.mapValues(x =&gt; (x, <span class="number">1</span>)).reduceByKey((x, y) =&gt; (x._1 + y._1, x._2 + y._2)).map&#123; <span class="keyword">case</span> (key, value) =&gt; (key, value._1 / value._2.toFloat) &#125;</span><br></pre></td></tr></table></figure>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/28-PairRDD-MapReduce.png?raw=true" alt="28-PairRDD-MapReduce"></p>
<p>combineByKey()是最为常用的基于键进行聚合的函数。大多数基于键聚合的函数都是用它实现的。和aggregate()一样，combineByKey()可以让用户返回与输入数据的类型不同的返回值。<br>要理解combineByKey()，要先理解它在处理数据时是如何处理每个元素的。由于combineByKey()会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。<br>如果这是一个新的元素，combineByKey()会使用一个叫作createCombiner()的函数来创建那个键对应的累加器的初始值。需要注意的是，这一过程会在每个分区中第一次出现各个键时发生，而不是在整个RDD中第一次出现一个键时发生。<br>如果这是一个在处理当前分区之前已经遇到的键，它会使用mergeValue()方法将该键的累加器对应的当前值与这个新的值进行合并。<br>由于每个分区都是独立处理的，因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器，就需要使用用户提供的mergeCombiners()方法将各个分区的结果进行合并。</p>
<p>求均值操作：版本二</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> result = input.combineByKey(</span><br><span class="line">  (v) =&gt; (v, <span class="number">1</span>),</span><br><span class="line">  (acc: (<span class="type">Int</span>, <span class="type">Int</span>), v) =&gt; (acc._1 + v, acc._2 + <span class="number">1</span>),</span><br><span class="line">  (acc1: (<span class="type">Int</span>, <span class="type">Int</span>), acc2: (<span class="type">Int</span>, <span class="type">Int</span>)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)</span><br><span class="line">).map&#123; <span class="keyword">case</span> (key, value) =&gt; (key, value._1 / value._2.toFloat) &#125;</span><br><span class="line"></span><br><span class="line">result.collectAsMap().map(println(_))</span><br></pre></td></tr></table></figure>
<h3 id="数据分组"><a href="#数据分组" class="headerlink" title="数据分组"></a>数据分组</h3><p>如果数据已经以预期的方式提取了键，groupByKey()就会使用RDD中的键来对数据进行分组。对于一个由类型K的键和类型V的值组成的RDD，所得到的结果RDD类型会是[K,Iterable[V]]。<br>groupBy()可以用于未成对的数据上，也可以根据除键相同以外的条件进行分组。它可以接收一个函数，对源RDD中的每个元素使用该函数，将返回结果作为键再进行分组。<br>多个RDD分组，可以使用cogroup函数，cogroup()的函数对多个共享同一个键的RDD进行分组。对两个键的类型均为K而值的类型分别为V和W的RDD进行cogroup()时，得到的结果RDD类型为[(K,(Iterable[V],Iterable[W]))]。如果其中的一个RDD对于另一个RDD中存在的某个键没有对应的记录，那么对应的迭代器则为空。cogroup()提供了为多个RDD进行数据分组的方法。</p>
<h3 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h3><p>连接主要用于多个PairRDD的操作，连接方式多种多样:右外连接、左外连接、交叉连接以及内连接。<br>普通的join操作符表示内连接2。只有在两个pairRDD中都存在的键才叫输出。当一个输入对应的某个键有多个值时，生成的pairRDD会包括来自两个输入RDD的每一组相对应的记录。<br>leftOuterJoin()产生的pairRDD中，源RDD的每一个键都有对应的记录。每个键相应的值是由一个源RDD中的值与一个包含第二个RDD的值的Option(在Java中为Optional)对象组成的二元组。<br>rightOuterJoin()几乎与leftOuterJoin()完全一样，只不过预期结果中的键必须出现在第二个RDD中，而二元组中的可缺失的部分则来自于源RDD而非第二个RDD。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/29-PairRDD-Connection.png?raw=true" alt="29-PairRDD-Connection"></p>
<h3 id="数据排序"><a href="#数据排序" class="headerlink" title="数据排序"></a>数据排序</h3><p>sortByKey()函数接收一个叫作ascending的参数，表示我们是否想要让结果按升序排序(默认值为true)。</p>
<h2 id="Pair-RDD-的-Action-操作"><a href="#Pair-RDD-的-Action-操作" class="headerlink" title="Pair RDD 的 Action 操作"></a>Pair RDD 的 Action 操作</h2><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/28-PairRDD-Actions.png?raw=true" alt="28-PairRDD-Actions"></p>
<h2 id="Pair-RDD-的数据分区"><a href="#Pair-RDD-的数据分区" class="headerlink" title="Pair RDD 的数据分区"></a>Pair RDD 的数据分区</h2><p>Spark目前支持Hash分区和Range分区，用户也可以自定义分区，Hash分区为当前的默认分区，Spark中分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle过程属于哪个分区和Reduce的个数。注意：</p>
<ol>
<li>只有Key-Value类型的RDD才有分区的，非Key-Value类型的RDD分区的值是None.</li>
<li>每个RDD的分区ID范围：0~numPartitions-1，决定这个值是属于那个分区的。</li>
</ol>
<h3 id="获取-RDD-的分区方式"><a href="#获取-RDD-的分区方式" class="headerlink" title="获取 RDD 的分区方式"></a>获取 RDD 的分区方式</h3><p>可以通过使用RDD的partitioner属性来获取RDD的分区方式。它会返回一个scala.Option对象，通过get方法获取其中的值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> pairs = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">3</span>)))</span><br><span class="line">pairs: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">33</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; pairs.partitioner</span><br><span class="line">res26: <span class="type">Option</span>[org.apache.spark.<span class="type">Partitioner</span>] = <span class="type">None</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> partitioned = pairs.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">partitioned: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">34</span>] at partitionBy at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; partitioned.partitioner</span><br><span class="line">res27: <span class="type">Option</span>[org.apache.spark.<span class="type">Partitioner</span>] = <span class="type">Some</span>(org.apache.spark.<span class="type">HashPartitioner</span>@<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Hash-分区方式"><a href="#Hash-分区方式" class="headerlink" title="Hash 分区方式"></a>Hash 分区方式</h3><p>HashPartitioner分区的原理：对于给定的key，计算其hashCode，并除于分区的个数取余，如果余数小于0，则用余数+分区的个数，最后返回的值就是这个key所属的分区ID。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> nopar = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">2</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">8</span>)</span><br><span class="line">nopar: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; nopar.partitioner</span><br><span class="line">res20: <span class="type">Option</span>[org.apache.spark.<span class="type">Partitioner</span>] = <span class="type">None</span></span><br><span class="line"></span><br><span class="line">scala&gt;nopar.mapPartitionsWithIndex((index,iter)=&gt;&#123; <span class="type">Iterator</span>(index.toString+<span class="string">" : "</span>+iter.mkString(<span class="string">"|"</span>)) &#125;).collect</span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="string">"0 : "</span>, <span class="number">1</span> : (<span class="number">1</span>,<span class="number">3</span>), <span class="number">2</span> : (<span class="number">1</span>,<span class="number">2</span>), <span class="number">3</span> : (<span class="number">2</span>,<span class="number">4</span>), <span class="string">"4 : "</span>, <span class="number">5</span> : (<span class="number">2</span>,<span class="number">3</span>), <span class="number">6</span> : (<span class="number">3</span>,<span class="number">6</span>), <span class="number">7</span> : (<span class="number">3</span>,<span class="number">8</span>)) </span><br><span class="line">scala&gt; <span class="keyword">val</span> hashpar = nopar.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">7</span>))</span><br><span class="line">hashpar: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">12</span>] at partitionBy at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; hashpar.count</span><br><span class="line">res18: <span class="type">Long</span> = <span class="number">6</span></span><br><span class="line"></span><br><span class="line">scala&gt; hashpar.partitioner</span><br><span class="line">res21: <span class="type">Option</span>[org.apache.spark.<span class="type">Partitioner</span>] = <span class="type">Some</span>(org.apache.spark.<span class="type">HashPartitioner</span>@<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; hashpar.mapPartitions(iter =&gt; <span class="type">Iterator</span>(iter.length)).collect()</span><br><span class="line">res19: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/30-PairRDD-HashPartition.png?raw=true" alt="30-PairRDD-HashPartition"></p>
<h3 id="Range-分区方式"><a href="#Range-分区方式" class="headerlink" title="Range 分区方式"></a>Range 分区方式</h3><p>HashPartitioner分区弊端：可能导致每个分区中数据量的不均匀，极端情况下会导致某些分区拥有RDD的全部数据。<br>RangePartitioner分区优势：尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大；<br>但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。<br>RangePartitioner作用：将一定范围内的数映射到某一个分区内，在实现中，分界的算法尤为重要。用到了水塘抽样算法。</p>
<h3 id="自定义分区方式"><a href="#自定义分区方式" class="headerlink" title="自定义分区方式"></a>自定义分区方式</h3><p>要实现自定义的分区器，你需要继承org.apache.spark.Partitioner类并实现下面三个方法。</p>
<ul>
<li>numPartitions:Int:返回创建出来的分区数。 </li>
<li>getPartition(key:Any):Int:返回给定键的分区编号(0到numPartitions-1)。</li>
<li>equals():Java判断相等性的标准方法。这个方法的实现非常重要，Spark需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样Spark才可以判断两个RDD的分区方式是否相同。 </li>
</ul>
<p>假设我们需要将相同后缀的数据写入相同的文件，我们通过将相同后缀的数据分区到相同的分区并保存输出来实现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.moqi.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">Partitioner</span>, <span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomerPartitioner</span>(<span class="params">numParts:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//覆盖分区数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = numParts</span><br><span class="line"></span><br><span class="line">  <span class="comment">//覆盖分区号获取函数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> ckey: <span class="type">String</span> = key.toString</span><br><span class="line">    ckey.substring(ckey.length<span class="number">-1</span>).toInt%numParts</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CustomerPartitioner</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"partitioner"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> data=sc.parallelize(<span class="type">List</span>(<span class="string">"aa.2"</span>,<span class="string">"bb.2"</span>,<span class="string">"cc.3"</span>,<span class="string">"dd.3"</span>,<span class="string">"ee.5"</span>))</span><br><span class="line">	</span><br><span class="line">    data.map((_,<span class="number">1</span>)).partitionBy(<span class="keyword">new</span> <span class="type">CustomerPartitioner</span>(<span class="number">5</span>)).keys.saveAsTextFile(<span class="string">"hdfs://slave1:9000/partitioner"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> data=sc.parallelize(<span class="type">List</span>(<span class="string">"aa.2"</span>,<span class="string">"bb.2"</span>,<span class="string">"cc.3"</span>,<span class="string">"dd.3"</span>,<span class="string">"ee.5"</span>).zipWithIndex,<span class="number">2</span>)</span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">7</span>] at parallelize at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; data.collect</span><br><span class="line">res4: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((aa<span class="number">.2</span>,<span class="number">0</span>), (bb<span class="number">.2</span>,<span class="number">1</span>), (cc<span class="number">.3</span>,<span class="number">2</span>), (dd<span class="number">.3</span>,<span class="number">3</span>), (ee<span class="number">.5</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; data.mapPartitionsWithIndex((index,iter)=&gt;<span class="type">Iterator</span>(index.toString +<span class="string">" : "</span>+ iter.mkString(<span class="string">"|"</span>))).collect</span><br><span class="line">res5: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">0</span> : (aa<span class="number">.2</span>,<span class="number">0</span>)|(bb<span class="number">.2</span>,<span class="number">1</span>), <span class="number">1</span> : (cc<span class="number">.3</span>,<span class="number">2</span>)|(dd<span class="number">.3</span>,<span class="number">3</span>)|(ee<span class="number">.5</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; :paste</span><br><span class="line"><span class="comment">// Entering paste mode (ctrl-D to finish)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomerPartitioner</span>(<span class="params">numParts:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">Partitioner</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//覆盖分区数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = numParts</span><br><span class="line"></span><br><span class="line">  <span class="comment">//覆盖分区号获取函数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> ckey: <span class="type">String</span> = key.toString</span><br><span class="line">    ckey.substring(ckey.length<span class="number">-1</span>).toInt%numParts</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Exiting paste mode, now interpreting.</span></span><br><span class="line"></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">CustomerPartitioner</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">data</span>.<span class="title">partitionBy</span>(<span class="params">new <span class="type">CustomerPartitioner</span>(4</span>))</span></span><br><span class="line"><span class="class"><span class="title">res7</span></span>: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">9</span>] at partitionBy at &lt;console&gt;:<span class="number">31</span></span><br><span class="line"></span><br><span class="line">scala&gt; res7.mapPartitionsWithIndex((index,iter)=&gt;<span class="type">Iterator</span>(index.toString +<span class="string">" : "</span>+ iter.mkString(<span class="string">"|"</span>))).collect</span><br><span class="line">res8: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="string">"0 : "</span>, <span class="number">1</span> : (ee<span class="number">.5</span>,<span class="number">4</span>), <span class="number">2</span> : (aa<span class="number">.2</span>,<span class="number">0</span>)|(bb<span class="number">.2</span>,<span class="number">1</span>), <span class="number">3</span> : (cc<span class="number">.3</span>,<span class="number">2</span>)|(dd<span class="number">.3</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<p>使用自定义的Partitioner是很容易的:只要把它传给partitionBy()方法即可。Spark中有许多依赖于数据混洗的方法，比如join()和groupByKey()，它们也可以接收一个可选的Partitioner对象来控制输出数据的分区方式。</p>
<h3 id="分区-Shuffle-优化"><a href="#分区-Shuffle-优化" class="headerlink" title="分区 Shuffle 优化"></a>分区 Shuffle 优化</h3><p>在分布式程序中，通信的代价是很大的，因此控制数据分布以获得最少的网络传输可以极大地提升整体性能。<br>Spark中所有的键值对RDD都可以进行分区。系统会根据一个针对键的函数对元素进行分组。主要有哈希分区和范围分区，当然用户也可以自定义分区函数。<br>通过分区可以有效提升程序性能。如下例子：<br>分析这样一个应用，它在内存中保存着一张很大的用户信息表——也就是一个由(UserID,UserInfo)对组成的RDD，其中UserInfo包含一个该用户所订阅的主题的列表。该应用会周期性地将这张表与一个小文件进行组合，这个小文件中存着过去五分钟内发生的事件——其实就是一个由(UserID,LinkInfo)对组成的表，存放着过去五分钟内某网站各用户的访问情况。例如，我们可能需要对用户访问其未订阅主题的页面的情况进行统计。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始化代码，从 HDFS 的一个 Hadoop SequenceFile 读取用户信息</span></span><br><span class="line"><span class="comment">// userData 中的元素会根据它们被读取时的来源，即 HDFS 块所在的节点来分布</span></span><br><span class="line"><span class="comment">// Spark 此时无法获知某个特定的 User ID 对应的记录位于那个节点上</span></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(...)</span><br><span class="line"><span class="keyword">val</span> userData = sc.sequenceFile[<span class="type">UserId</span>, <span class="type">UserInfo</span>](<span class="string">"hdfs://..."</span>).persist()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 周期性调用函数来处理过去五分钟产生的事件日志</span></span><br><span class="line"><span class="comment">// 假设这是一个包含(UserID, LinkInfo)键值对的 SequenceFile</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">processNewLogs</span></span>(logFileName: <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> events = sc.sequenceFile[<span class="type">UserID</span>, <span class="type">LinkInfo</span>](logFileName)</span><br><span class="line">    <span class="keyword">val</span> joined = userData.join(events) <span class="comment">// RDD of (UserID, (UserInfo, LinkInfo)) pairs</span></span><br><span class="line">    <span class="keyword">val</span> offTopicVisits = joined.filter &#123;</span><br><span class="line">        <span class="comment">// Expand the tuple into tis components</span></span><br><span class="line">        <span class="keyword">case</span> (userId, (userInfo, linkInfo)) =&gt; !userInfo.topics.contains(linkInfo.topic)</span><br><span class="line">    &#125;.count()</span><br><span class="line">    println(<span class="string">"Number of visits to non-subscribed topics: "</span> + offTopicVisits)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/31-PairRDD-Partition-Shuffle-Optimization-1.png?raw=true" alt="31-PairRDD-Partition-Shuffle-Optimization-1"></p>
<p>这段代码可以正确运行，但是不够高效。这是因为在每次调用processNewLogs()时都会用到join()操作，而我们对数据集是如何分区的却一无所知。默认情况下，连接操作会将两个数据集中的所有键的哈希值都求出来，将该哈希值相同的记录通过网络传到同一台机器上，然后在那台机器上对所有键相同的记录进行连接操作。因为userData表比每五分钟出现的访问日志表events要大得多，所以要浪费时间做很多额外工作:在每次调用时都对userData表进行哈希值计算和跨节点数据混洗，降低了程序的执行效率。<br>优化方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(...)</span><br><span class="line"><span class="keyword">val</span> userData = sc.sequenceFile[<span class="type">UserID</span>, <span class="type">UserInfo</span>](<span class="string">"hdfs://..."</span>)</span><br><span class="line">				 .partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">100</span>)) <span class="comment">// 构造 100 个分区</span></span><br><span class="line">				 .persist()</span><br></pre></td></tr></table></figure>
<p>我们在构建userData时调用了partitionBy()，Spark就知道了该RDD是根据键的哈希值来分区的，这样在调用join()时，Spark就会利用到这一点。具体来说，当调用userData.join(events)时，Spark只会对events进行数据混洗操作，将events中特定UserID的记录发送到userData的对应分区所在的那台机器上。这样，需要通过网络传输的数据就大大减少了，程序运行速度也可以显著提升了。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/32-PairRDD-Partition-Shuffle-Optimization-2.png?raw=true" alt="32-PairRDD-Partition-Shuffle-Optimization-2"></p>
<h3 id="基于分区进行操作"><a href="#基于分区进行操作" class="headerlink" title="基于分区进行操作"></a>基于分区进行操作</h3><p>基于分区对数据进行操作可以让我们避免为每个数据元素进行重复的配置工作。诸如打开数据库连接或创建随机数生成器等操作，都是我们应当尽量避免为每个元素都配置一次的工作。Spark提供基于分区的mapPartition和foreachPartition，让你的部分代码只对RDD的每个分区运行一次，这样可以帮助降低这些操作的代价。</p>
<h3 id="从分区中获益的操作"><a href="#从分区中获益的操作" class="headerlink" title="从分区中获益的操作"></a>从分区中获益的操作</h3><p>能够从数据分区中获得性能提升的操作有cogroup()、groupWith()、join()、leftOuterJoin()、rightOuterJoin()、groupByKey()、reduceByKey()、combineByKey()以及lookup()等。</p>
<h1 id="数据读取与保存主要方式"><a href="#数据读取与保存主要方式" class="headerlink" title="数据读取与保存主要方式"></a>数据读取与保存主要方式</h1><h2 id="文本文件"><a href="#文本文件" class="headerlink" title="文本文件"></a>文本文件</h2><p>当我们将一个文本文件读取为RDD时，输入的每一行都会成为RDD的一个元素。也可以将多个完整的文本文件一次性读取为一个pairRDD，其中键是文件名，值是文件内容。val input = sc.textFile(“./README.md”). 如果传递目录，则将目录下的所有文件读取作为RDD。文件路径支持通配符。通过wholeTextFiles()对于大量的小文件读取效率比较高，大文件效果没有那么高。<br>Spark通过saveAsTextFile()进行文本文件的输出，该方法接收一个路径，并将RDD中的内容都输入到路径对应的文件中。Spark将传入的路径作为目录对待，会在那个目录下输出多个文件。这样，Spark就可以从多个节点上并行输出了。result.saveAsTextFile(outputFile).</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.textFile(<span class="string">"./README.md"</span>)</span><br><span class="line">res6: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = ./<span class="type">README</span>.md <span class="type">MapPartitionsRDD</span>[<span class="number">7</span>] at textFile at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> readme = sc.textFile(<span class="string">"./README.md"</span>)</span><br><span class="line">readme: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = ./<span class="type">README</span>.md <span class="type">MapPartitionsRDD</span>[<span class="number">9</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; readme.collect()</span><br><span class="line">res7: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(# <span class="type">Apache</span> <span class="type">Spark</span>, <span class="string">""</span>, <span class="type">Spark</span> is a fast and general cluster...</span><br><span class="line">scala&gt; readme.saveAsTextFile(<span class="string">"hdfs://slave1:9000/test"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="JSON-文件"><a href="#JSON-文件" class="headerlink" title="JSON 文件"></a>JSON 文件</h2><p>如果JSON文件中每一行就是一个JSON记录，那么可以通过将JSON文件当做文本文件来读取，然后利用相关的JSON库对每一条数据进行JSON解析。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> org.json4s._  </span><br><span class="line"><span class="keyword">import</span> org.json4s._</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">import</span> org.json4s.jackson.<span class="type">JsonMethods</span>._  </span><br><span class="line"><span class="keyword">import</span> org.json4s.jackson.<span class="type">JsonMethods</span>._</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">import</span> org.json4s.jackson.<span class="type">Serialization</span>  </span><br><span class="line"><span class="keyword">import</span> org.json4s.jackson.<span class="type">Serialization</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">var</span> result = sc.textFile(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line">result: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = examples/src/main/resources/people.json <span class="type">MapPartitionsRDD</span>[<span class="number">7</span>] at textFile at &lt;console&gt;:<span class="number">47</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">implicit</span> <span class="keyword">val</span> formats = <span class="type">Serialization</span>.formats(<span class="type">ShortTypeHints</span>(<span class="type">List</span>())) </span><br><span class="line">formats: org.json4s.<span class="type">Formats</span>&#123;<span class="keyword">val</span> dateFormat: org.json4s.<span class="type">DateFormat</span>; <span class="keyword">val</span> typeHints: org.json4s.<span class="type">TypeHints</span>&#125; = org.json4s.<span class="type">Serialization</span>$$anon$<span class="number">1</span>@<span class="number">61</span>f2c1da</span><br><span class="line"></span><br><span class="line">scala&gt; result.collect().foreach(x =&gt; &#123;<span class="keyword">var</span> c = parse(x).extract[<span class="type">Person</span>];println(c.name + <span class="string">","</span> + c.age)&#125;)  </span><br><span class="line"><span class="type">Michael</span>,<span class="number">30</span></span><br><span class="line"><span class="type">Andy</span>,<span class="number">30</span></span><br><span class="line"><span class="type">Justin</span>,<span class="number">19</span></span><br></pre></td></tr></table></figure>
<p>如果JSON数据是跨行的，那么只能读入整个文件，然后对每个文件进行解析。JSON数据的输出主要是通过在输出之前将由结构化数据组成的RDD转为字符串RDD，然后使用Spark的文本文件API写出去。说白了还是以文本文件的形式存，只是文本的格式已经在程序中转换为JSON。</p>
<h2 id="CSV-文件"><a href="#CSV-文件" class="headerlink" title="CSV 文件"></a>CSV 文件</h2><p>读取CSV/TSV数据和读取JSON数据相似，都需要先把文件当作普通文本文件来读取数据，然后通过将每一行进行解析实现对CSV的读取。CSV/TSV数据的输出也是需要将结构化RDD通过相关的库转换成字符串RDD，然后使用Spark的文本文件API写出去。</p>
<h2 id="SequenceFile-文件"><a href="#SequenceFile-文件" class="headerlink" title="SequenceFile 文件"></a>SequenceFile 文件</h2><p>SequenceFile文件是Hadoop用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。Spark有专门用来读取SequenceFile的接口。在SparkContext中，可以调用sequenceFile[keyClass, valueClass](path).</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/33-Type-In-Scala-Java-Hadoop.png?raw=true" alt="33-Type-In-Scala-Java-Hadoop"></p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/34-SequenceFile-Structures.png?raw=true" alt="34-SequenceFile-Structures"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> data=sc.parallelize(<span class="type">List</span>((<span class="number">2</span>,<span class="string">"aa"</span>),(<span class="number">3</span>,<span class="string">"bb"</span>),(<span class="number">4</span>,<span class="string">"cc"</span>),(<span class="number">5</span>,<span class="string">"dd"</span>),(<span class="number">6</span>,<span class="string">"ee"</span>)))</span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">16</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; data.saveAsSequenceFile(<span class="string">"hdfs://slave1:9000/sequdata"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> sdata = sc.sequenceFile[<span class="type">Int</span>,<span class="type">String</span>](<span class="string">"hdfs://slave1:9000/sdata/p*"</span>)</span><br><span class="line">sdata: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">19</span>] at sequenceFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; sdata.collect()</span><br><span class="line">res14: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">2</span>,aa), (<span class="number">3</span>,bb), (<span class="number">4</span>,cc), (<span class="number">5</span>,dd), (<span class="number">6</span>,ee))</span><br></pre></td></tr></table></figure>
<p>可以直接调用saveAsSequenceFile(path)保存你的PairRDD，它会帮你写出数据。需要键和值能够自动转为Writable类型。</p>
<h2 id="对象文件"><a href="#对象文件" class="headerlink" title="对象文件"></a>对象文件</h2><p>对象文件是将对象序列化后保存的文件，采用Java的序列化机制。可以通过objectFile<a href="path">k,v</a>函数接收一个路径，读取对象文件，返回对应的RDD，也可以通过调用saveAsObjectFile()实现对对象文件的输出。因为是序列化所以要指定类型。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> data=sc.parallelize(<span class="type">List</span>((<span class="number">2</span>,<span class="string">"aa"</span>),(<span class="number">3</span>,<span class="string">"bb"</span>),(<span class="number">4</span>,<span class="string">"cc"</span>),(<span class="number">5</span>,<span class="string">"dd"</span>),(<span class="number">6</span>,<span class="string">"ee"</span>)))</span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">20</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; data.saveAsObjectFile(<span class="string">"hdfs://slave1:9000/objfile"</span>)</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> objrdd:<span class="type">RDD</span>[(<span class="type">Int</span>,<span class="type">String</span>)] = sc.objectFile[(<span class="type">Int</span>,<span class="type">String</span>)](<span class="string">"hdfs://slave1:9000/objfile/p*"</span>)</span><br><span class="line">objrdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">28</span>] at objectFile at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; objrdd.collect()</span><br><span class="line">res20: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">2</span>,aa), (<span class="number">3</span>,bb), (<span class="number">4</span>,cc), (<span class="number">5</span>,dd), (<span class="number">6</span>,ee))</span><br></pre></td></tr></table></figure>
<h2 id="Hadoop-文件"><a href="#Hadoop-文件" class="headerlink" title="Hadoop 文件"></a>Hadoop 文件</h2><p>Spark的整个生态系统与Hadoop是完全兼容的,所以对于Hadoop所支持的文件类型或者数据库类型,Spark也同样支持.另外,由于Hadoop的API有新旧两个版本,所以Spark为了能够兼容Hadoop所有的版本,也提供了两套创建操作接口.对于外部存储创建操作而言,hadoopRDD和newHadoopRDD是最为抽象的两个函数接口,主要包含以下四个参数。</p>
<ol>
<li>输入格式(InputFormat): 制定数据输入的类型,如TextInputFormat等,新旧两个版本所引用的版本分别是org.apache.hadoop.mapred.InputFormat和org.apache.hadoop.mapreduce.InputFormat(NewInputFormat)</li>
<li>键类型: 指定[K,V]键值对中K的类型</li>
<li>值类型: 指定[K,V]键值对中V的类型</li>
<li>分区值: 指定由外部存储生成的RDD的partition数量的最小值,如果没有指定,系统会使用默认值defaultMinSplits</li>
</ol>
<p>其他创建操作的API接口都是为了方便最终的Spark程序开发者而设置的,是这两个接口的高效实现版本.例如,对于textFile而言,只有path这个指定文件路径的参数,其他参数在系统内部指定了默认值。</p>
<p>注意:</p>
<ol>
<li>在Hadoop中以压缩形式存储的数据,不需要指定解压方式就能够进行读取,因为Hadoop本身有一个解压器会根据压缩文件的后缀推断解压算法进行解压。</li>
<li>如果用Spark从Hadoop中读取某种类型的数据不知道怎么读取的时候,上网查找一个使用map-reduce的时候是怎么读取这种这种数据的,然后再将对应的读取方式改写成兼容版本的的hadoopRDD和newAPIHadoopRDD两个类就行了。</li>
</ol>
<p>读取示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> data = sc.parallelize(<span class="type">Array</span>((<span class="number">30</span>,<span class="string">"hadoop"</span>), (<span class="number">71</span>,<span class="string">"hive"</span>), (<span class="number">11</span>,<span class="string">"cat"</span>)))</span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">47</span>] at parallelize at &lt;console&gt;:<span class="number">35</span></span><br><span class="line"></span><br><span class="line">scala&gt; data.saveAsNewAPIHadoopFile(<span class="string">"hdfs://slave1:9000/output4/"</span>,classOf[<span class="type">LongWritable</span>] ,classOf[<span class="type">Text</span>] ,classOf[org.apache.hadoop.mapreduce.lib.output.<span class="type">TextOutputFormat</span>[<span class="type">LongWritable</span>, <span class="type">Text</span>]])</span><br></pre></td></tr></table></figure>
<p>对于RDD最后的归宿除了返回为集合和标量，也可以将RDD存储到外部文件系统或者数据库中，Spark系统与Hadoop是完全兼容的，所以MapReduce所支持的读写文件或者数据库类型，Spark也同样支持。另外，由于Hadoop的API有新旧两个版本，所以Spark为了能够兼容Hadoop所有的版本，也提供了两套API。将RDD保存到HDFS中在通常情况下需要关注或者设置五个参数，即文件保存的路径，key值的class类型，Value值的class类型，RDD的输出格式(OutputFormat，如TextOutputFormat/SequenceFileOutputFormat)，以及最后一个相关的参数codec(这个参数表示压缩存储的压缩形式，如DefaultCodec，Gzip，Codec等等)。</p>
<table>
<thead>
<tr>
<th>兼容旧版API</th>
</tr>
</thead>
<tbody>
<tr>
<td>saveAsObjectFile(path:   String): Unit</td>
</tr>
<tr>
<td>saveAsTextFile(path:   String, codec: Class[_ &lt;: CompressionCodec]): Unit</td>
</tr>
<tr>
<td>saveAsTextFile(path:   String): Unit</td>
</tr>
<tr>
<td>saveAsHadoopFile[F   &lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unit</td>
</tr>
<tr>
<td>saveAsHadoopFile[F   &lt;: OutputFormat[K, V]](path: String, codec: Class[_ &lt;:   CompressionCodec])(implicit fm: ClassTag[F]): Unit</td>
</tr>
<tr>
<td>saveAsHadoopFile(path:   String, keyClass: Class[<em>], valueClass: Class[</em>], outputFormatClass: Class[<em>&lt;: OutputFormat[</em>, <em>]], codec: Class[</em> &lt;: CompressionCodec]): Unit</td>
</tr>
<tr>
<td>saveAsHadoopDataset(conf:   JobConf): Unit</td>
</tr>
</tbody>
</table>
<p>这里列出的API，前面6个都是saveAsHadoopDataset的简易实现版本，仅仅支持将RDD存储到HDFS中，而saveAsHadoopDataset的参数类型是JobConf，所以其不仅能够将RDD存储到HDFS中，也可以将RDD存储到其他数据库中，如Hbase，MangoDB，Cassandra等。</p>
<table>
<thead>
<tr>
<th>兼容新版API</th>
</tr>
</thead>
<tbody>
<tr>
<td>saveAsNewAPIHadoopFile(path:   String, keyClass: Class[<em>], valueClass: Class[</em>], outputFormatClass: Class[_   &lt;: OutputFormat[_, _]], conf: Configuration =   self.context.hadoopConfiguration): Unit</td>
</tr>
<tr>
<td>saveAsNewAPIHadoopFile[F   &lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unit</td>
</tr>
<tr>
<td>saveAsNewAPIHadoopDataset(conf:   Configuration): Unit</td>
</tr>
</tbody>
</table>
<p>同样的，前2个API是saveAsNewAPIHadoopDataset的简易实现，只能将RDD存到HDFS中，而saveAsNewAPIHadoopDataset比较灵活.新版的API没有codec的参数，所以要压缩存储文件到HDFS中每需要使用hadoopConfiguration参数，设置对应mapreduce.map.output.compress.codec参数和mapreduce.map.output.compress参数。<br>注意：如果不知道怎么将RDD存储到Hadoop生态的系统中，主要上网搜索一下对应的map-reduce是怎么将数据存储进去的，然后改写成对应的saveAsHadoopDataset或saveAsNewAPIHadoopDataset就可以了。</p>
<p>写入示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> read =  sc.newAPIHadoopFile[<span class="type">LongWritable</span>, <span class="type">Text</span>, org.apache.hadoop.mapreduce.lib.input.<span class="type">TextInputFormat</span>](<span class="string">"hdfs://slave1:9000/output3/part*"</span>, classOf[org.apache.hadoop.mapreduce.lib.input.<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>])</span><br><span class="line">read: org.apache.spark.rdd.<span class="type">RDD</span>[(org.apache.hadoop.io.<span class="type">LongWritable</span>, org.apache.hadoop.io.<span class="type">Text</span>)] = hdfs:<span class="comment">//slave1:9000/output3/part* NewHadoopRDD[48] at newAPIHadoopFile at &lt;console&gt;:35</span></span><br><span class="line"></span><br><span class="line">scala&gt; read.map&#123;<span class="keyword">case</span> (k, v) =&gt; v.toString&#125;.collect</span><br><span class="line">res44: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">30</span> hadoop, <span class="number">71</span> hive, <span class="number">11</span> cat)</span><br></pre></td></tr></table></figure>
<h2 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h2><p>Spark支持读写很多种文件系统，像本地文件系统、Amazon S3、HDFS 等。</p>
<h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><h3 id="关系型数据库连接"><a href="#关系型数据库连接" class="headerlink" title="关系型数据库连接"></a>关系型数据库连接</h3><p>支持通过Java JDBC访问关系型数据库。需要通过Jdbc RDD进行，示例如下:<br>MySQL 读取：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span> </span>(args: <span class="type">Array</span>[<span class="type">String</span>] ) &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span> ().setMaster (<span class="string">"local[2]"</span>).setAppName (<span class="string">"JdbcApp"</span>)</span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span> (sparkConf)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> rdd = <span class="keyword">new</span> org.apache.spark.rdd.<span class="type">JdbcRDD</span> (</span><br><span class="line">    sc,</span><br><span class="line">    () =&gt; &#123;</span><br><span class="line">      <span class="type">Class</span>.forName (<span class="string">"com.mysql.jdbc.Driver"</span>).newInstance()</span><br><span class="line">      java.sql.<span class="type">DriverManager</span>.getConnection (<span class="string">"jdbc:mysql://slave1:3306/rdd"</span>, <span class="string">"root"</span>, <span class="string">"hive"</span>)</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"select * from rddtable where id &gt;= ? and id &lt;= ?;"</span>,</span><br><span class="line">    <span class="number">1</span>,</span><br><span class="line">    <span class="number">10</span>,</span><br><span class="line">    <span class="number">1</span>,</span><br><span class="line">    r =&gt; (r.getInt(<span class="number">1</span>), r.getString(<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">  println (rdd.count () )</span><br><span class="line">  rdd.foreach (println (_) )</span><br><span class="line">  sc.stop ()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>MySQL 写入：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"HBaseApp"</span>)</span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">  <span class="keyword">val</span> data = sc.parallelize(<span class="type">List</span>(<span class="string">"Female"</span>, <span class="string">"Male"</span>,<span class="string">"Female"</span>))</span><br><span class="line"></span><br><span class="line">  data.foreachPartition(insertData)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertData</span></span>(iterator: <span class="type">Iterator</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="type">Class</span>.forName (<span class="string">"com.mysql.jdbc.Driver"</span>).newInstance()</span><br><span class="line">  <span class="keyword">val</span> conn = java.sql.<span class="type">DriverManager</span>.getConnection(<span class="string">"jdbc:mysql://slave1:3306/rdd"</span>, <span class="string">"root"</span>, <span class="string">"hive"</span>)</span><br><span class="line">  iterator.foreach(data =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> ps = conn.prepareStatement(<span class="string">"insert into rddtable(name) values (?)"</span>)</span><br><span class="line">    ps.setString(<span class="number">1</span>, data) </span><br><span class="line">    ps.executeUpdate()</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>JdbcRDD 接收这样几个参数：</p>
<ul>
<li>首先，要提供一个用于对数据库创建连接的函数。这个函数让每个节点在连接必要的配 置后创建自己读取数据的连接。 </li>
<li>接下来，要提供一个可以读取一定范围内数据的查询，以及查询参数中lowerBound和 upperBound 的值。这些参数可以让 Spark 在不同机器上查询不同范围的数据，这样就不 会因尝试在一个节点上读取所有数据而遭遇性能瓶颈。</li>
<li>这个函数的最后一个参数是一个可以将输出结果从转为对操作数据有用的格式的函数。如果这个参数空缺，Spark会自动将每行结果转为一个对象数组。 </li>
</ul>
<h3 id="HBase-数据库"><a href="#HBase-数据库" class="headerlink" title="HBase 数据库"></a>HBase 数据库</h3><p>由于 org.apache.hadoop.hbase.mapreduce.TableInputFormat 类的实现，Spark 可以通过Hadoop输入格式访问HBase。这个输入格式会返回键值对数据，其中键的类型为org.apache.hadoop.hbase.io.ImmutableBytesWritable，而值的类型为org.apache.hadoop.hbase.client.Result.<br>HBase 读取：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"HBaseApp"</span>)</span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">  <span class="comment">//HBase中的表名</span></span><br><span class="line">  conf.set(<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>, <span class="string">"fruit"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> hBaseRDD = sc.newAPIHadoopRDD(conf, classOf[<span class="type">TableInputFormat</span>],</span><br><span class="line">    classOf[org.apache.hadoop.hbase.io.<span class="type">ImmutableBytesWritable</span>],</span><br><span class="line">    classOf[org.apache.hadoop.hbase.client.<span class="type">Result</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> count = hBaseRDD.count()</span><br><span class="line">  println(<span class="string">"hBaseRDD RDD Count:"</span>+ count)</span><br><span class="line">  hBaseRDD.cache()</span><br><span class="line">  hBaseRDD.foreach &#123;</span><br><span class="line">    <span class="keyword">case</span> (_, result) =&gt;</span><br><span class="line">      <span class="keyword">val</span> key = <span class="type">Bytes</span>.toString(result.getRow)</span><br><span class="line">      <span class="keyword">val</span> name = <span class="type">Bytes</span>.toString(result.getValue(<span class="string">"info"</span>.getBytes, <span class="string">"name"</span>.getBytes))</span><br><span class="line">      <span class="keyword">val</span> color = <span class="type">Bytes</span>.toString(result.getValue(<span class="string">"info"</span>.getBytes, <span class="string">"color"</span>.getBytes))</span><br><span class="line">      println(<span class="string">"Row key:"</span> + key + <span class="string">" Name:"</span> + name + <span class="string">" Color:"</span> + color)</span><br><span class="line">  &#125;</span><br><span class="line">  sc.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>HBase 写入：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"HBaseApp"</span>)</span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">  <span class="keyword">val</span> jobConf = <span class="keyword">new</span> <span class="type">JobConf</span>(conf)</span><br><span class="line">  jobConf.setOutputFormat(classOf[<span class="type">TableOutputFormat</span>])</span><br><span class="line">  jobConf.set(<span class="type">TableOutputFormat</span>.<span class="type">OUTPUT_TABLE</span>, <span class="string">"fruit_spark"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fruitTable = <span class="type">TableName</span>.valueOf(<span class="string">"fruit_spark"</span>)</span><br><span class="line">  <span class="keyword">val</span> tableDescr = <span class="keyword">new</span> <span class="type">HTableDescriptor</span>(fruitTable)</span><br><span class="line">  tableDescr.addFamily(<span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(<span class="string">"info"</span>.getBytes))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> admin = <span class="keyword">new</span> <span class="type">HBaseAdmin</span>(conf)</span><br><span class="line">  <span class="keyword">if</span> (admin.tableExists(fruitTable)) &#123;</span><br><span class="line">    admin.disableTable(fruitTable)</span><br><span class="line">    admin.deleteTable(fruitTable)</span><br><span class="line">  &#125;</span><br><span class="line">  admin.createTable(tableDescr)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">convert</span></span>(triple: (<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>)) = &#123;</span><br><span class="line">    <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(<span class="type">Bytes</span>.toBytes(triple._1))</span><br><span class="line">    put.addImmutable(<span class="type">Bytes</span>.toBytes(<span class="string">"info"</span>), <span class="type">Bytes</span>.toBytes(<span class="string">"name"</span>), <span class="type">Bytes</span>.toBytes(triple._2))</span><br><span class="line">    put.addImmutable(<span class="type">Bytes</span>.toBytes(<span class="string">"info"</span>), <span class="type">Bytes</span>.toBytes(<span class="string">"price"</span>), <span class="type">Bytes</span>.toBytes(triple._3))</span><br><span class="line">    (<span class="keyword">new</span> <span class="type">ImmutableBytesWritable</span>, put)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> initialRDD = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="string">"apple"</span>,<span class="number">11</span>), (<span class="number">2</span>,<span class="string">"banana"</span>,<span class="number">12</span>), (<span class="number">3</span>,<span class="string">"pear"</span>,<span class="number">13</span>)))</span><br><span class="line">  <span class="keyword">val</span> localData = initialRDD.map(convert)</span><br><span class="line"></span><br><span class="line">  localData.saveAsHadoopDataset(jobConf)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Cassandra-和-ElasticSearch"><a href="#Cassandra-和-ElasticSearch" class="headerlink" title="Cassandra 和 ElasticSearch"></a>Cassandra 和 ElasticSearch</h3><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-Core/35-Cassandra-And-ElasticSearch.png?raw=true" alt="35-Cassandra-And-ElasticSearch"></p>
<h1 id="RDD-编程进阶"><a href="#RDD-编程进阶" class="headerlink" title="RDD 编程进阶"></a>RDD 编程进阶</h1><h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><p>累加器用来对信息进行聚合，通常在向Spark传递函数时，比如使用map()函数或者用filter()传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，更新这些副本的值也不会影响驱动器中的对应变量。如果我们想实现所有分片处理时更新共享变量的功能，那么累加器可以实现我们想要的效果。<br>针对一个输入的日志文件，如果我们想计算文件中所有空行的数量，我们可以编写以下程序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> notice = sc.textFile(<span class="string">"./NOTICE"</span>)</span><br><span class="line">notice: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = ./<span class="type">NOTICE</span> <span class="type">MapPartitionsRDD</span>[<span class="number">40</span>] at textFile at &lt;console&gt;:<span class="number">32</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> blanklines = sc.accumulator(<span class="number">0</span>)</span><br><span class="line">warning: there were two deprecation warnings; re-run <span class="keyword">with</span> -deprecation <span class="keyword">for</span> details</span><br><span class="line">blanklines: org.apache.spark.<span class="type">Accumulator</span>[<span class="type">Int</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> tmp = notice.flatMap(line =&gt; &#123;</span><br><span class="line">     |    <span class="keyword">if</span> (line == <span class="string">""</span>) &#123;</span><br><span class="line">     |       blanklines += <span class="number">1</span></span><br><span class="line">     |    &#125;</span><br><span class="line">     |    line.split(<span class="string">" "</span>)</span><br><span class="line">     | &#125;)</span><br><span class="line">tmp: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">41</span>] at flatMap at &lt;console&gt;:<span class="number">36</span></span><br><span class="line"></span><br><span class="line">scala&gt; tmp.count()</span><br><span class="line">res31: <span class="type">Long</span> = <span class="number">3213</span></span><br><span class="line"></span><br><span class="line">scala&gt; blanklines.value</span><br><span class="line">res32: <span class="type">Int</span> = <span class="number">171</span></span><br></pre></td></tr></table></figure>
<p>累加器的用法如下所示。<br>通过在驱动器中调用SparkContext.accumulator(initialValue)方法，创建出存有初始值的累加器。返回值为org.apache.spark.Accumulator[T]对象，其中T是初始值initialValue的类型。<br>Spark闭包里的执行器代码可以使用累加器的+=方法(在Java中是add)增加累加器的值。 <br>驱动器程序可以调用累加器的value属性(在Java中使用value()或setValue())来访问累加器的值。<br>注意：工作节点上的任务不能访问累加器的值。从这些任务的角度来看，累加器是一个只写变量。<br>对于要在行动操作中使用的累加器，Spark只会把每个任务对各累加器的修改应用一次。因此，如果想要一个无论在失败还是重复计算时都绝对可靠的累加器，我们必须把它放在foreach()这样的行动操作中。转化操作中累加器可能会发生不止一次更新。</p>
<h2 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h2><p>自定义累加器类型的功能在1.X版本中就已经提供了，但是使用起来比较麻烦，在2.0版本后，累加器的易用性有了较大的改进，而且官方还提供了一个新的抽象类：AccumulatorV2来提供更加友好的自定义类型累加器的实现方式。实现自定义类型累加器需要继承AccumulatorV2并至少覆写下例中出现的方法，下面这个累加器可以用于在程序运行过程中收集一些文本类信息，最终以Set[String]的形式返回。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.moqi.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.util.<span class="type">AccumulatorV2</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogAccumulator</span> <span class="keyword">extends</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">util</span>.<span class="title">AccumulatorV2</span>[<span class="type">String</span>, java.util.<span class="type">Set</span>[<span class="type">String</span>]] </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> _logArray: java.util.<span class="type">Set</span>[<span class="type">String</span>] = <span class="keyword">new</span> java.util.<span class="type">HashSet</span>[<span class="type">String</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    _logArray.isEmpty</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    _logArray.clear()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    _logArray.add(v)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: org.apache.spark.util.<span class="type">AccumulatorV2</span>[<span class="type">String</span>, java.util.<span class="type">Set</span>[<span class="type">String</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    other <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> o: <span class="type">LogAccumulator</span> =&gt; _logArray.addAll(o.value)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: java.util.<span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    java.util.<span class="type">Collections</span>.unmodifiableSet(_logArray)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>():org.apache.spark.util.<span class="type">AccumulatorV2</span>[<span class="type">String</span>, java.util.<span class="type">Set</span>[<span class="type">String</span>]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> newAcc = <span class="keyword">new</span> <span class="type">LogAccumulator</span>()</span><br><span class="line">    _logArray.synchronized&#123;</span><br><span class="line">      newAcc._logArray.addAll(_logArray)</span><br><span class="line">    &#125;</span><br><span class="line">    newAcc</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 过滤掉带字母的</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LogAccumulator</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"LogAccumulator"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> accum = <span class="keyword">new</span> <span class="type">LogAccumulator</span></span><br><span class="line">    sc.register(accum, <span class="string">"logAccum"</span>)</span><br><span class="line">    <span class="keyword">val</span> sum = sc.parallelize(<span class="type">Array</span>(<span class="string">"1"</span>, <span class="string">"2a"</span>, <span class="string">"3"</span>, <span class="string">"4b"</span>, <span class="string">"5"</span>, <span class="string">"6"</span>, <span class="string">"7cd"</span>, <span class="string">"8"</span>, <span class="string">"9"</span>), <span class="number">2</span>).filter(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> pattern = <span class="string">""</span><span class="string">"^-?(\d+)"</span><span class="string">""</span></span><br><span class="line">      <span class="keyword">val</span> flag = line.matches(pattern)</span><br><span class="line">      <span class="keyword">if</span> (!flag) &#123;</span><br><span class="line">        accum.add(line)</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;).map(_.toInt).reduce(_ + _)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"sum: "</span> + sum)</span><br><span class="line">    <span class="keyword">for</span> (v &lt;- accum.value) print(v + <span class="string">""</span>)</span><br><span class="line">    println()</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，甚至是机器学习算法中的一个很大的特征向量，广播变量用起来都很顺手。<br>传统方式下，Spark会自动把闭包中所有引用到的变量发送到工作节点上。虽然这很方便，但也很低效。原因有二:首先，默认的任务发射机制是专门为小任务进行优化的；其次，事实上你可能会在多个并行操作中使用同一个变量，但是Spark会为每个任务分别发送。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> broadcastVar = sc.broadcast(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">broadcastVar: org.apache.spark.broadcast.<span class="type">Broadcast</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Broadcast</span>(<span class="number">35</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; broadcastVar.value</span><br><span class="line">res33: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>使用广播变量的过程如下：</p>
<ol>
<li>通过对一个类型 T 的对象调用 SparkContext.broadcast 创建出一个 Broadcast[T] 对象。 任何可序列化的类型都可以这么实现。 </li>
<li>通过 value 属性访问该对象的值(在 Java 中为 value() 方法)。 </li>
<li>变量只会被发到各个节点一次，应作为只读值处理(修改这个值不会影响到别的节点)。</li>
</ol>
<p>End. </p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>您的支持将鼓励我继续创作</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="moqi 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="moqi 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Spark/" rel="tag"># Spark</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/06/16/JVM-GC-Development-Path/" rel="next" title="JVM GC 发展历程">
                <i class="fa fa-chevron-left"></i> JVM GC 发展历程
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/08/10/Spark-SQL-Analysis/" rel="prev" title="Spark SQL 应用解析">
                Spark SQL 应用解析 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://avatars3.githubusercontent.com/u/39821951?s=400&u=65c6d8145d7b591ca2051e7082fd842b56e62567&v=4"
                alt="moqi" />
            
              <p class="site-author-name" itemprop="name">moqi</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="mailto:moqimoqidea@gmail.com" target="_blank" title="E-mail">
                      
                        <i class="fa fa-fw fa-meh-o"></i>E-mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#RDD-概念"><span class="nav-number">1.</span> <span class="nav-text">RDD 概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-为什么会产生"><span class="nav-number">1.1.</span> <span class="nav-text">RDD 为什么会产生</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-概述"><span class="nav-number">1.2.</span> <span class="nav-text">RDD 概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是-RDD"><span class="nav-number">1.2.1.</span> <span class="nav-text">什么是 RDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD-的属性"><span class="nav-number">1.2.2.</span> <span class="nav-text">RDD 的属性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-弹性"><span class="nav-number">1.3.</span> <span class="nav-text">RDD 弹性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-特点"><span class="nav-number">1.4.</span> <span class="nav-text">RDD 特点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#分区"><span class="nav-number">1.4.1.</span> <span class="nav-text">分区</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#只读"><span class="nav-number">1.4.2.</span> <span class="nav-text">只读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#依赖"><span class="nav-number">1.4.3.</span> <span class="nav-text">依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#缓存"><span class="nav-number">1.4.4.</span> <span class="nav-text">缓存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CheckPoint"><span class="nav-number">1.4.5.</span> <span class="nav-text">CheckPoint</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RDD-编程"><span class="nav-number">2.</span> <span class="nav-text">RDD 编程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#编程模型"><span class="nav-number">2.1.</span> <span class="nav-text">编程模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建-RDD"><span class="nav-number">2.2.</span> <span class="nav-text">创建 RDD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-编程-1"><span class="nav-number">2.3.</span> <span class="nav-text">RDD 编程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Transformation"><span class="nav-number">2.3.1.</span> <span class="nav-text">Transformation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Action"><span class="nav-number">2.3.2.</span> <span class="nav-text">Action</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数值-RDD-的统计操作"><span class="nav-number">2.3.3.</span> <span class="nav-text">数值 RDD 的统计操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#向RDD操作传递函数注意"><span class="nav-number">2.3.4.</span> <span class="nav-text">向RDD操作传递函数注意</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#在不同-RDD-类型间转换"><span class="nav-number">2.3.5.</span> <span class="nav-text">在不同 RDD 类型间转换</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-持久化"><span class="nav-number">2.4.</span> <span class="nav-text">RDD 持久化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD-的缓存"><span class="nav-number">2.4.1.</span> <span class="nav-text">RDD 的缓存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD-的缓存方式"><span class="nav-number">2.4.2.</span> <span class="nav-text">RDD 的缓存方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-CheckPoint-机制"><span class="nav-number">2.5.</span> <span class="nav-text">RDD CheckPoint 机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CheckPoint-写流程"><span class="nav-number">2.5.1.</span> <span class="nav-text">CheckPoint 写流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CheckPoint-读流程"><span class="nav-number">2.5.2.</span> <span class="nav-text">CheckPoint 读流程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-的依赖关系"><span class="nav-number">2.6.</span> <span class="nav-text">RDD 的依赖关系</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DAG-的生成"><span class="nav-number">2.7.</span> <span class="nav-text">DAG 的生成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-相关概念关系"><span class="nav-number">2.8.</span> <span class="nav-text">RDD 相关概念关系</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#键值对-RDD"><span class="nav-number">3.</span> <span class="nav-text">键值对 RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Pair-RDD-的-Transformation-操作"><span class="nav-number">3.1.</span> <span class="nav-text">Pair RDD 的 Transformation 操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#转化操作"><span class="nav-number">3.1.1.</span> <span class="nav-text">转化操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#聚合操作"><span class="nav-number">3.1.2.</span> <span class="nav-text">聚合操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据分组"><span class="nav-number">3.1.3.</span> <span class="nav-text">数据分组</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#连接"><span class="nav-number">3.1.4.</span> <span class="nav-text">连接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据排序"><span class="nav-number">3.1.5.</span> <span class="nav-text">数据排序</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pair-RDD-的-Action-操作"><span class="nav-number">3.2.</span> <span class="nav-text">Pair RDD 的 Action 操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pair-RDD-的数据分区"><span class="nav-number">3.3.</span> <span class="nav-text">Pair RDD 的数据分区</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#获取-RDD-的分区方式"><span class="nav-number">3.3.1.</span> <span class="nav-text">获取 RDD 的分区方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hash-分区方式"><span class="nav-number">3.3.2.</span> <span class="nav-text">Hash 分区方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Range-分区方式"><span class="nav-number">3.3.3.</span> <span class="nav-text">Range 分区方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#自定义分区方式"><span class="nav-number">3.3.4.</span> <span class="nav-text">自定义分区方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分区-Shuffle-优化"><span class="nav-number">3.3.5.</span> <span class="nav-text">分区 Shuffle 优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于分区进行操作"><span class="nav-number">3.3.6.</span> <span class="nav-text">基于分区进行操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从分区中获益的操作"><span class="nav-number">3.3.7.</span> <span class="nav-text">从分区中获益的操作</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#数据读取与保存主要方式"><span class="nav-number">4.</span> <span class="nav-text">数据读取与保存主要方式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#文本文件"><span class="nav-number">4.1.</span> <span class="nav-text">文本文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#JSON-文件"><span class="nav-number">4.2.</span> <span class="nav-text">JSON 文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CSV-文件"><span class="nav-number">4.3.</span> <span class="nav-text">CSV 文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SequenceFile-文件"><span class="nav-number">4.4.</span> <span class="nav-text">SequenceFile 文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对象文件"><span class="nav-number">4.5.</span> <span class="nav-text">对象文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hadoop-文件"><span class="nav-number">4.6.</span> <span class="nav-text">Hadoop 文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#文件系统"><span class="nav-number">4.7.</span> <span class="nav-text">文件系统</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据库"><span class="nav-number">4.8.</span> <span class="nav-text">数据库</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#关系型数据库连接"><span class="nav-number">4.8.1.</span> <span class="nav-text">关系型数据库连接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HBase-数据库"><span class="nav-number">4.8.2.</span> <span class="nav-text">HBase 数据库</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cassandra-和-ElasticSearch"><span class="nav-number">4.8.3.</span> <span class="nav-text">Cassandra 和 ElasticSearch</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RDD-编程进阶"><span class="nav-number">5.</span> <span class="nav-text">RDD 编程进阶</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#累加器"><span class="nav-number">5.1.</span> <span class="nav-text">累加器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#自定义累加器"><span class="nav-number">5.2.</span> <span class="nav-text">自定义累加器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#广播变量"><span class="nav-number">5.3.</span> <span class="nav-text">广播变量</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">moqi</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">35.1k</span>
  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://moqimoqidea.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://moqimoqidea.github.io/2017/08/04/Spark-Core-Analysis/';
          this.page.identifier = '2017/08/04/Spark-Core-Analysis/';
          this.page.title = 'Spark Core 应用解析';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://moqimoqidea.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('3');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
