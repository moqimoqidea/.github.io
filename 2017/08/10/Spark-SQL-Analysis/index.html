<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="CgWpgpEd7fSixhMFnnAhPIXT3nCIz9VbArCcXf9ZC0c" />














  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Spark," />





  <link rel="alternate" href="/atom.xml" title="Coding and Talking" type="application/atom+xml" />






<meta name="description" content="本文主要解析部分 Spark SQL 的技术点。">
<meta name="keywords" content="Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark SQL 应用解析">
<meta property="og:url" content="http://moqimoqidea.github.io/2017/08/10/Spark-SQL-Analysis/index.html">
<meta property="og:site_name" content="Coding and Talking">
<meta property="og:description" content="本文主要解析部分 Spark SQL 的技术点。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/01-Spark-SQL-LOGO.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/02-Spark-SQL-Architecture.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/03-RDD-DataFrames-DataSet.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/04-RDD-Vs-DataFrame.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/05-OnHeap-OffHeap-GC.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/06-Spark-Catalyst.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/07-Spark-SQL-Optimization.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/08-Comparisons-Them.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/09-Parquet-File.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/10-Rule-And-RuleExecutor.png?raw=true">
<meta property="og:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/11-Spark-SQL-Run-Architecture.png?raw=true">
<meta property="og:updated_time" content="2018-11-25T07:38:41.517Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark SQL 应用解析">
<meta name="twitter:description" content="本文主要解析部分 Spark SQL 的技术点。">
<meta name="twitter:image" content="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/01-Spark-SQL-LOGO.png?raw=true">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://moqimoqidea.github.io/2017/08/10/Spark-SQL-Analysis/"/>





  <title>Spark SQL 应用解析 | Coding and Talking</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-120200111-1', 'auto');
  ga('send', 'pageview');
</script>





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Coding and Talking</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://moqimoqidea.github.io/2017/08/10/Spark-SQL-Analysis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="moqi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars3.githubusercontent.com/u/39821951?s=400&u=65c6d8145d7b591ca2051e7082fd842b56e62567&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Coding and Talking">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Spark SQL 应用解析</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-10T20:37:27+08:00">
                2017-08-10
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/10/Spark-SQL-Analysis/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/08/10/Spark-SQL-Analysis/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  10,171
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  47
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文主要解析部分 Spark SQL 的技术点。</p>
<a id="more"></a> 
<h1 id="Spark-SQL-概述"><a href="#Spark-SQL-概述" class="headerlink" title="Spark SQL 概述"></a>Spark SQL 概述</h1><h2 id="什么是-Spark-SQL"><a href="#什么是-Spark-SQL" class="headerlink" title="什么是 Spark SQL"></a>什么是 Spark SQL</h2><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/01-Spark-SQL-LOGO.png?raw=true" alt="01-Spark-SQL-LOGO"></p>
<p>Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。<br>我们已经学习了Hive，它是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！</p>
<p>Spark SQL 有四大特点：</p>
<ol>
<li>易整合。</li>
<li>统一的数据访问方式。</li>
<li>兼容 Hive.</li>
<li>标准的数据连接。</li>
</ol>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/02-Spark-SQL-Architecture.png?raw=true" alt="02-Spark-SQL-Architecture"></p>
<p>SparkSQL可以看做是一个转换层，向下对接各种不同的结构化数据源，向上提供不同的数据访问方式。</p>
<h2 id="RDD-vs-DataFrames-vs-DataSet"><a href="#RDD-vs-DataFrames-vs-DataSet" class="headerlink" title="RDD vs DataFrames vs DataSet"></a>RDD vs DataFrames vs DataSet</h2><p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/03-RDD-DataFrames-DataSet.png?raw=true" alt="03-RDD-DataFrames-DataSet"></p>
<p>在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。他们和RDD有什么区别呢？首先从版本的产生上来看：<br>RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt; Dataset(Spark1.6)<br>如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。<br>在后期的Spark版本中，DataSet会逐步取代RDD和DataFrame成为唯一的API接口。</p>
<h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><ul>
<li>RDD是一个懒执行的不可变的可以支持Lambda表达式的并行数据集合。</li>
<li>RDD的最大好处就是简单，API的人性化程度很高。</li>
<li>RDD的劣势是性能限制，它是一个JVM驻内存对象，这也就决定了存在GC的限制和数据增加时Java序列化成本的升高。</li>
</ul>
<h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。由于与R和Pandas的DataFrame类似，Spark DataFrame很好地继承了传统单机数据分析的开发体验。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/04-RDD-Vs-DataFrame.png?raw=true" alt="04-RDD-Vs-DataFrame"></p>
<p>上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。<br>DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待。<br>DataFrame也是懒执行的。<br>性能上比RDD要高，主要有两方面原因： </p>
<ol>
<li>定制化内存管理：数据以二进制的方式存在于非堆内存，节省了大量空间之外，还摆脱了GC的限制。</li>
</ol>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/05-OnHeap-OffHeap-GC.png?raw=true" alt="05-OnHeap-OffHeap-GC"></p>
<ol start="2">
<li>优化的执行计划：查询计划通过Spark catalyst optimiser进行优化。</li>
</ol>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/06-Spark-Catalyst.png?raw=true" alt="06-Spark-Catalyst"></p>
<p>比如下面这个例子：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">users.join(events, users(<span class="string">"id"</span>) === events(<span class="string">"uid"</span>))</span><br><span class="line">	 .filter(events(<span class="string">"date"</span>) &gt; <span class="string">"2015-01-01"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/07-Spark-SQL-Optimization.png?raw=true" alt="07-Spark-SQL-Optimization"></p>
<p>为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。<br>得到的优化执行计划在转换成物 理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。<br>对于普通开发者而言，查询优化器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。<br>Dataframe的劣势在于在编译期缺少类型安全检查，导致运行时出错。</p>
<h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><ul>
<li>是Dataframe API的一个扩展，是Spark最新的数据抽象。</li>
<li>用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。</li>
<li>Dataset支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。</li>
<li>样例类被用来在Dataset中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称。</li>
<li>Dataframe是Dataset的特列，DataFrame=Dataset[Row] ，所以可以通过as方法将Dataframe转换为Dataset。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息我都用Row来表示。</li>
<li>DataSet是强类型的。比如可以有Dataset[Car]，Dataset[Person].</li>
</ul>
<p>DataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比。</p>
<p>RDD让我们能够决定怎么做，而DataFrame和DataSet让我们决定做什么，控制的粒度不一样。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/08-Comparisons-Them.png?raw=true" alt="08-Comparisons-Them"></p>
<h3 id="三者的共性"><a href="#三者的共性" class="headerlink" title="三者的共性"></a>三者的共性</h3><ol>
<li>RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利。</li>
<li>三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkconf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"test"</span>).set(<span class="string">"spark.port.maxRetries"</span>,<span class="string">"1000"</span>)</span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().config(sparkconf).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> rdd=spark.sparkContext.parallelize(<span class="type">Seq</span>((<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">1</span>), (<span class="string">"a"</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="comment">// map不运行</span></span><br><span class="line">rdd.map&#123;line=&gt;</span><br><span class="line">  println(<span class="string">"运行"</span>)</span><br><span class="line">  line._1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出。</li>
<li>三者都有partition的概念。</li>
<li>三者有许多共同的函数，如filter，排序等。</li>
<li>在对DataFrame和Dataset进行操作许多操作都需要这个包进行支持。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure>
<ol start="7">
<li>DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型。<br>DataFrame:</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">testDF.map&#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(col1:<span class="type">String</span>,col2:<span class="type">Int</span>)=&gt;</span><br><span class="line">        println(col1);println(col2)</span><br><span class="line">        col1</span><br><span class="line">      <span class="keyword">case</span> _=&gt;</span><br><span class="line">        <span class="string">""</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>   Dataset:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class">    <span class="title">testDS</span>.<span class="title">map</span></span>&#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Coltest</span>(col1:<span class="type">String</span>,col2:<span class="type">Int</span>)=&gt;</span><br><span class="line">        println(col1);println(col2)</span><br><span class="line">        col1</span><br><span class="line">      <span class="keyword">case</span> _=&gt;</span><br><span class="line">        <span class="string">""</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="三者的区别"><a href="#三者的区别" class="headerlink" title="三者的区别"></a>三者的区别</h3><p><strong>RDD:</strong></p>
<ol>
<li>RDD一般和spark mlib同时使用。</li>
<li>RDD不支持spark sql操作。</li>
</ol>
<p><strong>DataFrame:</strong></p>
<ol>
<li>与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值，如</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">testDF.foreach&#123;</span><br><span class="line">  line =&gt;</span><br><span class="line">    <span class="keyword">val</span> col1=line.getAs[<span class="type">String</span>](<span class="string">"col1"</span>)</span><br><span class="line">    <span class="keyword">val</span> col2=line.getAs[<span class="type">String</span>](<span class="string">"col2"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>   每一列的值没法直接访问</p>
<ol start="2">
<li>DataFrame与Dataset一般不与spark ml同时使用。</li>
<li>DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作，如</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataDF.createOrReplaceTempView(<span class="string">"tmp"</span>)</span><br><span class="line">spark.sql(<span class="string">"select  ROW,DATE from tmp where DATE is not null order by DATE"</span>).show(<span class="number">100</span>,<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//保存</span></span><br><span class="line"><span class="keyword">val</span> saveoptions = <span class="type">Map</span>(<span class="string">"header"</span> -&gt; <span class="string">"true"</span>, <span class="string">"delimiter"</span> -&gt; <span class="string">"\t"</span>, <span class="string">"path"</span> -&gt; <span class="string">"hdfs://master01:9000/test"</span>)</span><br><span class="line">datawDF.write.format(<span class="string">"com.atguigu.spark.csv"</span>).mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).options(saveoptions).save()</span><br><span class="line"><span class="comment">//读取</span></span><br><span class="line"><span class="keyword">val</span> options = <span class="type">Map</span>(<span class="string">"header"</span> -&gt; <span class="string">"true"</span>, <span class="string">"delimiter"</span> -&gt; <span class="string">"\t"</span>, <span class="string">"path"</span> -&gt; <span class="string">"hdfs://master01:9000/test"</span>)</span><br><span class="line"><span class="keyword">val</span> datarDF= spark.read.options(options).format(<span class="string">"com.atguigu.spark.csv"</span>).load()</span><br></pre></td></tr></table></figure>
<p>利用这样的保存方式，可以方便的获得字段名和列的对应，而且分隔符（delimiter）可以自由指定。</p>
<p><strong>Dataset:</strong></p>
<ol>
<li>Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同。</li>
<li>DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的getAS方法或者共性中的第七条提到的模式匹配拿出特定字段。</li>
<li>而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class"><span class="title">/**</span></span></span><br><span class="line"><span class="class"> <span class="title">rdd</span></span></span><br><span class="line"><span class="class"> (<span class="params">"a", 1</span>)</span></span><br><span class="line"><span class="class"> (<span class="params">"b", 1</span>)</span></span><br><span class="line"><span class="class"> (<span class="params">"a", 1</span>)</span></span><br><span class="line"><span class="class"><span class="title">**/</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">test</span></span>: <span class="type">Dataset</span>[<span class="type">Coltest</span>]=rdd.map&#123;line=&gt;</span><br><span class="line">      <span class="type">Coltest</span>(line._1,line._2)</span><br><span class="line">    &#125;.toDS</span><br><span class="line">test.map&#123;</span><br><span class="line">      line=&gt;</span><br><span class="line">        println(line.col1)</span><br><span class="line">        println(line.col2)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>可以看出，Dataset在需要访问列中的某个字段时是非常方便的，然而，如果要写一些适配性很强的函数时，如果使用Dataset，行的类型又不确定，可能是各种case class，无法实现适配，这时候用DataFrame即Dataset[Row]就能比较好的解决问题。</p>
<h1 id="执行-Spark-SQL-查询"><a href="#执行-Spark-SQL-查询" class="headerlink" title="执行 Spark SQL 查询"></a>执行 Spark SQL 查询</h1><h2 id="命令行查询流程"><a href="#命令行查询流程" class="headerlink" title="命令行查询流程"></a>命令行查询流程</h2><p>打开Spark shell<br>例子：查询大于30岁的用户<br>创建如下JSON文件，注意JSON的格式：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Michael"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Andy"</span>, <span class="attr">"age"</span>:<span class="number">30</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Justin"</span>, <span class="attr">"age"</span>:<span class="number">19</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">"/opt/txt_data/json.txt"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">"persons"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">"SELECT * FROM persons"</span>).show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure>
<h2 id="IDEA-创建-Spark-SQL-程序"><a href="#IDEA-创建-Spark-SQL-程序" class="headerlink" title="IDEA 创建 Spark SQL 程序"></a>IDEA 创建 Spark SQL 程序</h2><p>Maven 依赖：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>
<p>程序如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.moqi.sparksql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.<span class="type">LoggerFactory</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloWorld</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> logger = <span class="type">LoggerFactory</span>.getLogger(<span class="type">HelloWorld</span>.getClass)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">//创建SparkConf()并设置App名称</span></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"Spark SQL basic example"</span>)</span><br><span class="line">      .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// For implicit conversions like converting RDDs to DataFrames</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">    df.filter($<span class="string">"age"</span> &gt; <span class="number">21</span>).show()</span><br><span class="line"></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"persons"</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">"SELECT * FROM persons where age &gt; 21"</span>).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Spark-SQL-解析"><a href="#Spark-SQL-解析" class="headerlink" title="Spark SQL 解析"></a>Spark SQL 解析</h1><h2 id="新的起点：SparkSession"><a href="#新的起点：SparkSession" class="headerlink" title="新的起点：SparkSession"></a>新的起点：SparkSession</h2><p>在老的版本中，SparkSQL提供两种SQL查询起始点，一个叫SQLContext，用于Spark自己提供的SQL查询，一个叫HiveContext，用于连接Hive的查询，SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">.builder()</span><br><span class="line">.appName(<span class="string">"Spark SQL basic example"</span>)</span><br><span class="line">.config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">// For implicit conversions like converting RDDs to DataFrames</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure>
<p>SparkSession.builder 用于创建一个SparkSession。<br>import spark.implicits._的引入是用于将DataFrames隐式转换成RDD，使df能够使用RDD中的方法。<br>如果需要Hive支持，则需要以下创建语句：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">.builder()</span><br><span class="line">.appName(<span class="string">"Spark SQL basic example"</span>)</span><br><span class="line">.config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment">// For implicit conversions like converting RDDs to DataFrames</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure>
<h2 id="创建-DataFrames"><a href="#创建-DataFrames" class="headerlink" title="创建 DataFrames"></a>创建 DataFrames</h2><p>在Spark SQL中SparkSession是创建DataFrames和执行SQL的入口，创建DataFrames有三种方式，一种是可以从一个存在的RDD进行转换，还可以从Hive Table进行查询返回，或者通过Spark的数据源进行创建。<br>从Spark数据源进行创建：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)	</span><br><span class="line"><span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>
<p>从 RDD 进行转换：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">Michael, 29</span></span><br><span class="line"><span class="comment">Andy, 30</span></span><br><span class="line"><span class="comment">Justin, 19</span></span><br><span class="line"><span class="comment">**/</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> peopleRdd = sc.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</span><br><span class="line">peopleRdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = examples/src/main/resources/people.txt <span class="type">MapPartitionsRDD</span>[<span class="number">18</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> peopleDF3 = peopleRdd.map(_.split(<span class="string">","</span>)).map(paras =&gt; (paras(<span class="number">0</span>),paras(<span class="number">1</span>).trim().toInt)).toDF(<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">peopleDF3: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.show()</span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">|<span class="type">Michael</span>| <span class="number">29</span>|</span><br><span class="line">|   <span class="type">Andy</span>| <span class="number">30</span>|</span><br><span class="line">| <span class="type">Justin</span>| <span class="number">19</span>|</span><br><span class="line">+-------+---+</span><br></pre></td></tr></table></figure>
<p>Hive 部分在后面数据源介绍。</p>
<h2 id="DataFrame-常用操作"><a href="#DataFrame-常用操作" class="headerlink" title="DataFrame 常用操作"></a>DataFrame 常用操作</h2><h3 id="DSL-风格语法"><a href="#DSL-风格语法" class="headerlink" title="DSL 风格语法"></a>DSL 风格语法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This import is needed to use the $-notation</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="comment">// Print the schema in a tree format</span></span><br><span class="line">df.printSchema()</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">// |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select only the "name" column</span></span><br><span class="line">df.select(<span class="string">"name"</span>).show()</span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |   name|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"><span class="comment">// |Michael|</span></span><br><span class="line"><span class="comment">// |   Andy|</span></span><br><span class="line"><span class="comment">// | Justin|</span></span><br><span class="line"><span class="comment">// +-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select everybody, but increment the age by 1</span></span><br><span class="line">df.select($<span class="string">"name"</span>, $<span class="string">"age"</span> + <span class="number">1</span>).show()</span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |   name|(age + 1)|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |Michael|     null|</span></span><br><span class="line"><span class="comment">// |   Andy|       31|</span></span><br><span class="line"><span class="comment">// | Justin|       20|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select people older than 21</span></span><br><span class="line">df.filter($<span class="string">"age"</span> &gt; <span class="number">21</span>).show()</span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// |age|name|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"><span class="comment">// | 30|Andy|</span></span><br><span class="line"><span class="comment">// +---+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Count people by age</span></span><br><span class="line">df.groupBy(<span class="string">"age"</span>).count().show()</span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// | age|count|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br><span class="line"><span class="comment">// |  19|    1|</span></span><br><span class="line"><span class="comment">// |null|    1|</span></span><br><span class="line"><span class="comment">// |  30|    1|</span></span><br><span class="line"><span class="comment">// +----+-----+</span></span><br></pre></td></tr></table></figure>
<h3 id="SQL-风格语法"><a href="#SQL-风格语法" class="headerlink" title="SQL 风格语法"></a>SQL 风格语法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Register the DataFrame as a SQL temporary view</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM people"</span>)</span><br><span class="line">sqlDF.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Register the DataFrame as a global temporary view</span></span><br><span class="line">df.createGlobalTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Global temporary view is tied to a system preserved database `global_temp`</span></span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Global temporary view is cross-session</span></span><br><span class="line">spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>
<p>临时表是Session范围内的，Session退出后，表就失效了。如果想应用范围内有效，可以使用全局表。注意使用全局表时需要全路径访问，如：global_temp.people.</p>
<h2 id="创建-DataSet"><a href="#创建-DataSet" class="headerlink" title="创建 DataSet"></a>创建 DataSet</h2><p>Dataset是具有强类型的数据集合，需要提供对应的类型信息。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,</span></span><br><span class="line"><span class="comment">// you can use custom classes that implement the Product interface</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">Encoders</span> <span class="title">are</span> <span class="title">created</span> <span class="title">for</span> <span class="title">case</span> <span class="title">classes</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">caseClassDS</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Andy"</span>, <span class="number">32</span>)).toDS()</span><br><span class="line">caseClassDS.show()</span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"><span class="comment">// |name|age|</span></span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"><span class="comment">// |Andy| 32|</span></span><br><span class="line"><span class="comment">// +----+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Encoders for most common types are automatically provided by importing spark.implicits._</span></span><br><span class="line"><span class="keyword">val</span> primitiveDS = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).toDS()</span><br><span class="line">primitiveDS.map(_ + <span class="number">1</span>).collect() <span class="comment">// Returns: Array(2, 3, 4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name</span></span><br><span class="line"><span class="keyword">val</span> path = <span class="string">"examples/src/main/resources/people.json"</span></span><br><span class="line"><span class="keyword">val</span> peopleDS = spark.read.json(path).as[<span class="type">Person</span>]</span><br><span class="line">peopleDS.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>
<h2 id="DataSet-和-RDD-互操作"><a href="#DataSet-和-RDD-互操作" class="headerlink" title="DataSet 和 RDD 互操作"></a>DataSet 和 RDD 互操作</h2><p>Spark SQL支持通过两种方式将存在的RDD转换为Dataset，转换的过程中需要让Dataset获取RDD中的Schema信息，主要有两种方式，一种是通过反射来获取RDD中的Schema信息。这种方式适合于列名已知的情况下。第二种是通过编程接口的方式将Schema信息应用于RDD，这种方式可以处理那种在运行时才能知道列的方式。</p>
<h3 id="通过反射获取-Schema"><a href="#通过反射获取-Schema" class="headerlink" title="通过反射获取 Schema"></a>通过反射获取 Schema</h3><p>SparkSQL能够自动将包含有case类的RDD转换成DataFrame，case类定义了table的结构，case类属性通过反射变成了表的列名。Case类可以包含诸如Seqs或者Array等复杂的结构。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// For implicit conversions from RDDs to DataFrames</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD of Person objects from a text file, convert it to a Dataframe</span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.sparkContext</span><br><span class="line">.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</span><br><span class="line">.map(_.split(<span class="string">","</span>))</span><br><span class="line">.map(attributes =&gt; <span class="type">Person</span>(attributes(<span class="number">0</span>), attributes(<span class="number">1</span>).trim.toInt))</span><br><span class="line">.toDF()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Register the DataFrame as a temporary view</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by Spark</span></span><br><span class="line"><span class="keyword">val</span> teenagersDF = spark.sql(<span class="string">"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index  ROW object</span></span><br><span class="line">teenagersDF.map(teenager =&gt; <span class="string">"Name: "</span> + teenager(<span class="number">0</span>)).show()</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// or by field name</span></span><br><span class="line">teenagersDF.map(teenager =&gt; <span class="string">"Name: "</span> + teenager.getAs[<span class="type">String</span>](<span class="string">"name"</span>)).show()</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// No pre-defined encoders for Dataset[Map[K,V]], define explicitly</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> mapEncoder = org.apache.spark.sql.<span class="type">Encoders</span>.kryo[<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Any</span>]]</span><br><span class="line"><span class="comment">// Primitive types and case classes can be also defined as</span></span><br><span class="line"><span class="comment">// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span></span><br><span class="line">teenagersDF.map(teenager =&gt; teenager.getValuesMap[<span class="type">Any</span>](<span class="type">List</span>(<span class="string">"name"</span>, <span class="string">"age"</span>))).collect()</span><br><span class="line"><span class="comment">// Array(Map("name" -&gt; "Justin", "age" -&gt; 19))</span></span><br></pre></td></tr></table></figure>
<h3 id="通过编程设置-Schema"><a href="#通过编程设置-Schema" class="headerlink" title="通过编程设置 Schema"></a>通过编程设置 Schema</h3><p>如果case类不能够提前定义，可以通过下面三个步骤定义一个DataFrame<br>创建一个多行结构的RDD;<br>创建用StructType来表示的行结构信息。<br>通过SparkSession提供的createDataFrame方法来应用Schema.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD</span></span><br><span class="line"><span class="keyword">val</span> peopleRDD = spark.sparkContext.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The schema is encoded in a string,应该是动态通过程序生成的</span></span><br><span class="line"><span class="keyword">val</span> schemaString = <span class="string">"name age"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Generate the schema based on the string of schema   Array[StructFiled]</span></span><br><span class="line"><span class="keyword">val</span> fields = schemaString.split(<span class="string">" "</span>)</span><br><span class="line">.map(fieldName =&gt; <span class="type">StructField</span>(fieldName, <span class="type">StringType</span>, nullable = <span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// val filed = schemaString.split(" ").map(filename=&gt; filename match&#123; case "name"=&gt; StructField(filename,StringType,nullable = true); case "age"=&gt;StructField(filename, IntegerType,nullable = true)&#125; )</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(fields)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Convert records of the RDD (people) to Rows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"><span class="keyword">val</span> rowRDD = peopleRDD</span><br><span class="line">.map(_.split(<span class="string">","</span>))</span><br><span class="line">.map(attributes =&gt; <span class="type">Row</span>(attributes(<span class="number">0</span>), attributes(<span class="number">1</span>).trim))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Apply the schema to the RDD</span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL can be run over a temporary view created using DataFrames</span></span><br><span class="line"><span class="keyword">val</span> results = spark.sql(<span class="string">"SELECT name FROM people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations</span></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index or by field name</span></span><br><span class="line">results.map(attributes =&gt; <span class="string">"Name: "</span> + attributes(<span class="number">0</span>)).show()</span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |        value|</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |Name: Michael|</span></span><br><span class="line"><span class="comment">// |   Name: Andy|</span></span><br><span class="line"><span class="comment">// | Name: Justin|</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br></pre></td></tr></table></figure>
<h2 id="类型之间的转换总结"><a href="#类型之间的转换总结" class="headerlink" title="类型之间的转换总结"></a>类型之间的转换总结</h2><p>RDD、DataFrame、Dataset三者有许多共性，有各自适用的场景常常需要在三者之间转换<br><strong>DataFrame/Dataset转RDD：</strong><br>这个转换很简单：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1=testDF.rdd</span><br><span class="line"><span class="keyword">val</span> rdd2=testDS.rdd</span><br></pre></td></tr></table></figure>
<p><strong>RDD转DataFrame：</strong></p>
<p>一般用元组把一行的数据写在一起，然后在toDF中指定字段名。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> testDF = rdd.map &#123;line=&gt;</span><br><span class="line">      (line._1,line._2)</span><br><span class="line">    &#125;.toDF(<span class="string">"col1"</span>,<span class="string">"col2"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>RDD转DataSet：</strong></p>
<p>可以注意到，定义每一行的类型（case class）时，已经给出了字段名和类型，后面只要往case class里面添加值即可。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">testDS</span> </span>= rdd.map &#123;line=&gt;</span><br><span class="line">      <span class="type">Coltest</span>(line._1,line._2)</span><br><span class="line">    &#125;.toDS</span><br></pre></td></tr></table></figure>
<p><strong>DataSet转DataFrame：</strong></p>
<p>这个也很简单，因为只是把case class封装成Row.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> testDF = testDS.toDF</span><br></pre></td></tr></table></figure>
<p><strong>DataFrame转DataSet：</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Coltest</span>(<span class="params">col1:<span class="type">String</span>,col2:<span class="type">Int</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> <span class="title">//定义字段名和类型</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">testDS</span> </span>= testDF.as[<span class="type">Coltest</span>]</span><br></pre></td></tr></table></figure>
<p>这种方法就是在给出每一列的类型后，使用as方法，转成Dataset，这在数据类型是DataFrame又需要针对各个字段处理时极为方便。<br>在使用一些特殊的操作时，一定要加上 import spark.implicits._ 不然toDF、toDS无法使用。</p>
<h2 id="用户自定义函数"><a href="#用户自定义函数" class="headerlink" title="用户自定义函数"></a>用户自定义函数</h2><p>通过spark.udf功能用户可以自定义函数。</p>
<h3 id="用户自定义UDF函数"><a href="#用户自定义UDF函数" class="headerlink" title="用户自定义UDF函数"></a>用户自定义UDF函数</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; spark.udf.register(<span class="string">"addName"</span>, (x:<span class="type">String</span>)=&gt; <span class="string">"Name:"</span>+x)</span><br><span class="line">res5: org.apache.spark.sql.expressions.<span class="type">UserDefinedFunction</span> = <span class="type">UserDefinedFunction</span>(&lt;function1&gt;,<span class="type">StringType</span>,<span class="type">Some</span>(<span class="type">List</span>(<span class="type">StringType</span>)))</span><br><span class="line"></span><br><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">"Select addName(name), age from people"</span>).show()</span><br><span class="line">+-----------------+----+</span><br><span class="line">|<span class="type">UDF</span>:addName(name)| age|</span><br><span class="line">+-----------------+----+</span><br><span class="line">|     <span class="type">Name</span>:<span class="type">Michael</span>|<span class="literal">null</span>|</span><br><span class="line">|        <span class="type">Name</span>:<span class="type">Andy</span>|  <span class="number">30</span>|</span><br><span class="line">|      <span class="type">Name</span>:<span class="type">Justin</span>|  <span class="number">19</span>|</span><br><span class="line">+-----------------+----+</span><br></pre></td></tr></table></figure>
<h3 id="用户自定义聚合函数"><a href="#用户自定义聚合函数" class="headerlink" title="用户自定义聚合函数"></a>用户自定义聚合函数</h3><p>强类型的Dataset和弱类型的DataFrame都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。</p>
<h4 id="弱类型"><a href="#弱类型" class="headerlink" title="弱类型"></a>弱类型</h4><p>通过继承UserDefinedAggregateFunction来实现用户自定义聚合函数。下面展示一个求平均工资的自定义聚合函数。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">MutableAggregationBuffer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">UserDefinedAggregateFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line"><span class="comment">// 聚合函数输入参数的数据类型 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"inputColumn"</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span><br><span class="line"><span class="comment">// 聚合缓冲区中值得数据类型 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line"><span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"sum"</span>, <span class="type">LongType</span>) :: <span class="type">StructField</span>(<span class="string">"count"</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 返回值的数据类型 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"><span class="comment">// 对于相同的输入是否一直返回相同的输出。 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">// 初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="comment">// 存工资的总额</span></span><br><span class="line">buffer(<span class="number">0</span>) = <span class="number">0</span>L</span><br><span class="line"><span class="comment">// 存工资的个数</span></span><br><span class="line">buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 相同Execute间的数据合并。 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>)</span><br><span class="line">buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 不同Execute间的数据合并 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>)</span><br><span class="line">buffer1(<span class="number">1</span>) = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 计算最终结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Double</span> = buffer.getLong(<span class="number">0</span>).toDouble / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册函数</span></span><br><span class="line">spark.udf.register(<span class="string">"myAverage"</span>, <span class="type">MyAverage</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">"examples/src/main/resources/employees.json"</span>)</span><br><span class="line">df.createOrReplaceTempView(<span class="string">"employees"</span>)</span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |   name|salary|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |Michael|  3000|</span></span><br><span class="line"><span class="comment">// |   Andy|  4500|</span></span><br><span class="line"><span class="comment">// | Justin|  3500|</span></span><br><span class="line"><span class="comment">// |  Berta|  4000|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> result = spark.sql(<span class="string">"SELECT myAverage(salary) as average_salary FROM employees"</span>)</span><br><span class="line">result.show()</span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |average_salary|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |        3750.0|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br></pre></td></tr></table></figure>
<h4 id="强类型"><a href="#强类型" class="headerlink" title="强类型"></a>强类型</h4><p>通过继承Aggregator来实现强类型自定义聚合函数，同样是求平均工资。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Encoder</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Encoders</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">// 既然是强类型，可能有case类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span>(<span class="params">name: <span class="type">String</span>, salary: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Average</span>(<span class="params">var sum: <span class="type">Long</span>, var count: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">MyAverage</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Employee</span>, <span class="type">Average</span>, <span class="type">Double</span>] </span>&#123;</span><br><span class="line"><span class="comment">// 定义一个数据结构，保存工资总数和工资总个数，初始都为0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Average</span> = <span class="type">Average</span>(<span class="number">0</span>L, <span class="number">0</span>L)</span><br><span class="line"><span class="comment">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span></span><br><span class="line"><span class="comment">// and return it instead of constructing a new object</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buffer: <span class="type">Average</span>, employee: <span class="type">Employee</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">buffer.sum += employee.salary</span><br><span class="line">buffer.count += <span class="number">1</span></span><br><span class="line">buffer</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 聚合不同execute的结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">Average</span>, b2: <span class="type">Average</span>): <span class="type">Average</span> = &#123;</span><br><span class="line">b1.sum += b2.sum</span><br><span class="line">b1.count += b2.count</span><br><span class="line">b1</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 计算输出</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">Average</span>): <span class="type">Double</span> = reduction.sum.toDouble / reduction.count</span><br><span class="line"><span class="comment">// 设定之间值类型的编码器，要转换成case类</span></span><br><span class="line"><span class="comment">// Encoders.product是进行scala元组和case类转换的编码器 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Average</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"><span class="comment">// 设定最终输出值的编码器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds = spark.read.json(<span class="string">"examples/src/main/resources/employees.json"</span>).as[<span class="type">Employee</span>]</span><br><span class="line">ds.show()</span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |   name|salary|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"><span class="comment">// |Michael|  3000|</span></span><br><span class="line"><span class="comment">// |   Andy|  4500|</span></span><br><span class="line"><span class="comment">// | Justin|  3500|</span></span><br><span class="line"><span class="comment">// |  Berta|  4000|</span></span><br><span class="line"><span class="comment">// +-------+------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Convert the function to a `TypedColumn` and give it a name</span></span><br><span class="line"><span class="keyword">val</span> averageSalary = <span class="type">MyAverage</span>.toColumn.name(<span class="string">"average_salary"</span>)</span><br><span class="line"><span class="keyword">val</span> result = ds.select(averageSalary)</span><br><span class="line">result.show()</span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |average_salary|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br><span class="line"><span class="comment">// |        3750.0|</span></span><br><span class="line"><span class="comment">// +--------------+</span></span><br></pre></td></tr></table></figure>
<h1 id="Spark-SQL-数据源"><a href="#Spark-SQL-数据源" class="headerlink" title="Spark SQL 数据源"></a>Spark SQL 数据源</h1><h2 id="通用加载-保存方法"><a href="#通用加载-保存方法" class="headerlink" title="通用加载 / 保存方法"></a>通用加载 / 保存方法</h2><h3 id="手动指定选项"><a href="#手动指定选项" class="headerlink" title="手动指定选项"></a>手动指定选项</h3><p>Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。<br>Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。修改配置项spark.sql.sources.default，可修改默认数据源格式。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.load(<span class="string">"examples/src/main/resources/users.parquet"</span>) df.select(<span class="string">"name"</span>, <span class="string">"favorite_color"</span>).write.save(<span class="string">"namesAndFavColors.parquet"</span>)</span><br></pre></td></tr></table></figure>
<p>当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：org.apache.spark.sql.parquet），如果数据源格式为内置格式，则只需要指定简称定json, parquet, jdbc, orc, libsvm, csv, text来指定数据的格式。<br>可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> peopleDF = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line">peopleDF.write.format(<span class="string">"parquet"</span>).save(<span class="string">"hdfs://master01:9000/namesAndAges.parquet"</span>)</span><br></pre></td></tr></table></figure>
<p>除此之外，可以直接运行SQL在文件上：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM parquet.`hdfs://master01:9000/namesAndAges.parquet`"</span>)</span><br><span class="line">sqlDF.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> peopleDF = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line">peopleDF: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.write.format(<span class="string">"parquet"</span>).save(<span class="string">"hdfs://master01:9000/namesAndAges.parquet"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM parquet.`hdfs://master01:9000/namesAndAges.parquet`"</span>)</span><br><span class="line"><span class="number">17</span>/<span class="number">09</span>/<span class="number">05</span> <span class="number">04</span>:<span class="number">21</span>:<span class="number">11</span> <span class="type">WARN</span> <span class="type">ObjectStore</span>: <span class="type">Failed</span> to get database parquet, returning <span class="type">NoSuchObjectException</span></span><br><span class="line">sqlDF: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; sqlDF.show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure>
<h3 id="文件保存选项"><a href="#文件保存选项" class="headerlink" title="文件保存选项"></a>文件保存选项</h3><p>可以采用SaveMode执行存储操作，SaveMode定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。SaveMode详细介绍如下表：</p>
<table>
<thead>
<tr>
<th><strong>Scala/Java</strong></th>
<th><strong>Any Language</strong></th>
<th><strong>Meaning</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SaveMode.ErrorIfExists(default)</strong></td>
<td>“error”(default)</td>
<td>如果文件存在，则报错</td>
</tr>
<tr>
<td><strong>SaveMode.Append</strong></td>
<td>“append”</td>
<td>追加</td>
</tr>
<tr>
<td><strong>SaveMode.Overwrite</strong></td>
<td>“overwrite”</td>
<td>覆写</td>
</tr>
<tr>
<td><strong>SaveMode.Ignore</strong></td>
<td>“ignore”</td>
<td>数据存在，则忽略</td>
</tr>
</tbody>
</table>
<h2 id="Parquet-文件"><a href="#Parquet-文件" class="headerlink" title="Parquet 文件"></a>Parquet 文件</h2><p>Parquet是一种流行的列式存储格式，可以高效地存储具有嵌套字段的记录。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/09-Parquet-File.png?raw=true" alt="09-Parquet-File"></p>
<h3 id="Parquet-读写"><a href="#Parquet-读写" class="headerlink" title="Parquet 读写"></a>Parquet 读写</h3><p>Parquet格式经常在Hadoop生态圈中被使用，它也支持Spark SQL的全部数据类型。Spark SQL 提供了直接读取和存储 Parquet 格式文件的方法。 </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Encoders for most common types are automatically provided by importing spark.implicits._</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrames can be saved as Parquet files, maintaining the schema information</span></span><br><span class="line">peopleDF.write.parquet(<span class="string">"hdfs://master01:9000/people.parquet"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read in the parquet file created above</span></span><br><span class="line"><span class="comment">// Parquet files are self-describing so the schema is preserved</span></span><br><span class="line"><span class="comment">// The result of loading a Parquet file is also a DataFrame</span></span><br><span class="line"><span class="keyword">val</span> parquetFileDF = spark.read.parquet(<span class="string">"hdfs://master01:9000/people.parquet"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Parquet files can also be used to create a temporary view and then used in SQL statements</span></span><br><span class="line">parquetFileDF.createOrReplaceTempView(<span class="string">"parquetFile"</span>)</span><br><span class="line"><span class="keyword">val</span> namesDF = spark.sql(<span class="string">"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19"</span>)</span><br><span class="line">namesDF.map(attributes =&gt; <span class="string">"Name: "</span> + attributes(<span class="number">0</span>)).show()</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       value|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |Name: Justin|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br></pre></td></tr></table></figure>
<h3 id="解析分区信息"><a href="#解析分区信息" class="headerlink" title="解析分区信息"></a>解析分区信息</h3><p>对表进行分区是对数据进行优化的方式之一。在分区的表内，数据通过分区列将数据存储在不同的目录下。Parquet数据源现在能够自动发现并解析分区信息。例如，对人口数据进行分区存储，分区列为gender和country，使用下面的目录结构：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">path</span><br><span class="line">└── to</span><br><span class="line">    └── table</span><br><span class="line">        ├── gender=male</span><br><span class="line">        │   ├── ...</span><br><span class="line">        │   │</span><br><span class="line">        │   ├── country=US</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   ├── country=CN</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   └── ...</span><br><span class="line">        └── gender=female</span><br><span class="line">            ├── ...</span><br><span class="line">            │</span><br><span class="line">            ├── country=US</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            ├── country=CN</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            └── ...</span><br></pre></td></tr></table></figure>
<p>通过传递path/to/table给 SQLContext.read.parquet或SQLContext.read.load，Spark SQL将自动解析分区信息。返回的DataFrame的Schema如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- name: string (nullable = <span class="literal">true</span>)</span><br><span class="line">|-- age: long (nullable = <span class="literal">true</span>)</span><br><span class="line">|-- gender: string (nullable = <span class="literal">true</span>)</span><br><span class="line">|-- country: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>
<p>需要注意的是，数据的分区列的数据类型是自动解析的。当前，支持数值类型和字符串类型。自动解析分区类型的参数为：spark.sql.sources.partitionColumnTypeInference.enabled，默认值为true。如果想关闭该功能，直接将该参数设置为disabled。此时，分区列数据格式将被默认设置为string类型，不再进行类型解析。</p>
<h3 id="Schema-合并"><a href="#Schema-合并" class="headerlink" title="Schema 合并"></a>Schema 合并</h3><p>像ProtocolBuffer、Avro和Thrift那样，Parquet也支持Schema evolution（Schema演变）。用户可以先定义一个简单的Schema，然后逐渐的向Schema中增加列描述。通过这种方式，用户可以获取多个有不同Schema但相互兼容的Parquet文件。现在Parquet数据源能自动检测这种情况，并合并这些文件的schemas。<br>因为Schema合并是一个高消耗的操作，在大多数情况下并不需要，所以Spark SQL从1.5.0开始默认关闭了该功能。可以通过下面两种方式开启该功能：<br>当数据源为Parquet文件时，将数据源选项mergeSchema设置为true<br>设置全局SQL选项spark.sql.parquet.mergeSchema为true<br>示例如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sqlContext from the previous example is used in this example.</span></span><br><span class="line"><span class="comment">// This is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a simple DataFrame, stored into a partition directory</span></span><br><span class="line"><span class="keyword">val</span> df1 = sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>).map(i =&gt; (i, i * <span class="number">2</span>)).toDF(<span class="string">"single"</span>, <span class="string">"double"</span>)</span><br><span class="line">df1.write.parquet(<span class="string">"hdfs://master01:9000/data/test_table/key=1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create another DataFrame in a new partition directory,</span></span><br><span class="line"><span class="comment">// adding a new column and dropping an existing column</span></span><br><span class="line"><span class="keyword">val</span> df2 = sc.makeRDD(<span class="number">6</span> to <span class="number">10</span>).map(i =&gt; (i, i * <span class="number">3</span>)).toDF(<span class="string">"single"</span>, <span class="string">"triple"</span>)</span><br><span class="line">df2.write.parquet(<span class="string">"hdfs://master01:9000/data/test_table/key=2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read the partitioned table</span></span><br><span class="line"><span class="keyword">val</span> df3 = spark.read.option(<span class="string">"mergeSchema"</span>, <span class="string">"true"</span>).parquet(<span class="string">"hdfs://master01:9000/data/test_table"</span>)</span><br><span class="line">df3.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment">// The final schema consists of all 3 columns in the Parquet files together</span></span><br><span class="line"><span class="comment">// with the partitioning column appeared in the partition directory paths.</span></span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">// |-- single: int (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- double: int (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- triple: int (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- key : int (nullable = true)</span></span><br></pre></td></tr></table></figure>
<h2 id="Hive-数据库"><a href="#Hive-数据库" class="headerlink" title="Hive 数据库"></a>Hive 数据库</h2><p>Apache Hive是Hadoop上的SQL引擎，Spark SQL编译时可以包含Hive支持，也可以不包含。包含Hive支持的Spark SQL可以支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的 一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。<br>若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好Hive，Spark SQL也可以运行。 需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span>(<span class="params">key: <span class="type">Int</span>, value: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">//</span> <span class="title">warehouseLocation</span> <span class="title">points</span> <span class="title">to</span> <span class="title">the</span> <span class="title">default</span> <span class="title">location</span> <span class="title">for</span> <span class="title">managed</span> <span class="title">databases</span> <span class="title">and</span> <span class="title">tables</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">warehouseLocation</span> </span>= <span class="keyword">new</span> <span class="type">File</span>(<span class="string">"spark-warehouse"</span>).getAbsolutePath</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">.builder()</span><br><span class="line">.appName(<span class="string">"Spark Hive Example"</span>)</span><br><span class="line">.config(<span class="string">"spark.sql.warehouse.dir"</span>, warehouseLocation)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">import</span> spark.sql</span><br><span class="line"></span><br><span class="line">sql(<span class="string">"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)"</span>)</span><br><span class="line">sql(<span class="string">"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries are expressed in HiveQL</span></span><br><span class="line">sql(<span class="string">"SELECT * FROM src"</span>).show()</span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |key|  value|</span></span><br><span class="line"><span class="comment">// +---+-------+</span></span><br><span class="line"><span class="comment">// |238|val_238|</span></span><br><span class="line"><span class="comment">// | 86| val_86|</span></span><br><span class="line"><span class="comment">// |311|val_311|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Aggregation queries are also supported.</span></span><br><span class="line">sql(<span class="string">"SELECT COUNT(*) FROM src"</span>).show()</span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |count(1)|</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |    500 |</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are themselves DataFrames and support all normal functions.</span></span><br><span class="line"><span class="keyword">val</span> sqlDF = sql(<span class="string">"SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span></span><br><span class="line"><span class="keyword">val</span> stringsDS = sqlDF.map &#123;</span><br><span class="line"><span class="keyword">case</span> <span class="type">Row</span>(key: <span class="type">Int</span>, value: <span class="type">String</span>) =&gt; <span class="string">s"Key: <span class="subst">$key</span>, Value: <span class="subst">$value</span>"</span></span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |               value|</span></span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// |Key: 0, Value: val_0|</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// You can also use DataFrames to create temporary views within a SparkSession.</span></span><br><span class="line"><span class="keyword">val</span> recordsDF = spark.createDataFrame((<span class="number">1</span> to <span class="number">100</span>).map(i =&gt; <span class="type">Record</span>(i, <span class="string">s"val_<span class="subst">$i</span>"</span>)))</span><br><span class="line">recordsDF.createOrReplaceTempView(<span class="string">"records"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Queries can then join DataFrame data with data stored in Hive.</span></span><br><span class="line">sql(<span class="string">"SELECT * FROM records r JOIN src s ON r.key = s.key"</span>).show()</span><br><span class="line"><span class="comment">// +---+------+---+------+</span></span><br><span class="line"><span class="comment">// |key| value|key| value|</span></span><br><span class="line"><span class="comment">// +---+------+---+------+</span></span><br><span class="line"><span class="comment">// |  2| val_2|  2| val_2|</span></span><br><span class="line"><span class="comment">// |  4| val_4|  4| val_4|</span></span><br><span class="line"><span class="comment">// |  5| val_5|  5| val_5|</span></span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></table></figure>
<h3 id="内联-Hive-应用"><a href="#内联-Hive-应用" class="headerlink" title="内联 Hive 应用"></a>内联 Hive 应用</h3><p>如果要使用内嵌的Hive，什么都不用做，直接用就可以了。 –conf : spark.sql.warehouse.dir=</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">"show tables"</span>).show</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|        |  persons|       <span class="literal">true</span>|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">"SELECT * FROM persons"</span>).show()</span><br><span class="line">+----+-------+</span><br><span class="line">| age|   name|</span><br><span class="line">+----+-------+</span><br><span class="line">|<span class="literal">null</span>|<span class="type">Michael</span>|</span><br><span class="line">|  <span class="number">30</span>|   <span class="type">Andy</span>|</span><br><span class="line">|  <span class="number">19</span>| <span class="type">Justin</span>|</span><br><span class="line">+----+-------+</span><br></pre></td></tr></table></figure>
<p>注意：如果你使用的是内部的Hive，在Spark2.0之后，spark.sql.warehouse.dir用于指定数据仓库的地址，如果你需要是用HDFS作为路径，那么需要将core-site.xml和hdfs-site.xml 加入到Spark conf目录，否则只会创建master节点上的warehouse目录，查询时会出现文件找不到的问题。所以如果需要使用HDFS，则需要将metastore删除，重启集群。</p>
<h3 id="外部-Hive-应用"><a href="#外部-Hive-应用" class="headerlink" title="外部 Hive 应用"></a>外部 Hive 应用</h3><p>如果想连接外部已经部署好的Hive，需要通过以下几个步骤。</p>
<ol>
<li>将Hive中的hive-site.xml拷贝或者软连接到Spark安装目录下的conf目录下。</li>
<li>打开spark shell，注意带上访问Hive元数据库的JDBC客户端。</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master spark://master01:7077 --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure>
<h2 id="JSON-数据集"><a href="#JSON-数据集" class="headerlink" title="JSON 数据集"></a>JSON 数据集</h2><p>Spark SQL 能够自动推测 JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载一个 Dataset[String]或者一个JSON 文件.注意，这个JSON文件不是一个传统的JSON文件，每一行都得是一个JSON串。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Michael"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Andy"</span>, <span class="attr">"age"</span>:<span class="number">30</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Justin"</span>, <span class="attr">"age"</span>:<span class="number">19</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Primitive types (Int, String, etc) and Product types (case classes) encoders are</span></span><br><span class="line"><span class="comment">// supported by importing this when creating a Dataset.</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// A JSON dataset is pointed to by path.</span></span><br><span class="line"><span class="comment">// The path can be either a single text file or a directory storing text files</span></span><br><span class="line"><span class="keyword">val</span> path = <span class="string">"examples/src/main/resources/people.json"</span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.read.json(path)</span><br><span class="line"></span><br><span class="line"><span class="comment">// The inferred schema can be visualized using the printSchema() method</span></span><br><span class="line">peopleDF.printSchema()</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Creates a temporary view using the DataFrame</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by spark</span></span><br><span class="line"><span class="keyword">val</span> teenagerNamesDF = spark.sql(<span class="string">"SELECT name FROM people WHERE age BETWEEN 13 AND 19"</span>)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line"><span class="comment">// +------+</span></span><br><span class="line"><span class="comment">// |  name|</span></span><br><span class="line"><span class="comment">// +------+</span></span><br><span class="line"><span class="comment">// |Justin|</span></span><br><span class="line"><span class="comment">// +------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span></span><br><span class="line"><span class="comment">// a Dataset[String] storing one JSON object per string</span></span><br><span class="line"><span class="keyword">val</span> otherPeopleDataset = spark.createDataset(</span><br><span class="line"><span class="string">""</span><span class="string">"&#123;"</span><span class="string">name":"</span><span class="type">Yin</span><span class="string">","</span><span class="string">address":&#123;"</span><span class="string">city":"</span><span class="type">Columbus</span><span class="string">","</span><span class="string">state":"</span><span class="type">Ohio</span><span class="string">"&#125;&#125;"</span><span class="string">""</span> :: <span class="type">Nil</span>)</span><br><span class="line"><span class="keyword">val</span> otherPeople = spark.read.json(otherPeopleDataset)</span><br><span class="line">otherPeople.show()</span><br><span class="line"><span class="comment">// +---------------+----+</span></span><br><span class="line"><span class="comment">// |        address|name|</span></span><br><span class="line"><span class="comment">// +---------------+----+</span></span><br><span class="line"><span class="comment">// |[Columbus,Ohio]| Yin|</span></span><br><span class="line"><span class="comment">// +---------------+----+</span></span><br></pre></td></tr></table></figure>
<h2 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h2><p>Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。<br>注意，需要将相关的数据库驱动放到spark的类路径下。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --master spark://master01:7077 --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span></span><br><span class="line"><span class="comment">// Loading data from a JDBC source</span></span><br><span class="line"><span class="keyword">val</span> jdbcDF = spark.read.format(<span class="string">"jdbc"</span>).option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://master01:3306/rdd"</span>).option(<span class="string">"dbtable"</span>, <span class="string">" rddtable"</span>).option(<span class="string">"user"</span>, <span class="string">"root"</span>).option(<span class="string">"password"</span>, <span class="string">"hive"</span>).load()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> connectionProperties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">connectionProperties.put(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">connectionProperties.put(<span class="string">"password"</span>, <span class="string">"hive"</span>)</span><br><span class="line"><span class="keyword">val</span> jdbcDF2 = spark.read</span><br><span class="line">.jdbc(<span class="string">"jdbc:mysql://master01:3306/rdd"</span>, <span class="string">"rddtable"</span>, connectionProperties)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Saving data to a JDBC source</span></span><br><span class="line">jdbcDF.write</span><br><span class="line">.format(<span class="string">"jdbc"</span>)</span><br><span class="line">.option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://master01:3306/rdd"</span>)</span><br><span class="line">.option(<span class="string">"dbtable"</span>, <span class="string">"rddtable2"</span>)</span><br><span class="line">.option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">.option(<span class="string">"password"</span>, <span class="string">"hive"</span>)</span><br><span class="line">.save()</span><br><span class="line"></span><br><span class="line">jdbcDF2.write</span><br><span class="line">.jdbc(<span class="string">"jdbc:mysql://master01:3306/mysql"</span>, <span class="string">"db"</span>, connectionProperties)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Specifying create table column data types on write</span></span><br><span class="line">jdbcDF.write</span><br><span class="line">.option(<span class="string">"createTableColumnTypes"</span>, <span class="string">"name CHAR(64), comments VARCHAR(1024)"</span>)</span><br><span class="line">.jdbc(<span class="string">"jdbc:mysql://master01:3306/mysql"</span>, <span class="string">"db"</span>, connectionProperties)</span><br></pre></td></tr></table></figure>
<h1 id="JDBC-ODBC-服务器"><a href="#JDBC-ODBC-服务器" class="headerlink" title="JDBC / ODBC 服务器"></a>JDBC / ODBC 服务器</h1><p>Spark SQL也提供JDBC连接支持，这对于让商业智能(BI)工具连接到Spark集群上以 及在多用户间共享一个集群的场景都非常有用。JDBC 服务器作为一个独立的 Spark 驱动 器程序运行，可以在多用户之间共享。任意一个客户端都可以在内存中缓存数据表，对表 进行查询。集群的资源以及缓存数据都在所有用户之间共享。<br>Spark SQL的JDBC服务器与Hive中的HiveServer2相一致。由于使用了Thrift通信协议，它也被称为“Thrift server”。<br>服务器可以通过 Spark 目录中的 sbin/start-thriftserver.sh 启动。这个 脚本接受的参数选项大多与 spark-submit 相同。默认情况下，服务器会在 localhost:10000 上进行监听，我们可以通过环境变量(HIVE_SERVER2_THRIFT_PORT 和 HIVE_SERVER2_THRIFT_BIND_HOST)修改这些设置，也可以通过 Hive配置选项(hive. server2.thrift.port 和 hive.server2.thrift.bind.host)来修改。你也可以通过命令行参 数–hiveconf property=value来设置Hive选项。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-thriftserver.sh \</span><br><span class="line">--hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \</span><br><span class="line">--hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \</span><br><span class="line">--master &lt;master-uri&gt;</span><br><span class="line">...</span><br><span class="line">./bin/beeline</span><br><span class="line">beeline&gt; !connect jdbc:hive2://master01:10000</span><br></pre></td></tr></table></figure>
<p>在 Beeline 客户端中，你可以使用标准的 HiveQL 命令来创建、列举以及查询数据表。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[bigdata<span class="meta">@master</span>01 spark<span class="number">-2.1</span><span class="number">.1</span>-bin-hadoop2<span class="number">.7</span>]$ ./sbin/start-thriftserver.sh</span><br><span class="line">starting org.apache.spark.sql.hive.thriftserver.<span class="type">HiveThriftServer2</span>, logging to /home/bigdata/hadoop/spark<span class="number">-2.1</span><span class="number">.1</span>-bin-hadoop2<span class="number">.7</span>/logs/spark-bigdata-org.apache.spark.sql.hive.thriftserver.<span class="type">HiveThriftServer2</span><span class="number">-1</span>-master01.out</span><br><span class="line"></span><br><span class="line">[bigdata<span class="meta">@master</span>01 spark<span class="number">-2.1</span><span class="number">.1</span>-bin-hadoop2<span class="number">.7</span>]$ ./bin/beeline</span><br><span class="line"><span class="type">Beeline</span> version <span class="number">1.2</span><span class="number">.1</span>.spark2 by <span class="type">Apache</span> <span class="type">Hive</span></span><br><span class="line"></span><br><span class="line">beeline&gt; !connect jdbc:hive2:<span class="comment">//master01:10000</span></span><br><span class="line"><span class="type">Connecting</span> to jdbc:hive2:<span class="comment">//master01:10000</span></span><br><span class="line"><span class="type">Enter</span> username <span class="keyword">for</span> jdbc:hive2:<span class="comment">//master01:10000: bigdata</span></span><br><span class="line"><span class="type">Enter</span> password <span class="keyword">for</span> jdbc:hive2:<span class="comment">//master01:10000: *******</span></span><br><span class="line">log4j:<span class="type">WARN</span> <span class="type">No</span> appenders could be found <span class="keyword">for</span> logger (org.apache.hive.jdbc.<span class="type">Utils</span>).</span><br><span class="line">log4j:<span class="type">WARN</span> <span class="type">Please</span> initialize the log4j system properly.</span><br><span class="line">log4j:<span class="type">WARN</span> <span class="type">See</span> http:<span class="comment">//logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span></span><br><span class="line"><span class="type">Connected</span> to: <span class="type">Spark</span> <span class="type">SQL</span> (version <span class="number">2.1</span><span class="number">.1</span>)</span><br><span class="line"><span class="type">Driver</span>: <span class="type">Hive</span> <span class="type">JDBC</span> (version <span class="number">1.2</span><span class="number">.1</span>.spark2)</span><br><span class="line"><span class="type">Transaction</span> isolation: <span class="type">TRANSACTION_REPEATABLE_READ</span></span><br><span class="line"></span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="comment">//master01:10000&gt; show tables;</span></span><br><span class="line">+-----------+------------+--------------+--+</span><br><span class="line">| database  | tableName  | isTemporary  |</span><br><span class="line">+-----------+------------+--------------+--+</span><br><span class="line">| <span class="keyword">default</span>   | src        | <span class="literal">false</span>        |</span><br><span class="line">+-----------+------------+--------------+--+</span><br><span class="line"><span class="number">1</span> row selected (<span class="number">0.726</span> seconds)</span><br><span class="line"></span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="comment">//master01:10000&gt;</span></span><br></pre></td></tr></table></figure>
<h1 id="Spark-SQL-CLI"><a href="#Spark-SQL-CLI" class="headerlink" title="Spark SQL CLI"></a>Spark SQL CLI</h1><p>Spark SQL CLI可以很方便的在本地运行Hive元数据服务以及从命令行执行查询任务。需要注意的是，Spark SQL CLI不能与Thrift JDBC服务交互。在Spark目录下执行如下命令启动Spark SQL CLI：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-sql</span><br></pre></td></tr></table></figure>
<p>配置Hive需要替换 conf/ 下的 hive-site.xml 。</p>
<h1 id="Spark-SQL-的运行原理"><a href="#Spark-SQL-的运行原理" class="headerlink" title="Spark SQL 的运行原理"></a>Spark SQL 的运行原理</h1><h2 id="Spark-SQL-运行架构"><a href="#Spark-SQL-运行架构" class="headerlink" title="Spark SQL 运行架构"></a>Spark SQL 运行架构</h2><p>Spark SQL对SQL语句的处理和关系型数据库类似，即词法/语法解析、绑定、优化、执行。Spark SQL会先将SQL语句解析成一棵树，然后使用规则(Rule)对Tree进行绑定、优化等处理过程。Spark SQL由Core、Catalyst、Hive、Hive-ThriftServer四部分构成：<br>Core: 负责处理数据的输入和输出，如获取数据，查询结果输出成DataFrame等<br>Catalyst: 负责处理整个查询过程，包括解析、绑定、优化等<br>Hive: 负责对Hive数据进行处理<br>Hive-ThriftServer: 主要用于对hive的访问</p>
<h3 id="TreeNode"><a href="#TreeNode" class="headerlink" title="TreeNode"></a>TreeNode</h3><p>逻辑计划、表达式等都可以用tree来表示，它只是在内存中维护，并不会进行磁盘的持久化，分析器和优化器对树的修改只是替换已有节点。<br>TreeNode有2个直接子类，QueryPlan和Expression。QueryPlam下又有LogicalPlan和SparkPlan. Expression是表达式体系，不需要执行引擎计算而是可以直接处理或者计算的节点，包括投影操作，操作符运算等。</p>
<h3 id="Rule-amp-RuleExecutor"><a href="#Rule-amp-RuleExecutor" class="headerlink" title="Rule &amp; RuleExecutor"></a>Rule &amp; RuleExecutor</h3><p>Rule就是指对逻辑计划要应用的规则，以到达绑定和优化。他的实现类就是RuleExecutor。优化器和分析器都需要继承RuleExecutor。每一个子类中都会定义Batch、Once、FixPoint. 其中每一个Batch代表着一套规则，Once表示对树进行一次操作，FixPoint表示对树进行多次的迭代操作。RuleExecutor内部提供一个Seq[Batch]属性，里面定义的是RuleExecutor的处理逻辑，具体的处理逻辑由具体的Rule子类实现。</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/10-Rule-And-RuleExecutor.png?raw=true" alt="10-Rule-And-RuleExecutor"></p>
<p>整个流程架构图：</p>
<p><img src="https://github.com/onefansofworld/hexo_images/blob/master/Spark/Spark-SQL/11-Spark-SQL-Run-Architecture.png?raw=true" alt="11-Spark-SQL-Run-Architecture"></p>
<h2 id="Spark-SQL-运行原理"><a href="#Spark-SQL-运行原理" class="headerlink" title="Spark SQL 运行原理"></a>Spark SQL 运行原理</h2><ol>
<li><strong>使用SessionCatalog保存元数据</strong>：在解析SQL语句之前，会创建SparkSession，或者如果是2.0之前的版本初始化SQLContext，SparkSession只是封装了SparkContext和SQLContext的创建而已。会把元数据保存在SessionCatalog中，涉及到表名，字段名称和字段类型。创建临时表或者视图，其实就会往SessionCatalog注册。</li>
<li><strong>解析SQL,使用ANTLR生成未绑定的逻辑计划</strong>：当调用SparkSession的sql或者SQLContext的sql方法，我们以2.0为准，就会使用SparkSqlParser进行解析SQL. 使用的ANTLR进行词法解析和语法解析。它分为2个步骤来生成Unresolved LogicalPlan：(1)词法分析：Lexical Analysis，负责将token分组成符号类； (2)构建一个分析树或者语法树AST。</li>
<li><strong>使用分析器Analyzer绑定逻辑计划</strong>：在该阶段，Analyzer会使用Analyzer Rules，并结合SessionCatalog，对未绑定的逻辑计划进行解析，生成已绑定的逻辑计划。</li>
<li><strong>使用优化器Optimizer优化逻辑计划</strong>：优化器也是会定义一套Rules，利用这些Rule对逻辑计划和Exepression进行迭代处理，从而使得树的节点进行和并和优化。</li>
<li><strong>使用SparkPlanner生成物理计划</strong>：SparkSpanner使用Planning Strategies，对优化后的逻辑计划进行转换，生成可以执行的物理计划SparkPlan.</li>
<li><strong>使用QueryExecution执行物理计划</strong>：此时调用SparkPlan的execute方法，底层其实已经再触发JOB了，然后返回RDD.</li>
</ol>
<p>End. </p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>您的支持将鼓励我继续创作</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="moqi 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="moqi 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Spark/" rel="tag"># Spark</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/08/04/Spark-Core-Analysis/" rel="next" title="Spark Core 应用解析">
                <i class="fa fa-chevron-left"></i> Spark Core 应用解析
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/08/13/Data-Warehouse-Construction/" rel="prev" title="数据仓库建设">
                数据仓库建设 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://avatars3.githubusercontent.com/u/39821951?s=400&u=65c6d8145d7b591ca2051e7082fd842b56e62567&v=4"
                alt="moqi" />
            
              <p class="site-author-name" itemprop="name">moqi</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="mailto:moqimoqidea@gmail.com" target="_blank" title="E-mail">
                      
                        <i class="fa fa-fw fa-meh-o"></i>E-mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-SQL-概述"><span class="nav-number">1.</span> <span class="nav-text">Spark SQL 概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#什么是-Spark-SQL"><span class="nav-number">1.1.</span> <span class="nav-text">什么是 Spark SQL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-vs-DataFrames-vs-DataSet"><span class="nav-number">1.2.</span> <span class="nav-text">RDD vs DataFrames vs DataSet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD"><span class="nav-number">1.2.1.</span> <span class="nav-text">RDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DataFrame"><span class="nav-number">1.2.2.</span> <span class="nav-text">DataFrame</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DataSet"><span class="nav-number">1.2.3.</span> <span class="nav-text">DataSet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#三者的共性"><span class="nav-number">1.2.4.</span> <span class="nav-text">三者的共性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#三者的区别"><span class="nav-number">1.2.5.</span> <span class="nav-text">三者的区别</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#执行-Spark-SQL-查询"><span class="nav-number">2.</span> <span class="nav-text">执行 Spark SQL 查询</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#命令行查询流程"><span class="nav-number">2.1.</span> <span class="nav-text">命令行查询流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IDEA-创建-Spark-SQL-程序"><span class="nav-number">2.2.</span> <span class="nav-text">IDEA 创建 Spark SQL 程序</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-SQL-解析"><span class="nav-number">3.</span> <span class="nav-text">Spark SQL 解析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#新的起点：SparkSession"><span class="nav-number">3.1.</span> <span class="nav-text">新的起点：SparkSession</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建-DataFrames"><span class="nav-number">3.2.</span> <span class="nav-text">创建 DataFrames</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DataFrame-常用操作"><span class="nav-number">3.3.</span> <span class="nav-text">DataFrame 常用操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DSL-风格语法"><span class="nav-number">3.3.1.</span> <span class="nav-text">DSL 风格语法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SQL-风格语法"><span class="nav-number">3.3.2.</span> <span class="nav-text">SQL 风格语法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建-DataSet"><span class="nav-number">3.4.</span> <span class="nav-text">创建 DataSet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DataSet-和-RDD-互操作"><span class="nav-number">3.5.</span> <span class="nav-text">DataSet 和 RDD 互操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#通过反射获取-Schema"><span class="nav-number">3.5.1.</span> <span class="nav-text">通过反射获取 Schema</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#通过编程设置-Schema"><span class="nav-number">3.5.2.</span> <span class="nav-text">通过编程设置 Schema</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#类型之间的转换总结"><span class="nav-number">3.6.</span> <span class="nav-text">类型之间的转换总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#用户自定义函数"><span class="nav-number">3.7.</span> <span class="nav-text">用户自定义函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#用户自定义UDF函数"><span class="nav-number">3.7.1.</span> <span class="nav-text">用户自定义UDF函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用户自定义聚合函数"><span class="nav-number">3.7.2.</span> <span class="nav-text">用户自定义聚合函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#弱类型"><span class="nav-number">3.7.2.1.</span> <span class="nav-text">弱类型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#强类型"><span class="nav-number">3.7.2.2.</span> <span class="nav-text">强类型</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-SQL-数据源"><span class="nav-number">4.</span> <span class="nav-text">Spark SQL 数据源</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#通用加载-保存方法"><span class="nav-number">4.1.</span> <span class="nav-text">通用加载 / 保存方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#手动指定选项"><span class="nav-number">4.1.1.</span> <span class="nav-text">手动指定选项</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#文件保存选项"><span class="nav-number">4.1.2.</span> <span class="nav-text">文件保存选项</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Parquet-文件"><span class="nav-number">4.2.</span> <span class="nav-text">Parquet 文件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Parquet-读写"><span class="nav-number">4.2.1.</span> <span class="nav-text">Parquet 读写</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解析分区信息"><span class="nav-number">4.2.2.</span> <span class="nav-text">解析分区信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Schema-合并"><span class="nav-number">4.2.3.</span> <span class="nav-text">Schema 合并</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hive-数据库"><span class="nav-number">4.3.</span> <span class="nav-text">Hive 数据库</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#内联-Hive-应用"><span class="nav-number">4.3.1.</span> <span class="nav-text">内联 Hive 应用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#外部-Hive-应用"><span class="nav-number">4.3.2.</span> <span class="nav-text">外部 Hive 应用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#JSON-数据集"><span class="nav-number">4.4.</span> <span class="nav-text">JSON 数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#JDBC"><span class="nav-number">4.5.</span> <span class="nav-text">JDBC</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#JDBC-ODBC-服务器"><span class="nav-number">5.</span> <span class="nav-text">JDBC / ODBC 服务器</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-SQL-CLI"><span class="nav-number">6.</span> <span class="nav-text">Spark SQL CLI</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-SQL-的运行原理"><span class="nav-number">7.</span> <span class="nav-text">Spark SQL 的运行原理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-SQL-运行架构"><span class="nav-number">7.1.</span> <span class="nav-text">Spark SQL 运行架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TreeNode"><span class="nav-number">7.1.1.</span> <span class="nav-text">TreeNode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Rule-amp-RuleExecutor"><span class="nav-number">7.1.2.</span> <span class="nav-text">Rule &amp; RuleExecutor</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-SQL-运行原理"><span class="nav-number">7.2.</span> <span class="nav-text">Spark SQL 运行原理</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">moqi</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">68.5k</span>
  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://moqimoqidea.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://moqimoqidea.github.io/2017/08/10/Spark-SQL-Analysis/';
          this.page.identifier = '2017/08/10/Spark-SQL-Analysis/';
          this.page.title = 'Spark SQL 应用解析';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://moqimoqidea.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('3');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
