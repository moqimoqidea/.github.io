<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spark Core 应用解析]]></title>
    <url>%2F2017%2F08%2F04%2FSpark-Core-Analysis%2F</url>
    <content type="text"><![CDATA[本文主要解析部分 Spark Core 的技术点。 RDD 概念RDD 为什么会产生RDD是Spark的基石，是实现Spark数据处理的核心抽象。那么RDD为什么会产生呢？ Hadoop的MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后写入物理存储设备。数据更多面临的是一次性处理。 MR的这种方式对数据领域两种常见的操作不是很高效。第一种是迭代式的算法。比如机器学习中ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR这种模式不太合适，即使多MR串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。 MR中的迭代： Spark中的迭代： 我们需要一个效率非常快，且能够支持迭代计算和有效数据共享的模型，Spark应运而生。RDD是基于工作集的工作模式，更多的是面向工作流。 但是无论是MR还是RDD都应该具有类似位置感知、容错和负载均衡等特性。 RDD 概述什么是 RDDRDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。在 Spark 中，对数据的所有操作不外乎创建 RDD、转化已有RDD 以及调用 RDD 操作进行求值。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。RDD 可以包含 Python、Java、Scala 中任意类型的对象， 甚至可以包含用户自定义的对象。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。RDD支持两种操作:转化操作和行动操作。RDD 的转化操作是返回一个新的 RDD的操作，比如 map()和 filter()，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作。比如 count() 和 first()。Spark采用惰性计算模式，RDD只有第一次在一个行动操作中用到时，才会真正计算。Spark可以优化整个计算过程。默认情况下，Spark 的 RDD 会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个 RDD，可以使用 RDD.persist() 让 Spark 把这个 RDD 缓存下来。 RDD 的属性 一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。 一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。 RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。 一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 RDD是一个应用层面的逻辑概念。一个RDD多个分片。RDD就是一个元数据记录集，记录了RDD内存所有的关系数据。 RDD 弹性 自动进行内存和磁盘数据存储的切换：Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换 基于血统的高效容错机制：在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。 Task如果失败会自动进行特定次数的重试：RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。 Stage如果失败会自动进行特定次数的重试：如果Job的某个Stage阶段计算失败，框架也会自动进行任务的重新计算，默认次数也是4次。 Checkpoint和Persist可主动或被动触发：RDD可以通过Persist持久化将RDD缓存到内存或者磁盘，当再次用到该RDD时直接读取就行。也可以将RDD进行检查点，检查点会将数据存储在HDFS中，该RDD的所有父RD都会被移除。 数据调度弹性：Spark把这个JOB执行模型抽象为通用的有向无环图DAG，可以将多Stage的任务串联或并行执行，调度引擎自动处理Stage的失败以及Task的失败。 数据分片的高度弹性：可以根据业务的特征，动态调整数据分片的个数，提升整体的应用执行效率。 RDD全称叫做弹性分布式数据集(Resilient Distributed Datasets)一种分布式的内存抽象，表示一个只读的记录分区的集合，它只能通过其他RDD转换而创建，为此，RDD支持丰富的转换操作(如map, join, filter, groupBy等过这种转换操作，新的RDD则包含了如何从其他RDDs衍生所必需的信息，所以说RDDs之间是有依赖关系的。基于RDDs之间的依赖，RDDs会形成一个有向无环图DAG，G描述了整个流式计算的流程，实际执行的时候，RDD是通过血缘关系(Lineage)成的，即使出现数据分区丢失，也可以通过血缘关系重建分区，总结起来，基于RDD的流式计算任务可描述为：从稳定的物理存储(如分布式文件系统)中加载记录，记入由一组确定性操作构成的DAG，然后写回稳定存储。另外RDD还可以将数据集缓存到内存中，使得在多个操作之间可以重用数据集，基于这个特点可以很方便地构建应用(图计算、机器学习等)或者交互式数据分析应用。可以说Spark最初也就是实现RDD的一个分布式系统，后面通过不断发展壮大成为现在较为完善的大数据生态系单来讲，Spark-RDD的关系类似于Hadoop-MapReduce关系。 RDD 特点RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。 分区RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。 只读如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。 由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。 RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。 依赖RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是一一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。 通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，如下图所示，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。 缓存如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。 CheckPoint虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。 给定一个RDD我们至少可以知道如下几点信息：1、分区数以及分区方式；2、由父RDDs衍生而来的相关依赖信息；3、计算每个分区的数据，计算步骤为：1）如果被缓存，则从缓存中取的分区的数据；2）如果被checkpoint，则从checkpoint处恢复数据；3）根据血缘关系计算分区的数据。 RDD 编程编程模型在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。 那么这其中的 Dirver、SparkContext、Executor、Master、Worker分别是什么？ 创建 RDD在Spark中创建RDD的创建方式大概可以分为三种：（1）、从集合中创建RDD；（2）、从外部存储创建RDD；（3）、从其他RDD创建。 由一个已经存在的Scala集合创建，集合并行化。val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))；而从集合中创建RDD，Spark主要提供了两种函数：parallelize和makeRDD。我们可以先看看这两个函数的声明： 123456789def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] 我们可以从上面看出makeRDD有两种实现，而且第一个makeRDD函数接收的参数和parallelize完全一致。其实第一种makeRDD函数实现是依赖了parallelize函数的实现，来看看Spark中是怎么实现这个makeRDD函数的： 12345def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; parallelize(seq, numSlices)&#125; 我们可以看出，这个makeRDD函数完全和parallelize函数一致。但是我们得看看第二种makeRDD函数函数实现了，它接收的参数类型是Seq[(T, Seq[String])]，Spark文档的说明是：Distribute a local Scala collection to form an RDD, with one or more location preferences (hostnames of Spark nodes) for each object. Create a new partition for each collection item.原来，这个函数还为数据提供了位置信息，来看看我们怎么使用： 12345678910111213141516171819202122scala&gt; val guigu1= sc.parallelize(List(1,2,3))guigu1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:21 scala&gt; val guigu2 = sc.makeRDD(List(1,2,3))guigu2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[11] at makeRDD at &lt;console&gt;:21 scala&gt; val seq = List((1, List("slave01")), | (2, List("slave02")))seq: List[(Int, List[String])] = List((1,List(slave01)), (2,List(slave02))) scala&gt; val guigu3 = sc.makeRDD(seq)guigu3: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at makeRDD at &lt;console&gt;:23 scala&gt; guigu3.preferredLocations(guigu3.partitions(1))res26: Seq[String] = List(slave02) scala&gt; guigu3.preferredLocations(guigu3.partitions(0))res27: Seq[String] = List(slave01) scala&gt; guigu1.preferredLocations(guigu1.partitions(0))res28: Seq[String] = List() 我们可以看到，makeRDD函数有两种实现，第一种实现其实完全和parallelize一致；而第二种实现可以为数据提供位置信息，而除此之外的实现和parallelize函数也是一致的，如下： 123456789101112def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())&#125; def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope &#123; assertNotStopped() val indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap new ParallelCollectionRDD[T](this, seq.map(_._1), seq.size, indexToPrefs)&#125; 都是返回ParallelCollectionRDD，而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。 由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等。 12scala&gt; val rdd1 = sc.textFile("hdfs://slave1:9000/txtFile")rdd1: org.apache.spark.rdd.RDD[String] = hdfs://slave1:9000/RELEASE MapPartitionsRDD[4] at textFile at &lt;console&gt;:24 RDD 编程RDD一般分为数值RDD和键值对RDD，本章不进行具体区分，先统一来看，下一章会对键值对RDD做专门说明。 TransformationRDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。常用的 Transformation： map(func): 返回通过函数func传递源的每个元素形成的新分布式数据集。 1234567891011scala&gt; var source = sc.parallelize(1 to 10)source: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at &lt;console&gt;:24scala&gt; source.collect()res7: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; val mapadd = source.map(_ * 2)mapadd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at map at &lt;console&gt;:26scala&gt; mapadd.collect()res8: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18, 20) filter(func): 返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成。 1234567891011scala&gt; var sourceFilter = sc.parallelize(Array("xiaoming","xiaojiang","xiaohe","dazhi"))sourceFilter: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24scala&gt; val filter = sourceFilter.filter(_.contains("xiao"))filter: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[11] at filter at &lt;console&gt;:26scala&gt; sourceFilter.collect()res9: Array[String] = Array(xiaoming, xiaojiang, xiaohe, dazhi)scala&gt; filter.collect()res10: Array[String] = Array(xiaoming, xiaojiang, xiaohe) flatMap(func): 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）。 1234567891011scala&gt; val sourceFlat = sc.parallelize(1 to 5)sourceFlat: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:24scala&gt; sourceFlat.collect()res11: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt; val flatMap = sourceFlat.flatMap(1 to _)flatMap: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[13] at flatMap at &lt;console&gt;:26scala&gt; flatMap.collect()res12: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5) mapPartitions(func): 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。 123456789101112131415161718192021222324252627scala&gt; val rdd = sc.parallelize(List(("kpop","female"),("zorro","male"),("mobin","male"),("lucy","female")))rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[16] at parallelize at &lt;console&gt;:24scala&gt; :paste// Entering paste mode (ctrl-D to finish)def partitionsFun(iter : Iterator[(String,String)]) : Iterator[String] = &#123; var woman = List[String]() while (iter.hasNext)&#123; val next = iter.next() next match &#123; case (_,"female") =&gt; woman = next._1 :: woman case _ =&gt; &#125; &#125; woman.iterator&#125;// Exiting paste mode, now interpreting.partitionsFun: (iter: Iterator[(String, String)])Iterator[String]scala&gt; val result = rdd.mapPartitions(partitionsFun)result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[17] at mapPartitions at &lt;console&gt;:28scala&gt; result.collect()res13: Array[String] = Array(kpop, lucy) mapPartitionsWithIndex(func): 类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]。 123456789101112131415161718192021222324252627scala&gt; val rdd = sc.parallelize(List(("kpop","female"),("zorro","male"),("mobin","male"),("lucy","female")))rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[18] at parallelize at &lt;console&gt;:24scala&gt; :paste// Entering paste mode (ctrl-D to finish)def partitionsFun(index : Int, iter : Iterator[(String,String)]) : Iterator[String] = &#123; var woman = List[String]() while (iter.hasNext)&#123; val next = iter.next() next match &#123; case (_,"female") =&gt; woman = "["+index+"]"+next._1 :: woman case _ =&gt; &#125; &#125; woman.iterator&#125;// Exiting paste mode, now interpreting.partitionsFun: (index: Int, iter: Iterator[(String, String)])Iterator[String]scala&gt; val result = rdd.mapPartitionsWithIndex(partitionsFun)result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[19] at mapPartitionsWithIndex at &lt;console&gt;:28scala&gt; result.collect()res14: Array[String] = Array([0]kpop, [3]lucy) sample(withReplacement, fraction, seed): 以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。例子从RDD中随机且有放回的抽出50%的数据，随机种子值为3（即可能以1 2 3的其中一个起始值）。 1234567891011121314151617scala&gt; val rdd = sc.parallelize(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[20] at parallelize at &lt;console&gt;:24scala&gt; rdd.collect()res15: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; var sample1 = rdd.sample(true,0.4,2)sample1: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[21] at sample at &lt;console&gt;:26scala&gt; sample1.collect()res16: Array[Int] = Array(1, 2, 2, 7, 7, 8, 9)scala&gt; var sample2 = rdd.sample(false,0.2,3)sample2: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[22] at sample at &lt;console&gt;:26scala&gt; sample2.collect()res17: Array[Int] = Array(1, 9) takeSample(withReplacement, num, seed): 和Sample的区别是：takeSample返回的是最终的结果集合。 12345678scala&gt; val rdd = sc.parallelize(1 to 20)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24scala&gt; val takeSample = rdd.takeSample(false, 10)takeSample: Array[Int] = Array(17, 13, 1, 5, 9, 20, 10, 3, 8, 7)scala&gt; val takeSample1 = rdd.takeSample(true, 10)takeSample1: Array[Int] = Array(12, 10, 18, 5, 4, 9, 1, 14, 5, 9) union(otherDataset): 对源RDD和参数RDD求并集后返回一个新的RDD。 1234567891011scala&gt; val rdd1 = sc.parallelize(1 to 5)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[23] at parallelize at &lt;console&gt;:24scala&gt; val rdd2 = sc.parallelize(5 to 10)rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[24] at parallelize at &lt;console&gt;:24scala&gt; val rdd3 = rdd1.union(rdd2)rdd3: org.apache.spark.rdd.RDD[Int] = UnionRDD[25] at union at &lt;console&gt;:28scala&gt; rdd3.collect()res18: Array[Int] = Array(1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10) intersection(otherDataset): 对源RDD和参数RDD求交集后返回一个新的RDD。 1234567891011scala&gt; val rdd1 = sc.parallelize(1 to 7)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[26] at parallelize at &lt;console&gt;:24scala&gt; val rdd2 = sc.parallelize(5 to 10)rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[27] at parallelize at &lt;console&gt;:24scala&gt; val rdd3 = rdd1.intersection(rdd2)rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[33] at intersection at &lt;console&gt;:28scala&gt; rdd3.collect()res19: Array[Int] = Array(5, 6, 7) distinct([numTasks])): 对源RDD进行去重后返回一个新的RDD. 默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。 1234567891011121314scala&gt; val distinctRdd = sc.parallelize(List(1,2,1,5,2,9,6,1))distinctRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at &lt;console&gt;:24scala&gt; val unionRDD = distinctRdd.distinct()unionRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[37] at distinct at &lt;console&gt;:26scala&gt; unionRDD.collect()res20: Array[Int] = Array(1, 9, 5, 6, 2)scala&gt; val unionRDD = distinctRdd.distinct(2)unionRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[40] at distinct at &lt;console&gt;:26scala&gt; unionRDD.collect()res21: Array[Int] = Array(6, 2, 1, 9, 5) partitionBy(partitioner): 对RDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区， 否则会生成ShuffleRDD。 1234567891011scala&gt; val rdd = sc.parallelize(Array((1,"aaa"),(2,"bbb"),(3,"ccc"),(4,"ddd")),4)rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[44] at parallelize at &lt;console&gt;:24scala&gt; rdd.partitions.sizeres24: Int = 4scala&gt; var rdd2 = rdd.partitionBy(new org.apache.spark.HashPartitioner(2))rdd2: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[45] at partitionBy at &lt;console&gt;:26scala&gt; rdd2.partitions.sizeres25: Int = 2 reduceByKey(func, [numTasks]): 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。 12345678scala&gt; val rdd = sc.parallelize(List(("female",1),("male",5),("female",5),("male",2)))rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[46] at parallelize at &lt;console&gt;:24scala&gt; val reduce = rdd.reduceByKey((x,y) =&gt; x+y)reduce: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[47] at reduceByKey at &lt;console&gt;:26scala&gt; reduce.collect()res29: Array[(String, Int)] = Array((female,6), (male,7)) groupByKey(): groupByKey也是对每个key进行操作，但只生成一个sequence。 1234567891011121314151617181920212223scala&gt; val words = Array("one", "two", "two", "three", "three", "three")words: Array[String] = Array(one, two, two, three, three, three)scala&gt; val wordPairsRDD = sc.parallelize(words).map(word =&gt; (word, 1))wordPairsRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[4] at map at &lt;console&gt;:26scala&gt; val group = wordPairsRDD.groupByKey()group: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[5] at groupByKey at &lt;console&gt;:28scala&gt; group.collect()res1: Array[(String, Iterable[Int])] = Array((two,CompactBuffer(1, 1)), (one,CompactBuffer(1)), (three,CompactBuffer(1, 1, 1)))scala&gt; group.map(t =&gt; (t._1, t._2.sum))res2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[6] at map at &lt;console&gt;:31scala&gt; res2.collect()res3: Array[(String, Int)] = Array((two,2), (one,1), (three,3))scala&gt; val map = group.map(t =&gt; (t._1, t._2.sum))map: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[7] at map at &lt;console&gt;:30scala&gt; map.collect()res4: Array[(String, Int)] = Array((two,2), (one,1), (three,3)) combineByKey[C]( createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): 对相同K，把V合并成一个集合. createCombiner: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。如果这是一个新的元素,combineByKey() 会使用一个叫作 createCombiner() 的函数来创建 那个键对应的累加器的初始值。 mergeValue: 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并。 mergeCombiners: 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。 123456789101112131415161718scala&gt; val scores = Array(("Fred", 88), ("Fred", 95), ("Fred", 91), ("Wilma", 93), ("Wilma", 95), ("Wilma", 98))scores: Array[(String, Int)] = Array((Fred,88), (Fred,95), (Fred,91), (Wilma,93), (Wilma,95), (Wilma,98))scala&gt; val input = sc.parallelize(scores)input: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[52] at parallelize at &lt;console&gt;:26scala&gt; val combine = input.combineByKey( | (v)=&gt;(v,1), | (acc:(Int,Int),v)=&gt;(acc._1+v,acc._2+1), | (acc1:(Int,Int),acc2:(Int,Int))=&gt;(acc1._1+acc2._1,acc1._2+acc2._2))combine: org.apache.spark.rdd.RDD[(String, (Int, Int))] = ShuffledRDD[53] at combineByKey at &lt;console&gt;:28scala&gt; val result = combine.map&#123; | case (key,value) =&gt; (key,value._1/value._2.toDouble)&#125;result: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[54] at map at &lt;console&gt;:30scala&gt; result.collect()res33: Array[(String, Double)] = Array((Wilma,95.33333333333333), (Fred,91.33333333333333)) aggregateByKey(zeroValue:U,[partitioner: Partitioner]) (seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U): 在kv对的RDD中，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。seqOp函数用于在每一个分区中用初始值逐步迭代value，combOp函数用于合并每个分区中的结果。 1234567891011121314151617scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:24scala&gt; val agg = rdd.aggregateByKey(0)(math.max(_,_),_+_)agg: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[13] at aggregateByKey at &lt;console&gt;:26scala&gt; agg.collect()res7: Array[(Int, Int)] = Array((3,8), (1,7), (2,3))scala&gt; agg.partitions.sizeres8: Int = 3scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),1)rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24scala&gt; val agg = rdd.aggregateByKey(0)(math.max(_,_),_+_).collect()agg: Array[(Int, Int)] = Array((1,4), (3,8), (2,3))]]></content>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM GC 发展历程]]></title>
    <url>%2F2017%2F06%2F16%2FJVM-GC-Development-Path%2F</url>
    <content type="text"><![CDATA[本文主要介绍 JVM GC 的发展。 较早的 From - To 阶段 最早的垃圾收集 From - To 架构的 GC，把整个堆内存分成大小差不多相等的两部分，中间有一个分配的指针（Free Point），对指针设定目标值（比如 From 区域的 80%）时，触发一次 GC。GC 触发时应用进入 Stop-The-World 状态，这时垃圾回收器检查 From 区域有哪些是可以回收的那些不是，将不可以回收的拷贝到 To 区域，其他回收。一次 GC 操作完成的时候完成区域交换（From 转换为 To 区域，To 转换为 From 区域），然后指针分配内存开始从新的 From 区域开始。 这种纯粹的拷贝垃圾回收方法最大的问题在于堆内存里面永远只可以用一半的内存，所以有一半的堆是浪费的。但在当时而言还是比较领先的，比如相对于引用计数的垃圾回收方法。引用计数垃圾回收的问题在于：引用计数在计数的时候需要维持一个锁的消耗，会降低分配内存的速度；另外一个是在循环引用中，这个消耗会更大。 回收分代的思想 将堆内存分代治理建立于这样一个假设之上：代际假设 , 核心论点有两个： 大多数对象很快就会被闲置； 少部分活下来的对象会存在相当长一段时间。 因此，JVM GC 分代治理的核心基础是以下两个：（1）大多数对象都会在年轻代死亡。（2）年老代引用年轻代的对象只占很小的一部分。分代治理会发生不同区域的 GC。在年轻代发生的 GC 称为 “minor GC”，在年老代出现的 GC 称为 “major GC” 或者 “full GC”。 根据代际假设构建的堆内存首先避免了全盘扫描，这个时期的 JVM GC 发展为如上图所示结构，分为年轻代，年老代，永生代。年轻代分为 eden 区与两个 survivor 区，s0 和 s1 实现是最初的 From - To 架构，在这里我们假设 s0 为 From 区，s1 为 To 区。创建的对象首先进入 eden 区，如果发生 minor GC，eden 区中大部分对象被回收，小部分对象拷贝到 To 区，From 也拷贝到 To 区；如果在 eden 中的对象太大不能拷贝到 To 区，则会被直接移动到年老代。每次 minor GC 时 From 和 To 会交换，每交换一次区内的对象年龄会加一，当年龄到达一定值（比如15）（注：这一块在后面的 GC 实现了动态调整）的时候，这些大龄的对象也被移动到了年老代。 但是这里会发生一个问题：如果年老代的对象需要引用年轻代的对象怎么办？为了处理这些情况，年老代中有一种称为”CardTable” (卡表) 的东西，它是一个 512 字节的块。每当年老代中的对象引用年轻代中的对象时，它就会记录在此表中。当发生 minor GC 时，仅搜索该卡表以确定它是否是年轻代 GC 需要回收的对象，而不是检查旧代中的所有对象的引用。 当数据已满时，触发年老代执行 GC 。执行程序因 GC 类型而异，根据JDK 7，有5种GC类型。 Serial GC Parallel GC Parallel Old GC (Parallel Compacting GC) Concurrent Mark &amp; Sweep GC (or “CMS”) Garbage First (G1) GCSerial GC(-XX:+UseSerialGC)，即串行 GC，使用被称为 “mark-sweep-compant” 的算法。 第一步：标记年老代中幸存的对象。（标记 - mark） 第二步：从堆的最前面开始检查，只留下幸存的堆。（扫描 - sweep） 第三步：把对象从最前面开始填充，以便连续堆积对象，并将堆分为包含对象和不包含对象的两部分。（紧凑 - compant） 串行 GC 适用于小内存和少量 CPU 内核的 JVM。Parallel GC(-XX:+UseParallelGC)，即并行 GC，和串行 GC 最大的区别是用多个线程来处理 GC，因此更快，当有足够的内存和 CPU 资源时，此 GC 非常有用，它也被称为”吞吐量 GC“。 Parallel Old GC (-XX:+UseParallelOldGC)，并行旧 GC，自 JDK 5 更新以来开始支持，与并行 GC 相比，唯一的区别是老年代的 GC 算法。它经历了三个步骤：mark – summary – compaction（标记 - 摘要 - 压缩）。”摘要“步骤经历了一些更复杂的步骤。 CMS GC （-XX：+ UseConcMarkSweepGC），从下图中可以看出，CMS GC 相对于前三个复杂得多。第一步 “Initial Mark” (初始标记) 很简单，搜索最接近类加载器的对象中的幸存对象，因此暂停时间很短。在 “Concurrent Mark” (并发标记) 步骤中，跟踪并检查刚刚确认的幸存对象引用的对象。这一步的不同之处在于它在同时处理其他线程的同时继续进行。在 “Remark” (再次标记) 步骤中，将检查在并发标记步骤中新增加或停止引用的对象。最后，在 “Concurrent Sweep” (并发扫描) 步骤中进行垃圾回收，垃圾回收和其他线程同步进行。由于 CMS GC 独特的运行方式，因此 GC 的暂停时间非常短。CMS GC 也称为低延迟 GC。在应用程序的响应时间至关重要时使用。这种 GC 的主要缺点如下：（1）它比其他 GC 类型使用更多的内存和 CPU。（2）默认情况下不提供压缩步骤。因而如果由于许多内存碎片而需要执行压缩任务，那么 GC 需要的静止时间可能会比其他任何 GC 方式都要长。所以，如果在使用 CMS GC 的时候要尤其注意压缩任务执行的频率和持续时间。 G1 GC G1 是 Garbage First 的缩写，最开始的 G1 完全抛弃分代收集的思想，开始于 JDK 1.7 Update 4，可以视为上图中所有的 “E”, “O”, “S” 标志消失，只分成不同的 Region (块)。实际上现在的 G1 GC 也有了分代的逻辑。G1 的实现一直以来都在不断的变化之中。 由前文所知每种 GC 都有出现的历史背景，比如串行 GC 是在内存和 CPU 比较小的情况下出现的；并行 GC 是在吞吐量出现巨大需求的时候出现的；而 CMS GC 则是对低延迟有了更高的需求，尽量拖延 full GC 出现的时间。那么，出现 G1这种形式的垃圾收集器的原因是什么？在 JVM GC 不断发展的过程中，已经出现了自适应的堆，也就是不需要手动调节年轻代与年老代的比例，以及年轻代对象进入年老代的年龄。这些自适应堆对更灵活的堆块产生了强烈的需求：如果将堆空间划分为很多个 Region，G1 可以将某一块指定为各种不同的代（可以是年老代，Eden 或者 Survivor 区），而且各个块在空间上不需要连续的在一起，有一个 List 将它们组织在一起，这样 G1 就很容易调整各个代之间的比例。 为什么 G1 被称为 G1？ G1 会在内部维护一个优先列表，通过一个合理的模型，计算出每个 Region 的收集成本和收益期望并量化，这样每次进行 GC 时，G1 总是会选择最适合的 Region（通常垃圾比较多）进行回收，使 GC 时间满足设置的条件。 G1通过引入 Remembered Set 来避免全堆扫描（前面所说的 CardTable 是其的一种实现）。Remembered Set 用于跟踪对象引用。G1 中每个 Region 都有对应的 Remembered Set 。当 JVM 发现内部的一个引用关系需要更新（对 Reference 类型进行写操作），则立即产生一个 Write Barrier 中断这个写操作，并检查Reference 引用的对象是否处于不同的 Region 之间（用分代的思想，就是新生代和老年代之间的引用）。如果是，则通过 CardTable 通知 G1，G1 根据 CardTable 把相关引用信息记录到被引用对象所属的 Region 的Remembered Set 中，并将 Remembered Set 加入 GC Root 。这样，在 G1 进行根节点枚举时就可以扫描到该对象而不会出现遗漏。 通俗解释第二条：如果一个 Region 的 Reference 越少，JVM 倾向于认为这块 Region 里面活着的对象越少，这个 Region 块是可回收的垃圾块的百分比就越大，这样回收这个 Region 的收益就越大。所以称这种算法为 Garbage First. G1 GC 年轻代的回收 当 JVM 启动时基于启动参数，JVM 要求操作系统分配一个大的连续内存块来托管 JVM 的堆，被划分为 2048 个 Region (块)。 年轻代的块发生 “minor GC”，将活着的对象拷贝到 survivor 块（依然是 From - To 算法）。如果对象过大或者对象的年龄足够，会拷贝到年老代。 结果：图三有新增的 “Recently Copied” 两块。 G1 GC 年老代的回收 初始标记阶段：初始标记的活着的对象在年轻代的垃圾收集上。在日志中，被标记为 GC pause (young) (inital-mark). 并行标记阶段，如果找到空区域（由”X”表示），则在 Remark 阶段立即将它们移除。此外，计算确定活跃度的信息。 Remark 阶段，空区域被移除并回收，现在计算所有区域的区域活跃度。 复制清理阶段，G1 选择具有最低”活跃度“的区域（比如引用其他 Region 最少的区域），那些可以最快收集的区域。这些区域与年轻代 GC 同时收集。这在日志中表示为 [GC pause (mixed)]。G1 实际上将 Stop-The-World 的操作放在一个时间区间，这样对应用性能和稳定性较好。 复制清理阶段后，选择的区域已经被收集并压缩成图中所示的深蓝色区域和深绿色区域。 JDK 8 中 JVM 的调整JDK 8 的 JVM 去掉了永生代(PermGen)，用 Metaspace 来代替。Metaspace 使用系统的内存。 参考https://www.cubrid.org/blog/understanding-java-garbage-collection https://www.oracle.com/technetwork/tutorials/tutorials-1876574.html https://blog.idrsolutions.com/2017/05/g1gc-java-9-garbage-collector-explained-5-minutes/ https://www.sczyh30.com/posts/Java/jvm-gc-hotspot-implements/ https://software.intel.com/en-us/blogs/2014/06/18/part-1-tuning-java-garbage-collection-for-hbase https://blog.csdn.net/elinespace/article/details/78852469]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 日志相关命令整理]]></title>
    <url>%2F2017%2F06%2F03%2FLinux-Check-Log-Commands%2F</url>
    <content type="text"><![CDATA[本文主要介绍开发中一些日志的常用操作。 tailn 是显示行号；相当于 nl 命令；例子如下： tail -100f test.log 实时监控 100 行日志 tail -n 10 test.log 查询日志尾部最后 10 行的日志; tail -n +10 test.log 查询 10 行之后的所有日志; head跟 tail 是相反的，tail 是看后多少行日志；例子如下： head -n 10 test.log 查询日志文件中的头 10 行日志; head -n -10 test.log 查询日志文件除了最后 10 行的其他所有日志; cattac 是倒序查看，是 cat 单词反写；例子如下： cat -n test.log |grep “debug” 查询关键字的日志 vim进入编辑查找：vi(vim) 进入vim编辑模式： vim filename vim +n filename 进入特定行号日志 输入命令“set nu” 显示行号 输入“/关键字”,按enter键查找 查找下一个，按“n”即可 退出：按ESC键后，接着再输入:号时，vi会在屏幕的最下方等待我们输入命令 wq! 保存退出； q! 不保存退出； 切换方向 /关键字 注：正向查找，按n键把光标移动到下一个符合条件的地方 ?关键字 注：反向查找，按shift+n 键，把光标移动到下一个符合条件的 搜索关键字附近的日志 最常用的：cat -n filename |grep “关键字” 其他情况： cat filename | grep -C 5 ‘关键字’ (显示日志里匹配字串那行以及前后5行) cat filename | grep -B 5 ‘关键字’ (显示匹配字串及前5行) cat filename | grep -A 5 ‘关键字’ (显示匹配字串及后5行) 按行号查看 - 过滤出关键字附近的日志 cat -n test.log |grep “debug” 得到关键日志的行号 cat -n test.log |tail -n +92|head -n 20 选择关键字所在的中间一行. 然后查看这个关键字后 20 行的日志: tail -n +92 表示查询第 92 行之后的日志 head -n 20 则表示在前面的查询结果里再查后 20 条记录 根据日期查询日志sed -n ‘/2014-12-17 16:17:20/,/2014-12-17 16:17:36/p’ test.log 特别说明:上面的两个日期必须是日志中打印出来的日志，否则无效； 先 grep ‘2014-12-17 16:17:20’ test.log 来确定日志中是否有该 时间点 日志内容特别多，打印在屏幕上不方便查看 使用 more 和 less 命令，如： cat -n test.log |grep “debug” |more 这样就分页打印了,通过点击空格键翻页 使用 &gt;xxx.txt 将其保存到文件中，到时可以拉下这个文件分析，如：cat -n test.log |grep “debug” &gt; debug.txt 参考https://blog.csdn.net/yangkai_hudong/article/details/47783487 https://www.cnblogs.com/hunt/p/7064886.html https://blog.csdn.net/dingnning/article/details/7189862]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA - Debugger 经验总结]]></title>
    <url>%2F2017%2F06%2F02%2FIDEA-Debugger%2F</url>
    <content type="text"><![CDATA[本文主要介绍 IDEA - Debugger 的一些操作。每个都有场景和操作说明，动态 Gif 图体积较大，请耐心等待。 觉得图比较小的单击查看大图。 分析外部堆栈跟踪把报错信息复制到 Analyze -&gt; Analyze Stacktrace，快速进入程序块。开发中经常可以看到生产环境有错误日志，依照此方法快速将日志导入项目，定位问题。 场景： 操作： 返回到前一个堆栈帧IDEA 可在程序的执行流程中回退到先前的堆栈帧。要求不是最上面入口方法，选择 Drop Frame 后，等于未进入调用的方法。请注意：已经对全局状态进行的更改不会被恢复，只有本地变量会被重置。 强制从当前方法返回在当前堆栈帧中右键单击选择 Force Return 然后根据需要的返回类型输入即可。 抛出一个异常在当前堆栈帧中右键单击选择 Throw Exception 然后手动输入异常即可，比如 new NullPointerException(); 重新加载修改的类一般而言应用于在 Debugger 时发现未调用的方法有需要改动的地方，这时候修改未调用的方法，然后选择 Run -&gt; Reload Changed Classes, 快捷键 Alt + U, 然后 A. 这时候 Debugger 继续进行调用，则执行的调用方法逻辑为重新编译之后。底层逻辑是用到 JVM 的 hotSwap. 分析 Java Stream 操作IDEA Debugger 时可以可视化 Java Stream 进行的操作和对值数据的影响，需要断点停留在 Stream 上点击 Trace Current Stream Chain 按钮。 参考https://www.jetbrains.com/help/idea/analyzing-external-stacktraces.html https://www.jetbrains.com/help/idea/altering-the-program-s-execution-flow.html https://www.jetbrains.com/help/idea/analyze-java-stream-operations.html]]></content>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Start]]></title>
    <url>%2F2017%2F06%2F01%2FStart%2F</url>
    <content type="text"><![CDATA[本文简要讨论了博客起源。 博客搭建于 20170601，儿童节做出的决定。 目前定位于分享技术和个人思考，狭义来讲技术在最近一段时间是开发工具 IntelliJ IDEA 的一些最佳实践，个人思考在最近一段时间是关于一些书的读后感。 目前对 Github + Hexo + NexT 刚开始熟悉，有建议欢迎联系邮箱。 Ernest Hemingway once wrote:”The world is a fine place，and worth fighting for.”I agree with the second part.]]></content>
      <tags>
        <tag>default</tag>
      </tags>
  </entry>
</search>
