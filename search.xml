<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spark Streaming 应用解析]]></title>
    <url>%2F2017%2F08%2F21%2FSpark-Streaming-Analysis%2F</url>
    <content type="text"><![CDATA[本文主要解析部分 Spark Streaming 的技术点。 Spark Streaming 概述什么是 Spark Streaming Spark Streaming类似于Apache Storm，用于流式数据的处理。根据其官方文档介绍，Spark Streaming有高吞吐量和容错能力强等特点。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等。数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算。而结果也能保存在很多地方，如HDFS，数据库等。另外Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。 和Spark基于RDD的概念很相似，Spark Streaming使用离散化流(discretized stream)作为抽象表示，叫作DStream。DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而 DStream 是由这些 RDD 所组成的序列(因此 得名“离散化”)。 DStream 可以从各种输入源创建，比如 Flume、Kafka 或者 HDFS。创建出来的DStream 支持两种操作，一种是转化操作(transformation)，会生成一个新的DStream，另一种是输出操作(output operation)，可以把数据写入外部系统中。DStream 提供了许多与 RDD 所支持的操作相类似的操作支持，还增加了与时间相关的新操作，比如滑动窗口。 Spark Streaming 特点 易用 容错 整合 Spark 体系 Spark Streaming 与 Storm 对比 Spark Streaming：开发语言 scala, 编程模型 DStream. Strom：开发语言：Clojure, 编程模型：Spout/Bolt. 运行 Spark StreamingMaven 依赖123456&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 程序如下123456789101112131415161718192021222324252627282930package com.moqi.streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object WorldCount &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount") val ssc = new StreamingContext(conf, Seconds(1)) // Create a DStream that will connect to hostname:port, like localhost:9999 val lines = ssc.socketTextStream("master01", 9999) // Split each line into words val words = lines.flatMap(_.split(" ")) //import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3 // Count each word in each batch val pairs = words.map(word =&gt; (word, 1)) val wordCounts = pairs.reduceByKey(_ + _) // Print the first ten elements of each RDD generated in this DStream to the console wordCounts.print() ssc.start() // Start the computation ssc.awaitTermination() // Wait for the computation to terminate &#125;&#125; 按照Spark Core中的方式进行打包，并将程序上传到Spark机器上。并运行： 1bin/spark-submit --class com.moqi.streaming.WorldCount ~/wordcount-jar-with-dependencies.jar 通过Netcat发送数据： 123456# TERMINAL 1:# Running Netcat$ nc -lk 9999hello world 如果程序运行时，log日志太多，可以将spark conf目录下的log4j文件里面的日志级别改成WARN。 架构与抽象Spark Streaming使用“微批次”的架构，把流式计算当作一系列连续的小规模批处理来对待。Spark Streaming从各种输入源中读取数据，并把数据分组为小的批次。新的批次按均匀的时间间隔创建出来。在每个时间区间开始的时候，一个新的批次就创建出来，在该区间内收到的数据都会被添加到这个批次中。在时间区间结束时，批次停止增长。时间区间的大小是由批次间隔这个参数决定的。批次间隔一般设在500毫秒到几秒之间，由应用开发者配置。每个输入批次都形成一个RDD，以 Spark 作业的方式处理并生成其他的 RDD。 处理的结果可以以批处理的方式传给外部系统。高层次的架构如图： Spark Streaming的编程抽象是离散化流，也就是DStream。它是一个 RDD 序列，每个RDD代表数据流中一个时间片内的数据。 Spark Streaming在Spark的驱动器程序—工作节点的结构的执行过程如下图所示。Spark Streaming为每个输入源启动对 应的接收器。接收器以任务的形式运行在应用的执行器进程中，从输入源收集数据并保存为 RDD。它们收集到输入数据后会把数据复制到另一个执行器进程来保障容错性(默 认行为)。数据保存在执行器进程的内存中，和缓存 RDD 的方式一样。驱动器程序中的 StreamingContext 会周期性地运行 Spark 作业来处理这些数据，把数据与之前时间区间中的 RDD 进行整合。 Spark Streaming 解析初始化 StreamingContext1234567891011import org.apache.spark._import org.apache.spark.streaming._val conf = new SparkConf().setAppName(appName).setMaster(master)val ssc = new StreamingContext(conf, Seconds(1))// 可以通过ssc.sparkContext 来访问SparkContext// 或者通过已经存在的SparkContext来创建StreamingContextimport org.apache.spark.streaming._val sc = ... // existing SparkContextval ssc = new StreamingContext(sc, Seconds(1)) 初始化完Context之后： 定义消息输入源来创建DStreams.。 定义DStreams的转化操作和输出操作。 通过 streamingContext.start()来启动消息采集和处理。 等待程序终止，可以通过streamingContext.awaitTermination()来设置。 通过streamingContext.stop()来手动终止处理程序。 StreamingContext和SparkContext什么关系？ StreamingContext一旦启动，对DStreams的操作就不能修改了。在同一时间一个JVM中只有一个StreamingContext可以启动stop() 方法将同时停止SparkContext，可以传入参数stopSparkContext用于只停止StreamingContext在Spark1.4版本后，如何优雅的停止SparkStreaming而不丢失数据，通过设置sparkConf.set(“spark.streaming.stopGracefullyOnShutdown”,”true”) 即可。在StreamingContext的start方法中已经注册了Hook方法。 什么是 DStreamsDiscretized Stream是Spark Streaming的基础抽象，代表持续性的数据流和经过各种Spark原语操作后的结果数据流。在内部实现上，DStream是一系列连续的RDD来表示。每个RDD含有一段时间间隔内的数据，如下图： 对数据的操作也是按照RDD为单位来进行的： 计算过程由Spark engine来完成： DStreams 输入Spark Streaming原生支持一些不同的数据源。一些“核心”数据源已经被打包到Spark Streaming 的 Maven 工件中，而其他的一些则可以通过 spark-streaming-kafka 等附加工件获取。每个接收器都以 Spark 执行器程序中一个长期运行的任务的形式运行，因此会占据分配给应用的 CPU 核心。此外，我们还需要有可用的 CPU 核心来处理数据。这意味着如果要运行多个接收器，就必须至少有和接收器数目相同的核心数，还要加上用来完成计算所需要的核心数。例如，如果我们想要在流计算应用中运行 10 个接收器，那么至少需要为应用分配 11 个 CPU 核心。所以如果在本地模式运行，不要使用local或者local[1]。 基本数据源文件数据源Socket数据流前面的例子已经看到过。文件数据流：能够读取所有HDFS API兼容的文件系统文件，通过fileStream方法进行读取： 1streamingContext.fileStream[KeyClass, ValueClass, InputFormatClass](dataDirectory) Spark Streaming 将会监控 dataDirectory 目录并不断处理移动进来的文件，记住目前不支持嵌套目录。 文件需要有相同的数据格式。 文件进入 dataDirectory的方式需要通过移动或者重命名来实现。 一旦文件移动进目录，则不能再修改，即便修改了也不会读取新数据。 如果文件比较简单，则可以使用 streamingContext.textFileStream(dataDirectory)方法来读取文件。文件流不需要接收器，不需要单独分配CPU核。Hdfs读取实例（需要提前需要在HDFS上建好目录）： 123456789101112131415161718scala&gt; import org.apache.spark.streaming._import org.apache.spark.streaming._scala&gt; val ssc = new StreamingContext(sc, Seconds(1))ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@4027edebscala&gt; val lines = ssc.textFileStream("hdfs://master01:9000/data/")lines: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.MappedDStream@61d9dd15scala&gt; val words = lines.flatMap(_.split(" "))words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@1e084a26scala&gt; val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _)wordCounts: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.ShuffledDStream@8947a4bscala&gt; wordCounts.print()scala&gt; ssc.start() 上传文件上去： 1234[bigdata@master01 hadoop-2.7.3]$ lsbin data etc include lib libexec LICENSE.txt logs NOTICE.txt README.txt sbin sdata share[bigdata@master01 hadoop-2.7.3]$ bin/hdfs dfs -put ./LICENSE.txt /data/[bigdata@master01 hadoop-2.7.3]$ bin/hdfs dfs -put ./README.txt /data/ 获取计算结果： 123456789101112131415161718192021222324252627282930313233343536373839404142-------------------------------------------Time: 1504665716000 ms--------------------------------------------------------------------------------------Time: 1504665717000 ms--------------------------------------------------------------------------------------Time: 1504665718000 ms-------------------------------------------(227.7202-1,2)(created,2)(offer,8)(BUSINESS,11)(agree,10)(hereunder,,1)(“control”,1)(Grant,2)(2.2.,2)(include,11)...-------------------------------------------Time: 1504665719000 ms-------------------------------------------Time: 1504665739000 ms--------------------------------------------------------------------------------------Time: 1504665740000 ms-------------------------------------------(under,1)(Technology,1)(distribution,2)(http://hadoop.apache.org/core/,1)(Unrestricted,1)(740.13),1)(check,1)(have,1)(policies,1)(uses,1)...-------------------------------------------Time: 1504665741000 ms------------------------------------------- 自定义数据源通过继承Receiver，并实现onStart、onStop方法来自定义数据源采集。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class CustomReceiver(host: String, port: Int) extends Receiver[String](StorageLevel.MEMORY_AND_DISK_2) with Logging &#123; def onStart() &#123; // Start the thread that receives data over a connection new Thread("Socket Receiver") &#123; override def run() &#123; receive() &#125; &#125;.start() &#125; def onStop() &#123; // There is nothing much to do as the thread calling receive() // is designed to stop by itself if isStopped() returns false &#125; /** Create a socket connection and receive data until receiver is stopped */ private def receive() &#123; var socket: Socket = null var userInput: String = null try &#123; // Connect to host:port socket = new Socket(host, port) // Until stopped or connection broken continue reading val reader = new BufferedReader( new InputStreamReader(socket.getInputStream(), StandardCharsets.UTF_8)) userInput = reader.readLine() while(!isStopped &amp;&amp; userInput != null) &#123; store(userInput) userInput = reader.readLine() &#125; reader.close() socket.close() // Restart in an attempt to connect again when server is active again restart("Trying to connect again") &#125; catch &#123; case e: java.net.ConnectException =&gt; // restart if could not connect to server restart("Error connecting to " + host + ":" + port, e) case t: Throwable =&gt; // restart if there is any other error restart("Error receiving data", t) &#125; &#125;&#125; 可以通过streamingContext.receiverStream(\)，来使用自定义的数据采集源： 1234// Assuming ssc is the StreamingContextval customReceiverStream = ssc.receiverStream(new CustomReceiver(host, port))val words = lines.flatMap(_.split(" "))... 模拟Spark内置的Socket链接： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package com.moqi.streamingimport java.io.&#123;BufferedReader, InputStreamReader&#125;import java.net.Socketimport java.nio.charset.StandardCharsetsimport org.apache.spark.SparkConfimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.streaming.receiver.Receiverclass CustomReceiver (host: String, port: Int) extends Receiver[String](StorageLevel.MEMORY_AND_DISK_2) &#123; override def onStart(): Unit = &#123; // Start the thread that receives data over a connection new Thread("Socket Receiver") &#123; override def run() &#123; receive() &#125; &#125;.start() &#125; override def onStop(): Unit = &#123; // There is nothing much to do as the thread calling receive() // is designed to stop by itself if isStopped() returns false &#125; /** Create a socket connection and receive data until receiver is stopped */ private def receive() &#123; var socket: Socket = null var userInput: String = null try &#123; // Connect to host:port socket = new Socket(host, port) // Until stopped or connection broken continue reading val reader = new BufferedReader(new InputStreamReader(socket.getInputStream(), StandardCharsets.UTF_8)) userInput = reader.readLine() while(!isStopped &amp;&amp; userInput != null) &#123; // 传送出来 store(userInput) userInput = reader.readLine() &#125; reader.close() socket.close() // Restart in an attempt to connect again when server is active again restart("Trying to connect again") &#125; catch &#123; case e: java.net.ConnectException =&gt; // restart if could not connect to server restart("Error connecting to " + host + ":" + port, e) case t: Throwable =&gt; // restart if there is any other error restart("Error receiving data", t) &#125; &#125;&#125;object CustomReceiver &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount") val ssc = new StreamingContext(conf, Seconds(1)) // Create a DStream that will connect to hostname:port, like localhost:9999 val lines = ssc.receiverStream(new CustomReceiver("master01", 9999)) // Split each line into words val words = lines.flatMap(_.split(" ")) //import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3 // Count each word in each batch val pairs = words.map(word =&gt; (word, 1)) val wordCounts = pairs.reduceByKey(_ + _) // Print the first ten elements of each RDD generated in this DStream to the console wordCounts.print() ssc.start() // Start the computation ssc.awaitTermination() // Wait for the computation to terminate //ssc.stop() &#125;&#125; RDD 队列测试过程中，可以通过使用streamingContext.queueStream(queueOfRDDs)来创建DStream，每一个推送到这个队列中的RDD，都会作为一个DStream处理。 1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.moqi.streamingimport org.apache.spark.SparkConfimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import scala.collection.mutableobject QueueRdd &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setMaster("local[2]").setAppName("QueueRdd") val ssc = new StreamingContext(conf, Seconds(1)) // Create the queue through which RDDs can be pushed to // a QueueInputDStream //创建RDD队列 val rddQueue = new mutable.SynchronizedQueue[RDD[Int]]() // Create the QueueInputDStream and use it do some processing // 创建QueueInputDStream val inputStream = ssc.queueStream(rddQueue) //处理队列中的RDD数据 val mappedStream = inputStream.map(x =&gt; (x % 10, 1)) val reducedStream = mappedStream.reduceByKey(_ + _) //打印结果 reducedStream.print() //启动计算 ssc.start() // Create and push some RDDs into for (i &lt;- 1 to 30) &#123; rddQueue += ssc.sparkContext.makeRDD(1 to 300, 10) Thread.sleep(2000) //通过程序停止StreamingContext的运行 //ssc.stop() &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233[bigdata@master01 spark-2.1.1-bin-hadoop2.7]$ bin/spark-submit --class com.atguigu.streaming.QueueRdd ~/queueRdd-jar-with-dependencies.jar17/09/05 23:28:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable-------------------------------------------Time: 1504668485000 ms-------------------------------------------(4,30)(0,30)(6,30)(8,30)(2,30)(1,30)(3,30)(7,30)(9,30)(5,30)-------------------------------------------Time: 1504668486000 ms--------------------------------------------------------------------------------------Time: 1504668487000 ms-------------------------------------------(4,30)(0,30)(6,30)(8,30)(2,30)(1,30)(3,30)(7,30)(9,30)(5,30) 高级数据源除核心数据源外，还可以用附加数据源接收器来从一些知名数据获取系统中接收的数据，这些接收器都作为Spark Streaming的组件进行独立打包了。它们仍然是Spark的一部分，不过你需要在构建文件中添加额外的包才能使用它们。现有的接收器包括 Twitter、Apache Kafka、Amazon Kinesis、Apache Flume，以及ZeroMQ。可以通过添加与Spark版本匹配 的 Maven 工件 spark-streaming-[projectname]_2.10 来引入这些附加接收器。 Apache Kafka在工程中需要引入 Maven 工件 spark- streaming-kafka_2.10 来使用它。包内提供的 KafkaUtils 对象可以在 StreamingContext 和 JavaStreamingContext 中以你的 Kafka 消息创建出 DStream。由于 KafkaUtils 可以订阅多个主题，因此它创建出的 DStream 由成对的主题和消息组成。要创建出一个流数据，需 要使用 StreamingContext 实例、一个由逗号隔开的 ZooKeeper 主机列表字符串、消费者组的名字(唯一名字)，以及一个从主题到针对这个主题的接收器线程数的映射表来调用 createStream() 方法。 1234import org.apache.spark.streaming.kafka._...// 创建一个从主题到接收器线程数的映射表val topics = List(("pandas", 1), ("logs", 1)).toMapval topicLines = KafkaUtils.createStream(ssc, zkQuorum, group, topics) topicLines.map(_._2) 下面我们进行一个实例，演示SparkStreaming如何从Kafka读取消息，如果通过连接池方法把消息处理完成后再写会Kafka： kafka Connection Pool程序： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package com.moqi.streamingimport java.util.Propertiesimport org.apache.commons.pool2.impl.DefaultPooledObjectimport org.apache.commons.pool2.&#123;BasePooledObjectFactory, PooledObject&#125;import org.apache.kafka.clients.producer.&#123;KafkaProducer, ProducerRecord&#125;case class KafkaProducerProxy(brokerList: String, producerConfig: Properties = new Properties, defaultTopic: Option[String] = None, producer: Option[KafkaProducer[String, String]] = None) &#123; type Key = String type Val = String require(brokerList == null || !brokerList.isEmpty, "Must set broker list") private val p = producer getOrElse &#123; var props:Properties= new Properties(); props.put("bootstrap.servers", brokerList); props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); new KafkaProducer[String,String](props) &#125; private def toMessage(value: Val, key: Option[Key] = None, topic: Option[String] = None): ProducerRecord[Key, Val] = &#123; val t = topic.getOrElse(defaultTopic.getOrElse(throw new IllegalArgumentException("Must provide topic or default topic"))) require(!t.isEmpty, "Topic must not be empty") key match &#123; case Some(k) =&gt; new ProducerRecord(t, k, value) case _ =&gt; new ProducerRecord(t, value) &#125; &#125; def send(key: Key, value: Val, topic: Option[String] = None) &#123; p.send(toMessage(value, Option(key), topic)) &#125; def send(value: Val, topic: Option[String]) &#123; send(null, value, topic) &#125; def send(value: Val, topic: String) &#123; send(null, value, Option(topic)) &#125; def send(value: Val) &#123; send(null, value, None) &#125; def shutdown(): Unit = p.close()&#125;abstract class KafkaProducerFactory(brokerList: String, config: Properties, topic: Option[String] = None) extends Serializable &#123; def newInstance(): KafkaProducerProxy&#125;class BaseKafkaProducerFactory(brokerList: String, config: Properties = new Properties, defaultTopic: Option[String] = None) extends KafkaProducerFactory(brokerList, config, defaultTopic) &#123; override def newInstance() = new KafkaProducerProxy(brokerList, config, defaultTopic)&#125;class PooledKafkaProducerAppFactory(val factory: KafkaProducerFactory) extends BasePooledObjectFactory[KafkaProducerProxy] with Serializable &#123; override def create(): KafkaProducerProxy = factory.newInstance() override def wrap(obj: KafkaProducerProxy): PooledObject[KafkaProducerProxy] = new DefaultPooledObject(obj) override def destroyObject(p: PooledObject[KafkaProducerProxy]): Unit = &#123; p.getObject.shutdown() super.destroyObject(p) &#125;&#125; KafkaStreaming main： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package com.moqi.streamingimport org.apache.commons.pool2.impl.&#123;GenericObjectPool, GenericObjectPoolConfig&#125;import org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.SparkConfimport org.apache.spark.api.java.function.VoidFunctionimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.kafka010.&#123;ConsumerStrategies, KafkaUtils, LocationStrategies&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object createKafkaProducerPool&#123; def apply(brokerList: String, topic: String): GenericObjectPool[KafkaProducerProxy] = &#123; val producerFactory = new BaseKafkaProducerFactory(brokerList, defaultTopic = Option(topic)) val pooledProducerFactory = new PooledKafkaProducerAppFactory(producerFactory) val poolConfig = &#123; val c = new GenericObjectPoolConfig val maxNumProducers = 10 c.setMaxTotal(maxNumProducers) c.setMaxIdle(maxNumProducers) c &#125; new GenericObjectPool[KafkaProducerProxy](pooledProducerFactory, poolConfig) &#125;&#125;object KafkaStreaming&#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setMaster("local[4]").setAppName("NetworkWordCount") val ssc = new StreamingContext(conf, Seconds(1)) //创建topic val brobrokers = "172.16.148.150:9092,172.16.148.151:9092,172.16.148.152:9092" val sourcetopic="source"; val targettopic="target"; //创建消费者组 var group="con-consumer-group" //消费者配置 val kafkaParam = Map( "bootstrap.servers" -&gt; brobrokers,//用于初始化链接到集群的地址 "key.deserializer" -&gt; classOf[StringDeserializer], "value.deserializer" -&gt; classOf[StringDeserializer], //用于标识这个消费者属于哪个消费团体 "group.id" -&gt; group, //如果没有初始化偏移量或者当前的偏移量不存在任何服务器上，可以使用这个配置属性 //可以使用这个配置，latest自动重置偏移量为最新的偏移量 "auto.offset.reset" -&gt; "latest", //如果是true，则这个消费者的偏移量会在后台自动提交 "enable.auto.commit" -&gt; (false: java.lang.Boolean) ); //ssc.sparkContext.broadcast(pool) //创建DStream，返回接收到的输入数据 var stream=KafkaUtils.createDirectStream[String,String](ssc, LocationStrategies.PreferConsistent,ConsumerStrategies.Subscribe[String,String](Array(sourcetopic),kafkaParam)) //每一个stream都是一个ConsumerRecord stream.map(s =&gt;("id:" + s.key(),"&gt;&gt;&gt;&gt;:"+s.value())).foreachRDD(rdd =&gt; &#123; rdd.foreachPartition(partitionOfRecords =&gt; &#123; // Get a producer from the shared pool val pool = createKafkaProducerPool(brobrokers, targettopic) val p = pool.borrowObject() partitionOfRecords.foreach &#123;message =&gt; System.out.println(message._2);p.send(message._2,Option(targettopic))&#125; // Returning the producer to the pool also shuts it down pool.returnObject(p) &#125;) &#125;) ssc.start() ssc.awaitTermination() &#125;&#125; 程序部署： 启动zookeeper和kafka： 1bin/kafka-server-start.sh -deamon ./config/server.properties 创建两个topic，一个为source，一个为target： 123bin/kafka-topics.sh --create --zookeeper 192.168.56.150:2181,192.168.56.151:2181,192.168.56.152:2181 --replication-factor 2 --partitions 2 --topic sourcebin/kafka-topics.sh --create --zookeeper 172.16.148.150:2181,172.16.148.151:2181,172.16.148.152:2181 --replication-factor 2 --partitions 2 --topic target 启动kafka console producer 写入source topic： 1bin/kafka-console-producer.sh --broker-list 192.168.56.150:9092, 192.168.56.151:9092, 192.168.56.152:9092 --topic source 启动kafka console consumer 监听target topic： 1bin/kafka-console-consumer.sh --bootstrap-server 192.168.56.150:9092, 192.168.56.151:9092, 192.168.56.152:9092 --topic source 启动kafkaStreaming程序： 1[bigdata@master01 ~]$ ./hadoop/spark-2.1.1-bin-hadoop2.7/bin/spark-submit --class com.moqi.streaming.KafkaStreaming ./kafkastreaming-jar-with-dependencies.jar 程序运行。 Spark 两种连接 Kafka 方式Spark对于Kafka的连接主要有两种方式，一种是DirectKafkaInputDStream，另外一种是KafkaInputDStream。DirectKafkaInputDStream 只在 driver 端接收数据，所以继承了 InputDStream，是没有 receivers 的。主要通过KafkaUtils#createDirectStream以及KafkaUtils#createStream这两个 API 来创建，除了要传入的参数不同外，接收 kafka 数据的节点、拉取数据的时机也完全不同。KafkaUtils#createStream【Receiver-based】这种方法使用一个 Receiver 来接收数据。在该 Receiver 的实现中使用了 Kafka high-level consumer API。Receiver 从 kafka 接收的数据将被存储到 Spark executor 中，随后启动的 job 将处理这些数据。在默认配置下，该方法失败后会丢失数据（保存在 executor 内存里的数据在 application 失败后就没了），若要保证数据不丢失，需要启用 WAL（即预写日志至 HDFS、S3等），这样再失败后可以从日志文件中恢复数据。在该函数中，会新建一个 KafkaInputDStream对象，KafkaInputDStream继承于 ReceiverInputDStream。KafkaInputDStream实现了getReceiver方法，返回接收器的实例： 123456789def getReceiver(): Receiver[(K, V)] = &#123; if (!useReliableReceiver) &#123; //&lt; 不启用 WAL new KafkaReceiver[K, V, U, T](kafkaParams, topics, storageLevel) &#125; else &#123; //&lt; 启用 WAL new ReliableKafkaReceiver[K, V, U, T](kafkaParams, topics, storageLevel) &#125; &#125; 根据是否启用 WAL，receiver 分为 KafkaReceiver 和 ReliableKafkaReceiver。下图描述了 KafkaReceiver 接收数据的具体流程： 需要注意的点： Kafka Topic 的 partitions 与RDD 的 partitions 没有直接关系，不能一一对应。如果增加 topic 的 partition 个数的话仅仅会增加单个 Receiver 接收数据的线程数。事实上，使用这种方法只会在一个 executor 上启用一个 Receiver，该 Receiver 包含一个线程池，线程池的线程个数与所有 topics 的 partitions 个数总和一致，每条线程接收一个 topic 的一个 partition 的数据。而并不会增加处理数据时的并行度。 对于一个 topic，可以使用多个 groupid 相同的 input DStream 来使用多个 Receivers 来增加并行度，然后 union 他们；对于多个 topics，除了可以用上个办法增加并行度外，还可以对不同的 topic 使用不同的 input DStream 然后 union 他们来增加并行度。 如果你启用了 WAL，为能将接收到的数据将以 log 的方式在指定的存储系统备份一份，需要指定输入数据的存储等级为 StorageLevel.MEMORY_AND_DISK_SER 或 StorageLevel.MEMORY_AND_DISK_SER_2. KafkaUtils#createDirectStream【WithOut Receiver】自 Spark-1.3.0 起，提供了不需要 Receiver 的方法。替代了使用 receivers 来接收数据，该方法定期查询每个 topic+partition 的 lastest offset，并据此决定每个 batch 要接收的 offsets 范围。KafkaUtils#createDirectStream调用中，会新建DirectKafkaInputDStream，DirectKafkaInputDStream#compute(validTime: Time)会从 kafka 拉取数据并生成 RDD，流程如下： 如上图所示，该函数主要做了以下三个事情： 确定要接收的 partitions 的 offsetRange，以作为第2步创建的 RDD 的数据来源。 创建 RDD 并执行 count 操作，使 RDD 真实具有数据。 以 streamId、数据条数，offsetRanges 信息初始化 inputInfo 并添加到 JobScheduler 中。 进一步看 KafkaRDD 的 getPartitions 实现： 123456override def getPartitions: Array[Partition] = &#123; offsetRanges.zipWithIndex.map &#123; case (o, i) =&gt; val (host, port) = leaders(TopicAndPartition(o.topic, o.partition)) new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset, host, port) &#125;.toArray &#125; 从上面的代码可以很明显看到，KafkaRDD 的 partition 数据与 Kafka topic 的某个 partition 的 o.fromOffset 至 o.untilOffset 数据是相对应的，也就是说 KafkaRDD 的 partition 与 Kafka partition 是一一对应的该方式相比使用 Receiver 的方式有以下好处： 简化并行：不再需要创建多个 kafka input DStream 然后再 union 这些 input DStream。使用 directStream，Spark Streaming会创建与 Kafka partitions 相同数量的 paritions 的 RDD，RDD 的 partition与 Kafka 的 partition 一一对应，这样更易于理解及调优。 高效：在方式一中要保证数据零丢失需要启用 WAL（预写日志），这会占用更多空间。而在方式二中，可以直接从 Kafka 指定的 topic 的指定 offsets 处恢复数据，不需要使用 WAL. 恰好一次语义保证：基于Receiver方式使用了 Kafka 的 high level API 来在 Zookeeper 中存储已消费的 offsets。这在某些情况下会导致一些数据被消费两次，比如 streaming app 在处理某个 batch 内已接受到的数据的过程中挂掉，但是数据已经处理了一部分，但这种情况下无法将已处理数据的 offsets 更新到 Zookeeper 中，下次重启时，这批数据将再次被消费且处理。基于direct的方式，使用kafka的简单api，Spark Streaming自己就负责追踪消费的offset，并保存在checkpoint中。Spark自己一定是同步的，因此可以保证数据是消费一次且仅消费一次。这种方式中，只要将 output 操作和保存 offsets 操作封装成一个原子操作就能避免失败后的重复消费和处理，从而达到恰好一次的语义（Exactly-once）。 通过以上分析，我们可以对这两种方式的区别做一个总结： createStream会使用 Receiver；而createDirectStream不会。 createStream使用的 Receiver 会分发到某个 executor 上去启动并接受数据；而createDirectStream直接在 driver 上接收数据。 createStream使用 Receiver 源源不断的接收数据并把数据交给 ReceiverSupervisor 处理最终存储为 blocks 作为 RDD 的输入，从 kafka 拉取数据与计算消费数据相互独立；而createDirectStream会在每个 batch 拉取数据并就地消费，到下个 batch 再次拉取消费，周而复始，从 kafka 拉取数据与计算消费数据是连续的，没有独立开。 createStream中创建的KafkaInputDStream 每个 batch 所对应的 RDD 的 partition 不与 Kafka partition 一一对应；而createDirectStream中创建的 DirectKafkaInputDStream 每个 batch 所对应的 RDD 的 partition 与 Kafka partition 一一对应。 Flume-ngSpark提供两个不同的接收器来使用Apache Flume。 两个接收器简介如下。 推式接收器该接收器以 Avro 数据池的方式工作，由 Flume 向其中推数据。 拉式接收器该接收器可以从自定义的中间数据池中拉数据，而其他进程可以使用 Flume 把数据推进 该中间数据池。 两种方式都需要重新配置 Flume，并在某个节点配置的端口上运行接收器(不是已有的 Spark 或者 Flume 使用的端口)。要使用其中任何一种方法，都需要在工程中引入 Maven 工件 spark-streaming-flume_2.10。 推式接收器的方法设置起来很容易，但是它不使用事务来接收数据。在这种方式中，接收 器以 Avro 数据池的方式工作，我们需要配置 Flume 来把数据发到 Avro 数据池。我们提供的 FlumeUtils 对象会把接收器配置在一个特定的工作节点的主机名及端口号上。这些设置必须和 Flume 配置相匹配。 123456789// Flume 对 Avro 池的配置a1.sinks = avroSinka1.sinks.avroSink.type = avroa1.sinks.avroSink.channel = memoryChannela1.sinks.avroSink.hostname = receiver-hostnamea1.sinks.avroSink.port = port-used-for-avro-sink-not-spark-port// Scala 中的 FlumeUtils 代理val events = FlumeUtils.createStream(ssc, receiverHostname, receiverPort) 虽然这种方式很简洁，但缺点是没有事务支持。这会增加运行接收器的工作节点发生错误 时丢失少量数据的几率。不仅如此，如果运行接收器的工作节点发生故障，系统会尝试从 另一个位置启动接收器，这时需要重新配置 Flume 才能将数据发给新的工作节点。这样配 置会比较麻烦。较新的方式是拉式接收器(在Spark 1.1中引入)，它设置了一个专用的Flume数据池供 Spark Streaming读取，并让接收器主动从数据池中拉取数据。这种方式的优点在于弹性较 好，Spark Streaming通过事务从数据池中读取并复制数据。在收到事务完成的通知前，这 些数据还保留在数据池中。我们需要先把自定义数据池配置为 Flume 的第三方插件。安装插件的最新方法请参考 Flume 文档的相关部分。由于插件是用 Scala 写的，因此需要把插件本身以及 Scala 库都添加到 Flume 插件 中。Spark 1.1 中对应的 Maven 索引如例 10-37 所示。 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-flume-sink_2.11&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;2.11.11&lt;/version&gt;&lt;/dependency&gt; 当你把自定义 Flume 数据池添加到一个节点上之后，就需要配置 Flume 来把数据推送到这个数据池中： 12345a1.sinks = sparka1.sinks.spark.type = org.apache.spark.streaming.flume.sink.SparkSinka1.sinks.spark.hostname = receiver-hostnamea1.sinks.spark.port = port-used-for-sync-not-spark-porta1.sinks.spark.channel = memoryChannel 等到数据已经在数据池中缓存起来，就可以调用 FlumeUtils 来读取数据了 : 12// 在 Scala 中使用 FlumeUtils 读取自定义数据池val events = FlumeUtils.createPollingStream(ssc, receiverHostname, receiverPort) DStreams 转换DStream上的原语与RDD的类似，分为Transformations（转换）和Output Operations（输出）两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种Window相关的原语。 Transformation Meaning map(func) 将源DStream中的每个元素通过一个函数func从而得到新的DStreams。 flatMap(func) 和map类似，但是每个输入的项可以被映射为0或更多项。 filter(func) 选择源DStream中函数func判为true的记录作为新DStreams repartition(numPartitions) 通过创建更多或者更少的partition来改变此DStream的并行级别。 union(otherStream) 联合源DStreams和其他DStreams来得到新DStream count() 统计源DStreams中每个RDD所含元素的个数得到单元素RDD的新DStreams。 reduce(func) 通过函数func(两个参数一个输出)来整合源DStreams中每个RDD元素得到单元素RDD的DStreams。这个函数需要关联从而可以被并行计算。 countByValue() 对于DStreams中元素类型为K调用此函数，得到包含(K,Long)对的新DStream，其中Long值表明相应的K在源DStream中每个RDD出现的频率。 reduceByKey(func, [numTasks]) 对(K,V)对的DStream调用此函数，返回同样（K,V)对的新DStream，但是新DStream中的对应V为使用reduce函数整合而来。Note：默认情况下，这个操作使用Spark默认数量的并行任务（本地模式为2，集群模式中的数量取决于配置参数spark.default.parallelism）。你也可以传入可选的参数numTaska来设置不同数量的任务。 join(otherStream, [numTasks]) 两DStream分别为(K,V)和(K,W)对，返回(K,(V,W))对的新DStream。 cogroup(otherStream, [numTasks]) 两DStream分别为(K,V)和(K,W)对，返回(K,(Seq[V],Seq[W])对新DStreams transform(func) 将RDD到RDD映射的函数func作用于源DStream中每个RDD上得到新DStream。这个可用于在DStream的RDD上做任意操作。 updateStateByKey(func) 得到”状态”DStream，其中每个key状态的更新是通过将给定函数用于此key的上一个状态和新值而得到。这个可用于保存每个key值的任意状态数据。 DStream 的转化操作可以分为无状态(stateless)和有状态(stateful)两种。 在无状态转化操作中，每个批次的处理不依赖于之前批次的数据。常见的 RDD 转化操作，例如 map()、filter()、reduceByKey() 等，都是无状态转化操作。 相对地，有状态转化操作需要使用之前批次的数据或者是中间结果来计算当前批次的数据。有状态转化操作包括基于滑动窗口的转化操作和追踪状态变化的转化操作。 无状态转化操作无状态转化操作就是把简单的 RDD 转化操作应用到每个批次上，也就是转化 DStream 中的每一个 RDD。部分无状态转化操作列在了下表中。 注意，针对键值对的 DStream 转化操作(比如 reduceByKey())要添加import StreamingContext._ 才能在 Scala中使用。 需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个 DStream 在内部是由许多 RDD(批次)组成，且无状态转化操作是分别应用到每个 RDD 上的。例如， reduceByKey() 会归约每个时间区间中的数据，但不会归约不同区间之间的数据。举个例子，在之前的wordcount程序中，我们只会统计1秒内接收到的数据的单词个数，而不会累加。无状态转化操作也能在多个 DStream 间整合数据，不过也是在各个时间区间内。例如，键 值对 DStream 拥有和 RDD 一样的与连接相关的转化操作，也就是 cogroup()、join()、 leftOuterJoin() 等。我们可以在 DStream 上使用这些操作，这样就对每个批次分别执行了对应的 RDD 操作。我们还可以像在常规的 Spark 中一样使用 DStream 的 union() 操作将它和另一个 DStream 的内容合并起来，也可以使用 StreamingContext.union() 来合并多个流。 有状态转化操作追踪状态变化 UpdateStateByKeyUpdateStateByKey原语用于记录历史记录，有时，我们需要在 DStream 中跨批次维护状态(例如流计算中累加wordcount)。针对这种情况，updateStateByKey() 为我们提供了对一个状态变量的访问，用于键值对形式的 DStream。给定一个由(键，事件)对构成的 DStream，并传递一个指定如何根据新的事件 更新每个键对应状态的函数，它可以构建出一个新的 DStream，其内部数据为(键，状态) 对。updateStateByKey() 的结果会是一个新的 DStream，其内部的 RDD 序列是由每个时间区间对应的(键，状态)对组成的。updateStateByKey操作使得我们可以在用新信息进行更新时保持任意的状态。为使用这个功能，你需要做下面两步： 定义状态，状态可以是一个任意的数据类型。 定义状态更新函数，用此函数阐明如何使用之前的状态和来自输入流的新值对状态进行更新。 使用updateStateByKey需要对检查点目录进行配置，会使用检查点来保存状态。更新版的wordcount： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.moqi.streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object WorldCount &#123; def main(args: Array[String]) &#123; // 定义更新状态方法，参数values为当前批次单词频度，state为以往批次单词频度 val updateFunc = (values: Seq[Int], state: Option[Int]) =&gt; &#123; val currentCount = values.foldLeft(0)(_ + _) val previousCount = state.getOrElse(0) Some(currentCount + previousCount) &#125; val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount") val ssc = new StreamingContext(conf, Seconds(3)) ssc.checkpoint(".") // Create a DStream that will connect to hostname:port, like localhost:9999 val lines = ssc.socketTextStream("master01", 9999) // Split each line into words val words = lines.flatMap(_.split(" ")) //import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3 // Count each word in each batch val pairs = words.map(word =&gt; (word, 1)) // 使用updateStateByKey来更新状态，统计从运行开始以来单词总的次数 val stateDstream = pairs.updateStateByKey[Int](updateFunc) stateDstream.print() //val wordCounts = pairs.reduceByKey(_ + _) // Print the first ten elements of each RDD generated in this DStream to the console //wordCounts.print() ssc.start() // Start the computation ssc.awaitTermination() // Wait for the computation to terminate //ssc.stop() &#125;&#125; 启动nc –lk 9999： 123[bigdata@master01 ~]$ nc -lk 9999ni shi shuini hao ma 启动统计程序： 123456789101112131415161718192021222324252627[bigdata@master01 ~]$ ./hadoop/spark-2.1.1-bin-hadoop2.7/bin/spark-submit --class com.moqi.streaming.WorldCount ./statefulwordcount-jar-with-dependencies.jar17/09/06 04:06:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable-------------------------------------------Time: 1504685175000 ms--------------------------------------------------------------------------------------Time: 1504685181000 ms-------------------------------------------(shi,1)(shui,1)(ni,1)-------------------------------------------Time: 1504685187000 ms-------------------------------------------(shi,1)(ma,1)(hao,1)(shui,1)(ni,2)[bigdata@master01 ~]$ ls2df8e0c3-174d-401a-b3a7-f7776c3987db checkpoint-1504685205000 databackup checkpoint-1504685205000.bk debug.logcheckpoint-1504685199000 checkpoint-1504685208000 hadoopcheckpoint-1504685199000.bk checkpoint-1504685208000.bk receivedBlockMetadatacheckpoint-1504685202000 checkpoint-1504685211000 softwarecheckpoint-1504685202000.bk checkpoint-1504685211000.bk statefulwordcount-jar-with-dependencies.jar Window OperationsWindow Operations有点类似于Storm中的State，可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming的允许状态。基于窗口的操作会在一个比 StreamingContext 的批次间隔更长的时间范围内，通过整合多个批次的结果，计算出整个窗口的结果。 所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长，两者都必须是 StreamContext 的批次间隔的整数倍。窗口时长控制每次计算最近的多少个批次的数据，其实就是最近的 windowDuration/batchInterval 个批次。如果有一个以 10 秒为批次间隔的源 DStream，要创建一个最近 30 秒的时间窗口(即最近 3 个批次)，就应当把 windowDuration 设为 30 秒。而滑动步长的默认值与批次间隔相等，用来控制对新的 DStream 进行计算的间隔。如果源 DStream 批次间隔为 10 秒，并且我们只希望每两个批次计算一次窗口结果， 就应该把滑动步长设置为 20 秒。假设，你想拓展前例从而每隔十秒对持续30秒的数据生成word count。为做到这个，我们需要在持续30秒数据的(word,1)对DStream上应用reduceByKey。使用操作reduceByKeyAndWindow. 12# reduce last 30 seconds of data, every 10 secondwindowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x -y, 30, 20) Transformation Meaning window(windowLength, slideInterval) 基于对源DStream窗化的批次进行计算返回一个新的DStream countByWindow(windowLength, slideInterval) 返回一个滑动窗口计数流中的元素。 reduceByWindow(func, windowLength, slideInterval) 通过使用自定义函数整合滑动区间流元素来创建一个新的单元素流。 reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]) 当在一个(K,V)对的DStream上调用此函数，会返回一个新(K,V)对的DStream，此处通过对滑动窗口中批次数据使用reduce函数来整合每个key的value值。Note:默认情况下，这个操作使用Spark的默认数量并行任务(本地是2)，在集群模式中依据配置属性(spark.default.parallelism)来做grouping。你可以通过设置可选参数numTasks来设置不同数量的tasks。 reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]) 这个函数是上述函数的更高效版本，每个窗口的reduce值都是通过用前一个窗的reduce值来递增计算。通过reduce进入到滑动窗口数据并”反向reduce”离开窗口的旧数据来实现这个操作。一个例子是随着窗口滑动对keys的“加”“减”计数。通过前边介绍可以想到，这个函数只适用于”可逆的reduce函数”，也就是这些reduce函数有相应的”反reduce”函数(以参数invFunc形式传入)。如前述函数，reduce任务的数量通过可选参数来配置。注意：为了使用这个操作，CheckPoint 必须可用。 countByValueAndWindow(windowLength,slideInterval, [numTasks]) 对(K,V)对的DStream调用，返回(K,Long)对的新DStream，其中每个key的值是其在滑动窗口中频率。如上，可配置reduce任务数量。 reduceByWindow() 和 reduceByKeyAndWindow() 让我们可以对每个窗口更高效地进行归约操作。它们接收一个归约函数，在整个窗口上执行，比如 +。除此以外，它们还有一种特殊形式，通过只考虑新进入窗口的数据和离开窗 口的数据，让 Spark 增量计算归约结果。这种特殊形式需要提供归约函数的一个逆函数，比 如 + 对应的逆函数为 -。对于较大的窗口，提供逆函数可以大大提高执行效率。 1234567val ipDStream = accessLogsDStream.map(logEntry =&gt; (logEntry.getIpAddress(), 1))val ipCountDStream = ipDStream.reduceByKeyAndWindow( &#123;(x, y) =&gt; x + y&#125;, &#123;(x, y) =&gt; x - y&#125;, Seconds(30), Seconds(10)) // 加上新进入窗口的批次中的元素 // 移除离开窗口的老批次中的元素 // 窗口时长 // 滑动步长 countByWindow() 和 countByValueAndWindow() 作为对数据进行 计数操作的简写。countByWindow() 返回一个表示每个窗口中元素个数的 DStream，而 countByValueAndWindow() 返回的 DStream 则包含窗口中每个值的个数。 123val ipDStream = accessLogsDStream.map&#123;entry =&gt; entry.getIpAddress()&#125; val ipAddressRequestCount = ipDStream.countByValueAndWindow(Seconds(30), Seconds(10)) val requestCount = accessLogsDStream.countByWindow(Seconds(30), Seconds(10)) WordCount第三版：3秒一个批次，窗口12秒，滑步6秒。 1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.moqi.streamingimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object WorldCount &#123; def main(args: Array[String]) &#123; // 定义更新状态方法，参数values为当前批次单词频度，state为以往批次单词频度 val updateFunc = (values: Seq[Int], state: Option[Int]) =&gt; &#123; val currentCount = values.foldLeft(0)(_ + _) val previousCount = state.getOrElse(0) Some(currentCount + previousCount) &#125; val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount") val ssc = new StreamingContext(conf, Seconds(3)) ssc.checkpoint(".") // Create a DStream that will connect to hostname:port, like localhost:9999 val lines = ssc.socketTextStream("master01", 9999) // Split each line into words val words = lines.flatMap(_.split(" ")) //import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3 // Count each word in each batch val pairs = words.map(word =&gt; (word, 1)) val wordCounts = pairs.reduceByKeyAndWindow((a:Int,b:Int) =&gt; (a + b),Seconds(12), Seconds(6)) // Print the first ten elements of each RDD generated in this DStream to the console wordCounts.print() ssc.start() // Start the computation ssc.awaitTermination() // Wait for the computation to terminate //ssc.stop() &#125;&#125; 重要操作Transform OperationTransform原语允许DStream上执行任意的RDD-to-RDD函数。即使这些函数并没有在DStream的API中暴露出来，通过该函数可以方便的扩展Spark API。该函数每一批次调度一次。比如下面的例子，在进行单词统计的时候，想要过滤掉spam的信息。其实也就是对DStream中的RDD应用转换。 123456val spamInfoRDD = ssc.sparkContext.newAPIHadoopRDD(...) // RDD containing spam informationval cleanedDStream = wordCounts.transform &#123; rdd =&gt; rdd.join(spamInfoRDD).filter(...) // join data stream with spam information to do data cleaning ...&#125; join 操作连接操作（leftOuterJoin, rightOuterJoin, fullOuterJoin也可以），可以连接Stream-Stream，windows-stream to windows-stream、stream-dataset. Stream-Stream Joins: 1234567val stream1: DStream[String, String] = ...val stream2: DStream[String, String] = ...val joinedStream = stream1.join(stream2)val windowedStream1 = stream1.window(Seconds(20))val windowedStream2 = stream2.window(Minutes(1))val joinedStream = windowedStream1.join(windowedStream2) Stream-dataset joins 123val dataset: RDD[String, String] = ...val windowedStream = stream.window(Seconds(20))...val joinedStream = windowedStream.transform &#123; rdd =&gt; rdd.join(dataset) &#125; DStreams 输出输出操作指定了对流数据经转化操作得到的数据所要执行的操作(例如把结果推入外部数据库或输出到屏幕上)。与 RDD 中的惰性求值类似，如果一个 DStream 及其派生出的 DStream 都没有被执行输出操作，那么这些 DStream 就都不会被求值。如果 StreamingContext 中没有设定输出操作，整个 context 就都不会启动。 Output Operation Meaning print() 在运行流程序的驱动结点上打印DStream中每一批次数据的最开始10个元素。这用于开发和调试。在Python API中，同样的操作叫pprint()。 saveAsTextFiles(prefix, [suffix]) 以text文件形式存储这个DStream的内容。每一批次的存储文件名基于参数中的prefix和suffix。”prefix-Time_IN_MS[.suffix]”. saveAsObjectFiles(prefix, [suffix]) 以Java对象序列化的方式将Stream中的数据保存为 SequenceFiles . 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”. Python中目前不可用。 saveAsHadoopFiles(prefix, [suffix]) 将Stream中的数据保存为 Hadoop files. 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”. Python API Python中目前不可用。 foreachRDD(func) 这是最通用的输出操作，即将函数func用于产生于stream的每一个RDD。其中参数传入的函数func应该实现将每一个RDD中数据推送到外部系统，如将RDD存入文件或者通过网络将其写入数据库。注意：函数func在运行流应用的驱动中被执行，同时其中一般函数RDD操作从而强制其对于流RDD的运算。 通用的输出操作 foreachRDD()，它用来对 DStream 中的 RDD 运行任意计算。这和transform() 有些类似，都可以让我们访问任意 RDD。在 foreachRDD() 中，可以重用我们在 Spark 中实现的所有行动操作。比如，常见的用例之一是把数据写到诸如 MySQL 的外部数据库中。需要注意的： 连接不能写在driver层面。 如果写在foreach则每个RDD都创建，得不偿失。 增加foreachPartition，在分区创建。 可以考虑使用连接池优化。 1234567891011dstream.foreachRDD &#123; rdd =&gt; // error val connection = createNewConnection() // executed at the driver 序列化错误 rdd.foreachPartition &#123; partitionOfRecords =&gt; // ConnectionPool is a static, lazily initialized pool of connections val connection = ConnectionPool.getConnection() partitionOfRecords.foreach(record =&gt; connection.send(record) // executed at the worker ) ConnectionPool.returnConnection(connection) // return to the pool for future reuse &#125;&#125; 累加器和广播变量累加器(Accumulators)和广播变量(Broadcast variables)不能从Spark Streaming的检查点中恢复。如果你启用检查并也使用了累加器和广播变量，那么你必须创建累加器和广播变量的延迟单实例从而在驱动因失效重启后他们可以被重新实例化。如下例述： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748object WordBlacklist &#123; @volatile private var instance: Broadcast[Seq[String]] = null def getInstance(sc: SparkContext): Broadcast[Seq[String]] = &#123; if (instance == null) &#123; synchronized &#123; if (instance == null) &#123; val wordBlacklist = Seq("a", "b", "c") instance = sc.broadcast(wordBlacklist) &#125; &#125; &#125; instance &#125;&#125;object DroppedWordsCounter &#123; @volatile private var instance: LongAccumulator = null def getInstance(sc: SparkContext): LongAccumulator = &#123; if (instance == null) &#123; synchronized &#123; if (instance == null) &#123; instance = sc.longAccumulator("WordsInBlacklistCounter") &#125; &#125; &#125; instance &#125;&#125;wordCounts.foreachRDD &#123; (rdd: RDD[(String, Int)], time: Time) =&gt; // Get or register the blacklist Broadcast val blacklist = WordBlacklist.getInstance(rdd.sparkContext) // Get or register the droppedWordsCounter Accumulator val droppedWordsCounter = DroppedWordsCounter.getInstance(rdd.sparkContext) // Use blacklist to drop words and use droppedWordsCounter to count them val counts = rdd.filter &#123; case (word, count) =&gt; if (blacklist.value.contains(word)) &#123; droppedWordsCounter.add(count) false &#125; else &#123; true &#125; &#125;.collect().mkString("[", ", ", "]") val output = "Counts at time " + time + " " + counts&#125;) DataFrame and SQL Operations你可以很容易地在流数据上使用DataFrames和SQL。你必须使用SparkContext来创建StreamingContext要用的SQLContext。此外，这一过程可以在驱动失效后重启。我们通过创建一个实例化的SQLContext单实例来实现这个工作。如下例所示。我们对前例word count进行修改从而使用DataFrames和SQL来产生word counts。每个RDD被转换为DataFrame，以临时表格配置并用SQL进行查询。 12345678910111213141516171819val words: DStream[String] = ...words.foreachRDD &#123; rdd =&gt; // Get the singleton instance of SparkSession val spark = SparkSession.builder.config(rdd.sparkContext.getConf).getOrCreate() import spark.implicits._ // Convert RDD[String] to DataFrame val wordsDataFrame = rdd.toDF("word") // Create a temporary view wordsDataFrame.createOrReplaceTempView("words") // Do word count on DataFrame using SQL and print it val wordCountsDataFrame = spark.sql("select word, count(*) as total from words group by word") wordCountsDataFrame.show()&#125; 你也可以从不同的线程在定义于流数据的表上运行SQL查询（也就是说，异步运行StreamingContext）。仅确定你设置StreamingContext记住了足够数量的流数据以使得查询操作可以运行。否则，StreamingContext不会意识到任何异步的SQL查询操作，那么其就会在查询完成之后删除旧的数据。例如，如果你要查询最后一批次，但是你的查询会运行5分钟，那么你需要调用streamingContext.remember(Minutes(5))(in Scala, 或者其他语言的等价操作)。 Caching / Persistence和RDDs类似，DStreams同样允许开发者将流数据保存在内存中。也就是说，在DStream上使用persist()方法将会自动把DStreams中的每个RDD保存在内存中。当DStream中的数据要被多次计算时，这个非常有用（如在同样数据上的多次操作）。对于像reduceByWindow和reduceByKeyAndWindow以及基于状态的(updateStateByKey)这种操作，保存是隐含默认的。因此，即使开发者没有调用persist()，由基于窗操作产生的DStreams会自动保存在内存中。 7 * 24 不间断运行CheckPoint 机制检查点机制是我们在Spark Streaming中用来保障容错性的主要机制。与应用程序逻辑无关的错误（即系统错位，JVM崩溃等）有迅速恢复的能力.它可以使Spark Streaming阶段性地把应用数据存储到诸如HDFS或Amazon S3这样的可靠存储系统中， 以供恢复时使用。具体来说，检查点机制主要为以下两个目的服务。 控制发生失败时需要重算的状态数。SparkStreaming可以通 过转化图的谱系图来重算状态，检查点机制则可以控制需要在转化图中回溯多远。 提供驱动器程序容错。如果流计算应用中的驱动器程序崩溃了，你可以重启驱动器程序 并让驱动器程序从检查点恢复，这样Spark Streaming就可以读取之前运行的程序处理 数据的进度，并从那里继续。 为了实现这个，Spark Streaming需要为容错存储系统checkpoint足够的信息从而使得其可以从失败中恢复过来。有两种类型的数据设置检查点。​ Metadata checkpointing：将定义流计算的信息存入容错的系统如HDFS。元数据包括：​ 配置 – 用于创建流应用的配置。​ DStreams操作 – 定义流应用的DStreams操作集合。​ 不完整批次 – 批次的工作已进行排队但是并未完成。​ Data checkpointing： 将产生的RDDs存入可靠的存储空间。对于在多批次间合并数据的状态转换，这个很有必要。在这样的转换中，RDDs的产生基于之前批次的RDDs，这样依赖链长度随着时间递增。为了避免在恢复期这种无限的时间增长（和链长度成比例），状态转换中间的RDDs周期性写入可靠地存储空间（如HDFS）从而切短依赖链。​ 总而言之，元数据检查点在由驱动失效中恢复是首要需要的。而数据或者RDD检查点甚至在使用了状态转换的基础函数中也是必要的。出于这些原因，检查点机制对于任何生产环境中的流计算应用都至关重要。你可以通过向 ssc.checkpoint() 方法传递一个路径参数(HDFS、S3 或者本地路径均可)来配置检查点机制,同时你的应用应该能够使用检查点的数据 当程序首次启动，其将创建一个新的StreamingContext，设置所有的流并调用start()。 当程序在失效后重启，其将依据检查点目录的检查点数据重新创建一个StreamingContext。 通过使用StraemingContext.getOrCreate很容易获得这个性能。 12345678910111213141516171819ssc.checkpoint("hdfs://...") # 创建和设置一个新的StreamingContextdef functionToCreateContext(): sc = SparkContext(...) # new context ssc = new StreamingContext(...) lines = ssc.socketTextStream(...) # create DStreams ... ssc.checkpoint(checkpointDirectory) # 设置检查点目录 return ssc# 从检查点数据中获取StreamingContext或者重新创建一个context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext)# 在需要完成的context上做额外的配置# 无论其有没有启动context ...# 启动contextcontext.start()contaxt.awaitTermination() 如果检查点目录(checkpointDirectory)存在，那么context将会由检查点数据重新创建。如果目录不存在（首次运行），那么函数functionToCreateContext将会被调用来创建一个新的context并设置DStreams。​ 注意RDDs的检查点引起存入可靠内存的开销。在RDDs需要检查点的批次里，处理的时间会因此而延长。所以，检查点的间隔需要很仔细地设置。在小尺寸批次（1秒钟）。每一批次检查点会显著减少操作吞吐量。反之，检查点设置的过于频繁导致“血统”和任务尺寸增长，这会有很不好的影响对于需要RDD检查点设置的状态转换，默认间隔是批次间隔的乘数一般至少为10秒钟。可以通过dstream.checkpoint(checkpointInterval)。通常，检查点设置间隔是5-10个DStream的滑动间隔。 WAL 预写日志WAL 即 write ahead log（预写日志），是在 1.2 版本中就添加的特性。作用就是，将数据通过日志的方式写到可靠的存储，比如 HDFS、s3，在 driver 或 worker failure 时可以从在可靠存储上的日志文件恢复数据。WAL 在 driver 端和 executor 端都有应用。WAL在 driver 端的应用用于写日志的对象 writeAheadLogOption: WriteAheadLog。在 StreamingContext 中的 JobScheduler 中的 ReceiverTracker 的 ReceivedBlockTracker 构造函数中被创建，ReceivedBlockTracker 用于管理已接收到的 blocks 信息。需要注意的是，这里只需要启用 checkpoint 就可以创建该 driver 端的 WAL 管理实例，而不需要将 spark.streaming.receiver.writeAheadLog.enable 设置为 true。写什么、何时写、写什么首选需要明确的是，ReceivedBlockTracker 通过 WAL 写入 log 文件的内容是3种事件（当然，会进行序列化）： case class BlockAdditionEvent(receivedBlockInfo: ReceivedBlockInfo)；即新增了一个 block 及该 block 的具体信息，包括 streamId、blockId、数据条数等 case class BatchAllocationEvent(time: Time, allocatedBlocks: AllocatedBlocks)；即为某个 batchTime 分配了哪些 blocks 作为该 batch RDD 的数据源 case class BatchCleanupEvent(times: Seq[Time])；即清理了哪些 batchTime 对应的 block 知道了写了什么内容，结合源码，也不难找出是什么时候写了这些内容。需要再次注意的是，写上面这三种事件，也不需要将 spark.streaming.receiver.writeAheadLog.enable 设置为 true。 WAL 在 executor 端的应用Receiver 接收到的数据会源源不断的传递给 ReceiverSupervisor，是否启用 WAL 机制（即是否将 spark.streaming.receiver.writeAheadLog.enable 设置为 true）会影响 ReceiverSupervisor 在存储 block 时的行为： 不启用 WAL：你设置的StorageLevel是什么，就怎么存储。比如MEMORY_ONLY只会在内存中存一份，MEMORY_AND_DISK会在内存和磁盘上各存一份等 启用 WAL：在StorageLevel指定的存储的基础上，写一份到 WAL 中。存储一份在 WAL 上，更不容易丢数据但性能损失也比较大 关于是否要启用 WAL，要视具体的业务而定： 若可以接受一定的数据丢失，则不需要启用 WAL，因为对性能影响较大 若完全不能接受数据丢失，那就需要同时启用 checkpoint 和 WAL，checkpoint 保存着执行进度（比如已生成但未完成的 jobs），WAL 中保存着 blocks 及 blocks 元数据（比如保存着未完成的 jobs 对应的 blocks 信息及 block 文件）。同时，这种情况可能要在数据源和 Streaming Application 中联合来保证 exactly once 语义 预写日志功能的流程是： 一个SparkStreaming应用开始时（也就是driver开始时），相关的StreamingContext使用SparkContext启动接收器成为长驻运行任务。这些接收器接收并保存流数据到Spark内存中以供处理。 接收器通知driver。 接收块中的元数据（metadata）被发送到driver的StreamingContext。这个元数据包括： 定位其在executor内存中数据的块referenceid. 块数据在日志中的偏移信息（如果启用了）。用户传送数据的生命周期如下图所示。 类似Kafka这样的系统可以通过复制数据保持可靠性。 背压机制默认情况下，Spark Streaming通过Receiver以生产者生产数据的速率接收数据，计算过程中会出现batch processing time &gt; batch interval的情况，其中batch processing time 为实际计算一个批次花费时间， batch interval为Streaming应用设置的批处理间隔。这意味着Spark Streaming的数据接收速率高于Spark从队列中移除数据的速率，也就是数据处理能力低，在设置间隔内不能完全处理当前接收速率接收的数据。如果这种情况持续过长的时间，会造成数据在内存中堆积，导致Receiver所在Executor内存溢出等问题（如果设置StorageLevel包含disk, 则内存存放不下的数据会溢写至disk, 加大延迟）。Spark 1.5以前版本，用户如果要限制Receiver的数据接收速率，可以通过设置静态配制参数“spark.streaming.receiver.maxRate”的值来实现，此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如：producer数据生产高于maxRate，当前集群处理能力也高于maxRate，这就会造成资源利用率下降等问题。为了更好的协调数据接收速率与资源处理能力，Spark Streaming 从v1.5开始引入反压机制（back-pressure）,通过动态控制数据接收速率来适配集群数据处理能力。Spark Streaming Backpressure: 根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。通过属性“spark.streaming.backpressure.enabled”来控制是否启用backpressure机制，默认值false，即不启用。Streaming架构如下图所示： 在原架构的基础上加上一个新的组件RateController,这个组件负责监听“OnBatchCompleted”事件，然后从中抽取processingDelay 及schedulingDelay信息. Estimator依据这些信息估算出最大处理速度（rate），最后由基于Receiver的Input Stream将rate通过ReceiverTracker与ReceiverSupervisorImpl转发给BlockGenerator（继承自RateLimiter）. 流量控制点当Receiver开始接收数据时，会通过supervisor.pushSingle()方法将接收的数据存入currentBuffer等待BlockGenerator定时将数据取走，包装成block. 在将数据存放入currentBuffer之时，要获取许可（令牌）。如果获取到许可就可以将数据存入buffer, 否则将被阻塞，进而阻塞Receiver从数据源拉取数据。其令牌投放采用令牌桶机制进行， 原理如下图所示: 令牌桶机制： 大小固定的令牌桶可自行以恒定的速率源源不断地产生令牌。如果令牌不被消耗，或者被消耗的速度小于产生的速度，令牌就会不断地增多，直到把桶填满。后面再产生的令牌就会从桶中溢出。最后桶中可以保存的最大令牌数永远不会超过桶的大小。当进行某操作时需要令牌时会从令牌桶中取出相应的令牌数，如果获取到则继续操作，否则阻塞。用完之后不用放回。 驱动器程序容错驱动器程序的容错要求我们以特殊的方式创建 StreamingContext。我们需要把检查点目录提供给 StreamingContext。与直接调用 new StreamingContext 不同，应该使用 StreamingContext.getOrCreate() 函数。配置过程如下： 启动Driver自动重启功能 standalone: 提交任务时添加 –supervise 参数 yarn:设置yarn.resourcemanager.am.max-attempts 或者spark.yarn.maxAppAttempts mesos: 提交任务时添加 –supervise 参数 设置checkpointStreamingContext.setCheckpoint(hdfsDirectory) 支持从checkpoint中重启配置 123456def createContext(checkpointDirectory: String): StreamingContext = &#123; val ssc = new StreamingContext ssc.checkpoint(checkpointDirectory) ssc&#125;val ssc = StreamingContext.getOrCreate(checkpointDirectory, createContext(checkpointDirectory)) 工作节点容错为了应对工作节点失败的问题，Spark Streaming使用与Spark的容错机制相同的方法。所 有从外部数据源中收到的数据都在多个工作节点上备份。所有从备份数据转化操作的过程 中创建出来的 RDD 都能容忍一个工作节点的失败，因为根据 RDD 谱系图，系统可以把丢 失的数据从幸存的输入数据备份中重算出来。对于reduceByKey等Stateful操作重做的lineage较长的，强制启动checkpoint，减少重做几率。 接收器容错运行接收器的工作节点的容错也是很重要的。如果这样的节点发生错误，Spark Streaming 会在集群中别的节点上重启失败的接收器。然而，这种情况会不会导致数据的丢失取决于 数据源的行为(数据源是否会重发数据)以及接收器的实现(接收器是否会向数据源确认 收到数据)。举个例子，使用 Flume 作为数据源时，两种接收器的主要区别在于数据丢失 时的保障。在“接收器从数据池中拉取数据”的模型中，Spark 只会在数据已经在集群中 备份时才会从数据池中移除元素。而在“向接收器推数据”的模型中，如果接收器在数据 备份之前失败，一些数据可能就会丢失。总的来说，对于任意一个接收器，你必须同时考 虑上游数据源的容错性(是否支持事务)来确保零数据丢失。一般主要是通过将接收到数据后先写日志（WAL）到可靠文件系统中，后才写入实际的RDD。如果后续处理失败则成功写入WAL的数据通过WAL进行恢复，未成功写入WAL的数据通过可回溯的Source进行重放总的来说，接收器提供以下保证： 所有从可靠文件系统中读取的数据(比如通过StreamingContext.hadoopFiles读取的) 都是可靠的，因为底层的文件系统是有备份的。Spark Streaming会记住哪些数据存放到 了检查点中，并在应用崩溃后从检查点处继续执行。 对于像Kafka、推式Flume、Twitter这样的不可靠数据源，Spark会把输入数据复制到其 他节点上，但是如果接收器任务崩溃，Spark 还是会丢失数据。在 Spark 1.1 以及更早的版 本中，收到的数据只被备份到执行器进程的内存中，所以一旦驱动器程序崩溃(此时所 有的执行器进程都会丢失连接)，数据也会丢失。在 Spark 1.2 中，收到的数据被记录到诸 如 HDFS 这样的可靠的文件系统中，这样即使驱动器程序重启也不会导致数据丢失。 综上所述，确保所有数据都被处理的最佳方式是使用可靠的数据源(例如 HDFS、拉式 Flume 等)。如果你还要在批处理作业中处理这些数据，使用可靠数据源是最佳方式，因为 这种方式确保了你的批处理作业和流计算作业能读取到相同的数据，因而可以得到相同的结果。操作过程如下： 启用checkpointssc.setCheckpoint(checkpointDir) 启用WALsparkConf.set(“spark.streaming.receiver.writeAheadLog.enable”, “true”) 对Receiver使用可靠性存储StoreageLevel.MEMORY_AND_DISK_SER or StoreageLevel.MEMORY_AND_DISK_SER2 处理保证由于Spark Streaming工作节点的容错保障，Spark Streaming可以为所有的转化操作提供 “精确一次”执行的语义，即使一个工作节点在处理部分数据时发生失败，最终的转化结果(即转化操作得到的 RDD)仍然与数据只被处理一次得到的结果一样。然而，当把转化操作得到的结果使用输出操作推入外部系统中时，写结果的任务可能因故 障而执行多次，一些数据可能也就被写了多次。由于这引入了外部系统，因此我们需要专 门针对各系统的代码来处理这样的情况。我们可以使用事务操作来写入外部系统(即原子 化地将一个 RDD 分区一次写入)，或者设计幂等的更新操作(即多次运行同一个更新操作 仍生成相同的结果)。比如 Spark Streaming 的 saveAs…File 操作会在一个文件写完时自动 将其原子化地移动到最终位置上，以此确保每个输出文件只存在一份。 性能考量最常见的问题是Spark Streaming可以使用的最小批次间隔是多少。总的来说，500毫秒已经被证实为对许多应用而言是比较好的最小批次大小。寻找最小批次大小的最佳实践是从一个比较大的批次大小(10 秒左右)开始，不断使用更小的批次大小。如果 Streaming 用 户界面中显示的处理时间保持不变，你就可以进一步减小批次大小。如果处理时间开始增 加，你可能已经达到了应用的极限。相似地，对于窗口操作，计算结果的间隔(也就是滑动步长)对于性能也有巨大的影响。 当计算代价巨大并成为系统瓶颈时，就应该考虑提高滑动步长了。减少批处理所消耗时间的常见方式还有提高并行度。有以下三种方式可以提高并行度： 增加接收器数目 有时如果记录太多导致单台机器来不及读入并分发的话，接收器会成为系统瓶颈。这时 你就需要通过创建多个输入 DStream(这样会创建多个接收器)来增加接收器数目，然 后使用 union 来把数据合并为一个数据源。 将收到的数据显式地重新分区如果接收器数目无法再增加，你可以通过使用 DStream.repartition 来显式重新分区输 入流(或者合并多个流得到的数据流)来重新分配收到的数据。 提高聚合计算的并行度 对于像 reduceByKey() 这样的操作，你可以在第二个参数中指定并行度，我们在介绍 RDD 时提到过类似的手段。 Spark Streaming 高级解析DStreamGraph 对象解析在 Spark Streaming 中，DStreamGraph 是一个非常重要的组件，主要用来： 通过成员 inputStreams 持有 Spark Streaming 输入源及接收数据的方式 通过成员 outputStreams 持有 Streaming app 的 output 操作，并记录 DStream 依赖关系 生成每个 batch 对应的 jobs 下面，通过分析一个简单的例子，结合源码分析来说明 DStreamGraph 是如何发挥作用的。例子如下： 123456789val sparkConf = new SparkConf().setAppName("HdfsWordCount")val ssc = new StreamingContext(sparkConf, Seconds(2))val lines = ssc.textFileStream(args(0))val words = lines.flatMap(_.split(" "))val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _)wordCounts.print()ssc.start()ssc.awaitTermination() 创建 DStreamGraph 实例代码val ssc = new StreamingContext(sparkConf, Seconds(2))创建了 StreamingContext 实例，StreamingContext 包含了 DStreamGraph 类型的成员graph，graph 在 StreamingContext主构造函数中被创建，如下： 123456789101112private[streaming] val graph: DStreamGraph = &#123; if (isCheckpointPresent) &#123; cp_.graph.setContext(this) cp_.graph.restoreCheckpointData() cp_.graph &#125; else &#123; require(batchDur_ != null, "Batch duration for StreamingContext cannot be null") val newGraph = new DStreamGraph() newGraph.setBatchDuration(batchDur_) newGraph &#125; &#125; 可以看到，若当前 checkpoint 可用，会优先从 checkpoint 恢复 graph，否则新建一个。还可以从这里知道的一点是：graph 是运行在 driver 上的 DStreamGraph记录输入源及如何接收数据DStreamGraph有和application 输入数据相关的成员和方法，如下： 1234567private val inputStreams = new ArrayBuffer[InputDStream[_]]() def addInputStream(inputStream: InputDStream[_]) &#123; this.synchronized &#123; inputStream.setGraph(this) inputStreams += inputStream &#125; &#125; 成员inputStreams为 InputDStream 类型的数组，InputDStream是所有 input streams(数据输入流) 的虚基类。该类提供了 start() 和 stop()方法供 streaming 系统来开始和停止接收数据。那些只需要在 driver 端接收数据并转成 RDD 的 input streams 可以直接继承 InputDStream，例如 FileInputDStream是 InputDStream 的子类，它监控一个 HDFS 目录并将新文件转成RDDs。而那些需要在 workers 上运行receiver 来接收数据的 Input DStream，需要继承 ReceiverInputDStream，比如 KafkaReceiver。我们来看看val lines = ssc.textFileStream(args(0))调用。为了更容易理解，我画出了val lines = ssc.textFileStream(args(0))的调用流程： 从上面的调用流程图我们可以知道： ssc.textFileStream会触发新建一个FileInputDStream。FileInputDStream继承于InputDStream，其start()方法定义了数据源及如何接收数据 在FileInputDStream构造函数中，会调用ssc.graph.addInputStream(this)，将自身添加到 DStreamGraph 的 inputStreams: ArrayBuffer[InputDStream[_]] 中，这样 DStreamGraph 就知道了这个 Streaming App 的输入源及如何接收数据。可能你会奇怪为什么inputStreams 是数组类型，举个例子，这里再来一个 val lines1 = ssc.textFileStream(args(0))，那么又将生成一个 FileInputStream 实例添加到inputStreams，所以这里需要集合类型 生成FileInputDStream调用其 map 方法，将以 FileInputDStream 本身作为 partent 来构造新的 MappedDStream。对于 DStream 的 transform 操作，都将生成一个新的 DStream，和 RDD transform 生成新的 RDD 类似与MappedDStream 不同，所有继承了 InputDStream 的定义了输入源及接收数据方式的 sreams 都没有 parent，因为它们就是最初的 streams。 DStream 的依赖链每个 DStream 的子类都会继承 def dependencies: List[DStream[_]] = List()方法，该方法用来返回自己的依赖的父 DStream 列表。比如，没有父DStream 的 InputDStream 的 dependencies方法返回List()。MappedDStream 的实现如下： 123456789class MappedDStream[T: ClassTag, U: ClassTag] ( parent: DStream[T], mapFunc: T =&gt; U ) extends DStream[U](parent.ssc) &#123; override def dependencies: List[DStream[_]] = List(parent) ...&#125; 在上例中，构造函数参数列表中的 parent 即在 ssc.textFileStream 中new 的定义了输入源及数据接收方式的最初的 FileInputDStream实例，这里的 dependencies方法将返回该FileInputDStream实例，这就构成了第一条依赖。可用如下图表示，这里特地将 input streams 用蓝色表示，以强调其与普通由 transform 产生的 DStream 的不同： 继续来看val words = lines.flatMap(_.split(“ “))，flatMap如下： 123def flatMapU: ClassTag: DStream[U] = ssc.withScope &#123; new FlatMappedDStream(this, context.sparkContext.clean(flatMapFunc)) &#125; 每一个 transform 操作都将创建一个新的 DStream，flatMap 操作也不例外，它会创建一个FlatMappedDStream，FlatMappedDStream的实现如下： 1234class FlatMappedDStreamT: ClassTag, U: ClassTag extends DStreamU &#123; override def dependencies: List[DStream[_]] = List(parent) ...&#125; 与 MappedDStream 相同，FlatMappedDStream#dependencies也返回其依赖的父 DStream，及 lines，到这里，依赖链就变成了下图： 之后的几步操作不再这样具体分析，到生成wordCounts时，依赖图将变成下面这样： 在 DStream 中，与 transofrm 相对应的是 output 操作，包括 print, saveAsTextFiles, saveAsObjectFiles, saveAsHadoopFiles, foreachRDD。output 操作中，会创建ForEachDStream实例并调用register方法将自身添加到DStreamGraph.outputStreams成员中，该ForEachDStream实例也会持有是调用的哪个 output 操作。本例的代码调用如下，只需看箭头所指几行代码： 12345678910111213141516171819202122232425262728293031323334353637/** * Print the first ten elements of each RDD generated in this DStream. This is an output * operator, so this DStream will be registered as an output stream and there materialized. */ def print(): Unit = ssc.withScope &#123; print(10) &#125; /** * Print the first num elements of each RDD generated in this DStream. This is an output * operator, so this DStream will be registered as an output stream and there materialized. */ def print(num: Int): Unit = ssc.withScope &#123; def foreachFunc: (RDD[T], Time) =&gt; Unit = &#123; (rdd: RDD[T], time: Time) =&gt; &#123; val firstNum = rdd.take(num + 1) // scalastyle:off println println("-------------------------------------------") println(s"Time: $time") println("-------------------------------------------") firstNum.take(num).foreach(println) if (firstNum.length &gt; num) println("...") println() // scalastyle:on println &#125; &#125; foreachRDD(context.sparkContext.clean(foreachFunc), displayInnerRDDOps = false) &#125;/** * Register this streaming as an output stream. This would ensure that RDDs of this * DStream will be generated. */ private[streaming] def register(): DStream[T] = &#123; ssc.graph.addOutputStream(this) this &#125; 与 DStream transform 操作返回一个新的 DStream 不同，output 操作不会返回任何东西，只会创建一个ForEachDStream作为依赖链的终结。至此，生成了完成的依赖链，也就是 DAG，如下图（这里将 ForEachDStream 标为黄色以显示其与众不同）： ReceiverTracker 与数据导入Spark Streaming 在数据接收与导入方面需要满足有以下三个特点： 兼容众多输入源，包括HDFS, Flume, Kafka, Twitter and ZeroMQ。还可以自定义数据源 要能为每个 batch 的 RDD 提供相应的输入数据 为适应 7*24h 不间断运行，要有接收数据挂掉的容错机制有容乃大，兼容众多数据源 InputDStream是所有 input streams(数据输入流) 的虚基类。该类提供了 start() 和 stop()方法供 streaming 系统来开始和停止接收数据。那些只需要在 driver 端接收数据并转成 RDD 的 input streams 可以直接继承 InputDStream，例如 FileInputDStream是 InputDStream 的子类，它监控一个 HDFS 目录并将新文件转成RDDs。而那些需要在 workers 上运行receiver 来接收数据的 Input DStream，需要继承 ReceiverInputDStream，比如 KafkaReceiver只需在 driver 端接收数据的 input stream 一般比较简单且在生产环境中使用的比较少，本文不作分析，只分析继承了 ReceiverInputDStream 的 input stream 是如何导入数据的。ReceiverInputDStream有一个def getReceiver(): Receiver[T]方法，每个继承了ReceiverInputDStream的 input stream 都必须实现这个方法。该方法用来获取将要分发到各个 worker 节点上用来接收数据的 receiver（接收器）。不同的 ReceiverInputDStream 子类都有它们对应的不同的 receiver，如KafkaInputDStream对应KafkaReceiver，FlumeInputDStream对应FlumeReceiver，TwitterInputDStream对应TwitterReceiver，如果你要实现自己的数据源，也需要定义相应的 receiver。继承 ReceiverInputDStream 并定义相应的 receiver，就是 Spark Streaming 能兼容众多数据源的原因。 为每个 batch 的 RDD 提供输入数据在 StreamingContext 中，有一个重要的组件叫做 ReceiverTracker，它是 Spark Streaming 作业调度器 JobScheduler 的成员，负责启动、管理各个 receiver 及管理各个 receiver 接收到的数据。 确定 receiver 要分发到哪些 executors 上执行创建 ReceiverTracker 实例我们来看 StreamingContext#start() 方法部分调用实现，如下： 可以看到，StreamingContext#start() 会调用 JobScheduler#start() 方法，在 JobScheduler#start() 中，会创建一个新的 ReceiverTracker 实例 receiverTracker，并调用其 start() 方法。 ReceiverTracker#start()继续跟进 ReceiverTracker#start()，如下图，它主要做了两件事： 初始化一个 endpoint: ReceiverTrackerEndpoint，用来接收和处理来自 ReceiverTracker 和 receivers 发送的消息 调用 launchReceivers 来自将各个 receivers 分发到 executors 上 ReceiverTracker#launchReceivers()继续跟进 launchReceivers，它也主要干了两件事： 获取 DStreamGraph.inputStreams 中继承了 ReceiverInputDStream 的 input streams 的 receivers。也就是数据接收器 给消息接收处理器 endpoint 发送 StartAllReceivers(receivers)消息。直接返回，不等待消息被处理 处理StartAllReceivers消息endpoint 在接收到消息后，会先判断消息类型，对不同的消息做不同处理。对于StartAllReceivers消息，处理流程如下：计算每个 receiver 要分发的目的 executors。遵循两条原则： 将 receiver 分布的尽量均匀 如果 receiver 的preferredLocation本身不均匀，以preferredLocation为准 遍历每个 receiver，根据第1步中得到的目的 executors 调用 startReceiver 方法。 到这里，已经确定了每个 receiver 要分发到哪些 executors 上启动 receivers接上，通过 ReceiverTracker#startReceiver(receiver: Receiver[_], scheduledExecutors: Seq[String]) 来启动 receivers，我们来看具体流程： 如上流程图所述，分发和启动 receiver 的方式不可谓不精彩。其中，startReceiverFunc 函数主要实现如下： 1234val supervisor = new ReceiverSupervisorImpl( receiver, SparkEnv.get, serializableHadoopConf.value, checkpointDirOption)supervisor.start()supervisor.awaitTermination() supervisor.start() 中会调用 receiver#onStart 后立即返回。receiver#onStart 一般自行新建线程或线程池来接收数据，比如在 KafkaReceiver 中，就新建了线程池，在线程池中接收 topics 的数据。supervisor.start() 返回后，由 supervisor.awaitTermination() 阻塞住线程，以让这个 task 一直不退出，从而可以源源不断接收数据。 数据流转 上图为 receiver 接收到的数据的流转过程，让我们来逐一分析Step1: Receiver -&gt; ReceiverSupervisor 这一步中，Receiver 将接收到的数据源源不断地传给 ReceiverSupervisor。Receiver 调用其 store(…) 方法，store 方法中继续调用 supervisor.pushSingle 或 supervisor.pushArrayBuffer 等方法来传递数据。Receiver#store 有多重形式， ReceiverSupervisor 也有 pushSingle、pushArrayBuffer、pushIterator、pushBytes 方法与不同的 store 对应。 pushSingle: 对应单条小数据 pushArrayBuffer: 对应数组形式的数据 pushIterator: 对应 iterator 形式数据 pushBytes: 对应 ByteBuffer 形式的块数据 对于细小的数据，存储时需要 BlockGenerator 聚集多条数据成一块，然后再成块存储；反之就不用聚集，直接成块存储。当然，存储操作并不在 Step1 中执行，只为说明之后不同的操作逻辑。 Step2.1: ReceiverSupervisor -&gt; BlockManager -&gt; disk/memory 在这一步中，主要将从 receiver 收到的数据以 block（数据块）的形式存储存储 block 的是receivedBlockHandler: ReceivedBlockHandler，根据参数spark.streaming.receiver.writeAheadLog.enable配置的不同，默认为 false，receivedBlockHandler对象对应的类也不同，如下： 12345678910private val receivedBlockHandler: ReceivedBlockHandler = &#123; if (WriteAheadLogUtils.enableReceiverLog(env.conf)) &#123; //&lt; 先写 WAL，再存储到 executor 的内存或硬盘 new WriteAheadLogBasedBlockHandler(env.blockManager, receiver.streamId, receiver.storageLevel, env.conf, hadoopConf, checkpointDirOption.get) &#125; else &#123; //&lt; 直接存到 executor 的内存或硬盘 new BlockManagerBasedBlockHandler(env.blockManager, receiver.storageLevel) &#125;&#125; 启动 WAL 的好处就是在application 挂掉之后，可以恢复数据。 123456//&lt; 调用 receivedBlockHandler.storeBlock 方法存储 block，并得到一个 blockStoreResultval blockStoreResult = receivedBlockHandler.storeBlock(blockId, receivedBlock)//&lt; 使用blockStoreResult初始化一个ReceivedBlockInfo实例val blockInfo = ReceivedBlockInfo(streamId, numRecords, metadataOption, blockStoreResult)//&lt; 发送消息通知 ReceiverTracker 新增并存储了 blocktrackerEndpoint.askWithRetry[Boolean](AddBlock(blockInfo)) 不管是 WriteAheadLogBasedBlockHandler 还是 BlockManagerBasedBlockHandler 最终都是通过 BlockManager 将 block 数据存储 execuor 内存或磁盘或还有 WAL 方式存入。这里需要说明的是 streamId，每个 InputDStream 都有它自己唯一的 id，即 streamId，blockInfo包含 streamId 是为了区分block 是哪个 InputDStream 的数据。之后为 batch 分配 blocks 时，需要知道每个 InputDStream 都有哪些未分配的 blocks。 Step2.2: ReceiverSupervisor -&gt; ReceiverTracker将 block 存储之后，获得 block 描述信息 blockInfo: ReceivedBlockInfo，这里面包含：streamId、数据位置、数据条数、数据 size 等信息。之后，封装以 block 作为参数的 AddBlock(blockInfo) 消息并发送给 ReceiverTracker 以通知其有新增 block 数据块。 Step3: ReceiverTracker -&gt; ReceivedBlockTracker ReceiverTracker 收到 ReceiverSupervisor 发来的 AddBlock(blockInfo) 消息后，直接调用以下代码将 block 信息传给 ReceivedBlockTracker： 123private def addBlock(receivedBlockInfo: ReceivedBlockInfo): Boolean = &#123; receivedBlockTracker.addBlock(receivedBlockInfo) &#125; receivedBlockTracker.addBlock中，如果启用了 WAL，会将新增的 block 信息以 WAL 方式保存。 无论 WAL 是否启用，都会将新增的 block 信息保存到 streamIdToUnallocatedBlockQueues: mutable.HashMap[Int, ReceivedBlockQueue]中，该变量 key 为 InputDStream 的唯一 id，value 为已存储未分配的 block 信息。之后为 batch 分配blocks，会访问该结构来获取每个 InputDStream 对应的未消费的 blocks。 动态生成 JobJobScheduler有两个重要成员，一是ReceiverTracker，负责分发 receivers 及源源不断地接收数据；二是JobGenerator，负责定时的生成 jobs 并 checkpoint。 定时逻辑 在 JobScheduler 的主构造函数中，会创建 JobGenerator 对象。在 JobGenerator 的主构造函数中，会创建一个定时器： 12private val timer = new RecurringTimer(clock, ssc.graph.batchDuration.milliseconds, longTime =&gt; eventLoop.post(GenerateJobs(new Time(longTime))), "JobGenerator") 该定时器每隔 ssc.graph.batchDuration.milliseconds 会执行一次 eventLoop.post(GenerateJobs(new Time(longTime))) 向 eventLoop 发送 GenerateJobs(new Time(longTime))消息，eventLoop**收到消息后会进行这个 batch 对应的 jobs 的生成及提交执行**，eventLoop 是一个消息接收处理器。 需要注意的是，timer 在创建之后并不会马上启动，将在 StreamingContext#start() 启动 Streaming Application 时间接调用到 timer.start(restartTime.milliseconds)才启动。 为 batch 生成 jobs eventLoop 在接收到 GenerateJobs(new Time(longTime))消息后的主要处理流程有以上图中三步： 将已接收到的 blocks 分配给 batch 生成该 batch 对应的 jobs 将 jobs 封装成 JobSet 并提交执行接下来我们就将逐一展开这三步进行分析 将已接受到的 blocks 分配给 batch 上图是根据源码画出的为 batch 分配 blocks 的流程图，这里对 『获得 batchTime 各个 InputDStream 未分配的 blocks』作进一步说明： 我们知道了各个 ReceiverInputDStream 对应的 receivers 接收并保存的 blocks 信息会保存在 ReceivedBlockTracker#streamIdToUnallocatedBlockQueues，该成员 key 为 streamId，value 为该 streamId 对应的 InputDStream 已接收保存但尚未分配的 blocks 信息。所以获取某 InputDStream 未分配的 blocks 只要以该 InputDStream 的 streamId 来从 streamIdToUnallocatedBlockQueues 来 get 就好。获取之后，会清楚该 streamId 对应的value，以保证 block 不会被重复分配。在实际调用中，为 batchTime 分配 blocks 时，会从streamIdToUnallocatedBlockQueues取出未分配的 blocks 塞进 timeToAllocatedBlocks: mutable.HashMap[Time, AllocatedBlocks] 中，以在之后作为该 batchTime 对应的 RDD 的输入数据。通过以上步骤，就可以为 batch 的所有 InputDStream 分配 blocks。也就是为 batch 分配了 blocks。 生成该 batch 对应的 jobs 为指定 batchTime 生成 jobs 的逻辑如上图所示。你可能会疑惑，为什么 DStreamGraph#generateJobs(time: Time)为什么返回 Seq[Job]，而不是单个 job。这是因为，在一个 batch 内，可能会有多个 OutputStream 执行了多次 output 操作，每次 output 操作都将产生一个 Job，最终就会产生多个 Jobs。我们结合上图对执行流程进一步分析。在DStreamGraph#generateJobs(time: Time)中，对于DStreamGraph成员ArrayBuffer[DStream[_]]的每一项，调用DStream#generateJob(time: Time)来生成这个 outputStream 在该 batchTime 的 job。该生成过程主要有三步： Step1: 获取该 outputStream 在该 batchTime 对应的 RDD 每个 DStream 实例都有一个 generatedRDDs: HashMap[Time, RDD[T]] 成员，用来保存该 DStream 在每个 batchTime 生成的 RDD，当 DStream#getOrCompute(time: Time)调用时 首先会查看generatedRDDs中是否已经有该 time 对应的 RDD，若有则直接返回 若无，则调用compute(validTime: Time)来生成 RDD，这一步根据每个 InputDStream继承 compute 的实现不同而不同。例如，对于 FileInputDStream，其 compute 实现逻辑如下： 先通过一个 findNewFiles() 方法，找到多个新 file 对每个新 file，都将其作为参数调用 sc.newAPIHadoopFile(file)，生成一个 RDD 实例 将 2 中的多个新 file 对应的多个 RDD 实例进行 union，返回一个 union 后的 UnionRDD Step2: 根据 Step1中得到的 RDD 生成最终 job 要执行的函数 jobFuncjobFunc定义如下： 1234val jobFunc = () =&gt; &#123; val emptyFunc = &#123; (iterator: Iterator[T]) =&gt; &#123;&#125; &#125; context.sparkContext.runJob(rdd, emptyFunc)&#125; 可以看到，每个 outputStream 的 output 操作生成的 Job 其实与 RDD action 一样，最终调用 SparkContext#runJob 来提交 RDD DAG 定义的任务。 Step3: 根据 Step2中得到的 jobFunc 生成最终要执行的 Job 并返回Step2中得到了定义 Job 要干嘛的函数-jobFunc，这里便以 jobFunc及 batchTime 生成 Job 实例：Some(new Job(time, jobFunc))该Job实例将最终封装在 JobHandler 中被执行至此，我们搞明白了 JobScheduler 是如何通过一步步调用来动态生成每个 batchTime 的 jobs。下文我们将分析这些动态生成的 jobs 如何被分发及如何执行。 Job 的提交与执行我们分析了 JobScheduler 是如何动态为每个 batch生成 jobs，那么生成的 jobs 是如何被提交的。在 JobScheduler 生成某个 batch 对应的 Seq[Job] 之后，会将 batch 及 Seq[Job] 封装成一个 JobSet 对象，JobSet 持有某个 batch 内所有的 jobs，并记录各个 job 的运行状态。之后，调用JobScheduler#submitJobSet(jobSet: JobSet)来提交 jobs，在该函数中，除了一些状态更新，主要任务就是执行 jobSet.jobs.foreach(job =&gt; jobExecutor.execute(new JobHandler(job))). 即，对于 jobSet 中的每一个 job，执行jobExecutor.execute(new JobHandler(job))，要搞懂这行代码干了什么，就必须了解 JobHandler 及 jobExecutor。JobHandlerJobHandler 继承了 Runnable，为了说明与 job 的关系，其精简后的实现如下： 123456789101112131415private class JobHandler(job: Job) extends Runnable with Logging &#123; import JobScheduler._ def run() &#123; _eventLoop.post(JobStarted(job)) PairRDDFunctions.disableOutputSpecValidation.withValue(true) &#123; job.run() &#125; _eventLoop = eventLoop if (_eventLoop != null) &#123; _eventLoop.post(JobCompleted(job)) &#125; &#125;&#125; JobHandler#run 方法主要执行了 job.run()，该方法最终将调用到『生成该 batch 对应的 jobs的Step2 定义的 jobFunc』，jonFunc 将提交对应 RDD DAG 定义的 job。JobExecutor知道了 JobHandler 是用来执行 job 的，那么 JobHandler 将在哪里执行 job 呢？答案是jobExecutor，jobExecutor为 JobScheduler 成员，是一个线程池，在JobScheduler 主构造函数中创建，如下： 12private val numConcurrentJobs = ssc.conf.getInt("spark.streaming.concurrentJobs", 1)private val jobExecutor = ThreadUtils.newDaemonFixedThreadPool(numConcurrentJobs, "streaming-job-executor") JobHandler 将最终在 线程池jobExecutor 的线程中被调用，jobExecutor的线程数可通过spark.streaming.concurrentJobs配置，默认为1。若配置多个线程，就能让多个 job 同时运行，若只有一个线程，那么同一时刻只能有一个 job 运行。以上，即 jobs 被执行的逻辑。 Block 的生成与存储ReceiverSupervisorImpl共提供了4个将从 receiver 传递过来的数据转换成 block 并存储的方法，分别是： pushSingle: 处理单条数据 pushArrayBuffer: 处理数组形式数据 pushIterator: 处理 iterator 形式处理 pushBytes: 处理 ByteBuffer 形式数据 其中，pushArrayBuffer、pushIterator、pushBytes最终调用pushAndReportBlock；而pushSingle将调用defaultBlockGenerator.addData(data)，我们分别就这两种形式做说明 pushAndReportBlock我们针对存储 block 简化 pushAndReportBlock 后的代码如下： 12345678910def pushAndReportBlock( receivedBlock: ReceivedBlock, metadataOption: Option[Any], blockIdOption: Option[StreamBlockId]) &#123; ... val blockId = blockIdOption.getOrElse(nextBlockId) receivedBlockHandler.storeBlock(blockId, receivedBlock) ...&#125; 首先获取一个新的 blockId，之后调用 receivedBlockHandler.storeBlock, receivedBlockHandler 在 ReceiverSupervisorImpl 构造函数中初始化。当启用了 checkpoint 且 spark.streaming.receiver.writeAheadLog.enable 为 true 时，receivedBlockHandler 被初始化为 WriteAheadLogBasedBlockHandler 类型；否则将初始化为 BlockManagerBasedBlockHandler类型。WriteAheadLogBasedBlockHandler#storeBlock 将 ArrayBuffer, iterator, bytes 类型的数据序列化后得到的 serializedBlock 交由 BlockManager 根据设置的 StorageLevel 存入 executor 的内存或磁盘中 通过 WAL 再存储一份 而BlockManagerBasedBlockHandler#storeBlock将 ArrayBuffer, iterator, bytes 类型的数据交由 BlockManager 根据设置的 StorageLevel 存入 executor 的内存或磁盘中，并不再通过 WAL 存储一份pushSinglepushSingle将调用 BlockGenerator#addData(data: Any) 通过积攒的方式来存储数据。接下来对 BlockGenerator 是如何积攒一条一条数据最后写入 block 的逻辑 上图为 BlockGenerator 的各个成员，首选对各个成员做介绍：currentBuffer变长数组，当 receiver 接收的一条一条的数据将会添加到该变长数组的尾部 可能会有一个 receiver 的多个线程同时进行添加数据，这里是同步操作 添加前，会由 rateLimiter 检查一下速率，是否加入的速度过快。如果过快的话就需要 block 住，等到下一秒再开始添加。最高频率由 spark.streaming.receiver.maxRate 控制，默认值为 Long.MaxValue，具体含义是单个 Receiver 每秒钟允许添加的条数。blockIntervalTimer &amp; blockIntervalMs 分别是定时器和时间间隔。blockIntervalTimer中有一个线程，每隔blockIntervalMs会执行以下操作： 将 currentBuffer 赋值给 newBlockBuffer 将 currentBuffer 指向新的空的 ArrayBuffer 对象 将 newBlockBuffer 封装成 newBlock 将 newBlock 添加到 blocksForPushing 队列中blockIntervalMs 由 spark.streaming.blockInterval 控制，默认是 200ms。 blockPushingThread &amp; blocksForPushing &amp; blockQueueSizeblocksForPushing 是一个定长数组，长度由 blockQueueSize 决定，默认为10，可通过 spark.streaming.blockQueueSize 改变。上面分析到，blockIntervalTimer中的线程会定时将 block 塞入该队列。还有另一条线程不断送该队列中取出 block，然后调用 ReceiverSupervisorImpl.pushArrayBuffer(…) 来将 block 存储，这条线程就是blockPushingThread。PS: blocksForPushing为ArrayBlockingQueue类型。ArrayBlockingQueue是一个阻塞队列，能够自定义队列大小，当插入时，如果队列已经没有空闲位置，那么新的插入线程将阻塞到该队列，一旦该队列有空闲位置，那么阻塞的线程将执行插入以上，通过分析各个成员，也说明了 BlockGenerator 是如何存储单条数据的。 End.]]></content>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库建设]]></title>
    <url>%2F2017%2F08%2F13%2FData-Warehouse-Construction%2F</url>
    <content type="text"><![CDATA[本文主要介绍数据仓库的起源和基本搭建。 什么是数据仓库数据仓库，英文名称为Data Warehouse，可简写为DW或DWH。数据仓库，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。它出于分析性报告和决策支持目的而创建。 为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制。数据仓库能干什么？ 年度销售目标的指定，需要根据以往的历史报表进行决策，不能拍脑袋。 如何优化业务流程？ 案例1：某公司需要对APP进行推广，考核的主要目标是下载安装，有些第三方渠道会对这些数据造假，比如某个渠道在凌晨批量下载，点赞操作，操作步骤一致。通过数据分析，分析出应用的名称和安装时间，来判断一个渠道的是否优质、是否作假。 案例2：一个电商网站订单的完成包括：浏览、下单、支付、物流，其中物流环节可能和中通、申通、韵达等快递公司合作。快递公司每派送一个订单，都会有订单派送的确认时间，可以根据订单派送时间来分析哪个快递公司比较快捷高效，从而选择与哪些快递公司合作，剔除哪些快递公司，增加用户友好型。 数据仓库的特点数据是面向主题的与传统数据库面向应用进行数据组织的特点相对应，数据仓库中的数据是面向主题进行组织的。什么是主题呢？首先，主题是一个抽象的概念，是较高层次上企业信息系统中的数据综合、归类并进行分析利用的抽象。在逻辑意义上，它是对应企业中某一宏观分析领域所涉及的分析对象。面向主题的数据组织方式，就是在较高层次上对分析对象的数据的一个完整、一致的描述，能完整、统一地刻划各个分析对象所涉及的企业的各项数据，以及数据之间的联系。所谓较高层次是相对面向应用的数据组织方式而言的，是指按照主题进行数据组织的方式具有更高的数据抽象级别。 数据是集成的数据仓库的数据是从原有的分散的数据库数据抽取来的。操作型数据与DSS分析型数据之间差别甚大。第一，数据仓库的每一个主题所对应的源数据在原有的各分散数据库中有许多重复和不一致的地方，且来源于不同的联机系统的数据都和不同的应用逻辑捆绑在一起；第二，数据仓库中的综合数据不能从原有的数据库系统直接得到。因此在数据进入数据仓库之前，必然要经过统一与综合，这一步是数据仓库建设中最关键、最复杂的一步，所要完成的工作有： 要统一源数据中所有矛盾之处，如字段的同名异义、异名同义、单位不统一、字长不一致，等等。 进行数据综合和计算。数据仓库中的数据综合工作可以在从原有数据库抽取 数据时生成，但许多是在数据仓库内部生成的，即进入数据仓库以后进行综合生成的。 数据是不可更新的数据仓库的数据主要供企业决策分析之用，所涉及的数据操作主要是数据查询，一般情况下并不进行修改操作。数据仓库的数据反映的是一段相当长的时间内历史数据的内容，是不同时点的数据库快照的集合，以及基于这些快照进行统计、综合和重组的导出数据，而不是联机处理的数据。数据库中进行联机处理的数据经过集成输入到数据仓库中，一旦数据仓库存放的数据已经超过数据仓库的数据存储期限，这些数据将从当前的数据仓库中删去。因为数据仓库只进行数据查询操作，所以数据仓库管理系统相比数据库管理系统而言要简单得多。数据库管理系统中许多技术难点，如完整性保护、并发控制等等，在数据仓库的管理中几乎可以省去。但是由于数据仓库的查询数据量往往很大，所以就对数据查询提出了更高的要求，它要求采用各种复杂的索引技术；同时由于数据仓库面向的是商业企业的高层管理者，他们会对数据查询的界面友好性和数据表示提出更高的要求。 数据是随时间不断变化的数据仓库中的数据不可更新是针对应用来说的，也就是说，数据仓库的用户进行分析处理时是不进行数据更新操作的。但并不是说，在从数据集成输入数据仓库开始到最终被删除的整个数据生存周期中，所有的数据仓库数据都是永远不变的。​ 数据仓库的数据是随时间的变化而不断变化的，这是数据仓库数据的第四个特征。这一特征表现在以下3方面： 数据仓库随时间变化不断增加新的数据内容。数据仓库系统必须不断捕捉OLTP数据库中变化的数据，追加到数据仓库中去，也就是要不断地生成OLTP数据库的快照，经统一集成后增加到数据仓库中去；但对于确实不再变化的数据库快照，如果捕捉到新的变化数据，则只生成一个新的数据库快照增加进去，而不会对原有的数据库快照进行修改。 数据仓库随时间变化不断删去旧的数据内容。数据仓库的数据也有存储期限，一旦超过了这一期限，过期数据就要被删除。只是数据仓库内的数据时限要远远长于操作型环境中的数据时限。在操作型环境中一般只保存有60~90天的数据，而在数据仓库中则需要保存较长时限的数据（如5~10年），以适应DSS进行趋势分析的要求。 数据仓库中包含有大量的综合数据，这些综合数据中很多跟时间有关，如数据经常按照时间段进行综合，或隔一定的时间片进行抽样等等。这些数据要随着时间的变化不断地进行重新综合。因此，数据仓库的数据特征都包含时间项，以标明数据的历史时期。 数据仓库的发展历程数据仓库的发展大致经历了这样的三个过程： 简单报表阶段这个阶段，系统的主要目标是解决一些日常的工作中业务人员需要的报表，以及生成一些简单的能够帮助领导进行决策所需要的汇总数据。这个阶段的大部分表现形式为数据库和前端报表工具。 数据集市阶段这个阶段，主要是根据某个业务部门的需要，进行一定的数据的采集，整理，按照业务人员的需要，进行多维报表的展现，能够提供对特定业务指导的数据，并且能够提供特定的领导决策数据。 数据仓库阶段这个阶段，主要是按照一定的数据模型，对整个企业的数据进行采集，整理，并且能够按照各个业务部门的需要，提供跨部门的，完全一致的业务报表数据，能够通过数据仓库生成对对业务具有指导性的数据，同时，为领导决策提供全面的数据支持。 通过数据仓库建设的发展阶段，我们能够看出，数据仓库的建设和数据集市的建设的重要区别就在于数据模型的支持。因此，数据模型的建设，对于我们数据仓库的建设，有着决定性的意义。 数据仓库和数据库的区别了解数据库与数据仓库的区别之前，首先掌握三个概念。数据库软件、数据库、数据仓库。数据库软件：是一种软件，可以看得见，可以操作。用来实现数据库逻辑功能。属于物理层。数据库：是一种逻辑概念，用来存放数据的仓库。通过数据库软件来实现。数据库由很多表组成，表是二维的，一张表里可以有很多字段。字段一字排开，对应的数据就一行一行写入表中。数据库的表，在于能够用二维表现多维关系。目前市面上流行的数据库都是二维数据库。如：Oracle、DB2、MySQL、Sybase、MS SQL Server等。数据仓库：是数据库概念的升级。从逻辑上理解，数据库和数据仓库没有区别，都是通过数据库软件实现的存放数据的地方，只不过从数据量来说，数据仓库要比数据库更庞大得多。数据仓库主要用于数据挖掘和数据分析，辅助领导做决策。在IT的架构体系中，数据库是必须存在的。必须要有地方存放数据。比如现在的网购，淘宝，京东等等。物品的存货数量，货品的价格，用户的账户余额之类的。这些数据都是存放在后台数据库中。或者最简单理解，我们现在微博，QQ等账户的用户名和密码。在后台数据库必然有一张user表，字段起码有两个，即用户名和密码，然后我们的数据就一行一行的存在表上面。当我们登录的时候，我们填写了用户名和密码，这些数据就会被传回到后台去，去跟表上面的数据匹配，匹配成功了，你就能登录了。匹配不成功就会报错说密码错误或者没有此用户名等。这个就是数据库，数据库在生产环境就是用来干活的。凡是跟业务应用挂钩的，我们都使用数据库。数据仓库则是BI下的其中一种技术。由于数据库是跟业务应用挂钩的，所以一个数据库不可能装下一家公司的所有数据。数据库的表设计往往是针对某一个应用进行设计的。比如刚才那个登录的功能，这张user表上就只有这两个字段，没有别的字段了。但是这张表符合应用，没有问题。但是这张表不符合分析。比如我想知道在哪个时间段，用户登录的量最多？哪个用户一年购物最多？诸如此类的指标。那就要重新设计数据库的表结构了。对于数据分析和数据挖掘，我们引入数据仓库概念。数据仓库的表结构是依照分析需求，分析维度，分析指标进行设计的。数据库与数据仓库的区别实际讲的是OLTP与OLAP的区别。操作型处理，叫联机事务处理OLTP（On-Line Transaction Processing），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。分析型处理，叫联机分析处理OLAP（On-Line Analytical Processing）一般针对某些主题的历史数据进行分析，支持管理决策。 数据仓库架构分层数据仓库标准上可以分为四层：ODS（临时存储层）、PDW（数据仓库层）、DM（数据集市层）、APP（应用层）。 ODS层为临时存储层，是接口数据的临时存储区域，为后一步的数据处理做准备。一般来说ODS层的数据和源系统的数据是同构的，主要目的是简化后续数据加工处理的工作。从数据粒度上来说ODS层的数据粒度是最细的。ODS层的表通常包括两类，一个用于存储当前需要加载的数据，一个用于存储处理完后的历史数据。历史数据一般保存3-6个月后需要清除，以节省空间。但不同的项目要区别对待，如果源系统的数据量不大，可以保留更长的时间，甚至全量保存； PDW层为数据仓库层，PDW层的数据应该是一致的、准确的、干净的数据，即对源系统数据进行了清洗（去除了杂质）后的数据。这一层的数据一般是遵循数据库第三范式的，其数据粒度通常和ODS的粒度相同。在PDW层会保存BI系统中所有的历史数据，例如保存10年的数据。 DM层为数据集市层，这层数据是面向主题来组织数据的，通常是星形或雪花结构的数据。从数据粒度来说，这层的数据是轻度汇总级的数据，已经不存在明细数据了。从数据的时间跨度来说，通常是PDW层的一部分，主要的目的是为了满足用户分析的需求，而从分析的角度来说，用户通常只需要分析近几年（如近三年的数据）的即可。从数据的广度来说，仍然覆盖了所有业务数据。 APP层为应用层，这层数据是完全为了满足具体的分析需求而构建的数据，也是星形或雪花结构的数据。从数据粒度来说是高度汇总的数据。从数据的广度来说，则并不一定会覆盖所有业务数据，而是DM层数据的一个真子集，从某种意义上来说是DM层数据的一个重复。从极端情况来说，可以为每一张报表在APP层构建一个模型来支持，达到以空间换时间的目的数据仓库的标准分层只是一个建议性质的标准，实际实施时需要根据实际情况确定数据仓库的分层，不同类型的数据也可能采取不同的分层方法。 为什么要对数据仓库分层 用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据； 如果不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。 通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。 数据质量检查保证报表数据的正确性、稳定性，通过告警机制尽可能快的发现异常、尽可能快的解决问题。 检查方法： 数据行数据的比较。 行数有变化，但是指标有变化。对重点指标进行筛选。 发现问题，及时通知相关模块负责人 元数据介绍当需要了解某地企业及其提供的服务时，电话黄页的重要性就体现出来了。元数据（Metadata）类似于这样的电话黄页。 元数据的定义 数据仓库的元数据是关于数据仓库中数据的数据。它的作用类似于数据库管理系统的数据字典，保存了逻辑数据结构、文件、地址和索引等信息。广义上讲，在数据仓库中，元数据描述了数据仓库内数据的结构和建立方法的数据。 元数据是数据仓库管理系统的重要组成部分，元数据管理器是企业级数据仓库中的关键组件，贯穿数据仓库构建的整个过程，直接影响着数据仓库的构建、使用和维护。 构建数据仓库的主要步骤之一是ETL。这时元数据将发挥重要的作用，它定义了源数据系统到数据仓库的映射、数据转换的规则、数据仓库的逻辑结构、数据更新的规则、数据导入历史记录以及装载周期等相关内容。数据抽取和转换的专家以及数据仓库管理员正是通过元数据高效地构建数据仓库。 用户在使用数据仓库时，通过元数据访问数据，明确数据项的含义以及定制报表。 数据仓库的规模及其复杂性离不开正确的元数据管理，包括增加或移除外部数据源，改变数据清洗方法，控制出错的查询以及安排备份等。 元数据可分为技术元数据和业务元数据。技术元数据为开发和管理数据仓库的IT 人员使用，它描述了与数据仓库开发、管理和维护相关的数据，包括数据源信息、数据转换描述、数据仓库模型、数据清洗与更新规则、数据映射和访问权限等。而业务元数据为管理层和业务分析人员服务，从业务角度描述数据，包括商务术语、数据仓库中有什么数据、数据的位置和数据的可用性等，帮助业务人员更好地理解数据仓库中哪些数据是可用的以及如何使用。由上可见，元数据不仅定义了数据仓库中数据的模式、来源、抽取和转换规则等，而且是整个数据仓库系统运行的基础，元数据把数据仓库系统中各个松散的组件联系起来，组成了一个有机的整体，如图所示： 元数据的存储方式 元数据有两种常见存储方式：一种是以数据集为基础，每一个数据集有对应的元数据文件，每一个元数据文件包含对应数据集的元数据内容；另一种存储方式是以数据库为基础，即元数据库。其中元数据文件由若干项组成，每一项表示元数据的一个要素，每条记录为数据集的元数据内容。上述存储方式各有优缺点，第一种存储方式的优点是调用数据时相应的元数据也作为一个独立的文件被传输，相对数据库有较强的独立性，在对元数据进行检索时可以利用数据库的功能实现，也可以把元数据文件调到其他数据库系统中操作；不足是如果每一数据集都对应一个元数据文档，在规模巨大的数据库中则会有大量的元数据文件，管理不方便。第二种存储方式下，元数据库中只有一个元数据文件，管理比较方便，添加或删除数据集，只要在该文件中添加或删除相应的记录项即可。在获取某数据集的元数据时，因为实际得到的只是关系表格数据的一条记录，所以要求用户系统可以接受这种特定形式的数据。因此推荐使用元数据库的方式。 元数据库用于存储元数据，因此元数据库最好选用主流的关系数据库管理系统。元数据库还包含用于操作和查询元数据的机制。建立元数据库的主要好处是提供统一的数据结构和业务规则，易于把企业内部的多个数据集市有机地集成起来。目前，一些企业倾向建立多个数据集市，而不是一个集中的数据仓库，这时可以考虑在建立数据仓库（或数据集市）之前，先建立一个用于描述数据、服务应用集成的元数据库，做好数据仓库实施的初期支持工作，对后续开发和维护有很大的帮助。元数据库保证了数据仓库数据的一致性和准确性，为企业进行数据质量管理提供基础。 元数据的作用 描述哪些数据在数据仓库中，帮助决策分析者对数据仓库的内容定位。 定义数据进入数据仓库的方式，作为数据汇总、映射和清洗的指南。 记录业务事件发生而随之进行的数据抽取工作时间安排。 记录并检测系统数据一致性的要求和执行情况。 评估数据质量。 什么是数据模型数据模型是抽象描述现实世界的一种工具和方法，是通过抽象的实体及实体之间联系的形式，来表示现实世界中事务的相互关系的一种映射。在这里，数据模型表现的抽象的是实体和实体之间的关系，通过对实体和实体之间关系的定义和描述，来表达实际的业务中具体的业务关系。数据仓库模型是数据模型中针对特定的数据仓库应用系统的一种特定的数据模型，一般的来说，我们数据仓库模型分为几下几个层次，如图所示： 通过上面的图形，我们能够很容易的看出在整个数据仓库得建模过程中，我们需要经历一般四个过程： 业务建模，生成业务模型，主要解决业务层面的分解和程序化。 领域建模，生成领域模型，主要是对业务模型进行抽象处理，生成领域概念模型。 逻辑建模，生成逻辑模型，主要是将领域模型的概念实体以及实体之间的关系进行数据库层次的逻辑化。 物理建模，生成物理模型，主要解决，逻辑模型针对不同关系型数据库的物理化以及性能等一些具体的技术问题。 因此，在整个数据仓库的模型的设计和架构中，既涉及到业务知识，也涉及到了具体的技术，我们既需要了解丰富的行业经验，同时，也需要一定的信息技术来帮助我们实现我们的数据模型，最重要的是，我们还需要一个非常适用的方法论，来指导我们自己针对我们的业务进行抽象，处理，生成各个阶段的模型。 为什么需要数据模型通过数据仓库建设的发展阶段，我们能够看出，数据仓库的建设和数据集市的建设的重要区别就在于数据模型的支持。因此，数据模型的建设，对于我们数据仓库的建设，有着决定性的意义。 一般来说，数据模型的建设主要能够帮助我们解决以下的一些问题： 进行全面的业务梳理，改进业务流程。在业务模型建设的阶段，能够帮助我们的企业或者是管理机关对本单位的业务进行全面的梳理。通过业务模型的建设，我们应该能够全面了解该单位的业务架构图和整个业务的运行情况，能够将业务按照特定的规律进行分门别类和程序化，同时，帮助我们进一步的改进业务的流程，提高业务效率，指导我们的业务部门的生产。 建立全方位的数据视角，消灭信息孤岛和数据差异。通过数据仓库的模型建设，能够为企业提供一个整体的数据视角，不再是各个部门只是关注自己的数据，而且通过模型的建设，勾勒出了部门之间内在的联系，帮助消灭各个部门之间的信息孤岛的问题，更为重要的是，通过数据模型的建设，能够保证整个企业的数据的一致性，各个部门之间数据的差异将会得到有效解决。 解决业务的变动和数据仓库的灵活性。通过数据模型的建设，能够很好的分离出底层技术的实现和上层业务的展现。当上层业务发生变化时，通过数据模型，底层的技术实现可以非常轻松的完成业务的变动，从而达到整个数据仓库系统的灵活性。 帮助数据仓库系统本身的建设。通过数据仓库的模型建设，开发人员和业务人员能够很容易的达成系统建设范围的界定，以及长期目标的规划，从而能够使整个项目组明确当前的任务，加快整个系统建设的速度。 如何建设数据仓库模型建设数据模型既然是整个数据仓库建设中一个非常重要的关键部分，那么，怎么建设我们的数据仓库模型就是我们需要解决的一个问题。这里我们将要详细介绍如何创建适合自己的数据模型。 数据仓库数据模型架构数据仓库的数据模型的架构和数据仓库的整体架构是紧密关联在一起的，我们首先来了解一下整个数据仓库的数据模型应该包含的几个部分。从下图我们可以很清楚地看到，整个数据模型的架构分成 5 大部分，每个部分其实都有其独特的功能。 从上图我们可以看出，整个数据仓库的数据模型可以分为大概 5 大部分： 系统记录域（System of Record）：这部分是主要的数据仓库业务数据存储区，数据模型在这里保证了数据的一致性。 内部管理域（Housekeeping）：这部分主要存储数据仓库用于内部管理的元数据，数据模型在这里能够帮助进行统一的元数据的管理。 汇总域（Summary of Area）：这部分数据来自于系统记录域的汇总，数据模型在这里保证了分析域的主题分析的性能，满足了部分的报表查询。 分析域（Analysis Area）：这部分数据模型主要用于各个业务部分的具体的主题业务分析。这部分数据模型可以单独存储在相应的数据集市中。 反馈域（Feedback Area）：可选项，这部分数据模型主要用于相应前端的反馈数据，数据仓库可以视业务的需要设置这一区域。 通过对整个数据仓库模型的数据区域的划分，我们可以了解到，一个好的数据模型，不仅仅是对业务进行抽象划分，而且对实现技术也进行具体的指导，它应该涵盖了从业务到实现技术的各个部分。 数据仓库建模阶段划分我们前面介绍了数据仓库模型的几个层次，下面我们讲一下，针对这几个层次的不同阶段的数据建模的工作的主要内容： 业务建模 划分整个单位的业务，一般按照业务部门的划分，进行各个部分之间业务工作的界定，理清各业务部门之间的关系。 深入了解各个业务部门的内具体业务流程并将其程序化。 提出修改和改进业务部门工作流程的方法并程序化。 数据建模的范围界定，整个数据仓库项目的目标和阶段划分。 领域概念建模 抽取关键业务概念，并将之抽象化。 将业务概念分组，按照业务主线聚合类似的分组概念。 细化分组概念，理清分组概念内的业务流程并抽象化。 理清分组概念之间的关联，形成完整的领域概念模型。 逻辑建模 业务概念实体化，并考虑其具体的属性。 事件实体化，并考虑其属性内容。 说明实体化，并考虑其属性内容。 物理建模 针对特定物理化平台，做出相应的技术调整。 针对模型的性能考虑，对特定平台作出相应的调整。 针对管理的需要，结合特定的平台，做出相应的调整。 生成最后的执行脚本，并完善之。 从我们上面对数据仓库的数据建模阶段的各个阶段的划分，我们能够了解到整个数据仓库建模的主要工作和工作量，希望能够对我们在实际的项目建设能够有所帮助。 数据仓库建模方法大千世界，表面看五彩缤纷，实质上，万物都遵循其自有的法则。数据仓库的建模方法同样也有很多种，每一种建模方法其实代表了哲学上的一个观点，代表了一种归纳，概括世界的一种方法。目前业界较为流行的数据仓库的建模方法非常多，这里主要介绍范式建模法，维度建模法，实体建模法等几种方法，每种方法其实从本质上讲就是从不同的角度看我们业务中的问题，不管从技术层面还是业务层面，其实代表的是哲学上的一种世界观。我们下面给大家详细介绍一下这些建模方法。 范式建模法范式建模法（Third Normal Form，3NF）：范式建模法其实是我们在构建数据模型常用的一个方法，该方法的主要由 Inmon 所提倡，主要解决关系型数据库得数据存储，利用的一种技术层面上的方法。目前，我们在关系型数据库中的建模方法，大部分采用的是三范式建模法。范式是数据库逻辑模型设计的基本理论，一个关系模型可以从第一范式到第五范式进行无损分解，这个过程也可称为规范化。在数据仓库的模型设计中目前一般采用第三范式，它有着严格的数学定义。从其表达的含义来看，一个符合第三范式的关系必须具有以下三个条件 : 每个属性值唯一，不具有多义性 ; 每个非主属性必须完全依赖于整个主键，而非主键的一部分 ; 每个非主属性不能依赖于其他关系中的属性，因为这样的话，这种属性应该归到其他关系中去。 由于范式是基于整个关系型数据库的理论基础之上发展而来的，因此，本人在这里不多做介绍，有兴趣的读者可以通过阅读相应的材料来获得这方面的知识。根据 Inmon 的观点，数据仓库模型得建设方法和业务系统的企业数据模型类似。在业务系统中，企业数据模型决定了数据的来源，而企业数据模型也分为两个层次，即主题域模型和逻辑模型。同样，主题域模型可以看成是业务模型的概念模型，而逻辑模型则是域模型在关系型数据库上的实例。 从业务数据模型转向数据仓库模型时，同样也需要有数据仓库的域模型，即概念模型，同时也存在域模型的逻辑模型。这里，业务模型中的数据模型和数据仓库的模型稍微有一些不同。主要区别在于： 数据仓库的域模型应该包含企业数据模型的域模型之间的关系，以及各主题域定义。数据仓库的域模型的概念应该比业务系统的主题域模型范围更加广。 在数据仓库的逻辑模型需要从业务系统的数据模型中的逻辑模型中抽象实体，实体的属性，实体的子类，以及实体的关系等。 Inmon 的范式建模法的最大优点就是从关系型数据库的角度出发，结合了业务系统的数据模型，能够比较方便的实现数据仓库的建模。但其缺点也是明显的，由于建模方法限定在关系型数据库之上，在某些时候反而限制了整个数据仓库模型的灵活性，性能等，特别是考虑到数据仓库的底层数据向数据集市的数据进行汇总时，需要进行一定的变通才能满足相应的需求。因此，笔者建议读者们在实际的使用中，参考使用这一建模方式。 维度建模法维度建模法，Kimball 最先提出这一概念。其最简单的描述就是，按照事实表，维表来构建数据仓库，数据集市。这种方法的最被人广泛知晓的名字就是星型模式（Star-schema）。 上图的这个架构中是典型的星型架构。星型模式之所以广泛被使用，在于针对各个维作了大量的预处理，如按照维进行预先的统计、分类、排序等。通过这些预处理，能够极大的提升数据仓库的处理能力。特别是针对 3NF 的建模方法，星型模式在性能上占据明显的优势。同时，维度建模法的另外一个优点是，维度建模非常直观，紧紧围绕着业务模型，可以直观的反映出业务模型中的业务问题。不需要经过特别的抽象处理，即可以完成维度建模。这一点也是维度建模的优势。但是，维度建模法的缺点也是非常明显的，由于在构建星型模式之前需要进行大量的数据预处理，因此会导致大量的数据处理工作。而且，当业务发生变化，需要重新进行维度的定义时，往往需要重新进行维度数据的预处理。而在这些与处理过程中，往往会导致大量的数据冗余。另外一个维度建模法的缺点就是，如果只是依靠单纯的维度建模，不能保证数据来源的一致性和准确性，而且在数据仓库的底层，不是特别适用于维度建模的方法。因此以笔者的观点看，维度建模的领域主要适用与数据集市层，它的最大的作用其实是为了解决数据仓库建模中的性能问题。维度建模很难能够提供一个完整地描述真实业务实体之间的复杂关系的抽象方法。 实体建模法实体建模法并不是数据仓库建模中常见的一个方法，它来源于哲学的一个流派。从哲学的意义上说，客观世界应该是可以细分的，客观世界应该可以分成由一个个实体，以及实体与实体之间的关系组成。那么我们在数据仓库的建模过程中完全可以引入这个抽象的方法，将整个业务也可以划分成一个个的实体，而每个实体之间的关系，以及针对这些关系的说明就是我们数据建模需要做的工作。虽然实体法粗看起来好像有一些抽象，其实理解起来很容易。即我们可以将任何一个业务过程划分成 3 个部分，实体，事件和说明，如下图所示： 上图表述的是一个抽象的含义，如果我们描述一个简单的事实：“小明开车去学校上学”。以这个业务事实为例，我们可以把“小明”，“学校”看成是一个实体，“上学”描述的是一个业务过程，我们在这里可以抽象为一个具体“事件”，而“开车去”则可以看成是事件“上学”的一个说明。从上面的举例我们可以了解，我们使用的抽象归纳方法其实很简单，任何业务可以看成 3 个部分： 实体，主要指领域模型中特定的概念主体，指发生业务关系的对象。 事件，主要指概念主体之间完成一次业务流程的过程，特指特定的业务过程。 说明，主要是针对实体和事件的特殊说明。 由于实体建模法，能够很轻松的实现业务模型的划分，因此，在业务建模阶段和领域概念建模阶段，实体建模法有着广泛的应用。从笔者的经验来看，再没有现成的行业模型的情况下，我们可以采用实体建模的方法，和客户一起理清整个业务的模型，进行领域概念模型的划分，抽象出具体的业务概念，结合客户的使用特点，完全可以创建出一个符合自己需要的数据仓库模型来。但是，实体建模法也有着自己先天的缺陷，由于实体说明法只是一种抽象客观世界的方法，因此，注定了该建模方法只能局限在业务建模和领域概念建模阶段。因此，到了逻辑建模阶段和物理建模阶段，则是范式建模和维度建模发挥长处的阶段。因此，笔者建议读者在创建自己的数据仓库模型的时候，可以参考使用上述的三种数据仓库得建模方法，在各个不同阶段采用不同的方法，从而能够保证整个数据仓库建模的质量。 维度建模概述在多维分析的商业智能解决方案中，根据事实表和维度表的关系，又可将常见的模型分为星型模型和雪花型模型。在设计逻辑型数据的模型的时候，就应考虑数据是按照星型模型还是雪花型模型进行组织。当所有维表都直接连接到“ 事实表”上时，整个图解就像星星一样，故将该模型称为星型模型，如图 1 。星型架构是一种非正规化的结构，多维数据集的每一个维度都直接与事实表相连接，不存在渐变维度，所以数据有一定的冗余，如在地域维度表中，存在国家 A 省 B 的城市 C 以及国家 A 省 B 的城市 D 两条记录，那么国家 A 和省 B 的信息分别存储了两次，即存在冗余。 销售数据仓库中的星型模型： 当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的 “ 层次 “ 区域，这些被分解的表都连接到主维度表而不是事实表。如图 2，将地域维表又分解为国家，省份，城市等维表。它的优点是 : 通过最大限度地减少数据存储量以及联合较小的维表来改善查询性能。雪花型结构去除了数据冗余。销售数据仓库中的雪花型模型： 星型模型因为数据的冗余所以很多统计查询不需要做外部的连接，因此一般情况下效率比雪花型模型要高。星型结构不用考虑很多正规化的因素，设计与实现都比较简单。雪花型模型由于去除了冗余，有些统计就需要通过表的联接才能产生，所以效率不一定有星型模型高。正规化也是一种比较复杂的过程，相应的数据库结构设计、数据的 ETL、以及后期的维护都要复杂一些。因此在冗余可以接受的前提下，实际运用中星型模型使用更多，也更有效率。 使用选择星形模型(Star Schema)和雪花模型(Snowflake Schema)是数据仓库中常用到的两种方式，而它们之间的对比要从四个角度来进行讨论。 数据优化雪花模型使用的是规范化数据，也就是说数据在数据库内部是组织好的，以便消除冗余，因此它能够有效地减少数据量。通过引用完整性，其业务层级和维度都将存储在数据模型之中。 相比较而言，星形模型实用的是反规范化数据。在星形模型中，维度直接指的是事实表，业务层级不会通过维度之间的参照完整性来部署。 业务模型主键是一个单独的唯一键(数据属性)，为特殊数据所选择。在上面的例子中，Advertiser_ID就将是一个主键。外键(参考属性)仅仅是一个表中的字段，用来匹配其他维度表中的主键。在我们所引用的例子中，Advertiser_ID将是Account_dimension的一个外键。在雪花模型中，数据模型的业务层级是由一个不同维度表主键-外键的关系来代表的。而在星形模型中，所有必要的维度表在事实表中都只拥有外键。 性能第三个区别在于性能的不同。雪花模型在维度表、事实表之间的连接很多，因此性能方面会比较低。举个例子，如果你想要知道Advertiser 的详细信息，雪花模型就会请求许多信息，比如Advertiser Name、ID以及那些广告主和客户表的地址需要连接起来，然后再与事实表连接。而星形模型的连接就少的多，在这个模型中，如果你需要上述信息，你只要将Advertiser的维度表和事实表连接即可。 ETL雪花模型加载数据集市，因此ETL操作在设计上更加复杂，而且由于附属模型的限制，不能并行化。星形模型加载维度表，不需要再维度之间添加附属模型，因此ETL就相对简单，而且可以实现高度的并行化。 总结雪花模型使得维度分析更加容易，比如“针对特定的广告主，有哪些客户或者公司是在线的?”星形模型用来做指标分析更适合，比如“给定的一个客户他们的收入是多少?” 缓慢变化维维度建模的数据仓库中，有一个概念叫 Slowly Changing Dimensions, 中文一般翻译成”缓慢变化维“，经常被简写为 SCD. 缓慢变化维的提出是因为在现实世界中，维度的属性并不是静态的，它会随着时间的流失发生缓慢的变化。这种随着时间发生变化的维度我们一般称之为缓慢变化维，并且把处理温度表的历史变化信息的问题称为处理缓慢变化维问题，有时也简称为处理 SCD 的问题。 如何解决缓慢变化维带来的影响？ 第一种方法直接在原来维度的基础上进行更新，不会产生新的记录。 更新前： emp_rid emp_id emp_name position 101212 12345 Jack Developer 更新后： emp_rid emp_id emp_name position 101212 12345 Jack Manager position 发生变化。 第二种方法不修改原有的数据，重新产生一条新的记录，这样就可以追溯所有的历史记录。 更新前： emp_rid emp_id emp_name position start_date end_date 101212 12345 Jack Developer 2010-02-05 2012-06-12 更新后： emp_rid emp_id emp_name position start_date end_date 101212 12345 Jack Manager 2012-06-12 新增了一条记录。 第三种方法直接在原来维度的基础上进行更新，不会产生新的记录但是会记录上一次的。 更新前： emp_rid emp_id emp_name position old_position 101212 12345 Jack Developer null 更新后： emp_rid emp_id emp_name position old_position 101212 12345 Jack Manager Developer 多一个字段，用来存放以前的 position. End.]]></content>
      <tags>
        <tag>Data-Warehouse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark SQL 应用解析]]></title>
    <url>%2F2017%2F08%2F10%2FSpark-SQL-Analysis%2F</url>
    <content type="text"><![CDATA[本文主要解析部分 Spark SQL 的技术点。 Spark SQL 概述什么是 Spark SQL Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。我们已经学习了Hive，它是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！ Spark SQL 有四大特点： 易整合。 统一的数据访问方式。 兼容 Hive. 标准的数据连接。 SparkSQL可以看做是一个转换层，向下对接各种不同的结构化数据源，向上提供不同的数据访问方式。 RDD vs DataFrames vs DataSet 在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。他们和RDD有什么区别呢？首先从版本的产生上来看：RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt; Dataset(Spark1.6)如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。在后期的Spark版本中，DataSet会逐步取代RDD和DataFrame成为唯一的API接口。 RDD RDD是一个懒执行的不可变的可以支持Lambda表达式的并行数据集合。 RDD的最大好处就是简单，API的人性化程度很高。 RDD的劣势是性能限制，它是一个JVM驻内存对象，这也就决定了存在GC的限制和数据增加时Java序列化成本的升高。 DataFrame与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。由于与R和Pandas的DataFrame类似，Spark DataFrame很好地继承了传统单机数据分析的开发体验。 上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待。DataFrame也是懒执行的。性能上比RDD要高，主要有两方面原因： 定制化内存管理：数据以二进制的方式存在于非堆内存，节省了大量空间之外，还摆脱了GC的限制。 优化的执行计划：查询计划通过Spark catalyst optimiser进行优化。 比如下面这个例子： 12users.join(events, users("id") === events("uid")) .filter(events("date") &gt; "2015-01-01") 为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。得到的优化执行计划在转换成物 理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。对于普通开发者而言，查询优化器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。Dataframe的劣势在于在编译期缺少类型安全检查，导致运行时出错。 DataSet 是Dataframe API的一个扩展，是Spark最新的数据抽象。 用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。 Dataset支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。 样例类被用来在Dataset中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称。 Dataframe是Dataset的特列，DataFrame=Dataset[Row] ，所以可以通过as方法将Dataframe转换为Dataset。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息我都用Row来表示。 DataSet是强类型的。比如可以有Dataset[Car]，Dataset[Person]. DataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比。 RDD让我们能够决定怎么做，而DataFrame和DataSet让我们决定做什么，控制的粒度不一样。 三者的共性 RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利。 三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过。 12345678val sparkconf = new SparkConf().setMaster("local").setAppName("test").set("spark.port.maxRetries","1000")val spark = SparkSession.builder().config(sparkconf).getOrCreate()val rdd=spark.sparkContext.parallelize(Seq(("a", 1), ("b", 1), ("a", 1)))// map不运行rdd.map&#123;line=&gt; println("运行") line._1&#125; 三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出。 三者都有partition的概念。 三者有许多共同的函数，如filter，排序等。 在对DataFrame和Dataset进行操作许多操作都需要这个包进行支持。 1import spark.implicits._ DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型。DataFrame: 1234567testDF.map&#123; case Row(col1:String,col2:Int)=&gt; println(col1);println(col2) col1 case _=&gt; "" &#125; Dataset: 12345678case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型 testDS.map&#123; case Coltest(col1:String,col2:Int)=&gt; println(col1);println(col2) col1 case _=&gt; "" &#125; 三者的区别RDD: RDD一般和spark mlib同时使用。 RDD不支持spark sql操作。 DataFrame: 与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值，如 12345testDF.foreach&#123; line =&gt; val col1=line.getAs[String]("col1") val col2=line.getAs[String]("col2")&#125; 每一列的值没法直接访问 DataFrame与Dataset一般不与spark ml同时使用。 DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作，如 12dataDF.createOrReplaceTempView("tmp")spark.sql("select ROW,DATE from tmp where DATE is not null order by DATE").show(100,false) DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然。 123456//保存val saveoptions = Map("header" -&gt; "true", "delimiter" -&gt; "\t", "path" -&gt; "hdfs://master01:9000/test")datawDF.write.format("com.atguigu.spark.csv").mode(SaveMode.Overwrite).options(saveoptions).save()//读取val options = Map("header" -&gt; "true", "delimiter" -&gt; "\t", "path" -&gt; "hdfs://master01:9000/test")val datarDF= spark.read.options(options).format("com.atguigu.spark.csv").load() 利用这样的保存方式，可以方便的获得字段名和列的对应，而且分隔符（delimiter）可以自由指定。 Dataset: Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同。 DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的getAS方法或者共性中的第七条提到的模式匹配拿出特定字段。 而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息。 123456789101112131415case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型/** rdd ("a", 1) ("b", 1) ("a", 1)**/val test: Dataset[Coltest]=rdd.map&#123;line=&gt; Coltest(line._1,line._2) &#125;.toDStest.map&#123; line=&gt; println(line.col1) println(line.col2) &#125; 可以看出，Dataset在需要访问列中的某个字段时是非常方便的，然而，如果要写一些适配性很强的函数时，如果使用Dataset，行的类型又不确定，可能是各种case class，无法实现适配，这时候用DataFrame即Dataset[Row]就能比较好的解决问题。 执行 Spark SQL 查询命令行查询流程打开Spark shell例子：查询大于30岁的用户创建如下JSON文件，注意JSON的格式： 123&#123;"name":"Michael"&#125;&#123;"name":"Andy", "age":30&#125;&#123;"name":"Justin", "age":19&#125; 1234567891011121314151617181920212223scala&gt; val df = spark.read.json("/opt/txt_data/json.txt")df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]scala&gt; df.show+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+scala&gt; df.createOrReplaceTempView("persons")scala&gt; spark.sql("SELECT * FROM persons").show()+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+ IDEA 创建 Spark SQL 程序Maven 依赖： 123456&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 程序如下： 12345678910111213141516171819202122232425262728293031323334353637package com.moqi.sparksqlimport org.apache.spark.sql.SparkSessionimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.slf4j.LoggerFactoryobject HelloWorld &#123; val logger = LoggerFactory.getLogger(HelloWorld.getClass) def main(args: Array[String]) &#123; //创建SparkConf()并设置App名称 val spark = SparkSession .builder() .appName("Spark SQL basic example") .config("spark.some.config.option", "some-value") .getOrCreate() // For implicit conversions like converting RDDs to DataFrames import spark.implicits._ val df = spark.read.json("examples/src/main/resources/people.json") // Displays the content of the DataFrame to stdout df.show() df.filter($"age" &gt; 21).show() df.createOrReplaceTempView("persons") spark.sql("SELECT * FROM persons where age &gt; 21").show() spark.stop() &#125;&#125; Spark SQL 解析新的起点：SparkSession在老的版本中，SparkSQL提供两种SQL查询起始点，一个叫SQLContext，用于Spark自己提供的SQL查询，一个叫HiveContext，用于连接Hive的查询，SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。 12345678910import org.apache.spark.sql.SparkSessionval spark = SparkSession.builder().appName("Spark SQL basic example").config("spark.some.config.option", "some-value").getOrCreate()// For implicit conversions like converting RDDs to DataFramesimport spark.implicits._ SparkSession.builder 用于创建一个SparkSession。import spark.implicits._的引入是用于将DataFrames隐式转换成RDD，使df能够使用RDD中的方法。如果需要Hive支持，则需要以下创建语句： 1234567891011import org.apache.spark.sql.SparkSessionval spark = SparkSession.builder().appName("Spark SQL basic example").config("spark.some.config.option", "some-value").enableHiveSupport().getOrCreate()// For implicit conversions like converting RDDs to DataFramesimport spark.implicits._ 创建 DataFrames在Spark SQL中SparkSession是创建DataFrames和执行SQL的入口，创建DataFrames有三种方式，一种是可以从一个存在的RDD进行转换，还可以从Hive Table进行查询返回，或者通过Spark的数据源进行创建。从Spark数据源进行创建： 12345678910val df = spark.read.json("examples/src/main/resources/people.json") // Displays the content of the DataFrame to stdoutdf.show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+ 从 RDD 进行转换： 12345678910111213141516171819/**Michael, 29Andy, 30Justin, 19**/scala&gt; val peopleRdd = sc.textFile("examples/src/main/resources/people.txt")peopleRdd: org.apache.spark.rdd.RDD[String] = examples/src/main/resources/people.txt MapPartitionsRDD[18] at textFile at &lt;console&gt;:24scala&gt; val peopleDF3 = peopleRdd.map(_.split(",")).map(paras =&gt; (paras(0),paras(1).trim().toInt)).toDF("name","age")peopleDF3: org.apache.spark.sql.DataFrame = [name: string, age: int]scala&gt; peopleDF.show()+-------+---+| name|age|+-------+---+|Michael| 29|| Andy| 30|| Justin| 19|+-------+---+ Hive 部分在后面数据源介绍。 DataFrame 常用操作DSL 风格语法123456789101112131415161718192021222324252627282930313233343536373839404142434445// This import is needed to use the $-notationimport spark.implicits._// Print the schema in a tree formatdf.printSchema()// root// |-- age: long (nullable = true)// |-- name: string (nullable = true)// Select only the "name" columndf.select("name").show()// +-------+// | name|// +-------+// |Michael|// | Andy|// | Justin|// +-------+// Select everybody, but increment the age by 1df.select($"name", $"age" + 1).show()// +-------+---------+// | name|(age + 1)|// +-------+---------+// |Michael| null|// | Andy| 31|// | Justin| 20|// +-------+---------+// Select people older than 21df.filter($"age" &gt; 21).show()// +---+----+// |age|name|// +---+----+// | 30|Andy|// +---+----+// Count people by agedf.groupBy("age").count().show()// +----+-----+// | age|count|// +----+-----+// | 19| 1|// |null| 1|// | 30| 1|// +----+-----+ SQL 风格语法123456789101112131415161718192021222324252627282930313233343536// Register the DataFrame as a SQL temporary viewdf.createOrReplaceTempView("people")val sqlDF = spark.sql("SELECT * FROM people")sqlDF.show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+// Register the DataFrame as a global temporary viewdf.createGlobalTempView("people")// Global temporary view is tied to a system preserved database `global_temp`spark.sql("SELECT * FROM global_temp.people").show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+// Global temporary view is cross-sessionspark.newSession().sql("SELECT * FROM global_temp.people").show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+ 临时表是Session范围内的，Session退出后，表就失效了。如果想应用范围内有效，可以使用全局表。注意使用全局表时需要全路径访问，如：global_temp.people. 创建 DataSetDataset是具有强类型的数据集合，需要提供对应的类型信息。 12345678910111213141516171819202122232425262728// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,// you can use custom classes that implement the Product interfacecase class Person(name: String, age: Long)// Encoders are created for case classesval caseClassDS = Seq(Person("Andy", 32)).toDS()caseClassDS.show()// +----+---+// |name|age|// +----+---+// |Andy| 32|// +----+---+// Encoders for most common types are automatically provided by importing spark.implicits._val primitiveDS = Seq(1, 2, 3).toDS()primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by nameval path = "examples/src/main/resources/people.json"val peopleDS = spark.read.json(path).as[Person]peopleDS.show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+ DataSet 和 RDD 互操作Spark SQL支持通过两种方式将存在的RDD转换为Dataset，转换的过程中需要让Dataset获取RDD中的Schema信息，主要有两种方式，一种是通过反射来获取RDD中的Schema信息。这种方式适合于列名已知的情况下。第二种是通过编程接口的方式将Schema信息应用于RDD，这种方式可以处理那种在运行时才能知道列的方式。 通过反射获取 SchemaSparkSQL能够自动将包含有case类的RDD转换成DataFrame，case类定义了table的结构，case类属性通过反射变成了表的列名。Case类可以包含诸如Seqs或者Array等复杂的结构。 12345678910111213141516171819202122232425262728293031323334353637383940// For implicit conversions from RDDs to DataFramesimport spark.implicits._// Create an RDD of Person objects from a text file, convert it to a Dataframeval peopleDF = spark.sparkContext.textFile("examples/src/main/resources/people.txt").map(_.split(",")).map(attributes =&gt; Person(attributes(0), attributes(1).trim.toInt)).toDF()// Register the DataFrame as a temporary viewpeopleDF.createOrReplaceTempView("people")// SQL statements can be run by using the sql methods provided by Sparkval teenagersDF = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 19")// The columns of a row in the result can be accessed by field index ROW objectteenagersDF.map(teenager =&gt; "Name: " + teenager(0)).show()// +------------+// | value|// +------------+// |Name: Justin|// +------------+// or by field nameteenagersDF.map(teenager =&gt; "Name: " + teenager.getAs[String]("name")).show()// +------------+// | value|// +------------+// |Name: Justin|// +------------+// No pre-defined encoders for Dataset[Map[K,V]], define explicitlyimplicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]// Primitive types and case classes can be also defined as// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]teenagersDF.map(teenager =&gt; teenager.getValuesMap[Any](List("name", "age"))).collect()// Array(Map("name" -&gt; "Justin", "age" -&gt; 19)) 通过编程设置 Schema如果case类不能够提前定义，可以通过下面三个步骤定义一个DataFrame创建一个多行结构的RDD;创建用StructType来表示的行结构信息。通过SparkSession提供的createDataFrame方法来应用Schema. 1234567891011121314151617181920212223242526272829303132333435363738394041import org.apache.spark.sql.types._// Create an RDDval peopleRDD = spark.sparkContext.textFile("examples/src/main/resources/people.txt")// The schema is encoded in a string,应该是动态通过程序生成的val schemaString = "name age"// Generate the schema based on the string of schema Array[StructFiled]val fields = schemaString.split(" ").map(fieldName =&gt; StructField(fieldName, StringType, nullable = true))// val filed = schemaString.split(" ").map(filename=&gt; filename match&#123; case "name"=&gt; StructField(filename,StringType,nullable = true); case "age"=&gt;StructField(filename, IntegerType,nullable = true)&#125; )val schema = StructType(fields)// Convert records of the RDD (people) to Rowsimport org.apache.spark.sql._val rowRDD = peopleRDD.map(_.split(",")).map(attributes =&gt; Row(attributes(0), attributes(1).trim))// Apply the schema to the RDDval peopleDF = spark.createDataFrame(rowRDD, schema)// Creates a temporary view using the DataFramepeopleDF.createOrReplaceTempView("people")// SQL can be run over a temporary view created using DataFramesval results = spark.sql("SELECT name FROM people")// The results of SQL queries are DataFrames and support all the normal RDD operations// The columns of a row in the result can be accessed by field index or by field nameresults.map(attributes =&gt; "Name: " + attributes(0)).show()// +-------------+// | value|// +-------------+// |Name: Michael|// | Name: Andy|// | Name: Justin|// +-------------+ 类型之间的转换总结RDD、DataFrame、Dataset三者有许多共性，有各自适用的场景常常需要在三者之间转换DataFrame/Dataset转RDD：这个转换很简单： 12val rdd1=testDF.rddval rdd2=testDS.rdd RDD转DataFrame： 一般用元组把一行的数据写在一起，然后在toDF中指定字段名。 1234import spark.implicits._val testDF = rdd.map &#123;line=&gt; (line._1,line._2) &#125;.toDF("col1","col2") RDD转DataSet： 可以注意到，定义每一行的类型（case class）时，已经给出了字段名和类型，后面只要往case class里面添加值即可。 12345import spark.implicits._case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型val testDS = rdd.map &#123;line=&gt; Coltest(line._1,line._2) &#125;.toDS DataSet转DataFrame： 这个也很简单，因为只是把case class封装成Row. 12import spark.implicits._val testDF = testDS.toDF DataFrame转DataSet： 123import spark.implicits._case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型val testDS = testDF.as[Coltest] 这种方法就是在给出每一列的类型后，使用as方法，转成Dataset，这在数据类型是DataFrame又需要针对各个字段处理时极为方便。在使用一些特殊的操作时，一定要加上 import spark.implicits._ 不然toDF、toDS无法使用。 用户自定义函数通过spark.udf功能用户可以自定义函数。 用户自定义UDF函数1234567891011121314151617181920212223242526scala&gt; val df = spark.read.json("examples/src/main/resources/people.json")df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]scala&gt; df.show()+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+scala&gt; spark.udf.register("addName", (x:String)=&gt; "Name:"+x)res5: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType)))scala&gt; df.createOrReplaceTempView("people")scala&gt; spark.sql("Select addName(name), age from people").show()+-----------------+----+|UDF:addName(name)| age|+-----------------+----+| Name:Michael|null|| Name:Andy| 30|| Name:Justin| 19|+-----------------+----+ 用户自定义聚合函数强类型的Dataset和弱类型的DataFrame都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。 弱类型通过继承UserDefinedAggregateFunction来实现用户自定义聚合函数。下面展示一个求平均工资的自定义聚合函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import org.apache.spark.sql.expressions.MutableAggregationBufferimport org.apache.spark.sql.expressions.UserDefinedAggregateFunctionimport org.apache.spark.sql.types._import org.apache.spark.sql.Rowimport org.apache.spark.sql.SparkSessionobject MyAverage extends UserDefinedAggregateFunction &#123;// 聚合函数输入参数的数据类型 def inputSchema: StructType = StructType(StructField("inputColumn", LongType) :: Nil)// 聚合缓冲区中值得数据类型 def bufferSchema: StructType = &#123;StructType(StructField("sum", LongType) :: StructField("count", LongType) :: Nil)&#125;// 返回值的数据类型 def dataType: DataType = DoubleType// 对于相同的输入是否一直返回相同的输出。 def deterministic: Boolean = true// 初始化def initialize(buffer: MutableAggregationBuffer): Unit = &#123;// 存工资的总额buffer(0) = 0L// 存工资的个数buffer(1) = 0L&#125;// 相同Execute间的数据合并。 def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123;if (!input.isNullAt(0)) &#123;buffer(0) = buffer.getLong(0) + input.getLong(0)buffer(1) = buffer.getLong(1) + 1&#125;&#125;// 不同Execute间的数据合并 def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123;buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)&#125;// 计算最终结果def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)&#125;// 注册函数spark.udf.register("myAverage", MyAverage)val df = spark.read.json("examples/src/main/resources/employees.json")df.createOrReplaceTempView("employees")df.show()// +-------+------+// | name|salary|// +-------+------+// |Michael| 3000|// | Andy| 4500|// | Justin| 3500|// | Berta| 4000|// +-------+------+val result = spark.sql("SELECT myAverage(salary) as average_salary FROM employees")result.show()// +--------------+// |average_salary|// +--------------+// | 3750.0|// +--------------+ 强类型通过继承Aggregator来实现强类型自定义聚合函数，同样是求平均工资。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import org.apache.spark.sql.expressions.Aggregatorimport org.apache.spark.sql.Encoderimport org.apache.spark.sql.Encodersimport org.apache.spark.sql.SparkSession// 既然是强类型，可能有case类case class Employee(name: String, salary: Long)case class Average(var sum: Long, var count: Long)object MyAverage extends Aggregator[Employee, Average, Double] &#123;// 定义一个数据结构，保存工资总数和工资总个数，初始都为0def zero: Average = Average(0L, 0L)// Combine two values to produce a new value. For performance, the function may modify `buffer`// and return it instead of constructing a new objectdef reduce(buffer: Average, employee: Employee): Average = &#123;buffer.sum += employee.salarybuffer.count += 1buffer&#125;// 聚合不同execute的结果def merge(b1: Average, b2: Average): Average = &#123;b1.sum += b2.sumb1.count += b2.countb1&#125;// 计算输出def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count// 设定之间值类型的编码器，要转换成case类// Encoders.product是进行scala元组和case类转换的编码器 def bufferEncoder: Encoder[Average] = Encoders.product// 设定最终输出值的编码器def outputEncoder: Encoder[Double] = Encoders.scalaDouble&#125;import spark.implicits._val ds = spark.read.json("examples/src/main/resources/employees.json").as[Employee]ds.show()// +-------+------+// | name|salary|// +-------+------+// |Michael| 3000|// | Andy| 4500|// | Justin| 3500|// | Berta| 4000|// +-------+------+// Convert the function to a `TypedColumn` and give it a nameval averageSalary = MyAverage.toColumn.name("average_salary")val result = ds.select(averageSalary)result.show()// +--------------+// |average_salary|// +--------------+// | 3750.0|// +--------------+ Spark SQL 数据源通用加载 / 保存方法手动指定选项Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。修改配置项spark.sql.sources.default，可修改默认数据源格式。 1val df = spark.read.load("examples/src/main/resources/users.parquet") df.select("name", "favorite_color").write.save("namesAndFavColors.parquet") 当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：org.apache.spark.sql.parquet），如果数据源格式为内置格式，则只需要指定简称定json, parquet, jdbc, orc, libsvm, csv, text来指定数据的格式。可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。 12val peopleDF = spark.read.format("json").load("examples/src/main/resources/people.json")peopleDF.write.format("parquet").save("hdfs://master01:9000/namesAndAges.parquet") 除此之外，可以直接运行SQL在文件上： 12val sqlDF = spark.sql("SELECT * FROM parquet.`hdfs://master01:9000/namesAndAges.parquet`")sqlDF.show() 1234567891011121314151617181920212223242526scala&gt; val peopleDF = spark.read.format("json").load("examples/src/main/resources/people.json")peopleDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]scala&gt; peopleDF.write.format("parquet").save("hdfs://master01:9000/namesAndAges.parquet")scala&gt; peopleDF.show()+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+scala&gt; val sqlDF = spark.sql("SELECT * FROM parquet.`hdfs://master01:9000/namesAndAges.parquet`")17/09/05 04:21:11 WARN ObjectStore: Failed to get database parquet, returning NoSuchObjectExceptionsqlDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]scala&gt; sqlDF.show()+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+ 文件保存选项可以采用SaveMode执行存储操作，SaveMode定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。SaveMode详细介绍如下表： Scala/Java Any Language Meaning SaveMode.ErrorIfExists(default) “error”(default) 如果文件存在，则报错 SaveMode.Append “append” 追加 SaveMode.Overwrite “overwrite” 覆写 SaveMode.Ignore “ignore” 数据存在，则忽略 Parquet 文件Parquet是一种流行的列式存储格式，可以高效地存储具有嵌套字段的记录。 Parquet 读写Parquet格式经常在Hadoop生态圈中被使用，它也支持Spark SQL的全部数据类型。Spark SQL 提供了直接读取和存储 Parquet 格式文件的方法。 12345678910111213141516171819202122// Encoders for most common types are automatically provided by importing spark.implicits._import spark.implicits._val peopleDF = spark.read.json("examples/src/main/resources/people.json")// DataFrames can be saved as Parquet files, maintaining the schema informationpeopleDF.write.parquet("hdfs://master01:9000/people.parquet")// Read in the parquet file created above// Parquet files are self-describing so the schema is preserved// The result of loading a Parquet file is also a DataFrameval parquetFileDF = spark.read.parquet("hdfs://master01:9000/people.parquet")// Parquet files can also be used to create a temporary view and then used in SQL statementsparquetFileDF.createOrReplaceTempView("parquetFile")val namesDF = spark.sql("SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19")namesDF.map(attributes =&gt; "Name: " + attributes(0)).show()// +------------+// | value|// +------------+// |Name: Justin|// +------------+ 解析分区信息对表进行分区是对数据进行优化的方式之一。在分区的表内，数据通过分区列将数据存储在不同的目录下。Parquet数据源现在能够自动发现并解析分区信息。例如，对人口数据进行分区存储，分区列为gender和country，使用下面的目录结构： 12345678910111213141516171819path└── to └── table ├── gender=male │ ├── ... │ │ │ ├── country=US │ │ └── data.parquet │ ├── country=CN │ │ └── data.parquet │ └── ... └── gender=female ├── ... │ ├── country=US │ └── data.parquet ├── country=CN │ └── data.parquet └── ... 通过传递path/to/table给 SQLContext.read.parquet或SQLContext.read.load，Spark SQL将自动解析分区信息。返回的DataFrame的Schema如下： 12345root|-- name: string (nullable = true)|-- age: long (nullable = true)|-- gender: string (nullable = true)|-- country: string (nullable = true) 需要注意的是，数据的分区列的数据类型是自动解析的。当前，支持数值类型和字符串类型。自动解析分区类型的参数为：spark.sql.sources.partitionColumnTypeInference.enabled，默认值为true。如果想关闭该功能，直接将该参数设置为disabled。此时，分区列数据格式将被默认设置为string类型，不再进行类型解析。 Schema 合并像ProtocolBuffer、Avro和Thrift那样，Parquet也支持Schema evolution（Schema演变）。用户可以先定义一个简单的Schema，然后逐渐的向Schema中增加列描述。通过这种方式，用户可以获取多个有不同Schema但相互兼容的Parquet文件。现在Parquet数据源能自动检测这种情况，并合并这些文件的schemas。因为Schema合并是一个高消耗的操作，在大多数情况下并不需要，所以Spark SQL从1.5.0开始默认关闭了该功能。可以通过下面两种方式开启该功能：当数据源为Parquet文件时，将数据源选项mergeSchema设置为true设置全局SQL选项spark.sql.parquet.mergeSchema为true示例如下： 123456789101112131415161718192021222324// sqlContext from the previous example is used in this example.// This is used to implicitly convert an RDD to a DataFrame.import spark.implicits._// Create a simple DataFrame, stored into a partition directoryval df1 = sc.makeRDD(1 to 5).map(i =&gt; (i, i * 2)).toDF("single", "double")df1.write.parquet("hdfs://master01:9000/data/test_table/key=1")// Create another DataFrame in a new partition directory,// adding a new column and dropping an existing columnval df2 = sc.makeRDD(6 to 10).map(i =&gt; (i, i * 3)).toDF("single", "triple")df2.write.parquet("hdfs://master01:9000/data/test_table/key=2")// Read the partitioned tableval df3 = spark.read.option("mergeSchema", "true").parquet("hdfs://master01:9000/data/test_table")df3.printSchema()// The final schema consists of all 3 columns in the Parquet files together// with the partitioning column appeared in the partition directory paths.// root// |-- single: int (nullable = true)// |-- double: int (nullable = true)// |-- triple: int (nullable = true)// |-- key : int (nullable = true) Hive 数据库Apache Hive是Hadoop上的SQL引擎，Spark SQL编译时可以包含Hive支持，也可以不包含。包含Hive支持的Spark SQL可以支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的 一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好Hive，Spark SQL也可以运行。 需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import java.io.Fileimport org.apache.spark.sql.Rowimport org.apache.spark.sql.SparkSessioncase class Record(key: Int, value: String)// warehouseLocation points to the default location for managed databases and tablesval warehouseLocation = new File("spark-warehouse").getAbsolutePathval spark = SparkSession.builder().appName("Spark Hive Example").config("spark.sql.warehouse.dir", warehouseLocation).enableHiveSupport().getOrCreate()import spark.implicits._import spark.sqlsql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src")// Queries are expressed in HiveQLsql("SELECT * FROM src").show()// +---+-------+// |key| value|// +---+-------+// |238|val_238|// | 86| val_86|// |311|val_311|// ...// Aggregation queries are also supported.sql("SELECT COUNT(*) FROM src").show()// +--------+// |count(1)|// +--------+// | 500 |// +--------+// The results of SQL queries are themselves DataFrames and support all normal functions.val sqlDF = sql("SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key")// The items in DataFrames are of type Row, which allows you to access each column by ordinal.val stringsDS = sqlDF.map &#123;case Row(key: Int, value: String) =&gt; s"Key: $key, Value: $value"&#125;stringsDS.show()// +--------------------+// | value|// +--------------------+// |Key: 0, Value: val_0|// |Key: 0, Value: val_0|// |Key: 0, Value: val_0|// ...// You can also use DataFrames to create temporary views within a SparkSession.val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s"val_$i")))recordsDF.createOrReplaceTempView("records")// Queries can then join DataFrame data with data stored in Hive.sql("SELECT * FROM records r JOIN src s ON r.key = s.key").show()// +---+------+---+------+// |key| value|key| value|// +---+------+---+------+// | 2| val_2| 2| val_2|// | 4| val_4| 4| val_4|// | 5| val_5| 5| val_5|// ... 内联 Hive 应用如果要使用内嵌的Hive，什么都不用做，直接用就可以了。 –conf : spark.sql.warehouse.dir= 12345678910111213141516scala&gt; spark.sql("show tables").show+--------+---------+-----------+|database|tableName|isTemporary|+--------+---------+-----------+| | persons| true|+--------+---------+-----------+scala&gt; spark.sql("SELECT * FROM persons").show()+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+ 注意：如果你使用的是内部的Hive，在Spark2.0之后，spark.sql.warehouse.dir用于指定数据仓库的地址，如果你需要是用HDFS作为路径，那么需要将core-site.xml和hdfs-site.xml 加入到Spark conf目录，否则只会创建master节点上的warehouse目录，查询时会出现文件找不到的问题。所以如果需要使用HDFS，则需要将metastore删除，重启集群。 外部 Hive 应用如果想连接外部已经部署好的Hive，需要通过以下几个步骤。 将Hive中的hive-site.xml拷贝或者软连接到Spark安装目录下的conf目录下。 打开spark shell，注意带上访问Hive元数据库的JDBC客户端。 1$ bin/spark-shell --master spark://master01:7077 --jars mysql-connector-java-5.1.27-bin.jar JSON 数据集Spark SQL 能够自动推测 JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载一个 Dataset[String]或者一个JSON 文件.注意，这个JSON文件不是一个传统的JSON文件，每一行都得是一个JSON串。 123&#123;"name":"Michael"&#125;&#123;"name":"Andy", "age":30&#125;&#123;"name":"Justin", "age":19&#125; 1234567891011121314151617181920212223242526272829303132333435363738// Primitive types (Int, String, etc) and Product types (case classes) encoders are// supported by importing this when creating a Dataset.import spark.implicits._// A JSON dataset is pointed to by path.// The path can be either a single text file or a directory storing text filesval path = "examples/src/main/resources/people.json"val peopleDF = spark.read.json(path)// The inferred schema can be visualized using the printSchema() methodpeopleDF.printSchema()// root// |-- age: long (nullable = true)// |-- name: string (nullable = true)// Creates a temporary view using the DataFramepeopleDF.createOrReplaceTempView("people")// SQL statements can be run by using the sql methods provided by sparkval teenagerNamesDF = spark.sql("SELECT name FROM people WHERE age BETWEEN 13 AND 19")teenagerNamesDF.show()// +------+// | name|// +------+// |Justin|// +------+// Alternatively, a DataFrame can be created for a JSON dataset represented by// a Dataset[String] storing one JSON object per stringval otherPeopleDataset = spark.createDataset("""&#123;"name":"Yin","address":&#123;"city":"Columbus","state":"Ohio"&#125;&#125;""" :: Nil)val otherPeople = spark.read.json(otherPeopleDataset)otherPeople.show()// +---------------+----+// | address|name|// +---------------+----+// |[Columbus,Ohio]| Yin|// +---------------+----+ JDBCSpark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。注意，需要将相关的数据库驱动放到spark的类路径下。 1$ bin/spark-shell --master spark://master01:7077 --jars mysql-connector-java-5.1.27-bin.jar 1234567891011121314151617181920212223242526// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods// Loading data from a JDBC sourceval jdbcDF = spark.read.format("jdbc").option("url", "jdbc:mysql://master01:3306/rdd").option("dbtable", " rddtable").option("user", "root").option("password", "hive").load()val connectionProperties = new Properties()connectionProperties.put("user", "root")connectionProperties.put("password", "hive")val jdbcDF2 = spark.read.jdbc("jdbc:mysql://master01:3306/rdd", "rddtable", connectionProperties)// Saving data to a JDBC sourcejdbcDF.write.format("jdbc").option("url", "jdbc:mysql://master01:3306/rdd").option("dbtable", "rddtable2").option("user", "root").option("password", "hive").save()jdbcDF2.write.jdbc("jdbc:mysql://master01:3306/mysql", "db", connectionProperties)// Specifying create table column data types on writejdbcDF.write.option("createTableColumnTypes", "name CHAR(64), comments VARCHAR(1024)").jdbc("jdbc:mysql://master01:3306/mysql", "db", connectionProperties) JDBC / ODBC 服务器Spark SQL也提供JDBC连接支持，这对于让商业智能(BI)工具连接到Spark集群上以 及在多用户间共享一个集群的场景都非常有用。JDBC 服务器作为一个独立的 Spark 驱动 器程序运行，可以在多用户之间共享。任意一个客户端都可以在内存中缓存数据表，对表 进行查询。集群的资源以及缓存数据都在所有用户之间共享。Spark SQL的JDBC服务器与Hive中的HiveServer2相一致。由于使用了Thrift通信协议，它也被称为“Thrift server”。服务器可以通过 Spark 目录中的 sbin/start-thriftserver.sh 启动。这个 脚本接受的参数选项大多与 spark-submit 相同。默认情况下，服务器会在 localhost:10000 上进行监听，我们可以通过环境变量(HIVE_SERVER2_THRIFT_PORT 和 HIVE_SERVER2_THRIFT_BIND_HOST)修改这些设置，也可以通过 Hive配置选项(hive. server2.thrift.port 和 hive.server2.thrift.bind.host)来修改。你也可以通过命令行参 数–hiveconf property=value来设置Hive选项。 1234567./sbin/start-thriftserver.sh \--hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \--hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \--master &lt;master-uri&gt;..../bin/beelinebeeline&gt; !connect jdbc:hive2://master01:10000 在 Beeline 客户端中，你可以使用标准的 HiveQL 命令来创建、列举以及查询数据表。 1234567891011121314151617181920212223242526[bigdata@master01 spark-2.1.1-bin-hadoop2.7]$ ./sbin/start-thriftserver.shstarting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /home/bigdata/hadoop/spark-2.1.1-bin-hadoop2.7/logs/spark-bigdata-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-master01.out[bigdata@master01 spark-2.1.1-bin-hadoop2.7]$ ./bin/beelineBeeline version 1.2.1.spark2 by Apache Hivebeeline&gt; !connect jdbc:hive2://master01:10000Connecting to jdbc:hive2://master01:10000Enter username for jdbc:hive2://master01:10000: bigdataEnter password for jdbc:hive2://master01:10000: *******log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.Connected to: Spark SQL (version 2.1.1)Driver: Hive JDBC (version 1.2.1.spark2)Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2://master01:10000&gt; show tables;+-----------+------------+--------------+--+| database | tableName | isTemporary |+-----------+------------+--------------+--+| default | src | false |+-----------+------------+--------------+--+1 row selected (0.726 seconds)0: jdbc:hive2://master01:10000&gt; Spark SQL CLISpark SQL CLI可以很方便的在本地运行Hive元数据服务以及从命令行执行查询任务。需要注意的是，Spark SQL CLI不能与Thrift JDBC服务交互。在Spark目录下执行如下命令启动Spark SQL CLI： 1./bin/spark-sql 配置Hive需要替换 conf/ 下的 hive-site.xml 。 Spark SQL 的运行原理Spark SQL 运行架构Spark SQL对SQL语句的处理和关系型数据库类似，即词法/语法解析、绑定、优化、执行。Spark SQL会先将SQL语句解析成一棵树，然后使用规则(Rule)对Tree进行绑定、优化等处理过程。Spark SQL由Core、Catalyst、Hive、Hive-ThriftServer四部分构成：Core: 负责处理数据的输入和输出，如获取数据，查询结果输出成DataFrame等Catalyst: 负责处理整个查询过程，包括解析、绑定、优化等Hive: 负责对Hive数据进行处理Hive-ThriftServer: 主要用于对hive的访问 TreeNode逻辑计划、表达式等都可以用tree来表示，它只是在内存中维护，并不会进行磁盘的持久化，分析器和优化器对树的修改只是替换已有节点。TreeNode有2个直接子类，QueryPlan和Expression。QueryPlam下又有LogicalPlan和SparkPlan. Expression是表达式体系，不需要执行引擎计算而是可以直接处理或者计算的节点，包括投影操作，操作符运算等。 Rule &amp; RuleExecutorRule就是指对逻辑计划要应用的规则，以到达绑定和优化。他的实现类就是RuleExecutor。优化器和分析器都需要继承RuleExecutor。每一个子类中都会定义Batch、Once、FixPoint. 其中每一个Batch代表着一套规则，Once表示对树进行一次操作，FixPoint表示对树进行多次的迭代操作。RuleExecutor内部提供一个Seq[Batch]属性，里面定义的是RuleExecutor的处理逻辑，具体的处理逻辑由具体的Rule子类实现。 整个流程架构图： Spark SQL 运行原理 使用SessionCatalog保存元数据：在解析SQL语句之前，会创建SparkSession，或者如果是2.0之前的版本初始化SQLContext，SparkSession只是封装了SparkContext和SQLContext的创建而已。会把元数据保存在SessionCatalog中，涉及到表名，字段名称和字段类型。创建临时表或者视图，其实就会往SessionCatalog注册。 解析SQL,使用ANTLR生成未绑定的逻辑计划：当调用SparkSession的sql或者SQLContext的sql方法，我们以2.0为准，就会使用SparkSqlParser进行解析SQL. 使用的ANTLR进行词法解析和语法解析。它分为2个步骤来生成Unresolved LogicalPlan：(1)词法分析：Lexical Analysis，负责将token分组成符号类； (2)构建一个分析树或者语法树AST。 使用分析器Analyzer绑定逻辑计划：在该阶段，Analyzer会使用Analyzer Rules，并结合SessionCatalog，对未绑定的逻辑计划进行解析，生成已绑定的逻辑计划。 使用优化器Optimizer优化逻辑计划：优化器也是会定义一套Rules，利用这些Rule对逻辑计划和Exepression进行迭代处理，从而使得树的节点进行和并和优化。 使用SparkPlanner生成物理计划：SparkSpanner使用Planning Strategies，对优化后的逻辑计划进行转换，生成可以执行的物理计划SparkPlan. 使用QueryExecution执行物理计划：此时调用SparkPlan的execute方法，底层其实已经再触发JOB了，然后返回RDD. End.]]></content>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Core 应用解析]]></title>
    <url>%2F2017%2F08%2F04%2FSpark-Core-Analysis%2F</url>
    <content type="text"><![CDATA[本文主要解析部分 Spark Core 的技术点。 RDD 概念RDD 为什么会产生RDD是Spark的基石，是实现Spark数据处理的核心抽象。那么RDD为什么会产生呢？ Hadoop的MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后写入物理存储设备。数据更多面临的是一次性处理。 MR的这种方式对数据领域两种常见的操作不是很高效。第一种是迭代式的算法。比如机器学习中ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR这种模式不太合适，即使多MR串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。 MR中的迭代： Spark中的迭代： 我们需要一个效率非常快，且能够支持迭代计算和有效数据共享的模型，Spark应运而生。RDD是基于工作集的工作模式，更多的是面向工作流。 但是无论是MR还是RDD都应该具有类似位置感知、容错和负载均衡等特性。 RDD 概述什么是 RDDRDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。在 Spark 中，对数据的所有操作不外乎创建 RDD、转化已有RDD 以及调用 RDD 操作进行求值。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。RDD 可以包含 Python、Java、Scala 中任意类型的对象， 甚至可以包含用户自定义的对象。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。RDD支持两种操作:转化操作和行动操作。RDD 的转化操作是返回一个新的 RDD的操作，比如 map()和 filter()，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作。比如 count() 和 first()。Spark采用惰性计算模式，RDD只有第一次在一个行动操作中用到时，才会真正计算。Spark可以优化整个计算过程。默认情况下，Spark 的 RDD 会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个 RDD，可以使用 RDD.persist() 让 Spark 把这个 RDD 缓存下来。 RDD 的属性 一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。 一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。 RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。 一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 RDD是一个应用层面的逻辑概念。一个RDD多个分片。RDD就是一个元数据记录集，记录了RDD内存所有的关系数据。 RDD 弹性 自动进行内存和磁盘数据存储的切换：Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换 基于血统的高效容错机制：在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。 Task如果失败会自动进行特定次数的重试：RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。 Stage如果失败会自动进行特定次数的重试：如果Job的某个Stage阶段计算失败，框架也会自动进行任务的重新计算，默认次数也是4次。 Checkpoint和Persist可主动或被动触发：RDD可以通过Persist持久化将RDD缓存到内存或者磁盘，当再次用到该RDD时直接读取就行。也可以将RDD进行检查点，检查点会将数据存储在HDFS中，该RDD的所有父RD都会被移除。 数据调度弹性：Spark把这个JOB执行模型抽象为通用的有向无环图DAG，可以将多Stage的任务串联或并行执行，调度引擎自动处理Stage的失败以及Task的失败。 数据分片的高度弹性：可以根据业务的特征，动态调整数据分片的个数，提升整体的应用执行效率。 RDD全称叫做弹性分布式数据集(Resilient Distributed Datasets)一种分布式的内存抽象，表示一个只读的记录分区的集合，它只能通过其他RDD转换而创建，为此，RDD支持丰富的转换操作(如map, join, filter, groupBy等过这种转换操作，新的RDD则包含了如何从其他RDDs衍生所必需的信息，所以说RDDs之间是有依赖关系的。基于RDDs之间的依赖，RDDs会形成一个有向无环图DAG，G描述了整个流式计算的流程，实际执行的时候，RDD是通过血缘关系(Lineage)成的，即使出现数据分区丢失，也可以通过血缘关系重建分区，总结起来，基于RDD的流式计算任务可描述为：从稳定的物理存储(如分布式文件系统)中加载记录，记入由一组确定性操作构成的DAG，然后写回稳定存储。另外RDD还可以将数据集缓存到内存中，使得在多个操作之间可以重用数据集，基于这个特点可以很方便地构建应用(图计算、机器学习等)或者交互式数据分析应用。可以说Spark最初也就是实现RDD的一个分布式系统，后面通过不断发展壮大成为现在较为完善的大数据生态系单来讲，Spark-RDD的关系类似于Hadoop-MapReduce关系。 RDD 特点RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。 分区RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。 只读如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。 由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。 RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。 依赖RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是一一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。 通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，如下图所示，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。 缓存如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。 CheckPoint虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。 给定一个RDD我们至少可以知道如下几点信息：1、分区数以及分区方式；2、由父RDDs衍生而来的相关依赖信息；3、计算每个分区的数据，计算步骤为：1）如果被缓存，则从缓存中取的分区的数据；2）如果被checkpoint，则从checkpoint处恢复数据；3）根据血缘关系计算分区的数据。 RDD 编程编程模型在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。 那么这其中的 Dirver、SparkContext、Executor、Master、Worker分别是什么？ 创建 RDD在Spark中创建RDD的创建方式大概可以分为三种：（1）、从集合中创建RDD；（2）、从外部存储创建RDD；（3）、从其他RDD创建。 由一个已经存在的Scala集合创建，集合并行化。val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))；而从集合中创建RDD，Spark主要提供了两种函数：parallelize和makeRDD。我们可以先看看这两个函数的声明： 123456789def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] 我们可以从上面看出makeRDD有两种实现，而且第一个makeRDD函数接收的参数和parallelize完全一致。其实第一种makeRDD函数实现是依赖了parallelize函数的实现，来看看Spark中是怎么实现这个makeRDD函数的： 12345def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; parallelize(seq, numSlices)&#125; 我们可以看出，这个makeRDD函数完全和parallelize函数一致。但是我们得看看第二种makeRDD函数函数实现了，它接收的参数类型是Seq[(T, Seq[String])]，Spark文档的说明是：Distribute a local Scala collection to form an RDD, with one or more location preferences (hostnames of Spark nodes) for each object. Create a new partition for each collection item.原来，这个函数还为数据提供了位置信息，来看看我们怎么使用： 12345678910111213141516171819202122scala&gt; val guigu1= sc.parallelize(List(1,2,3))guigu1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:21 scala&gt; val guigu2 = sc.makeRDD(List(1,2,3))guigu2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[11] at makeRDD at &lt;console&gt;:21 scala&gt; val seq = List((1, List("slave01")), | (2, List("slave02")))seq: List[(Int, List[String])] = List((1,List(slave01)), (2,List(slave02))) scala&gt; val guigu3 = sc.makeRDD(seq)guigu3: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at makeRDD at &lt;console&gt;:23 scala&gt; guigu3.preferredLocations(guigu3.partitions(1))res26: Seq[String] = List(slave02) scala&gt; guigu3.preferredLocations(guigu3.partitions(0))res27: Seq[String] = List(slave01) scala&gt; guigu1.preferredLocations(guigu1.partitions(0))res28: Seq[String] = List() 我们可以看到，makeRDD函数有两种实现，第一种实现其实完全和parallelize一致；而第二种实现可以为数据提供位置信息，而除此之外的实现和parallelize函数也是一致的，如下： 123456789101112def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())&#125; def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope &#123; assertNotStopped() val indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap new ParallelCollectionRDD[T](this, seq.map(_._1), seq.size, indexToPrefs)&#125; 都是返回ParallelCollectionRDD，而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。 由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等。 12scala&gt; val rdd1 = sc.textFile("hdfs://slave1:9000/txtFile")rdd1: org.apache.spark.rdd.RDD[String] = hdfs://slave1:9000/RELEASE MapPartitionsRDD[4] at textFile at &lt;console&gt;:24 RDD 编程RDD一般分为数值RDD和键值对RDD，本章不进行具体区分，先统一来看，下一章会对键值对RDD做专门说明。 TransformationRDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。常用的 Transformation： map(func): 返回通过函数func传递源的每个元素形成的新分布式数据集。 1234567891011scala&gt; var source = sc.parallelize(1 to 10)source: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at &lt;console&gt;:24scala&gt; source.collect()res7: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; val mapadd = source.map(_ * 2)mapadd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at map at &lt;console&gt;:26scala&gt; mapadd.collect()res8: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18, 20) filter(func): 返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成。 1234567891011scala&gt; var sourceFilter = sc.parallelize(Array("xiaoming","xiaojiang","xiaohe","dazhi"))sourceFilter: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24scala&gt; val filter = sourceFilter.filter(_.contains("xiao"))filter: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[11] at filter at &lt;console&gt;:26scala&gt; sourceFilter.collect()res9: Array[String] = Array(xiaoming, xiaojiang, xiaohe, dazhi)scala&gt; filter.collect()res10: Array[String] = Array(xiaoming, xiaojiang, xiaohe) flatMap(func): 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）。 1234567891011scala&gt; val sourceFlat = sc.parallelize(1 to 5)sourceFlat: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:24scala&gt; sourceFlat.collect()res11: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt; val flatMap = sourceFlat.flatMap(1 to _)flatMap: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[13] at flatMap at &lt;console&gt;:26scala&gt; flatMap.collect()res12: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5) mapPartitions(func): 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。 123456789101112131415161718192021222324252627scala&gt; val rdd = sc.parallelize(List(("kpop","female"),("zorro","male"),("mobin","male"),("lucy","female")))rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[16] at parallelize at &lt;console&gt;:24scala&gt; :paste// Entering paste mode (ctrl-D to finish)def partitionsFun(iter : Iterator[(String,String)]) : Iterator[String] = &#123; var woman = List[String]() while (iter.hasNext)&#123; val next = iter.next() next match &#123; case (_,"female") =&gt; woman = next._1 :: woman case _ =&gt; &#125; &#125; woman.iterator&#125;// Exiting paste mode, now interpreting.partitionsFun: (iter: Iterator[(String, String)])Iterator[String]scala&gt; val result = rdd.mapPartitions(partitionsFun)result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[17] at mapPartitions at &lt;console&gt;:28scala&gt; result.collect()res13: Array[String] = Array(kpop, lucy) mapPartitionsWithIndex(func): 类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]。 123456789101112131415161718192021222324252627scala&gt; val rdd = sc.parallelize(List(("kpop","female"),("zorro","male"),("mobin","male"),("lucy","female")))rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[18] at parallelize at &lt;console&gt;:24scala&gt; :paste// Entering paste mode (ctrl-D to finish)def partitionsFun(index : Int, iter : Iterator[(String,String)]) : Iterator[String] = &#123; var woman = List[String]() while (iter.hasNext)&#123; val next = iter.next() next match &#123; case (_,"female") =&gt; woman = "["+index+"]"+next._1 :: woman case _ =&gt; &#125; &#125; woman.iterator&#125;// Exiting paste mode, now interpreting.partitionsFun: (index: Int, iter: Iterator[(String, String)])Iterator[String]scala&gt; val result = rdd.mapPartitionsWithIndex(partitionsFun)result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[19] at mapPartitionsWithIndex at &lt;console&gt;:28scala&gt; result.collect()res14: Array[String] = Array([0]kpop, [3]lucy) sample(withReplacement, fraction, seed): 以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。例子从RDD中随机且有放回的抽出50%的数据，随机种子值为3（即可能以1 2 3的其中一个起始值）。 1234567891011121314151617scala&gt; val rdd = sc.parallelize(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[20] at parallelize at &lt;console&gt;:24scala&gt; rdd.collect()res15: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; var sample1 = rdd.sample(true,0.4,2)sample1: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[21] at sample at &lt;console&gt;:26scala&gt; sample1.collect()res16: Array[Int] = Array(1, 2, 2, 7, 7, 8, 9)scala&gt; var sample2 = rdd.sample(false,0.2,3)sample2: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[22] at sample at &lt;console&gt;:26scala&gt; sample2.collect()res17: Array[Int] = Array(1, 9) union(otherDataset): 对源RDD和参数RDD求并集后返回一个新的RDD。 1234567891011scala&gt; val rdd1 = sc.parallelize(1 to 5)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[23] at parallelize at &lt;console&gt;:24scala&gt; val rdd2 = sc.parallelize(5 to 10)rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[24] at parallelize at &lt;console&gt;:24scala&gt; val rdd3 = rdd1.union(rdd2)rdd3: org.apache.spark.rdd.RDD[Int] = UnionRDD[25] at union at &lt;console&gt;:28scala&gt; rdd3.collect()res18: Array[Int] = Array(1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10) intersection(otherDataset): 对源RDD和参数RDD求交集后返回一个新的RDD。 1234567891011scala&gt; val rdd1 = sc.parallelize(1 to 7)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[26] at parallelize at &lt;console&gt;:24scala&gt; val rdd2 = sc.parallelize(5 to 10)rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[27] at parallelize at &lt;console&gt;:24scala&gt; val rdd3 = rdd1.intersection(rdd2)rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[33] at intersection at &lt;console&gt;:28scala&gt; rdd3.collect()res19: Array[Int] = Array(5, 6, 7) distinct([numTasks])): 对源RDD进行去重后返回一个新的RDD. 默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。 1234567891011121314scala&gt; val distinctRdd = sc.parallelize(List(1,2,1,5,2,9,6,1))distinctRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at &lt;console&gt;:24scala&gt; val unionRDD = distinctRdd.distinct()unionRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[37] at distinct at &lt;console&gt;:26scala&gt; unionRDD.collect()res20: Array[Int] = Array(1, 9, 5, 6, 2)scala&gt; val unionRDD = distinctRdd.distinct(2)unionRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[40] at distinct at &lt;console&gt;:26scala&gt; unionRDD.collect()res21: Array[Int] = Array(6, 2, 1, 9, 5) partitionBy(partitioner): 对RDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区， 否则会生成ShuffleRDD。 1234567891011scala&gt; val rdd = sc.parallelize(Array((1,"aaa"),(2,"bbb"),(3,"ccc"),(4,"ddd")),4)rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[44] at parallelize at &lt;console&gt;:24scala&gt; rdd.partitions.sizeres24: Int = 4scala&gt; var rdd2 = rdd.partitionBy(new org.apache.spark.HashPartitioner(2))rdd2: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[45] at partitionBy at &lt;console&gt;:26scala&gt; rdd2.partitions.sizeres25: Int = 2 reduceByKey(func, [numTasks]): 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。 12345678scala&gt; val rdd = sc.parallelize(List(("female",1),("male",5),("female",5),("male",2)))rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[46] at parallelize at &lt;console&gt;:24scala&gt; val reduce = rdd.reduceByKey((x,y) =&gt; x+y)reduce: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[47] at reduceByKey at &lt;console&gt;:26scala&gt; reduce.collect()res29: Array[(String, Int)] = Array((female,6), (male,7)) groupByKey(): groupByKey也是对每个key进行操作，但只生成一个sequence。 1234567891011121314151617181920212223scala&gt; val words = Array("one", "two", "two", "three", "three", "three")words: Array[String] = Array(one, two, two, three, three, three)scala&gt; val wordPairsRDD = sc.parallelize(words).map(word =&gt; (word, 1))wordPairsRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[4] at map at &lt;console&gt;:26scala&gt; val group = wordPairsRDD.groupByKey()group: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[5] at groupByKey at &lt;console&gt;:28scala&gt; group.collect()res1: Array[(String, Iterable[Int])] = Array((two,CompactBuffer(1, 1)), (one,CompactBuffer(1)), (three,CompactBuffer(1, 1, 1)))scala&gt; group.map(t =&gt; (t._1, t._2.sum))res2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[6] at map at &lt;console&gt;:31scala&gt; res2.collect()res3: Array[(String, Int)] = Array((two,2), (one,1), (three,3))scala&gt; val map = group.map(t =&gt; (t._1, t._2.sum))map: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[7] at map at &lt;console&gt;:30scala&gt; map.collect()res4: Array[(String, Int)] = Array((two,2), (one,1), (three,3)) combineByKey[C]( createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): 对相同K，把V合并成一个集合. createCombiner: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。如果这是一个新的元素,combineByKey() 会使用一个叫作 createCombiner() 的函数来创建 那个键对应的累加器的初始值。 mergeValue: 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并。 mergeCombiners: 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。 123456789101112131415161718scala&gt; val scores = Array(("Fred", 88), ("Fred", 95), ("Fred", 91), ("Wilma", 93), ("Wilma", 95), ("Wilma", 98))scores: Array[(String, Int)] = Array((Fred,88), (Fred,95), (Fred,91), (Wilma,93), (Wilma,95), (Wilma,98))scala&gt; val input = sc.parallelize(scores)input: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[52] at parallelize at &lt;console&gt;:26scala&gt; val combine = input.combineByKey( | (v)=&gt;(v,1), | (acc:(Int,Int),v)=&gt;(acc._1+v,acc._2+1), | (acc1:(Int,Int),acc2:(Int,Int))=&gt;(acc1._1+acc2._1,acc1._2+acc2._2))combine: org.apache.spark.rdd.RDD[(String, (Int, Int))] = ShuffledRDD[53] at combineByKey at &lt;console&gt;:28scala&gt; val result = combine.map&#123; | case (key,value) =&gt; (key,value._1/value._2.toDouble)&#125;result: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[54] at map at &lt;console&gt;:30scala&gt; result.collect()res33: Array[(String, Double)] = Array((Wilma,95.33333333333333), (Fred,91.33333333333333)) aggregateByKey(zeroValue:U,[partitioner: Partitioner]) (seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U): 在kv对的RDD中，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。seqOp函数用于在每一个分区中用初始值逐步迭代value，combOp函数用于合并每个分区中的结果。 1234567891011121314151617scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:24scala&gt; val agg = rdd.aggregateByKey(0)(math.max(_,_),_+_)agg: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[13] at aggregateByKey at &lt;console&gt;:26scala&gt; agg.collect()res7: Array[(Int, Int)] = Array((3,8), (1,7), (2,3))scala&gt; agg.partitions.sizeres8: Int = 3scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),1)rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24scala&gt; val agg = rdd.aggregateByKey(0)(math.max(_,_),_+_).collect()agg: Array[(Int, Int)] = Array((1,4), (3,8), (2,3)) foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]: aggregateByKey的简化操作，seqop和combop相同。 12345678scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[91] at parallelize at &lt;console&gt;:24scala&gt; val agg = rdd.foldByKey(0)(_+_)agg: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[92] at foldByKey at &lt;console&gt;:26scala&gt; agg.collect()res61: Array[(Int, Int)] = Array((3,14), (1,9), (2,3)) sortByKey([ascending], [numTasks]): 在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD。 12345678scala&gt; val rdd = sc.parallelize(Array((3,"aa"),(6,"cc"),(2,"bb"),(1,"dd")))rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24scala&gt; rdd.sortByKey(true).collect()res9: Array[(Int, String)] = Array((1,dd), (2,bb), (3,aa), (6,cc))scala&gt; rdd.sortByKey(false).collect()res10: Array[(Int, String)] = Array((6,cc), (3,aa), (2,bb), (1,dd)) sortBy(func,[ascending], [numTasks]): 与sortByKey类似，但是更灵活,可以用func先对数据进行处理，按照处理后的数据比较结果排序。 12345678scala&gt; val rdd = sc.parallelize(List(1,2,3,4))rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[21] at parallelize at &lt;console&gt;:24scala&gt; rdd.sortBy(x =&gt; x).collect()res11: Array[Int] = Array(1, 2, 3, 4)scala&gt; rdd.sortBy(x =&gt; x%3).collect()res12: Array[Int] = Array(3, 4, 1, 2) join(otherDataset, [numTasks]): 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD。 12345678scala&gt; val rdd = sc.parallelize(Array((1,"a"),(2,"b"),(3,"c")))rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[32] at parallelize at &lt;console&gt;:24scala&gt; val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6)))rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:24scala&gt; rdd.join(rdd1).collect()res13: Array[(Int, (String, Int))] = Array((1,(a,4)), (2,(b,5)), (3,(c,6))) cogroup(otherDataset, [numTasks]): 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable\&lt;V>,Iterable\&lt;W>))类型的RDD。 1234567891011121314151617181920scala&gt; val rdd = sc.parallelize(Array((1,"a"),(2,"b"),(3,"c")))rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[37] at parallelize at &lt;console&gt;:24scala&gt; val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6)))rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[38] at parallelize at &lt;console&gt;:24scala&gt; rdd.cogroup(rdd1).collect()res14: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((1,(CompactBuffer(a),CompactBuffer(4))), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6))))scala&gt; val rdd2 = sc.parallelize(Array((4,4),(2,5),(3,6)))rdd2: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[41] at parallelize at &lt;console&gt;:24scala&gt; rdd.cogroup(rdd2).collect()res15: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((4,(CompactBuffer(),CompactBuffer(4))), (1,(CompactBuffer(a),CompactBuffer())), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6))))scala&gt; val rdd3 = sc.parallelize(Array((1,"a"),(1,"d"),(2,"b"),(3,"c")))rdd3: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[44] at parallelize at &lt;console&gt;:24scala&gt; rdd3.cogroup(rdd2).collect()res16: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((4,(CompactBuffer(),CompactBuffer(4))), (1,(CompactBuffer(d, a),CompactBuffer())), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6)))) cartesian(otherDataset): 笛卡尔积。 12345678scala&gt; val rdd1 = sc.parallelize(1 to 3)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[47] at parallelize at &lt;console&gt;:24scala&gt; val rdd2 = sc.parallelize(2 to 5)rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[48] at parallelize at &lt;console&gt;:24scala&gt; rdd1.cartesian(rdd2).collect()res17: Array[(Int, Int)] = Array((1,2), (1,3), (1,4), (1,5), (2,2), (2,3), (2,4), (2,5), (3,2), (3,3), (3,4), (3,5)) pipe(command, [envVars]): 对于每个分区，都执行一个perl或者shell脚本，返回输出的RDD。 1234567891011scala&gt; val rdd = sc.parallelize(List("hi","Hello","how","are","you"),1)rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[50] at parallelize at &lt;console&gt;:24scala&gt; rdd.pipe("/home/spark_shell/pipe.sh").collect()res18: Array[String] = Array(AA, &gt;&gt;&gt;hi, &gt;&gt;&gt;Hello, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you)scala&gt; val rdd = sc.parallelize(List("hi","Hello","how","are","you"),2)rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[52] at parallelize at &lt;console&gt;:24scala&gt; rdd.pipe("/home/spark_shell/pipe.sh").collect()res19: Array[String] = Array(AA, &gt;&gt;&gt;hi, &gt;&gt;&gt;Hello, AA, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you) 123456pipe.sh:#!/bin/shecho "AA"while read LINE; do echo "&gt;&gt;&gt;"$&#123;LINE&#125;done coalesce(numPartitions): 缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。 1234567891011scala&gt; val rdd = sc.parallelize(1 to 16,4)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[54] at parallelize at &lt;console&gt;:24scala&gt; rdd.partitions.sizeres20: Int = 4scala&gt; val coalesceRDD = rdd.coalesce(3)coalesceRDD: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[55] at coalesce at &lt;console&gt;:26scala&gt; coalesceRDD.partitions.sizeres21: Int = 3 repartition(numPartitions): 根据分区数，从新通过网络随机洗牌所有数据。 1234567891011121314151617scala&gt; val rdd = sc.parallelize(1 to 16,4)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[56] at parallelize at &lt;console&gt;:24scala&gt; rdd.partitions.sizeres22: Int = 4scala&gt; val rerdd = rdd.repartition(2)rerdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[60] at repartition at &lt;console&gt;:26scala&gt; rerdd.partitions.sizeres23: Int = 2scala&gt; val rerdd = rdd.repartition(4)rerdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[64] at repartition at &lt;console&gt;:26scala&gt; rerdd.partitions.sizeres24: Int = 4 glom(): 将每一个分区形成一个数组，形成新的RDD类型时RDD[Array[T]]。 12345scala&gt; val rdd = sc.parallelize(1 to 16,4)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[65] at parallelize at &lt;console&gt;:24scala&gt; rdd.glom().collect()res25: Array[Array[Int]] = Array(Array(1, 2, 3, 4), Array(5, 6, 7, 8), Array(9, 10, 11, 12), Array(13, 14, 15, 16)) mapValues(func): 针对于(K,V)形式的类型只对V进行操作。 12345scala&gt; val rdd3 = sc.parallelize(Array((1,"a"),(1,"d"),(2,"b"),(3,"c")))rdd3: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[67] at parallelize at &lt;console&gt;:24scala&gt; rdd3.mapValues(_+"|||").collect()res26: Array[(Int, String)] = Array((1,a|||), (1,d|||), (2,b|||), (3,c|||)) subtract(other:RDD): 计算差的一种函数去除两个RDD中相同的元素，不同的RDD将保留下来 。 12345678scala&gt; val rdd = sc.parallelize(3 to 8)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[70] at parallelize at &lt;console&gt;:24scala&gt; val rdd1 = sc.parallelize(1 to 5)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[71] at parallelize at &lt;console&gt;:24scala&gt; rdd.subtract(rdd1).collect()res27: Array[Int] = Array(8, 6, 7) Action常用的 Action 如下： reduce(func): 通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的。 1234567891011scala&gt; val rdd1 = sc.makeRDD(1 to 10,2)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[85] at makeRDD at &lt;console&gt;:24scala&gt; rdd1.reduce(_+_)res50: Int = 55scala&gt; val rdd2 = sc.makeRDD(Array(("a",1),("a",3),("c",3),("d",5)))rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[86] at makeRDD at &lt;console&gt;:24scala&gt; rdd2.reduce((x,y)=&gt;(x._1 + y._1,x._2 + y._2))res51: (String, Int) = (adca,12) collect(): 在驱动程序中，以数组的形式返回数据集的所有元素。 12345scala&gt; val rdd = sc.makeRDD(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at makeRDD at &lt;console&gt;:24scala&gt; rdd.collect()res7: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) count(): 返回RDD的元素个数。 12345scala&gt; val rdd = sc.makeRDD(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[18] at makeRDD at &lt;console&gt;:24scala&gt; rdd.count()res8: Long = 10 first(): 返回RDD的第一个元素（类似于take(1)）。 12345scala&gt; val rdd = sc.makeRDD(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[19] at makeRDD at &lt;console&gt;:24scala&gt; rdd.first()res9: Int = 1 take(n): 返回一个由数据集的前n个元素组成的数组。 12345scala&gt; val rdd = sc.makeRDD(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[20] at makeRDD at &lt;console&gt;:24scala&gt; rdd.take(7)res10: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7) takeSample(withReplacement,num, [seed]): 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子。 12345scala&gt; val rdd = sc.makeRDD(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[21] at makeRDD at &lt;console&gt;:24scala&gt; rdd.takeSample(false, 2)res11: Array[Int] = Array(6, 2) takeOrdered(n): 返回前几个的排序。 12345678scala&gt; val rdd1 = sc.makeRDD(Seq(10, 5, 3, 19, 4))rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[23] at makeRDD at &lt;console&gt;:24scala&gt; rdd1.top(2)res12: Array[Int] = Array(19, 10)scala&gt; rdd1.takeOrdered(2)res13: Array[Int] = Array(3, 4) aggregate (zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U): aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。 1234567891011121314scala&gt; var rdd1 = sc.makeRDD(1 to 10,2)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[88] at makeRDD at &lt;console&gt;:24scala&gt; rdd1.aggregate(1)( | &#123;(x : Int,y : Int) =&gt; x + y&#125;, | &#123;(a : Int,b : Int) =&gt; a + b&#125; | )res56: Int = 58scala&gt; rdd1.aggregate(1)( | &#123;(x : Int,y : Int) =&gt; x * y&#125;, | &#123;(a : Int,b : Int) =&gt; a + b&#125; | )res57: Int = 30361 fold(num)(func): 折叠操作，aggregate的简化操作，seqop和combop一样。 1234567891011scala&gt; var rdd1 = sc.makeRDD(1 to 4,2)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[90] at makeRDD at &lt;console&gt;:24scala&gt; rdd1.aggregate(1)( | &#123;(x : Int,y : Int) =&gt; x + y&#125;, | &#123;(a : Int,b : Int) =&gt; a + b&#125; | )res59: Int = 13scala&gt; rdd1.fold(1)(_+_)res60: Int = 13 countByKey(): 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。 12345scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[95] at parallelize at &lt;console&gt;:24scala&gt; rdd.countByKey()res63: scala.collection.Map[Int,Long] = Map(3 -&gt; 2, 1 -&gt; 3, 2 -&gt; 1) foreach(func): 在数据集的每一个元素上，运行函数func进行更新。 123456789scala&gt; var rdd1 = sc.makeRDD(1 to 5)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[30] at makeRDD at &lt;console&gt;:24scala&gt; rdd1.collect().foreach(println)12345 saveAsTextFile(path): 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本。 saveAsSequenceFile(path): 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。 saveAsObjectFile(path): 用于将RDD中的元素序列化成对象，存储到文件中。 数值 RDD 的统计操作Spark对包含数值数据的RDD提供了一些描述性的统计操作。Spark的数值操作是通过流式算法实现的，允许以每次一个元素的方式构建出模型。这些统计数据都会在调用stats()时通过一次遍历数据计算出来，并以StatsCounter对象返回。 12345678scala&gt; var rdd1 = sc.makeRDD(1 to 100)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[42] at makeRDD at &lt;console&gt;:32scala&gt; rdd1.sum()res34: Double = 5050.0scala&gt; rdd1.max()res35: Int = 100 向RDD操作传递函数注意Spark的大部分转化操作和一部分行动操作，都需要依赖用户传递的函数来计算。在Scala中，我们可以把定义的内联函数、方法的引用或静态方法传递给Spark，就像Scala的其他函数式API一样。我们还要考虑其他一些细节，比如所传递的函数及其引用的数据需要是可序列化的(实现了Java的Serializable接口)。传递一个对象的方法或者字段时，会包含对整个对象的引用。 123456789101112131415161718class SearchFunctions(val query: String) extends java.io.Serializable&#123; def isMatch(s: String): Boolean = &#123; s.contains(query) &#125; def getMatchesFunctionReference(rdd: org.apache.spark.rdd.RDD[String]): org.apache.spark.rdd.RDD[String] = &#123; // 问题:"isMatch"表示"this.isMatch"，因此我们要传递整个"this" rdd.filter(isMatch) &#125; def getMatchesFieldReference(rdd: org.apache.spark.rdd.RDD[String]): org.apache.spark.rdd.RDD[String] = &#123; // 问题:"query"表示"this.query"，因此我们要传递整个"this" rdd.filter(x =&gt; x.contains(query)) &#125; def getMatchesNoReference(rdd: org.apache.spark.rdd.RDD[String]): org.apache.spark.rdd.RDD[String] = &#123; // 安全:只把我们需要的字段拿出来放入局部变量中 val query_ = this.query rdd.filter(x =&gt; x.contains(query_)) &#125; &#125; 如果在Scala中出现了NotSerializableException，通常问题就在于我们传递了一个不可序列化的类中的函数或字段。 在不同 RDD 类型间转换有些函数只能用于特定类型的RDD，比如mean()和variance()只能用在数值RDD上，而join()只能用在键值对RDD上。在Scala和Java中，这些函数都没有定义在标准的RDD类中，所以要访问这些附加功能，必须要确保获得了正确的专用RDD类。在Scala中，将RDD转为有特定函数的RDD(比如在RDD[Double]上进行数值操作)是由隐式转换来自动处理的。 RDD 持久化RDD 的缓存Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存个数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。如果一个有持久化数据的节点发生故障，Spark会在需要用到缓存的数据时重算丢失的数据分区。如果希望节点故障的情况不会拖累我们的执行速度，也可以把数据备份到多个节点上。 RDD 的缓存方式RDD通过persist方法或cache方法可以将前面的计算结果缓存，默认情况下persist()会把数据以序列化的形式缓存在JVM的堆空间中。但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。 通过查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在objectStorageLevel中定义的。 12345678910111213141516171819202122232425262728293031scala&gt; val rdd = sc.makeRDD(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[19] at makeRDD at &lt;console&gt;:25scala&gt; val nocache = rdd.map(_.toString+"["+System.currentTimeMillis+"]")nocache: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[20] at map at &lt;console&gt;:27scala&gt; val cache = rdd.map(_.toString+"["+System.currentTimeMillis+"]")cache: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[21] at map at &lt;console&gt;:27scala&gt; cache.cacheres24: cache.type = MapPartitionsRDD[21] at map at &lt;console&gt;:27scala&gt; nocache.collectres25: Array[String] = Array(1[1505479375155], 2[1505479374674], 3[1505479374674], 4[1505479375153], 5[1505479375153], 6[1505479374675], 7[1505479375154], 8[1505479375154], 9[1505479374676], 10[1505479374676])scala&gt; nocache.collectres26: Array[String] = Array(1[1505479375679], 2[1505479376157], 3[1505479376157], 4[1505479375680], 5[1505479375680], 6[1505479376159], 7[1505479375680], 8[1505479375680], 9[1505479376158], 10[1505479376158])scala&gt; nocache.collectres27: Array[String] = Array(1[1505479376743], 2[1505479377218], 3[1505479377218], 4[1505479376745], 5[1505479376745], 6[1505479377219], 7[1505479376747], 8[1505479376747], 9[1505479377218], 10[1505479377218])scala&gt; cache.collectres28: Array[String] = Array(1[1505479382745], 2[1505479382253], 3[1505479382253], 4[1505479382748], 5[1505479382748], 6[1505479382257], 7[1505479382747], 8[1505479382747], 9[1505479382253], 10[1505479382253])scala&gt; cache.collectres29: Array[String] = Array(1[1505479382745], 2[1505479382253], 3[1505479382253], 4[1505479382748], 5[1505479382748], 6[1505479382257], 7[1505479382747], 8[1505479382747], 9[1505479382253], 10[1505479382253])scala&gt; cache.collectres30: Array[String] = Array(1[1505479382745], 2[1505479382253], 3[1505479382253], 4[1505479382748], 5[1505479382748], 6[1505479382257], 7[1505479382747], 8[1505479382747], 9[1505479382253], 10[1505479382253])scala&gt; cache.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY) 在存储级别的末尾加上“_2”来把持久化数据存为两份 缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。 注意：使用Tachyon可以实现堆外缓存。 RDD CheckPoint 机制Spark中对于数据的保存除了持久化操作之外，还提供了一种CheckPoint的机制，CheckPoint（本质是通过将RDD写入Disk做CheckPoint）是为了通过lineage做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做CheckPoint容错，如果之后有节点出现问题而丢失分区，从做CheckPoint的RDD开始重做Lineage，就会减少开销。CheckPoint通过将数据写入到HDFS文件系统实现了RDD的CheckPoint功能。cache和CheckPoint是有显著区别的，缓存把RDD计算出来然后放在内存中，但是RDD的依赖链（相当于数据库中的redo日志），也不能丢掉，当某个点某个executor宕了，上面cache的RDD就会丢掉，需要通过依赖链重放计算出来，不同的是，CheckPoint是把RDD保存在HDFS中，是多副本可靠存储，所以依赖链就可以丢掉了，就斩断了依赖链，是通过复制实现的高容错。如果存在以下场景，则比较适合使用CheckPoint机制： DAG中的Lineage过长，如果重算，则开销太大（如在PageRank中）。 在宽依赖上做Checkpoint获得的收益更大。 为当前RDD设置 CheckPoint。该函数将会创建一个二进制的文件，并存储到checkpoint目录中，该目录是用SparkContext.setCheckpointDir()设置的。在checkpoint的过程中，该RDD的所有依赖于父RDD中的信息将全部被移出。对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。 1234567891011121314151617181920212223242526272829303132333435363738scala&gt; val data = sc.parallelize(1 to 100 , 5)data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:12 scala&gt; sc.setCheckpointDir("hdfs://slave1:9000/checkpoint") scala&gt; data.checkpoint scala&gt; data.countscala&gt; val ch1 = sc.parallelize(1 to 2)ch1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:25scala&gt; val ch2 = ch1.map(_.toString+"["+System.currentTimeMillis+"]")ch2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[36] at map at &lt;console&gt;:27scala&gt; val ch3 = ch1.map(_.toString+"["+System.currentTimeMillis+"]")ch3: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[37] at map at &lt;console&gt;:27scala&gt; ch3.checkpointscala&gt; ch2.collectres62: Array[String] = Array(1[1505480940726], 2[1505480940243])scala&gt; ch2.collectres63: Array[String] = Array(1[1505480941957], 2[1505480941480])scala&gt; ch2.collectres64: Array[String] = Array(1[1505480942736], 2[1505480942257])scala&gt; ch3.collectres65: Array[String] = Array(1[1505480949080], 2[1505480948603])scala&gt; ch3.collectres66: Array[String] = Array(1[1505480948683], 2[1505480949161])scala&gt; ch3.collectres67: Array[String] = Array(1[1505480948683], 2[1505480949161]) CheckPoint 写流程RDD checkpoint 过程中会经过以下几个状态：[ Initialized → marked for checkpointing → checkpointing in progress → checkpointed ]，转换流程如下： data.CheckPoint这个函数调用中，设置的目录中，所有依赖的RDD都会被删除，函数必须在job运行之前调用执行，强烈建议RDD缓存在内存中（又提到一次，千万要注意哟），否则保存到文件的时候需要从头计算。初始化RDD的CheckPointData变量为ReliableRDDCheckpointData。这时候标记为Initialized状态。 在所有jobaction的时候，runJob方法中都会调用rdd.doCheckpoint,这个会向前递归调用所有的依赖的RDD，看看需不需要CheckPoint。需要需要CheckPoint，然后调用CheckPointData.get.CheckPoint()，里面标记状态为CheckpointingInProgress，里面调用具体实现类的ReliableRDDCheckpointData的doCheckpoint方法。 doCheckpoint-&gt;writeRDDToCheckpointDirectory，注意这里会把job再运行一次，如果已经cache了，就可以直接使用缓存中的RDD了，就不需要重头计算一遍了（怎么又说了一遍），这时候直接把RDD，输出到hdfs，每个分区一个文件，会先写到一个临时文件，如果全部输出完，进行rename，如果输出失败，就回滚delete。 标记状态为Checkpointed，markCheckpointed方法中清除所有的依赖，怎么清除依赖的呢，就是把RDD变量的强引用设置为null，垃圾回收了，会触发ContextCleaner里面监听清除实际BlockManager缓存中的数据。 CheckPoint 读流程如果一个RDD我们已经CheckPoint了那么是什么时候用呢，CheckPoint将RDD持久化到HDFS或本地文件夹，如果不被手动remove掉，是一直存在的，也就是说可以被下一个driverprogram使用。比如sparkstreaming挂掉了，重启后就可以使用之前CheckPoint的数据进行recover,当然在同一个driverprogram也可以使用。我们讲下在同一个driverprogram中是怎么使用CheckPoint数据的。如果一个RDD被CheckPoint了，当这个RDD上有action操作时候，或者回溯的这个RDD的时候，触发这个RDD进行计算，里面判断是否CheckPoint过，对分区和依赖的处理都是使用的RDD内部的CheckPointRDD变量。具体细节如下：如果一个RDD被CheckPoint了，那么这个RDD中对分区和依赖的处理都是使用的RDD内部的CheckPointRDD变量，具体实现是ReliableCheckpointRDD类型。这个是在CheckPoint写流程中创建的。依赖和获取分区方法中先判断是否已经CheckPoint，如果已经CheckPoint了，就斩断依赖，使用ReliableCheckpointRDD，来处理依赖和获取分区。如果没有，才往前回溯依赖。依赖就是没有依赖，因为已经斩断了依赖，获取分区数据就是读取CheckPoint到HDFS目录中不同分区保存下来的文件。 RDD 的依赖关系RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。 窄依赖：窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用。总结：窄依赖我们形象的比喻为独生子女。 宽依赖：宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition，会引起shuffle。总结：宽依赖我们形象的比喻为超生。 Lineage：RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。 1234567891011121314151617181920212223scala&gt; val text = sc.textFile("README.md")text: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:24scala&gt; val words = text.flatMap(_.split)split splitAtscala&gt; val words = text.flatMap(_.split(" "))words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at &lt;console&gt;:26scala&gt; words.map((_,1))res0: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at &lt;console&gt;:29scala&gt; res0.reduceByKeyreduceByKey reduceByKeyLocallyscala&gt; res0.reduceByKey(_+_)res1: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at &lt;console&gt;:31scala&gt; res1.dependenciesres2: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@6cfe48a4)scala&gt; res0.dependenciesres3: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.OneToOneDependency@6c9e24c4) DAG 的生成DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。 RDD 相关概念关系 输入可能以多个文件的形式存储在HDFS上，每个File都包含了很多块，称为Block。当Spark读取这些文件作为输入时，会根据具体数据格式对应的InputFormat进行解析，一般是将若干个Block合并成一个输入分片，称为InputSplit，注意InputSplit不能跨越文件。随后将为这些输入分片生成具体的Task。InputSplit与Task是一一对应的关系。随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。 每个节点可以起一个或多个Executor。 每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。 每个Task执行的结果就是生成了目标RDD的一个partiton。 注意: 这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。而Task被执行的并发度 = Executor数目 * 每个Executor核数。至于partition的数目： 对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。 在Map阶段partition数目保持不变。 在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。 RDD在计算的时候，每个分区都会起一个task，所以rdd的分区数目决定了总的的task数目。申请的计算节点（Executor）数目和每个计算节点核数，决定了你同一时刻可以并行执行的task。比如的RDD有100个分区，那么计算的时候就会生成100个task，你的资源配置为10个计算节点，每个两2个核，同一时刻可以并行的task数目为20，计算这个RDD就需要5个轮次。如果计算资源不变，你有101个task的话，就需要6个轮次，在最后一轮中，只有一个task在执行，其余核都在空转。如果资源不变，你的RDD只有2个分区，那么同一时刻只有2个task运行，其余18个核空转，造成资源浪费。这就是在spark调优中，增大RDD分区数目，增大任务并行度的做法。 键值对 RDD键值对RDD是Spark中许多操作所需要的常见数据类型。本章做特别讲解。除了在基础RDD类中定义的操作之外，Spark为包含键值对类型的RDD提供了一些专有的操作在PairRDDFunctions专门进行了定义。这些RDD被称为pairRDD。有很多种方式创建pairRDD，在输入输出章节会讲解。一般如果从一个普通的RDD转为pairRDD时，可以调用map()函数来实现，传递的函数需要返回键值对。 1val pairs = lines.map(x =&gt; (x.split(" ")(0), x)) Pair RDD 的 Transformation 操作转化操作上一章进行了练习，这一章会重点讲解。针对一个Pair RDD的转化操作： 聚合操作当数据集以键值对形式组织的时候，聚合具有相同键的元素进行一些统计是很常见的操作。之前讲解过基础RDD上的fold()、combine()、reduce()等行动操作，pairRDD上则有相应的针对键的转化操作。Spark有一组类似的操作，可以组合具有相同键的值。这些操作返回RDD，因此它们是转化操作而不是行动操作。reduceByKey()与reduce()相当类似;它们都接收一个函数，并使用该函数对值进行合并。reduceByKey()会为数据集中的每个键进行并行的归约操作，每个归约操作会将键相同的值合并起来。因为数据集中可能有大量的键，所以reduceByKey()没有被实现为向用户程序返回一个值的行动操作。实际上，它会返回一个由各键和对应键归约出来的结果值组成的新的RDD。foldByKey()则与fold()相当类似;它们都使用一个与RDD和合并函数中的数据类型相同的零值作为初始值。与fold()一样，foldByKey()操作所使用的合并函数对零值与另一个元素进行合并，结果仍为该元素。求均值操作：版本一 1input.mapValues(x =&gt; (x, 1)).reduceByKey((x, y) =&gt; (x._1 + y._1, x._2 + y._2)).map&#123; case (key, value) =&gt; (key, value._1 / value._2.toFloat) &#125; combineByKey()是最为常用的基于键进行聚合的函数。大多数基于键聚合的函数都是用它实现的。和aggregate()一样，combineByKey()可以让用户返回与输入数据的类型不同的返回值。要理解combineByKey()，要先理解它在处理数据时是如何处理每个元素的。由于combineByKey()会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。如果这是一个新的元素，combineByKey()会使用一个叫作createCombiner()的函数来创建那个键对应的累加器的初始值。需要注意的是，这一过程会在每个分区中第一次出现各个键时发生，而不是在整个RDD中第一次出现一个键时发生。如果这是一个在处理当前分区之前已经遇到的键，它会使用mergeValue()方法将该键的累加器对应的当前值与这个新的值进行合并。由于每个分区都是独立处理的，因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器，就需要使用用户提供的mergeCombiners()方法将各个分区的结果进行合并。 求均值操作：版本二 1234567val result = input.combineByKey( (v) =&gt; (v, 1), (acc: (Int, Int), v) =&gt; (acc._1 + v, acc._2 + 1), (acc1: (Int, Int), acc2: (Int, Int)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)).map&#123; case (key, value) =&gt; (key, value._1 / value._2.toFloat) &#125;result.collectAsMap().map(println(_)) 数据分组如果数据已经以预期的方式提取了键，groupByKey()就会使用RDD中的键来对数据进行分组。对于一个由类型K的键和类型V的值组成的RDD，所得到的结果RDD类型会是[K,Iterable[V]]。groupBy()可以用于未成对的数据上，也可以根据除键相同以外的条件进行分组。它可以接收一个函数，对源RDD中的每个元素使用该函数，将返回结果作为键再进行分组。多个RDD分组，可以使用cogroup函数，cogroup()的函数对多个共享同一个键的RDD进行分组。对两个键的类型均为K而值的类型分别为V和W的RDD进行cogroup()时，得到的结果RDD类型为[(K,(Iterable[V],Iterable[W]))]。如果其中的一个RDD对于另一个RDD中存在的某个键没有对应的记录，那么对应的迭代器则为空。cogroup()提供了为多个RDD进行数据分组的方法。 连接连接主要用于多个PairRDD的操作，连接方式多种多样:右外连接、左外连接、交叉连接以及内连接。普通的join操作符表示内连接2。只有在两个pairRDD中都存在的键才叫输出。当一个输入对应的某个键有多个值时，生成的pairRDD会包括来自两个输入RDD的每一组相对应的记录。leftOuterJoin()产生的pairRDD中，源RDD的每一个键都有对应的记录。每个键相应的值是由一个源RDD中的值与一个包含第二个RDD的值的Option(在Java中为Optional)对象组成的二元组。rightOuterJoin()几乎与leftOuterJoin()完全一样，只不过预期结果中的键必须出现在第二个RDD中，而二元组中的可缺失的部分则来自于源RDD而非第二个RDD。 数据排序sortByKey()函数接收一个叫作ascending的参数，表示我们是否想要让结果按升序排序(默认值为true)。 Pair RDD 的 Action 操作 Pair RDD 的数据分区Spark目前支持Hash分区和Range分区，用户也可以自定义分区，Hash分区为当前的默认分区，Spark中分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle过程属于哪个分区和Reduce的个数。注意： 只有Key-Value类型的RDD才有分区的，非Key-Value类型的RDD分区的值是None. 每个RDD的分区ID范围：0~numPartitions-1，决定这个值是属于那个分区的。 获取 RDD 的分区方式可以通过使用RDD的partitioner属性来获取RDD的分区方式。它会返回一个scala.Option对象，通过get方法获取其中的值。 1234567891011scala&gt; val pairs = sc.parallelize(List((1, 1), (2, 2), (3, 3)))pairs: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:24scala&gt; pairs.partitionerres26: Option[org.apache.spark.Partitioner] = Nonescala&gt; val partitioned = pairs.partitionBy(new org.apache.spark.HashPartitioner(2))partitioned: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[34] at partitionBy at &lt;console&gt;:26scala&gt; partitioned.partitionerres27: Option[org.apache.spark.Partitioner] = Some(org.apache.spark.HashPartitioner@2) Hash 分区方式HashPartitioner分区的原理：对于给定的key，计算其hashCode，并除于分区的个数取余，如果余数小于0，则用余数+分区的个数，最后返回的值就是这个key所属的分区ID。 12345678910111213141516171819scala&gt; val nopar = sc.parallelize(List((1,3),(1,2),(2,4),(2,3),(3,6),(3,8)),8)nopar: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24scala&gt; nopar.partitionerres20: Option[org.apache.spark.Partitioner] = Nonescala&gt;nopar.mapPartitionsWithIndex((index,iter)=&gt;&#123; Iterator(index.toString+" : "+iter.mkString("|")) &#125;).collectres0: Array[String] = Array("0 : ", 1 : (1,3), 2 : (1,2), 3 : (2,4), "4 : ", 5 : (2,3), 6 : (3,6), 7 : (3,8)) scala&gt; val hashpar = nopar.partitionBy(new org.apache.spark.HashPartitioner(7))hashpar: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[12] at partitionBy at &lt;console&gt;:26scala&gt; hashpar.countres18: Long = 6scala&gt; hashpar.partitionerres21: Option[org.apache.spark.Partitioner] = Some(org.apache.spark.HashPartitioner@7)scala&gt; hashpar.mapPartitions(iter =&gt; Iterator(iter.length)).collect()res19: Array[Int] = Array(0, 3, 1, 2, 0, 0, 0) Range 分区方式HashPartitioner分区弊端：可能导致每个分区中数据量的不均匀，极端情况下会导致某些分区拥有RDD的全部数据。RangePartitioner分区优势：尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大；但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。RangePartitioner作用：将一定范围内的数映射到某一个分区内，在实现中，分界的算法尤为重要。用到了水塘抽样算法。 自定义分区方式要实现自定义的分区器，你需要继承org.apache.spark.Partitioner类并实现下面三个方法。 numPartitions:Int:返回创建出来的分区数。 getPartition(key:Any):Int:返回给定键的分区编号(0到numPartitions-1)。 equals():Java判断相等性的标准方法。这个方法的实现非常重要，Spark需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样Spark才可以判断两个RDD的分区方式是否相同。 假设我们需要将相同后缀的数据写入相同的文件，我们通过将相同后缀的数据分区到相同的分区并保存输出来实现。 1234567891011121314151617181920212223242526package com.moqi.sparkimport org.apache.spark.&#123;Partitioner, SparkConf, SparkContext&#125;class CustomerPartitioner(numParts:Int) extends Partitioner &#123; //覆盖分区数 override def numPartitions: Int = numParts //覆盖分区号获取函数 override def getPartition(key: Any): Int = &#123; val ckey: String = key.toString ckey.substring(ckey.length-1).toInt%numParts &#125;&#125;object CustomerPartitioner &#123; def main(args: Array[String]) &#123; val conf=new SparkConf().setAppName("partitioner") val sc=new SparkContext(conf) val data=sc.parallelize(List("aa.2","bb.2","cc.3","dd.3","ee.5")) data.map((_,1)).partitionBy(new CustomerPartitioner(5)).keys.saveAsTextFile("hdfs://slave1:9000/partitioner") &#125;&#125; 1234567891011121314151617181920212223242526272829303132scala&gt; val data=sc.parallelize(List("aa.2","bb.2","cc.3","dd.3","ee.5").zipWithIndex,2)data: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[7] at parallelize at &lt;console&gt;:27scala&gt; data.collectres4: Array[(String, Int)] = Array((aa.2,0), (bb.2,1), (cc.3,2), (dd.3,3), (ee.5,4))scala&gt; data.mapPartitionsWithIndex((index,iter)=&gt;Iterator(index.toString +" : "+ iter.mkString("|"))).collectres5: Array[String] = Array(0 : (aa.2,0)|(bb.2,1), 1 : (cc.3,2)|(dd.3,3)|(ee.5,4))scala&gt; :paste// Entering paste mode (ctrl-D to finish)class CustomerPartitioner(numParts:Int) extends org.apache.spark.Partitioner&#123; //覆盖分区数 override def numPartitions: Int = numParts //覆盖分区号获取函数 override def getPartition(key: Any): Int = &#123; val ckey: String = key.toString ckey.substring(ckey.length-1).toInt%numParts &#125;&#125;// Exiting paste mode, now interpreting.defined class CustomerPartitionerscala&gt; data.partitionBy(new CustomerPartitioner(4))res7: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[9] at partitionBy at &lt;console&gt;:31scala&gt; res7.mapPartitionsWithIndex((index,iter)=&gt;Iterator(index.toString +" : "+ iter.mkString("|"))).collectres8: Array[String] = Array("0 : ", 1 : (ee.5,4), 2 : (aa.2,0)|(bb.2,1), 3 : (cc.3,2)|(dd.3,3)) 使用自定义的Partitioner是很容易的:只要把它传给partitionBy()方法即可。Spark中有许多依赖于数据混洗的方法，比如join()和groupByKey()，它们也可以接收一个可选的Partitioner对象来控制输出数据的分区方式。 分区 Shuffle 优化在分布式程序中，通信的代价是很大的，因此控制数据分布以获得最少的网络传输可以极大地提升整体性能。Spark中所有的键值对RDD都可以进行分区。系统会根据一个针对键的函数对元素进行分组。主要有哈希分区和范围分区，当然用户也可以自定义分区函数。通过分区可以有效提升程序性能。如下例子：分析这样一个应用，它在内存中保存着一张很大的用户信息表——也就是一个由(UserID,UserInfo)对组成的RDD，其中UserInfo包含一个该用户所订阅的主题的列表。该应用会周期性地将这张表与一个小文件进行组合，这个小文件中存着过去五分钟内发生的事件——其实就是一个由(UserID,LinkInfo)对组成的表，存放着过去五分钟内某网站各用户的访问情况。例如，我们可能需要对用户访问其未订阅主题的页面的情况进行统计。 1234567891011121314151617// 初始化代码，从 HDFS 的一个 Hadoop SequenceFile 读取用户信息// userData 中的元素会根据它们被读取时的来源，即 HDFS 块所在的节点来分布// Spark 此时无法获知某个特定的 User ID 对应的记录位于那个节点上val sc = new SparkContext(...)val userData = sc.sequenceFile[UserId, UserInfo]("hdfs://...").persist()// 周期性调用函数来处理过去五分钟产生的事件日志// 假设这是一个包含(UserID, LinkInfo)键值对的 SequenceFiledef processNewLogs(logFileName: String) &#123; val events = sc.sequenceFile[UserID, LinkInfo](logFileName) val joined = userData.join(events) // RDD of (UserID, (UserInfo, LinkInfo)) pairs val offTopicVisits = joined.filter &#123; // Expand the tuple into tis components case (userId, (userInfo, linkInfo)) =&gt; !userInfo.topics.contains(linkInfo.topic) &#125;.count() println("Number of visits to non-subscribed topics: " + offTopicVisits)&#125; 这段代码可以正确运行，但是不够高效。这是因为在每次调用processNewLogs()时都会用到join()操作，而我们对数据集是如何分区的却一无所知。默认情况下，连接操作会将两个数据集中的所有键的哈希值都求出来，将该哈希值相同的记录通过网络传到同一台机器上，然后在那台机器上对所有键相同的记录进行连接操作。因为userData表比每五分钟出现的访问日志表events要大得多，所以要浪费时间做很多额外工作:在每次调用时都对userData表进行哈希值计算和跨节点数据混洗，降低了程序的执行效率。优化方法： 1234val sc = new SparkContext(...)val userData = sc.sequenceFile[UserID, UserInfo]("hdfs://...") .partitionBy(new HashPartitioner(100)) // 构造 100 个分区 .persist() 我们在构建userData时调用了partitionBy()，Spark就知道了该RDD是根据键的哈希值来分区的，这样在调用join()时，Spark就会利用到这一点。具体来说，当调用userData.join(events)时，Spark只会对events进行数据混洗操作，将events中特定UserID的记录发送到userData的对应分区所在的那台机器上。这样，需要通过网络传输的数据就大大减少了，程序运行速度也可以显著提升了。 基于分区进行操作基于分区对数据进行操作可以让我们避免为每个数据元素进行重复的配置工作。诸如打开数据库连接或创建随机数生成器等操作，都是我们应当尽量避免为每个元素都配置一次的工作。Spark提供基于分区的mapPartition和foreachPartition，让你的部分代码只对RDD的每个分区运行一次，这样可以帮助降低这些操作的代价。 从分区中获益的操作能够从数据分区中获得性能提升的操作有cogroup()、groupWith()、join()、leftOuterJoin()、rightOuterJoin()、groupByKey()、reduceByKey()、combineByKey()以及lookup()等。 数据读取与保存主要方式文本文件当我们将一个文本文件读取为RDD时，输入的每一行都会成为RDD的一个元素。也可以将多个完整的文本文件一次性读取为一个pairRDD，其中键是文件名，值是文件内容。val input = sc.textFile(“./README.md”). 如果传递目录，则将目录下的所有文件读取作为RDD。文件路径支持通配符。通过wholeTextFiles()对于大量的小文件读取效率比较高，大文件效果没有那么高。Spark通过saveAsTextFile()进行文本文件的输出，该方法接收一个路径，并将RDD中的内容都输入到路径对应的文件中。Spark将传入的路径作为目录对待，会在那个目录下输出多个文件。这样，Spark就可以从多个节点上并行输出了。result.saveAsTextFile(outputFile). 123456789scala&gt; sc.textFile("./README.md")res6: org.apache.spark.rdd.RDD[String] = ./README.md MapPartitionsRDD[7] at textFile at &lt;console&gt;:25scala&gt; val readme = sc.textFile("./README.md")readme: org.apache.spark.rdd.RDD[String] = ./README.md MapPartitionsRDD[9] at textFile at &lt;console&gt;:24scala&gt; readme.collect()res7: Array[String] = Array(# Apache Spark, "", Spark is a fast and general cluster...scala&gt; readme.saveAsTextFile("hdfs://slave1:9000/test") JSON 文件如果JSON文件中每一行就是一个JSON记录，那么可以通过将JSON文件当做文本文件来读取，然后利用相关的JSON库对每一条数据进行JSON解析。 12345678910111213141516171819scala&gt; import org.json4s._ import org.json4s._scala&gt; import org.json4s.jackson.JsonMethods._ import org.json4s.jackson.JsonMethods._scala&gt; import org.json4s.jackson.Serialization import org.json4s.jackson.Serializationscala&gt; var result = sc.textFile("examples/src/main/resources/people.json")result: org.apache.spark.rdd.RDD[String] = examples/src/main/resources/people.json MapPartitionsRDD[7] at textFile at &lt;console&gt;:47scala&gt; implicit val formats = Serialization.formats(ShortTypeHints(List())) formats: org.json4s.Formats&#123;val dateFormat: org.json4s.DateFormat; val typeHints: org.json4s.TypeHints&#125; = org.json4s.Serialization$$anon$1@61f2c1dascala&gt; result.collect().foreach(x =&gt; &#123;var c = parse(x).extract[Person];println(c.name + "," + c.age)&#125;) Michael,30Andy,30Justin,19 如果JSON数据是跨行的，那么只能读入整个文件，然后对每个文件进行解析。JSON数据的输出主要是通过在输出之前将由结构化数据组成的RDD转为字符串RDD，然后使用Spark的文本文件API写出去。说白了还是以文本文件的形式存，只是文本的格式已经在程序中转换为JSON。 CSV 文件读取CSV/TSV数据和读取JSON数据相似，都需要先把文件当作普通文本文件来读取数据，然后通过将每一行进行解析实现对CSV的读取。CSV/TSV数据的输出也是需要将结构化RDD通过相关的库转换成字符串RDD，然后使用Spark的文本文件API写出去。 SequenceFile 文件SequenceFile文件是Hadoop用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。Spark有专门用来读取SequenceFile的接口。在SparkContext中，可以调用sequenceFile[keyClass, valueClass](path). 12345678910scala&gt; val data=sc.parallelize(List((2,"aa"),(3,"bb"),(4,"cc"),(5,"dd"),(6,"ee")))data: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[16] at parallelize at &lt;console&gt;:24scala&gt; data.saveAsSequenceFile("hdfs://slave1:9000/sequdata")scala&gt; val sdata = sc.sequenceFile[Int,String]("hdfs://slave1:9000/sdata/p*")sdata: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[19] at sequenceFile at &lt;console&gt;:24scala&gt; sdata.collect()res14: Array[(Int, String)] = Array((2,aa), (3,bb), (4,cc), (5,dd), (6,ee)) 可以直接调用saveAsSequenceFile(path)保存你的PairRDD，它会帮你写出数据。需要键和值能够自动转为Writable类型。 对象文件对象文件是将对象序列化后保存的文件，采用Java的序列化机制。可以通过objectFilek,v函数接收一个路径，读取对象文件，返回对应的RDD，也可以通过调用saveAsObjectFile()实现对对象文件的输出。因为是序列化所以要指定类型。 123456789101112scala&gt; val data=sc.parallelize(List((2,"aa"),(3,"bb"),(4,"cc"),(5,"dd"),(6,"ee")))data: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[20] at parallelize at &lt;console&gt;:24scala&gt; data.saveAsObjectFile("hdfs://slave1:9000/objfile")scala&gt; import org.apache.spark.rdd.RDDimport org.apache.spark.rdd.RDDscala&gt; val objrdd:RDD[(Int,String)] = sc.objectFile[(Int,String)]("hdfs://slave1:9000/objfile/p*")objrdd: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[28] at objectFile at &lt;console&gt;:25scala&gt; objrdd.collect()res20: Array[(Int, String)] = Array((2,aa), (3,bb), (4,cc), (5,dd), (6,ee)) Hadoop 文件Spark的整个生态系统与Hadoop是完全兼容的,所以对于Hadoop所支持的文件类型或者数据库类型,Spark也同样支持.另外,由于Hadoop的API有新旧两个版本,所以Spark为了能够兼容Hadoop所有的版本,也提供了两套创建操作接口.对于外部存储创建操作而言,hadoopRDD和newHadoopRDD是最为抽象的两个函数接口,主要包含以下四个参数。 输入格式(InputFormat): 制定数据输入的类型,如TextInputFormat等,新旧两个版本所引用的版本分别是org.apache.hadoop.mapred.InputFormat和org.apache.hadoop.mapreduce.InputFormat(NewInputFormat) 键类型: 指定[K,V]键值对中K的类型 值类型: 指定[K,V]键值对中V的类型 分区值: 指定由外部存储生成的RDD的partition数量的最小值,如果没有指定,系统会使用默认值defaultMinSplits 其他创建操作的API接口都是为了方便最终的Spark程序开发者而设置的,是这两个接口的高效实现版本.例如,对于textFile而言,只有path这个指定文件路径的参数,其他参数在系统内部指定了默认值。 注意: 在Hadoop中以压缩形式存储的数据,不需要指定解压方式就能够进行读取,因为Hadoop本身有一个解压器会根据压缩文件的后缀推断解压算法进行解压。 如果用Spark从Hadoop中读取某种类型的数据不知道怎么读取的时候,上网查找一个使用map-reduce的时候是怎么读取这种这种数据的,然后再将对应的读取方式改写成兼容版本的的hadoopRDD和newAPIHadoopRDD两个类就行了。 读取示例： 1234scala&gt; val data = sc.parallelize(Array((30,"hadoop"), (71,"hive"), (11,"cat")))data: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[47] at parallelize at &lt;console&gt;:35scala&gt; data.saveAsNewAPIHadoopFile("hdfs://slave1:9000/output4/",classOf[LongWritable] ,classOf[Text] ,classOf[org.apache.hadoop.mapreduce.lib.output.TextOutputFormat[LongWritable, Text]]) 对于RDD最后的归宿除了返回为集合和标量，也可以将RDD存储到外部文件系统或者数据库中，Spark系统与Hadoop是完全兼容的，所以MapReduce所支持的读写文件或者数据库类型，Spark也同样支持。另外，由于Hadoop的API有新旧两个版本，所以Spark为了能够兼容Hadoop所有的版本，也提供了两套API。将RDD保存到HDFS中在通常情况下需要关注或者设置五个参数，即文件保存的路径，key值的class类型，Value值的class类型，RDD的输出格式(OutputFormat，如TextOutputFormat/SequenceFileOutputFormat)，以及最后一个相关的参数codec(这个参数表示压缩存储的压缩形式，如DefaultCodec，Gzip，Codec等等)。 兼容旧版API saveAsObjectFile(path: String): Unit saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec]): Unit saveAsTextFile(path: String): Unit saveAsHadoopFile[F &lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unit saveAsHadoopFile[F &lt;: OutputFormat[K, V]](path: String, codec: Class[_ &lt;: CompressionCodec])(implicit fm: ClassTag[F]): Unit saveAsHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[&lt;: OutputFormat[, ]], codec: Class[ &lt;: CompressionCodec]): Unit saveAsHadoopDataset(conf: JobConf): Unit 这里列出的API，前面6个都是saveAsHadoopDataset的简易实现版本，仅仅支持将RDD存储到HDFS中，而saveAsHadoopDataset的参数类型是JobConf，所以其不仅能够将RDD存储到HDFS中，也可以将RDD存储到其他数据库中，如Hbase，MangoDB，Cassandra等。 兼容新版API saveAsNewAPIHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[_ &lt;: OutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration): Unit saveAsNewAPIHadoopFile[F &lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unit saveAsNewAPIHadoopDataset(conf: Configuration): Unit 同样的，前2个API是saveAsNewAPIHadoopDataset的简易实现，只能将RDD存到HDFS中，而saveAsNewAPIHadoopDataset比较灵活.新版的API没有codec的参数，所以要压缩存储文件到HDFS中每需要使用hadoopConfiguration参数，设置对应mapreduce.map.output.compress.codec参数和mapreduce.map.output.compress参数。注意：如果不知道怎么将RDD存储到Hadoop生态的系统中，主要上网搜索一下对应的map-reduce是怎么将数据存储进去的，然后改写成对应的saveAsHadoopDataset或saveAsNewAPIHadoopDataset就可以了。 写入示例： 12345scala&gt; val read = sc.newAPIHadoopFile[LongWritable, Text, org.apache.hadoop.mapreduce.lib.input.TextInputFormat]("hdfs://slave1:9000/output3/part*", classOf[org.apache.hadoop.mapreduce.lib.input.TextInputFormat], classOf[LongWritable], classOf[Text])read: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = hdfs://slave1:9000/output3/part* NewHadoopRDD[48] at newAPIHadoopFile at &lt;console&gt;:35scala&gt; read.map&#123;case (k, v) =&gt; v.toString&#125;.collectres44: Array[String] = Array(30 hadoop, 71 hive, 11 cat) 文件系统Spark支持读写很多种文件系统，像本地文件系统、Amazon S3、HDFS 等。 数据库关系型数据库连接支持通过Java JDBC访问关系型数据库。需要通过Jdbc RDD进行，示例如下:MySQL 读取： 1234567891011121314151617181920def main (args: Array[String] ) &#123; val sparkConf = new SparkConf ().setMaster ("local[2]").setAppName ("JdbcApp") val sc = new SparkContext (sparkConf) val rdd = new org.apache.spark.rdd.JdbcRDD ( sc, () =&gt; &#123; Class.forName ("com.mysql.jdbc.Driver").newInstance() java.sql.DriverManager.getConnection ("jdbc:mysql://slave1:3306/rdd", "root", "hive") &#125;, "select * from rddtable where id &gt;= ? and id &lt;= ?;", 1, 10, 1, r =&gt; (r.getInt(1), r.getString(2))) println (rdd.count () ) rdd.foreach (println (_) ) sc.stop ()&#125; MySQL 写入： 1234567891011121314151617def main(args: Array[String]) &#123; val sparkConf = new SparkConf().setMaster("local[2]").setAppName("HBaseApp") val sc = new SparkContext(sparkConf) val data = sc.parallelize(List("Female", "Male","Female")) data.foreachPartition(insertData)&#125;def insertData(iterator: Iterator[String]): Unit = &#123;Class.forName ("com.mysql.jdbc.Driver").newInstance() val conn = java.sql.DriverManager.getConnection("jdbc:mysql://slave1:3306/rdd", "root", "hive") iterator.foreach(data =&gt; &#123; val ps = conn.prepareStatement("insert into rddtable(name) values (?)") ps.setString(1, data) ps.executeUpdate() &#125;)&#125; JdbcRDD 接收这样几个参数： 首先，要提供一个用于对数据库创建连接的函数。这个函数让每个节点在连接必要的配 置后创建自己读取数据的连接。 接下来，要提供一个可以读取一定范围内数据的查询，以及查询参数中lowerBound和 upperBound 的值。这些参数可以让 Spark 在不同机器上查询不同范围的数据，这样就不 会因尝试在一个节点上读取所有数据而遭遇性能瓶颈。 这个函数的最后一个参数是一个可以将输出结果从转为对操作数据有用的格式的函数。如果这个参数空缺，Spark会自动将每行结果转为一个对象数组。 HBase 数据库由于 org.apache.hadoop.hbase.mapreduce.TableInputFormat 类的实现，Spark 可以通过Hadoop输入格式访问HBase。这个输入格式会返回键值对数据，其中键的类型为org.apache.hadoop.hbase.io.ImmutableBytesWritable，而值的类型为org.apache.hadoop.hbase.client.Result.HBase 读取： 123456789101112131415161718192021222324def main(args: Array[String]) &#123; val sparkConf = new SparkConf().setMaster("local[2]").setAppName("HBaseApp") val sc = new SparkContext(sparkConf) val conf = HBaseConfiguration.create() //HBase中的表名 conf.set(TableInputFormat.INPUT_TABLE, "fruit") val hBaseRDD = sc.newAPIHadoopRDD(conf, classOf[TableInputFormat], classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable], classOf[org.apache.hadoop.hbase.client.Result]) val count = hBaseRDD.count() println("hBaseRDD RDD Count:"+ count) hBaseRDD.cache() hBaseRDD.foreach &#123; case (_, result) =&gt; val key = Bytes.toString(result.getRow) val name = Bytes.toString(result.getValue("info".getBytes, "name".getBytes)) val color = Bytes.toString(result.getValue("info".getBytes, "color".getBytes)) println("Row key:" + key + " Name:" + name + " Color:" + color) &#125; sc.stop()&#125; HBase 写入： 12345678910111213141516171819202122232425262728293031def main(args: Array[String]) &#123; val sparkConf = new SparkConf().setMaster("local[2]").setAppName("HBaseApp") val sc = new SparkContext(sparkConf) val conf = HBaseConfiguration.create() val jobConf = new JobConf(conf) jobConf.setOutputFormat(classOf[TableOutputFormat]) jobConf.set(TableOutputFormat.OUTPUT_TABLE, "fruit_spark") val fruitTable = TableName.valueOf("fruit_spark") val tableDescr = new HTableDescriptor(fruitTable) tableDescr.addFamily(new HColumnDescriptor("info".getBytes)) val admin = new HBaseAdmin(conf) if (admin.tableExists(fruitTable)) &#123; admin.disableTable(fruitTable) admin.deleteTable(fruitTable) &#125; admin.createTable(tableDescr) def convert(triple: (Int, String, Int)) = &#123; val put = new Put(Bytes.toBytes(triple._1)) put.addImmutable(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes(triple._2)) put.addImmutable(Bytes.toBytes("info"), Bytes.toBytes("price"), Bytes.toBytes(triple._3)) (new ImmutableBytesWritable, put) &#125; val initialRDD = sc.parallelize(List((1,"apple",11), (2,"banana",12), (3,"pear",13))) val localData = initialRDD.map(convert) localData.saveAsHadoopDataset(jobConf)&#125; Cassandra 和 ElasticSearch RDD 编程进阶累加器累加器用来对信息进行聚合，通常在向Spark传递函数时，比如使用map()函数或者用filter()传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，更新这些副本的值也不会影响驱动器中的对应变量。如果我们想实现所有分片处理时更新共享变量的功能，那么累加器可以实现我们想要的效果。针对一个输入的日志文件，如果我们想计算文件中所有空行的数量，我们可以编写以下程序： 1234567891011121314151617181920scala&gt; val notice = sc.textFile("./NOTICE")notice: org.apache.spark.rdd.RDD[String] = ./NOTICE MapPartitionsRDD[40] at textFile at &lt;console&gt;:32scala&gt; val blanklines = sc.accumulator(0)warning: there were two deprecation warnings; re-run with -deprecation for detailsblanklines: org.apache.spark.Accumulator[Int] = 0scala&gt; val tmp = notice.flatMap(line =&gt; &#123; | if (line == "") &#123; | blanklines += 1 | &#125; | line.split(" ") | &#125;)tmp: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[41] at flatMap at &lt;console&gt;:36scala&gt; tmp.count()res31: Long = 3213scala&gt; blanklines.valueres32: Int = 171 累加器的用法如下所示。通过在驱动器中调用SparkContext.accumulator(initialValue)方法，创建出存有初始值的累加器。返回值为org.apache.spark.Accumulator[T]对象，其中T是初始值initialValue的类型。Spark闭包里的执行器代码可以使用累加器的+=方法(在Java中是add)增加累加器的值。 驱动器程序可以调用累加器的value属性(在Java中使用value()或setValue())来访问累加器的值。注意：工作节点上的任务不能访问累加器的值。从这些任务的角度来看，累加器是一个只写变量。对于要在行动操作中使用的累加器，Spark只会把每个任务对各累加器的修改应用一次。因此，如果想要一个无论在失败还是重复计算时都绝对可靠的累加器，我们必须把它放在foreach()这样的行动操作中。转化操作中累加器可能会发生不止一次更新。 自定义累加器自定义累加器类型的功能在1.X版本中就已经提供了，但是使用起来比较麻烦，在2.0版本后，累加器的易用性有了较大的改进，而且官方还提供了一个新的抽象类：AccumulatorV2来提供更加友好的自定义类型累加器的实现方式。实现自定义类型累加器需要继承AccumulatorV2并至少覆写下例中出现的方法，下面这个累加器可以用于在程序运行过程中收集一些文本类信息，最终以Set[String]的形式返回。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.moqi.sparkimport org.apache.spark.util.AccumulatorV2import org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.JavaConversions._class LogAccumulator extends org.apache.spark.util.AccumulatorV2[String, java.util.Set[String]] &#123; private val _logArray: java.util.Set[String] = new java.util.HashSet[String]() override def isZero: Boolean = &#123; _logArray.isEmpty &#125; override def reset(): Unit = &#123; _logArray.clear() &#125; override def add(v: String): Unit = &#123; _logArray.add(v) &#125; override def merge(other: org.apache.spark.util.AccumulatorV2[String, java.util.Set[String]]): Unit = &#123; other match &#123; case o: LogAccumulator =&gt; _logArray.addAll(o.value) &#125; &#125; override def value: java.util.Set[String] = &#123; java.util.Collections.unmodifiableSet(_logArray) &#125; override def copy():org.apache.spark.util.AccumulatorV2[String, java.util.Set[String]] = &#123; val newAcc = new LogAccumulator() _logArray.synchronized&#123; newAcc._logArray.addAll(_logArray) &#125; newAcc &#125;&#125;// 过滤掉带字母的object LogAccumulator &#123; def main(args: Array[String]) &#123; val conf=new SparkConf().setAppName("LogAccumulator") val sc=new SparkContext(conf) val accum = new LogAccumulator sc.register(accum, "logAccum") val sum = sc.parallelize(Array("1", "2a", "3", "4b", "5", "6", "7cd", "8", "9"), 2).filter(line =&gt; &#123; val pattern = """^-?(\d+)""" val flag = line.matches(pattern) if (!flag) &#123; accum.add(line) &#125; flag &#125;).map(_.toInt).reduce(_ + _) println("sum: " + sum) for (v &lt;- accum.value) print(v + "") println() sc.stop() &#125;&#125; 广播变量广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，甚至是机器学习算法中的一个很大的特征向量，广播变量用起来都很顺手。传统方式下，Spark会自动把闭包中所有引用到的变量发送到工作节点上。虽然这很方便，但也很低效。原因有二:首先，默认的任务发射机制是专门为小任务进行优化的；其次，事实上你可能会在多个并行操作中使用同一个变量，但是Spark会为每个任务分别发送。 12345scala&gt; val broadcastVar = sc.broadcast(Array(1, 2, 3))broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(35)scala&gt; broadcastVar.valueres33: Array[Int] = Array(1, 2, 3) 使用广播变量的过程如下： 通过对一个类型 T 的对象调用 SparkContext.broadcast 创建出一个 Broadcast[T] 对象。 任何可序列化的类型都可以这么实现。 通过 value 属性访问该对象的值(在 Java 中为 value() 方法)。 变量只会被发到各个节点一次，应作为只读值处理(修改这个值不会影响到别的节点)。 End.]]></content>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM GC 发展历程]]></title>
    <url>%2F2017%2F06%2F16%2FJVM-GC-Development-Path%2F</url>
    <content type="text"><![CDATA[本文主要介绍 JVM GC 的发展。 较早的 From - To 阶段 最早的垃圾收集 From - To 架构的 GC，把整个堆内存分成大小差不多相等的两部分，中间有一个分配的指针（Free Point），对指针设定目标值（比如 From 区域的 80%）时，触发一次 GC。GC 触发时应用进入 Stop-The-World 状态，这时垃圾回收器检查 From 区域有哪些是可以回收的那些不是，将不可以回收的拷贝到 To 区域，其他回收。一次 GC 操作完成的时候完成区域交换（From 转换为 To 区域，To 转换为 From 区域），然后指针分配内存开始从新的 From 区域开始。 这种纯粹的拷贝垃圾回收方法最大的问题在于堆内存里面永远只可以用一半的内存，所以有一半的堆是浪费的。但在当时而言还是比较领先的，比如相对于引用计数的垃圾回收方法。引用计数垃圾回收的问题在于：引用计数在计数的时候需要维持一个锁的消耗，会降低分配内存的速度；另外一个是在循环引用中，这个消耗会更大。 回收分代的思想 将堆内存分代治理建立于这样一个假设之上：代际假设 , 核心论点有两个： 大多数对象很快就会被闲置； 少部分活下来的对象会存在相当长一段时间。 因此，JVM GC 分代治理的核心基础是以下两个：（1）大多数对象都会在年轻代死亡。（2）年老代引用年轻代的对象只占很小的一部分。分代治理会发生不同区域的 GC。在年轻代发生的 GC 称为 “minor GC”，在年老代出现的 GC 称为 “major GC” 或者 “full GC”。 根据代际假设构建的堆内存首先避免了全盘扫描，这个时期的 JVM GC 发展为如上图所示结构，分为年轻代，年老代，永生代。年轻代分为 eden 区与两个 survivor 区，s0 和 s1 实现是最初的 From - To 架构，在这里我们假设 s0 为 From 区，s1 为 To 区。创建的对象首先进入 eden 区，如果发生 minor GC，eden 区中大部分对象被回收，小部分对象拷贝到 To 区，From 也拷贝到 To 区；如果在 eden 中的对象太大不能拷贝到 To 区，则会被直接移动到年老代。每次 minor GC 时 From 和 To 会交换，每交换一次区内的对象年龄会加一，当年龄到达一定值（比如15）（注：这一块在后面的 GC 实现了动态调整）的时候，这些大龄的对象也被移动到了年老代。 但是这里会发生一个问题：如果年老代的对象需要引用年轻代的对象怎么办？为了处理这些情况，年老代中有一种称为”CardTable” (卡表) 的东西，它是一个 512 字节的块。每当年老代中的对象引用年轻代中的对象时，它就会记录在此表中。当发生 minor GC 时，仅搜索该卡表以确定它是否是年轻代 GC 需要回收的对象，而不是检查旧代中的所有对象的引用。 当数据已满时，触发年老代执行 GC 。执行程序因 GC 类型而异，根据JDK 7，有5种GC类型。 Serial GC Parallel GC Parallel Old GC (Parallel Compacting GC) Concurrent Mark &amp; Sweep GC (or “CMS”) Garbage First (G1) GCSerial GC(-XX:+UseSerialGC)，即串行 GC，使用被称为 “mark-sweep-compant” 的算法。 第一步：标记年老代中幸存的对象。（标记 - mark） 第二步：从堆的最前面开始检查，只留下幸存的堆。（扫描 - sweep） 第三步：把对象从最前面开始填充，以便连续堆积对象，并将堆分为包含对象和不包含对象的两部分。（紧凑 - compant） 串行 GC 适用于小内存和少量 CPU 内核的 JVM。Parallel GC(-XX:+UseParallelGC)，即并行 GC，和串行 GC 最大的区别是用多个线程来处理 GC，因此更快，当有足够的内存和 CPU 资源时，此 GC 非常有用，它也被称为”吞吐量 GC“。 Parallel Old GC (-XX:+UseParallelOldGC)，并行旧 GC，自 JDK 5 更新以来开始支持，与并行 GC 相比，唯一的区别是老年代的 GC 算法。它经历了三个步骤：mark – summary – compaction（标记 - 摘要 - 压缩）。”摘要“步骤经历了一些更复杂的步骤。 CMS GC （-XX：+ UseConcMarkSweepGC），从下图中可以看出，CMS GC 相对于前三个复杂得多。第一步 “Initial Mark” (初始标记) 很简单，搜索最接近类加载器的对象中的幸存对象，因此暂停时间很短。在 “Concurrent Mark” (并发标记) 步骤中，跟踪并检查刚刚确认的幸存对象引用的对象。这一步的不同之处在于它在同时处理其他线程的同时继续进行。在 “Remark” (再次标记) 步骤中，将检查在并发标记步骤中新增加或停止引用的对象。最后，在 “Concurrent Sweep” (并发扫描) 步骤中进行垃圾回收，垃圾回收和其他线程同步进行。由于 CMS GC 独特的运行方式，因此 GC 的暂停时间非常短。CMS GC 也称为低延迟 GC。在应用程序的响应时间至关重要时使用。这种 GC 的主要缺点如下：（1）它比其他 GC 类型使用更多的内存和 CPU。（2）默认情况下不提供压缩步骤。因而如果由于许多内存碎片而需要执行压缩任务，那么 GC 需要的静止时间可能会比其他任何 GC 方式都要长。所以，如果在使用 CMS GC 的时候要尤其注意压缩任务执行的频率和持续时间。 G1 GC G1 是 Garbage First 的缩写，最开始的 G1 完全抛弃分代收集的思想，开始于 JDK 1.7 Update 4，可以视为上图中所有的 “E”, “O”, “S” 标志消失，只分成不同的 Region (块)。实际上现在的 G1 GC 也有了分代的逻辑。G1 的实现一直以来都在不断的变化之中。 由前文所知每种 GC 都有出现的历史背景，比如串行 GC 是在内存和 CPU 比较小的情况下出现的；并行 GC 是在吞吐量出现巨大需求的时候出现的；而 CMS GC 则是对低延迟有了更高的需求，尽量拖延 full GC 出现的时间。那么，出现 G1这种形式的垃圾收集器的原因是什么？在 JVM GC 不断发展的过程中，已经出现了自适应的堆，也就是不需要手动调节年轻代与年老代的比例，以及年轻代对象进入年老代的年龄。这些自适应堆对更灵活的堆块产生了强烈的需求：如果将堆空间划分为很多个 Region，G1 可以将某一块指定为各种不同的代（可以是年老代，Eden 或者 Survivor 区），而且各个块在空间上不需要连续的在一起，有一个 List 将它们组织在一起，这样 G1 就很容易调整各个代之间的比例。 为什么 G1 被称为 G1？ G1 会在内部维护一个优先列表，通过一个合理的模型，计算出每个 Region 的收集成本和收益期望并量化，这样每次进行 GC 时，G1 总是会选择最适合的 Region（通常垃圾比较多）进行回收，使 GC 时间满足设置的条件。 G1通过引入 Remembered Set 来避免全堆扫描（前面所说的 CardTable 是其的一种实现）。Remembered Set 用于跟踪对象引用。G1 中每个 Region 都有对应的 Remembered Set 。当 JVM 发现内部的一个引用关系需要更新（对 Reference 类型进行写操作），则立即产生一个 Write Barrier 中断这个写操作，并检查Reference 引用的对象是否处于不同的 Region 之间（用分代的思想，就是新生代和老年代之间的引用）。如果是，则通过 CardTable 通知 G1，G1 根据 CardTable 把相关引用信息记录到被引用对象所属的 Region 的Remembered Set 中，并将 Remembered Set 加入 GC Root 。这样，在 G1 进行根节点枚举时就可以扫描到该对象而不会出现遗漏。 通俗解释第二条：如果一个 Region 的 Reference 越少，JVM 倾向于认为这块 Region 里面活着的对象越少，这个 Region 块是可回收的垃圾块的百分比就越大，这样回收这个 Region 的收益就越大。所以称这种算法为 Garbage First. G1 GC 年轻代的回收 当 JVM 启动时基于启动参数，JVM 要求操作系统分配一个大的连续内存块来托管 JVM 的堆，被划分为 2048 个 Region (块)。 年轻代的块发生 “minor GC”，将活着的对象拷贝到 survivor 块（依然是 From - To 算法）。如果对象过大或者对象的年龄足够，会拷贝到年老代。 结果：图三有新增的 “Recently Copied” 两块。 G1 GC 年老代的回收 初始标记阶段：初始标记的活着的对象在年轻代的垃圾收集上。在日志中，被标记为 GC pause (young) (inital-mark). 并行标记阶段，如果找到空区域（由”X”表示），则在 Remark 阶段立即将它们移除。此外，计算确定活跃度的信息。 Remark 阶段，空区域被移除并回收，现在计算所有区域的区域活跃度。 复制清理阶段，G1 选择具有最低”活跃度“的区域（比如引用其他 Region 最少的区域），那些可以最快收集的区域。这些区域与年轻代 GC 同时收集。这在日志中表示为 [GC pause (mixed)]。G1 实际上将 Stop-The-World 的操作放在一个时间区间，这样对应用性能和稳定性较好。 复制清理阶段后，选择的区域已经被收集并压缩成图中所示的深蓝色区域和深绿色区域。 JDK 8 中 JVM 的调整JDK 8 的 JVM 去掉了永生代(PermGen)，用 Metaspace 来代替。Metaspace 使用系统的内存。 参考https://www.cubrid.org/blog/understanding-java-garbage-collection https://www.oracle.com/technetwork/tutorials/tutorials-1876574.html https://blog.idrsolutions.com/2017/05/g1gc-java-9-garbage-collector-explained-5-minutes/ https://www.sczyh30.com/posts/Java/jvm-gc-hotspot-implements/ https://software.intel.com/en-us/blogs/2014/06/18/part-1-tuning-java-garbage-collection-for-hbase https://blog.csdn.net/elinespace/article/details/78852469]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 日志相关命令整理]]></title>
    <url>%2F2017%2F06%2F03%2FLinux-Check-Log-Commands%2F</url>
    <content type="text"><![CDATA[本文主要介绍开发中一些日志的常用操作。 tailn 是显示行号；相当于 nl 命令；例子如下： tail -100f test.log 实时监控 100 行日志 tail -n 10 test.log 查询日志尾部最后 10 行的日志; tail -n +10 test.log 查询 10 行之后的所有日志; head跟 tail 是相反的，tail 是看后多少行日志；例子如下： head -n 10 test.log 查询日志文件中的头 10 行日志; head -n -10 test.log 查询日志文件除了最后 10 行的其他所有日志; cattac 是倒序查看，是 cat 单词反写；例子如下： cat -n test.log |grep “debug” 查询关键字的日志 vim进入编辑查找：vi(vim) 进入vim编辑模式： vim filename vim +n filename 进入特定行号日志 输入命令“set nu” 显示行号 输入“/关键字”,按enter键查找 查找下一个，按“n”即可 退出：按ESC键后，接着再输入:号时，vi会在屏幕的最下方等待我们输入命令 wq! 保存退出； q! 不保存退出； 切换方向 /关键字 注：正向查找，按n键把光标移动到下一个符合条件的地方 ?关键字 注：反向查找，按shift+n 键，把光标移动到下一个符合条件的 搜索关键字附近的日志 最常用的：cat -n filename |grep “关键字” 其他情况： cat filename | grep -C 5 ‘关键字’ (显示日志里匹配字串那行以及前后5行) cat filename | grep -B 5 ‘关键字’ (显示匹配字串及前5行) cat filename | grep -A 5 ‘关键字’ (显示匹配字串及后5行) 按行号查看 - 过滤出关键字附近的日志 cat -n test.log |grep “debug” 得到关键日志的行号 cat -n test.log |tail -n +92|head -n 20 选择关键字所在的中间一行. 然后查看这个关键字后 20 行的日志: tail -n +92 表示查询第 92 行之后的日志 head -n 20 则表示在前面的查询结果里再查后 20 条记录 根据日期查询日志sed -n ‘/2014-12-17 16:17:20/,/2014-12-17 16:17:36/p’ test.log 特别说明:上面的两个日期必须是日志中打印出来的日志，否则无效； 先 grep ‘2014-12-17 16:17:20’ test.log 来确定日志中是否有该 时间点 日志内容特别多，打印在屏幕上不方便查看 使用 more 和 less 命令，如： cat -n test.log |grep “debug” |more 这样就分页打印了,通过点击空格键翻页 使用 &gt;xxx.txt 将其保存到文件中，到时可以拉下这个文件分析，如：cat -n test.log |grep “debug” &gt; debug.txt 参考https://blog.csdn.net/yangkai_hudong/article/details/47783487 https://www.cnblogs.com/hunt/p/7064886.html https://blog.csdn.net/dingnning/article/details/7189862]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA - Debugger 经验总结]]></title>
    <url>%2F2017%2F06%2F02%2FIDEA-Debugger%2F</url>
    <content type="text"><![CDATA[本文主要介绍 IDEA - Debugger 的一些操作。每个都有场景和操作说明，动态 Gif 图体积较大，请耐心等待。 觉得图比较小的单击查看大图。 分析外部堆栈跟踪把报错信息复制到 Analyze -&gt; Analyze Stacktrace，快速进入程序块。开发中经常可以看到生产环境有错误日志，依照此方法快速将日志导入项目，定位问题。 场景： 操作： 返回到前一个堆栈帧IDEA 可在程序的执行流程中回退到先前的堆栈帧。要求不是最上面入口方法，选择 Drop Frame 后，等于未进入调用的方法。请注意：已经对全局状态进行的更改不会被恢复，只有本地变量会被重置。 强制从当前方法返回在当前堆栈帧中右键单击选择 Force Return 然后根据需要的返回类型输入即可。 抛出一个异常在当前堆栈帧中右键单击选择 Throw Exception 然后手动输入异常即可，比如 new NullPointerException(); 重新加载修改的类一般而言应用于在 Debugger 时发现未调用的方法有需要改动的地方，这时候修改未调用的方法，然后选择 Run -&gt; Reload Changed Classes, 快捷键 Alt + U, 然后 A. 这时候 Debugger 继续进行调用，则执行的调用方法逻辑为重新编译之后。底层逻辑是用到 JVM 的 hotSwap. 分析 Java Stream 操作IDEA Debugger 时可以可视化 Java Stream 进行的操作和对值数据的影响，需要断点停留在 Stream 上点击 Trace Current Stream Chain 按钮。 参考https://www.jetbrains.com/help/idea/analyzing-external-stacktraces.html https://www.jetbrains.com/help/idea/altering-the-program-s-execution-flow.html https://www.jetbrains.com/help/idea/analyze-java-stream-operations.html]]></content>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Start]]></title>
    <url>%2F2017%2F06%2F01%2FStart%2F</url>
    <content type="text"><![CDATA[本文简要讨论了博客起源。 博客搭建于 20170601，儿童节做出的决定。 目前定位于分享技术和个人思考，狭义来讲技术在最近一段时间是开发工具 IntelliJ IDEA 的一些最佳实践，个人思考在最近一段时间是关于一些书的读后感。 目前对 Github + Hexo + NexT 刚开始熟悉，有建议欢迎联系邮箱。 Ernest Hemingway once wrote:”The world is a fine place，and worth fighting for.”I agree with the second part.]]></content>
      <tags>
        <tag>default</tag>
      </tags>
  </entry>
</search>
