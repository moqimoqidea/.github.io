<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spark SQL 应用解析]]></title>
    <url>%2F2017%2F08%2F10%2FSpark-SQL-Analysis%2F</url>
    <content type="text"><![CDATA[本文主要解析部分 Spark SQL 的技术点。 Spark SQL 概述什么是 Spark SQL Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。我们已经学习了Hive，它是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！ Spark SQL 有四大特点：(1).易整合。(2).统一的数据访问方式。(3).兼容 Hive. (4).标准的数据连接。 SparkSQL可以看做是一个转换层，向下对接各种不同的结构化数据源，向上提供不同的数据访问方式。 RDD vs DataFrames vs DataSet 在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。他们和RDD有什么区别呢？首先从版本的产生上来看：RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt; Dataset(Spark1.6)如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。在后期的Spark版本中，DataSet会逐步取代RDD和DataFrame成为唯一的API接口。 RDD RDD是一个懒执行的不可变的可以支持Lambda表达式的并行数据集合。 RDD的最大好处就是简单，API的人性化程度很高。 RDD的劣势是性能限制，它是一个JVM驻内存对象，这也就决定了存在GC的限制和数据增加时Java序列化成本的升高。 DataFrame与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。由于与R和Pandas的DataFrame类似，Spark DataFrame很好地继承了传统单机数据分析的开发体验。 上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待。DataFrame也是懒执行的。性能上比RDD要高，主要有两方面原因： 定制化内存管理：数据以二进制的方式存在于非堆内存，节省了大量空间之外，还摆脱了GC的限制。 优化的执行计划：查询计划通过Spark catalyst optimiser进行优化。 比如下面这个例子： 12users.join(events, users("id") === events("uid")) .filter(events("date") &gt; "2015-01-01") 为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。得到的优化执行计划在转换成物 理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。对于普通开发者而言，查询优化器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。Dataframe的劣势在于在编译期缺少类型安全检查，导致运行时出错。 DataSet 是Dataframe API的一个扩展，是Spark最新的数据抽象。 用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。 Dataset支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。 样例类被用来在Dataset中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称。 Dataframe是Dataset的特列，DataFrame=Dataset[Row] ，所以可以通过as方法将Dataframe转换为Dataset。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息我都用Row来表示。 DataSet是强类型的。比如可以有Dataset[Car]，Dataset[Person]. DataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比。 RDD让我们能够决定怎么做，而DataFrame和DataSet让我们决定做什么，控制的粒度不一样。 三者的共性 RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利。 三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过。 12345678val sparkconf = new SparkConf().setMaster("local").setAppName("test").set("spark.port.maxRetries","1000")val spark = SparkSession.builder().config(sparkconf).getOrCreate()val rdd=spark.sparkContext.parallelize(Seq(("a", 1), ("b", 1), ("a", 1)))// map不运行rdd.map&#123;line=&gt; println("运行") line._1&#125; 三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出。 三者都有partition的概念。 三者有许多共同的函数，如filter，排序等。 在对DataFrame和Dataset进行操作许多操作都需要这个包进行支持。 1import spark.implicits._ DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型。DataFrame: 1234567testDF.map&#123; case Row(col1:String,col2:Int)=&gt; println(col1);println(col2) col1 case _=&gt; "" &#125; Dataset: 12345678case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型 testDS.map&#123; case Coltest(col1:String,col2:Int)=&gt; println(col1);println(col2) col1 case _=&gt; "" &#125; 三者的区别RDD: RDD一般和spark mlib同时使用。 RDD不支持spark sql操作。 DataFrame: 与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值，如 12345testDF.foreach&#123; line =&gt; val col1=line.getAs[String]("col1") val col2=line.getAs[String]("col2")&#125; 每一列的值没法直接访问 DataFrame与Dataset一般不与spark ml同时使用。 DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作，如 12dataDF.createOrReplaceTempView("tmp")spark.sql("select ROW,DATE from tmp where DATE is not null order by DATE").show(100,false) DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然。 123456//保存val saveoptions = Map("header" -&gt; "true", "delimiter" -&gt; "\t", "path" -&gt; "hdfs://master01:9000/test")datawDF.write.format("com.atguigu.spark.csv").mode(SaveMode.Overwrite).options(saveoptions).save()//读取val options = Map("header" -&gt; "true", "delimiter" -&gt; "\t", "path" -&gt; "hdfs://master01:9000/test")val datarDF= spark.read.options(options).format("com.atguigu.spark.csv").load() 利用这样的保存方式，可以方便的获得字段名和列的对应，而且分隔符（delimiter）可以自由指定。 Dataset: Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同。 DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的getAS方法或者共性中的第七条提到的模式匹配拿出特定字段。 而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息。 123456789101112131415case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型/** rdd ("a", 1) ("b", 1) ("a", 1)**/val test: Dataset[Coltest]=rdd.map&#123;line=&gt; Coltest(line._1,line._2) &#125;.toDStest.map&#123; line=&gt; println(line.col1) println(line.col2) &#125; 可以看出，Dataset在需要访问列中的某个字段时是非常方便的，然而，如果要写一些适配性很强的函数时，如果使用Dataset，行的类型又不确定，可能是各种case class，无法实现适配，这时候用DataFrame即Dataset[Row]就能比较好的解决问题。 执行 Spark SQL 查询命令行查询流程打开Spark shell例子：查询大于30岁的用户创建如下JSON文件，注意JSON的格式： 123&#123;"name":"Michael"&#125;&#123;"name":"Andy", "age":30&#125;&#123;"name":"Justin", "age":19&#125; 1234567891011121314151617181920212223scala&gt; val df = spark.read.json("/opt/txt_data/json.txt")df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]scala&gt; df.show+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+scala&gt; df.createOrReplaceTempView("persons")scala&gt; spark.sql("SELECT * FROM persons").show()+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+ IDEA 创建 Spark SQL 程序Maven 依赖： 123456&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 程序如下： 12345678910111213141516171819202122232425262728293031323334353637package com.moqi.sparksqlimport org.apache.spark.sql.SparkSessionimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.slf4j.LoggerFactoryobject HelloWorld &#123; val logger = LoggerFactory.getLogger(HelloWorld.getClass) def main(args: Array[String]) &#123; //创建SparkConf()并设置App名称 val spark = SparkSession .builder() .appName("Spark SQL basic example") .config("spark.some.config.option", "some-value") .getOrCreate() // For implicit conversions like converting RDDs to DataFrames import spark.implicits._ val df = spark.read.json("examples/src/main/resources/people.json") // Displays the content of the DataFrame to stdout df.show() df.filter($"age" &gt; 21).show() df.createOrReplaceTempView("persons") spark.sql("SELECT * FROM persons where age &gt; 21").show() spark.stop() &#125;&#125; Spark SQL 解析新的起点：SparkSession在老的版本中，SparkSQL提供两种SQL查询起始点，一个叫SQLContext，用于Spark自己提供的SQL查询，一个叫HiveContext，用于连接Hive的查询，SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。 12345678910import org.apache.spark.sql.SparkSessionval spark = SparkSession.builder().appName("Spark SQL basic example").config("spark.some.config.option", "some-value").getOrCreate()// For implicit conversions like converting RDDs to DataFramesimport spark.implicits._ SparkSession.builder 用于创建一个SparkSession。import spark.implicits._的引入是用于将DataFrames隐式转换成RDD，使df能够使用RDD中的方法。如果需要Hive支持，则需要以下创建语句： 1234567891011import org.apache.spark.sql.SparkSessionval spark = SparkSession.builder().appName("Spark SQL basic example").config("spark.some.config.option", "some-value").enableHiveSupport().getOrCreate()// For implicit conversions like converting RDDs to DataFramesimport spark.implicits._ 创建 DataFrames在Spark SQL中SparkSession是创建DataFrames和执行SQL的入口，创建DataFrames有三种方式，一种是可以从一个存在的RDD进行转换，还可以从Hive Table进行查询返回，或者通过Spark的数据源进行创建。从Spark数据源进行创建： 12345678910val df = spark.read.json("examples/src/main/resources/people.json") // Displays the content of the DataFrame to stdoutdf.show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+ 从 RDD 进行转换： 12345678910111213141516171819/**Michael, 29Andy, 30Justin, 19**/scala&gt; val peopleRdd = sc.textFile("examples/src/main/resources/people.txt")peopleRdd: org.apache.spark.rdd.RDD[String] = examples/src/main/resources/people.txt MapPartitionsRDD[18] at textFile at &lt;console&gt;:24scala&gt; val peopleDF3 = peopleRdd.map(_.split(",")).map(paras =&gt; (paras(0),paras(1).trim().toInt)).toDF("name","age")peopleDF3: org.apache.spark.sql.DataFrame = [name: string, age: int]scala&gt; peopleDF.show()+-------+---+| name|age|+-------+---+|Michael| 29|| Andy| 30|| Justin| 19|+-------+---+ Hive 部分在后面数据源介绍。 DataFrame 常用操作DSL 风格语法123456789101112131415161718192021222324252627282930313233343536373839404142434445// This import is needed to use the $-notationimport spark.implicits._// Print the schema in a tree formatdf.printSchema()// root// |-- age: long (nullable = true)// |-- name: string (nullable = true)// Select only the "name" columndf.select("name").show()// +-------+// | name|// +-------+// |Michael|// | Andy|// | Justin|// +-------+// Select everybody, but increment the age by 1df.select($"name", $"age" + 1).show()// +-------+---------+// | name|(age + 1)|// +-------+---------+// |Michael| null|// | Andy| 31|// | Justin| 20|// +-------+---------+// Select people older than 21df.filter($"age" &gt; 21).show()// +---+----+// |age|name|// +---+----+// | 30|Andy|// +---+----+// Count people by agedf.groupBy("age").count().show()// +----+-----+// | age|count|// +----+-----+// | 19| 1|// |null| 1|// | 30| 1|// +----+-----+ SQL 风格语法123456789101112131415161718192021222324252627282930313233343536// Register the DataFrame as a SQL temporary viewdf.createOrReplaceTempView("people")val sqlDF = spark.sql("SELECT * FROM people")sqlDF.show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+// Register the DataFrame as a global temporary viewdf.createGlobalTempView("people")// Global temporary view is tied to a system preserved database `global_temp`spark.sql("SELECT * FROM global_temp.people").show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+// Global temporary view is cross-sessionspark.newSession().sql("SELECT * FROM global_temp.people").show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+ 临时表是Session范围内的，Session退出后，表就失效了。如果想应用范围内有效，可以使用全局表。注意使用全局表时需要全路径访问，如：global_temp.people. 创建 DataSetDataset是具有强类型的数据集合，需要提供对应的类型信息。 12345678910111213141516171819202122232425262728// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,// you can use custom classes that implement the Product interfacecase class Person(name: String, age: Long)// Encoders are created for case classesval caseClassDS = Seq(Person("Andy", 32)).toDS()caseClassDS.show()// +----+---+// |name|age|// +----+---+// |Andy| 32|// +----+---+// Encoders for most common types are automatically provided by importing spark.implicits._val primitiveDS = Seq(1, 2, 3).toDS()primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by nameval path = "examples/src/main/resources/people.json"val peopleDS = spark.read.json(path).as[Person]peopleDS.show()// +----+-------+// | age| name|// +----+-------+// |null|Michael|// | 30| Andy|// | 19| Justin|// +----+-------+ DataSet 和 RDD 互操作Spark SQL支持通过两种方式将存在的RDD转换为Dataset，转换的过程中需要让Dataset获取RDD中的Schema信息，主要有两种方式，一种是通过反射来获取RDD中的Schema信息。这种方式适合于列名已知的情况下。第二种是通过编程接口的方式将Schema信息应用于RDD，这种方式可以处理那种在运行时才能知道列的方式。 通过反射获取 SchemaSparkSQL能够自动将包含有case类的RDD转换成DataFrame，case类定义了table的结构，case类属性通过反射变成了表的列名。Case类可以包含诸如Seqs或者Array等复杂的结构。 12345678910111213141516171819202122232425262728293031323334353637383940// For implicit conversions from RDDs to DataFramesimport spark.implicits._// Create an RDD of Person objects from a text file, convert it to a Dataframeval peopleDF = spark.sparkContext.textFile("examples/src/main/resources/people.txt").map(_.split(",")).map(attributes =&gt; Person(attributes(0), attributes(1).trim.toInt)).toDF()// Register the DataFrame as a temporary viewpeopleDF.createOrReplaceTempView("people")// SQL statements can be run by using the sql methods provided by Sparkval teenagersDF = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 19")// The columns of a row in the result can be accessed by field index ROW objectteenagersDF.map(teenager =&gt; "Name: " + teenager(0)).show()// +------------+// | value|// +------------+// |Name: Justin|// +------------+// or by field nameteenagersDF.map(teenager =&gt; "Name: " + teenager.getAs[String]("name")).show()// +------------+// | value|// +------------+// |Name: Justin|// +------------+// No pre-defined encoders for Dataset[Map[K,V]], define explicitlyimplicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]// Primitive types and case classes can be also defined as// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]teenagersDF.map(teenager =&gt; teenager.getValuesMap[Any](List("name", "age"))).collect()// Array(Map("name" -&gt; "Justin", "age" -&gt; 19)) 通过编程设置 Schema如果case类不能够提前定义，可以通过下面三个步骤定义一个DataFrame创建一个多行结构的RDD;创建用StructType来表示的行结构信息。通过SparkSession提供的createDataFrame方法来应用Schema. 1234567891011121314151617181920212223242526272829303132333435363738394041import org.apache.spark.sql.types._// Create an RDDval peopleRDD = spark.sparkContext.textFile("examples/src/main/resources/people.txt")// The schema is encoded in a string,应该是动态通过程序生成的val schemaString = "name age"// Generate the schema based on the string of schema Array[StructFiled]val fields = schemaString.split(" ").map(fieldName =&gt; StructField(fieldName, StringType, nullable = true))// val filed = schemaString.split(" ").map(filename=&gt; filename match&#123; case "name"=&gt; StructField(filename,StringType,nullable = true); case "age"=&gt;StructField(filename, IntegerType,nullable = true)&#125; )val schema = StructType(fields)// Convert records of the RDD (people) to Rowsimport org.apache.spark.sql._val rowRDD = peopleRDD.map(_.split(",")).map(attributes =&gt; Row(attributes(0), attributes(1).trim))// Apply the schema to the RDDval peopleDF = spark.createDataFrame(rowRDD, schema)// Creates a temporary view using the DataFramepeopleDF.createOrReplaceTempView("people")// SQL can be run over a temporary view created using DataFramesval results = spark.sql("SELECT name FROM people")// The results of SQL queries are DataFrames and support all the normal RDD operations// The columns of a row in the result can be accessed by field index or by field nameresults.map(attributes =&gt; "Name: " + attributes(0)).show()// +-------------+// | value|// +-------------+// |Name: Michael|// | Name: Andy|// | Name: Justin|// +-------------+ 类型之间的转换总结RDD、DataFrame、Dataset三者有许多共性，有各自适用的场景常常需要在三者之间转换DataFrame/Dataset转RDD：这个转换很简单： 12val rdd1=testDF.rddval rdd2=testDS.rdd RDD转DataFrame： 一般用元组把一行的数据写在一起，然后在toDF中指定字段名。 1234import spark.implicits._val testDF = rdd.map &#123;line=&gt; (line._1,line._2) &#125;.toDF("col1","col2") RDD转DataSet： 可以注意到，定义每一行的类型（case class）时，已经给出了字段名和类型，后面只要往case class里面添加值即可。 12345import spark.implicits._case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型val testDS = rdd.map &#123;line=&gt; Coltest(line._1,line._2) &#125;.toDS DataSet转DataFrame： 这个也很简单，因为只是把case class封装成Row. 12import spark.implicits._val testDF = testDS.toDF DataFrame转DataSet： 123import spark.implicits._case class Coltest(col1:String,col2:Int)extends Serializable //定义字段名和类型val testDS = testDF.as[Coltest] 这种方法就是在给出每一列的类型后，使用as方法，转成Dataset，这在数据类型是DataFrame又需要针对各个字段处理时极为方便。在使用一些特殊的操作时，一定要加上 import spark.implicits._ 不然toDF、toDS无法使用。 用户自定义函数通过spark.udf功能用户可以自定义函数。 用户自定义UDF函数1234567891011121314151617181920212223242526scala&gt; val df = spark.read.json("examples/src/main/resources/people.json")df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]scala&gt; df.show()+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+scala&gt; spark.udf.register("addName", (x:String)=&gt; "Name:"+x)res5: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType)))scala&gt; df.createOrReplaceTempView("people")scala&gt; spark.sql("Select addName(name), age from people").show()+-----------------+----+|UDF:addName(name)| age|+-----------------+----+| Name:Michael|null|| Name:Andy| 30|| Name:Justin| 19|+-----------------+----+ 用户自定义聚合函数强类型的Dataset和弱类型的DataFrame都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。 弱类型通过继承UserDefinedAggregateFunction来实现用户自定义聚合函数。下面展示一个求平均工资的自定义聚合函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import org.apache.spark.sql.expressions.MutableAggregationBufferimport org.apache.spark.sql.expressions.UserDefinedAggregateFunctionimport org.apache.spark.sql.types._import org.apache.spark.sql.Rowimport org.apache.spark.sql.SparkSessionobject MyAverage extends UserDefinedAggregateFunction &#123;// 聚合函数输入参数的数据类型 def inputSchema: StructType = StructType(StructField("inputColumn", LongType) :: Nil)// 聚合缓冲区中值得数据类型 def bufferSchema: StructType = &#123;StructType(StructField("sum", LongType) :: StructField("count", LongType) :: Nil)&#125;// 返回值的数据类型 def dataType: DataType = DoubleType// 对于相同的输入是否一直返回相同的输出。 def deterministic: Boolean = true// 初始化def initialize(buffer: MutableAggregationBuffer): Unit = &#123;// 存工资的总额buffer(0) = 0L// 存工资的个数buffer(1) = 0L&#125;// 相同Execute间的数据合并。 def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123;if (!input.isNullAt(0)) &#123;buffer(0) = buffer.getLong(0) + input.getLong(0)buffer(1) = buffer.getLong(1) + 1&#125;&#125;// 不同Execute间的数据合并 def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123;buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)&#125;// 计算最终结果def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)&#125;// 注册函数spark.udf.register("myAverage", MyAverage)val df = spark.read.json("examples/src/main/resources/employees.json")df.createOrReplaceTempView("employees")df.show()// +-------+------+// | name|salary|// +-------+------+// |Michael| 3000|// | Andy| 4500|// | Justin| 3500|// | Berta| 4000|// +-------+------+val result = spark.sql("SELECT myAverage(salary) as average_salary FROM employees")result.show()// +--------------+// |average_salary|// +--------------+// | 3750.0|// +--------------+ 强类型通过继承Aggregator来实现强类型自定义聚合函数，同样是求平均工资。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import org.apache.spark.sql.expressions.Aggregatorimport org.apache.spark.sql.Encoderimport org.apache.spark.sql.Encodersimport org.apache.spark.sql.SparkSession// 既然是强类型，可能有case类case class Employee(name: String, salary: Long)case class Average(var sum: Long, var count: Long)object MyAverage extends Aggregator[Employee, Average, Double] &#123;// 定义一个数据结构，保存工资总数和工资总个数，初始都为0def zero: Average = Average(0L, 0L)// Combine two values to produce a new value. For performance, the function may modify `buffer`// and return it instead of constructing a new objectdef reduce(buffer: Average, employee: Employee): Average = &#123;buffer.sum += employee.salarybuffer.count += 1buffer&#125;// 聚合不同execute的结果def merge(b1: Average, b2: Average): Average = &#123;b1.sum += b2.sumb1.count += b2.countb1&#125;// 计算输出def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count// 设定之间值类型的编码器，要转换成case类// Encoders.product是进行scala元组和case类转换的编码器 def bufferEncoder: Encoder[Average] = Encoders.product// 设定最终输出值的编码器def outputEncoder: Encoder[Double] = Encoders.scalaDouble&#125;import spark.implicits._val ds = spark.read.json("examples/src/main/resources/employees.json").as[Employee]ds.show()// +-------+------+// | name|salary|// +-------+------+// |Michael| 3000|// | Andy| 4500|// | Justin| 3500|// | Berta| 4000|// +-------+------+// Convert the function to a `TypedColumn` and give it a nameval averageSalary = MyAverage.toColumn.name("average_salary")val result = ds.select(averageSalary)result.show()// +--------------+// |average_salary|// +--------------+// | 3750.0|// +--------------+ Spark SQL 数据源通用加载 / 保存方法手动指定选项Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。修改配置项spark.sql.sources.default，可修改默认数据源格式。 1val df = spark.read.load("examples/src/main/resources/users.parquet") df.select("name", "favorite_color").write.save("namesAndFavColors.parquet") 当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：org.apache.spark.sql.parquet），如果数据源格式为内置格式，则只需要指定简称定json, parquet, jdbc, orc, libsvm, csv, text来指定数据的格式。可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。 12val peopleDF = spark.read.format("json").load("examples/src/main/resources/people.json")peopleDF.write.format("parquet").save("hdfs://master01:9000/namesAndAges.parquet") 除此之外，可以直接运行SQL在文件上： 12val sqlDF = spark.sql("SELECT * FROM parquet.`hdfs://master01:9000/namesAndAges.parquet`")sqlDF.show() 1234567891011121314151617181920212223242526scala&gt; val peopleDF = spark.read.format("json").load("examples/src/main/resources/people.json")peopleDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]scala&gt; peopleDF.write.format("parquet").save("hdfs://master01:9000/namesAndAges.parquet")scala&gt; peopleDF.show()+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+scala&gt; val sqlDF = spark.sql("SELECT * FROM parquet.`hdfs://master01:9000/namesAndAges.parquet`")17/09/05 04:21:11 WARN ObjectStore: Failed to get database parquet, returning NoSuchObjectExceptionsqlDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]scala&gt; sqlDF.show()+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+ 文件保存选项可以采用SaveMode执行存储操作，SaveMode定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。SaveMode详细介绍如下表： Scala/Java Any Language Meaning SaveMode.ErrorIfExists(default) “error”(default) 如果文件存在，则报错 SaveMode.Append “append” 追加 SaveMode.Overwrite “overwrite” 覆写 SaveMode.Ignore “ignore” 数据存在，则忽略 Parquet 文件Parquet是一种流行的列式存储格式，可以高效地存储具有嵌套字段的记录。 Parquet 读写Parquet格式经常在Hadoop生态圈中被使用，它也支持Spark SQL的全部数据类型。Spark SQL 提供了直接读取和存储 Parquet 格式文件的方法。 12345678910111213141516171819202122// Encoders for most common types are automatically provided by importing spark.implicits._import spark.implicits._val peopleDF = spark.read.json("examples/src/main/resources/people.json")// DataFrames can be saved as Parquet files, maintaining the schema informationpeopleDF.write.parquet("hdfs://master01:9000/people.parquet")// Read in the parquet file created above// Parquet files are self-describing so the schema is preserved// The result of loading a Parquet file is also a DataFrameval parquetFileDF = spark.read.parquet("hdfs://master01:9000/people.parquet")// Parquet files can also be used to create a temporary view and then used in SQL statementsparquetFileDF.createOrReplaceTempView("parquetFile")val namesDF = spark.sql("SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19")namesDF.map(attributes =&gt; "Name: " + attributes(0)).show()// +------------+// | value|// +------------+// |Name: Justin|// +------------+ 解析分区信息对表进行分区是对数据进行优化的方式之一。在分区的表内，数据通过分区列将数据存储在不同的目录下。Parquet数据源现在能够自动发现并解析分区信息。例如，对人口数据进行分区存储，分区列为gender和country，使用下面的目录结构： 12345678910111213141516171819path└── to └── table ├── gender=male │ ├── ... │ │ │ ├── country=US │ │ └── data.parquet │ ├── country=CN │ │ └── data.parquet │ └── ... └── gender=female ├── ... │ ├── country=US │ └── data.parquet ├── country=CN │ └── data.parquet └── ... 通过传递path/to/table给 SQLContext.read.parquet或SQLContext.read.load，Spark SQL将自动解析分区信息。返回的DataFrame的Schema如下： 12345root|-- name: string (nullable = true)|-- age: long (nullable = true)|-- gender: string (nullable = true)|-- country: string (nullable = true) 需要注意的是，数据的分区列的数据类型是自动解析的。当前，支持数值类型和字符串类型。自动解析分区类型的参数为：spark.sql.sources.partitionColumnTypeInference.enabled，默认值为true。如果想关闭该功能，直接将该参数设置为disabled。此时，分区列数据格式将被默认设置为string类型，不再进行类型解析。 Schema 合并像ProtocolBuffer、Avro和Thrift那样，Parquet也支持Schema evolution（Schema演变）。用户可以先定义一个简单的Schema，然后逐渐的向Schema中增加列描述。通过这种方式，用户可以获取多个有不同Schema但相互兼容的Parquet文件。现在Parquet数据源能自动检测这种情况，并合并这些文件的schemas。因为Schema合并是一个高消耗的操作，在大多数情况下并不需要，所以Spark SQL从1.5.0开始默认关闭了该功能。可以通过下面两种方式开启该功能：当数据源为Parquet文件时，将数据源选项mergeSchema设置为true设置全局SQL选项spark.sql.parquet.mergeSchema为true示例如下： 123456789101112131415161718192021222324// sqlContext from the previous example is used in this example.// This is used to implicitly convert an RDD to a DataFrame.import spark.implicits._// Create a simple DataFrame, stored into a partition directoryval df1 = sc.makeRDD(1 to 5).map(i =&gt; (i, i * 2)).toDF("single", "double")df1.write.parquet("hdfs://master01:9000/data/test_table/key=1")// Create another DataFrame in a new partition directory,// adding a new column and dropping an existing columnval df2 = sc.makeRDD(6 to 10).map(i =&gt; (i, i * 3)).toDF("single", "triple")df2.write.parquet("hdfs://master01:9000/data/test_table/key=2")// Read the partitioned tableval df3 = spark.read.option("mergeSchema", "true").parquet("hdfs://master01:9000/data/test_table")df3.printSchema()// The final schema consists of all 3 columns in the Parquet files together// with the partitioning column appeared in the partition directory paths.// root// |-- single: int (nullable = true)// |-- double: int (nullable = true)// |-- triple: int (nullable = true)// |-- key : int (nullable = true) Hive 数据库Apache Hive是Hadoop上的SQL引擎，Spark SQL编译时可以包含Hive支持，也可以不包含。包含Hive支持的Spark SQL可以支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的 一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好Hive，Spark SQL也可以运行。 需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import java.io.Fileimport org.apache.spark.sql.Rowimport org.apache.spark.sql.SparkSessioncase class Record(key: Int, value: String)// warehouseLocation points to the default location for managed databases and tablesval warehouseLocation = new File("spark-warehouse").getAbsolutePathval spark = SparkSession.builder().appName("Spark Hive Example").config("spark.sql.warehouse.dir", warehouseLocation).enableHiveSupport().getOrCreate()import spark.implicits._import spark.sqlsql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src")// Queries are expressed in HiveQLsql("SELECT * FROM src").show()// +---+-------+// |key| value|// +---+-------+// |238|val_238|// | 86| val_86|// |311|val_311|// ...// Aggregation queries are also supported.sql("SELECT COUNT(*) FROM src").show()// +--------+// |count(1)|// +--------+// | 500 |// +--------+// The results of SQL queries are themselves DataFrames and support all normal functions.val sqlDF = sql("SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key")// The items in DataFrames are of type Row, which allows you to access each column by ordinal.val stringsDS = sqlDF.map &#123;case Row(key: Int, value: String) =&gt; s"Key: $key, Value: $value"&#125;stringsDS.show()// +--------------------+// | value|// +--------------------+// |Key: 0, Value: val_0|// |Key: 0, Value: val_0|// |Key: 0, Value: val_0|// ...// You can also use DataFrames to create temporary views within a SparkSession.val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s"val_$i")))recordsDF.createOrReplaceTempView("records")// Queries can then join DataFrame data with data stored in Hive.sql("SELECT * FROM records r JOIN src s ON r.key = s.key").show()// +---+------+---+------+// |key| value|key| value|// +---+------+---+------+// | 2| val_2| 2| val_2|// | 4| val_4| 4| val_4|// | 5| val_5| 5| val_5|// ... 内联 Hive 应用如果要使用内嵌的Hive，什么都不用做，直接用就可以了。 –conf : spark.sql.warehouse.dir= 12345678910111213141516scala&gt; spark.sql("show tables").show+--------+---------+-----------+|database|tableName|isTemporary|+--------+---------+-----------+| | persons| true|+--------+---------+-----------+scala&gt; spark.sql("SELECT * FROM persons").show()+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+ 注意：如果你使用的是内部的Hive，在Spark2.0之后，spark.sql.warehouse.dir用于指定数据仓库的地址，如果你需要是用HDFS作为路径，那么需要将core-site.xml和hdfs-site.xml 加入到Spark conf目录，否则只会创建master节点上的warehouse目录，查询时会出现文件找不到的问题。所以如果需要使用HDFS，则需要将metastore删除，重启集群。 外部 Hive 应用如果想连接外部已经部署好的Hive，需要通过以下几个步骤。 将Hive中的hive-site.xml拷贝或者软连接到Spark安装目录下的conf目录下。 打开spark shell，注意带上访问Hive元数据库的JDBC客户端。 1$ bin/spark-shell --master spark://master01:7077 --jars mysql-connector-java-5.1.27-bin.jar JSON 数据集Spark SQL 能够自动推测 JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载一个 Dataset[String]或者一个JSON 文件.注意，这个JSON文件不是一个传统的JSON文件，每一行都得是一个JSON串。 123&#123;"name":"Michael"&#125;&#123;"name":"Andy", "age":30&#125;&#123;"name":"Justin", "age":19&#125; 1234567891011121314151617181920212223242526272829303132333435363738// Primitive types (Int, String, etc) and Product types (case classes) encoders are// supported by importing this when creating a Dataset.import spark.implicits._// A JSON dataset is pointed to by path.// The path can be either a single text file or a directory storing text filesval path = "examples/src/main/resources/people.json"val peopleDF = spark.read.json(path)// The inferred schema can be visualized using the printSchema() methodpeopleDF.printSchema()// root// |-- age: long (nullable = true)// |-- name: string (nullable = true)// Creates a temporary view using the DataFramepeopleDF.createOrReplaceTempView("people")// SQL statements can be run by using the sql methods provided by sparkval teenagerNamesDF = spark.sql("SELECT name FROM people WHERE age BETWEEN 13 AND 19")teenagerNamesDF.show()// +------+// | name|// +------+// |Justin|// +------+// Alternatively, a DataFrame can be created for a JSON dataset represented by// a Dataset[String] storing one JSON object per stringval otherPeopleDataset = spark.createDataset("""&#123;"name":"Yin","address":&#123;"city":"Columbus","state":"Ohio"&#125;&#125;""" :: Nil)val otherPeople = spark.read.json(otherPeopleDataset)otherPeople.show()// +---------------+----+// | address|name|// +---------------+----+// |[Columbus,Ohio]| Yin|// +---------------+----+ JDBCSpark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。注意，需要将相关的数据库驱动放到spark的类路径下。 1$ bin/spark-shell --master spark://master01:7077 --jars mysql-connector-java-5.1.27-bin.jar 1234567891011121314151617181920212223242526// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods// Loading data from a JDBC sourceval jdbcDF = spark.read.format("jdbc").option("url", "jdbc:mysql://master01:3306/rdd").option("dbtable", " rddtable").option("user", "root").option("password", "hive").load()val connectionProperties = new Properties()connectionProperties.put("user", "root")connectionProperties.put("password", "hive")val jdbcDF2 = spark.read.jdbc("jdbc:mysql://master01:3306/rdd", "rddtable", connectionProperties)// Saving data to a JDBC sourcejdbcDF.write.format("jdbc").option("url", "jdbc:mysql://master01:3306/rdd").option("dbtable", "rddtable2").option("user", "root").option("password", "hive").save()jdbcDF2.write.jdbc("jdbc:mysql://master01:3306/mysql", "db", connectionProperties)// Specifying create table column data types on writejdbcDF.write.option("createTableColumnTypes", "name CHAR(64), comments VARCHAR(1024)").jdbc("jdbc:mysql://master01:3306/mysql", "db", connectionProperties) JDBC / ODBC 服务器Spark SQL也提供JDBC连接支持，这对于让商业智能(BI)工具连接到Spark集群上以 及在多用户间共享一个集群的场景都非常有用。JDBC 服务器作为一个独立的 Spark 驱动 器程序运行，可以在多用户之间共享。任意一个客户端都可以在内存中缓存数据表，对表 进行查询。集群的资源以及缓存数据都在所有用户之间共享。Spark SQL的JDBC服务器与Hive中的HiveServer2相一致。由于使用了Thrift通信协议，它也被称为“Thrift server”。服务器可以通过 Spark 目录中的 sbin/start-thriftserver.sh 启动。这个 脚本接受的参数选项大多与 spark-submit 相同。默认情况下，服务器会在 localhost:10000 上进行监听，我们可以通过环境变量(HIVE_SERVER2_THRIFT_PORT 和 HIVE_SERVER2_THRIFT_BIND_HOST)修改这些设置，也可以通过 Hive配置选项(hive. server2.thrift.port 和 hive.server2.thrift.bind.host)来修改。你也可以通过命令行参 数–hiveconf property=value来设置Hive选项。 1234567./sbin/start-thriftserver.sh \--hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \--hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \--master &lt;master-uri&gt;..../bin/beelinebeeline&gt; !connect jdbc:hive2://master01:10000 在 Beeline 客户端中，你可以使用标准的 HiveQL 命令来创建、列举以及查询数据表。 1234567891011121314151617181920212223242526[bigdata@master01 spark-2.1.1-bin-hadoop2.7]$ ./sbin/start-thriftserver.shstarting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /home/bigdata/hadoop/spark-2.1.1-bin-hadoop2.7/logs/spark-bigdata-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-master01.out[bigdata@master01 spark-2.1.1-bin-hadoop2.7]$ ./bin/beelineBeeline version 1.2.1.spark2 by Apache Hivebeeline&gt; !connect jdbc:hive2://master01:10000Connecting to jdbc:hive2://master01:10000Enter username for jdbc:hive2://master01:10000: bigdataEnter password for jdbc:hive2://master01:10000: *******log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.Connected to: Spark SQL (version 2.1.1)Driver: Hive JDBC (version 1.2.1.spark2)Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2://master01:10000&gt; show tables;+-----------+------------+--------------+--+| database | tableName | isTemporary |+-----------+------------+--------------+--+| default | src | false |+-----------+------------+--------------+--+1 row selected (0.726 seconds)0: jdbc:hive2://master01:10000&gt; Spark SQL CLISpark SQL CLI可以很方便的在本地运行Hive元数据服务以及从命令行执行查询任务。需要注意的是，Spark SQL CLI不能与Thrift JDBC服务交互。在Spark目录下执行如下命令启动Spark SQL CLI： 1./bin/spark-sql 配置Hive需要替换 conf/ 下的 hive-site.xml 。 Spark SQL 的运行原理Spark SQL 运行架构Spark SQL对SQL语句的处理和关系型数据库类似，即词法/语法解析、绑定、优化、执行。Spark SQL会先将SQL语句解析成一棵树，然后使用规则(Rule)对Tree进行绑定、优化等处理过程。Spark SQL由Core、Catalyst、Hive、Hive-ThriftServer四部分构成：Core: 负责处理数据的输入和输出，如获取数据，查询结果输出成DataFrame等Catalyst: 负责处理整个查询过程，包括解析、绑定、优化等Hive: 负责对Hive数据进行处理Hive-ThriftServer: 主要用于对hive的访问 TreeNode逻辑计划、表达式等都可以用tree来表示，它只是在内存中维护，并不会进行磁盘的持久化，分析器和优化器对树的修改只是替换已有节点。TreeNode有2个直接子类，QueryPlan和Expression。QueryPlam下又有LogicalPlan和SparkPlan. Expression是表达式体系，不需要执行引擎计算而是可以直接处理或者计算的节点，包括投影操作，操作符运算等。 Rule &amp; RuleExecutorRule就是指对逻辑计划要应用的规则，以到达绑定和优化。他的实现类就是RuleExecutor。优化器和分析器都需要继承RuleExecutor。每一个子类中都会定义Batch、Once、FixPoint. 其中每一个Batch代表着一套规则，Once表示对树进行一次操作，FixPoint表示对树进行多次的迭代操作。RuleExecutor内部提供一个Seq[Batch]属性，里面定义的是RuleExecutor的处理逻辑，具体的处理逻辑由具体的Rule子类实现。 整个流程架构图： Spark SQL 运行原理 使用SessionCatalog保存元数据：在解析SQL语句之前，会创建SparkSession，或者如果是2.0之前的版本初始化SQLContext，SparkSession只是封装了SparkContext和SQLContext的创建而已。会把元数据保存在SessionCatalog中，涉及到表名，字段名称和字段类型。创建临时表或者视图，其实就会往SessionCatalog注册。 解析SQL,使用ANTLR生成未绑定的逻辑计划：当调用SparkSession的sql或者SQLContext的sql方法，我们以2.0为准，就会使用SparkSqlParser进行解析SQL. 使用的ANTLR进行词法解析和语法解析。它分为2个步骤来生成Unresolved LogicalPlan：(1)词法分析：Lexical Analysis，负责将token分组成符号类； (2)构建一个分析树或者语法树AST。 使用分析器Analyzer绑定逻辑计划：在该阶段，Analyzer会使用Analyzer Rules，并结合SessionCatalog，对未绑定的逻辑计划进行解析，生成已绑定的逻辑计划。 使用优化器Optimizer优化逻辑计划：优化器也是会定义一套Rules，利用这些Rule对逻辑计划和Exepression进行迭代处理，从而使得树的节点进行和并和优化。 使用SparkPlanner生成物理计划：SparkSpanner使用Planning Strategies，对优化后的逻辑计划进行转换，生成可以执行的物理计划SparkPlan. 使用QueryExecution执行物理计划：此时调用SparkPlan的execute方法，底层其实已经再触发JOB了，然后返回RDD. End.]]></content>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Core 应用解析]]></title>
    <url>%2F2017%2F08%2F04%2FSpark-Core-Analysis%2F</url>
    <content type="text"><![CDATA[本文主要解析部分 Spark Core 的技术点。 RDD 概念RDD 为什么会产生RDD是Spark的基石，是实现Spark数据处理的核心抽象。那么RDD为什么会产生呢？ Hadoop的MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后写入物理存储设备。数据更多面临的是一次性处理。 MR的这种方式对数据领域两种常见的操作不是很高效。第一种是迭代式的算法。比如机器学习中ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR这种模式不太合适，即使多MR串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。 MR中的迭代： Spark中的迭代： 我们需要一个效率非常快，且能够支持迭代计算和有效数据共享的模型，Spark应运而生。RDD是基于工作集的工作模式，更多的是面向工作流。 但是无论是MR还是RDD都应该具有类似位置感知、容错和负载均衡等特性。 RDD 概述什么是 RDDRDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。在 Spark 中，对数据的所有操作不外乎创建 RDD、转化已有RDD 以及调用 RDD 操作进行求值。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。RDD 可以包含 Python、Java、Scala 中任意类型的对象， 甚至可以包含用户自定义的对象。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。RDD支持两种操作:转化操作和行动操作。RDD 的转化操作是返回一个新的 RDD的操作，比如 map()和 filter()，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作。比如 count() 和 first()。Spark采用惰性计算模式，RDD只有第一次在一个行动操作中用到时，才会真正计算。Spark可以优化整个计算过程。默认情况下，Spark 的 RDD 会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个 RDD，可以使用 RDD.persist() 让 Spark 把这个 RDD 缓存下来。 RDD 的属性 一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。 一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。 RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。 一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 RDD是一个应用层面的逻辑概念。一个RDD多个分片。RDD就是一个元数据记录集，记录了RDD内存所有的关系数据。 RDD 弹性 自动进行内存和磁盘数据存储的切换：Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换 基于血统的高效容错机制：在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。 Task如果失败会自动进行特定次数的重试：RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。 Stage如果失败会自动进行特定次数的重试：如果Job的某个Stage阶段计算失败，框架也会自动进行任务的重新计算，默认次数也是4次。 Checkpoint和Persist可主动或被动触发：RDD可以通过Persist持久化将RDD缓存到内存或者磁盘，当再次用到该RDD时直接读取就行。也可以将RDD进行检查点，检查点会将数据存储在HDFS中，该RDD的所有父RD都会被移除。 数据调度弹性：Spark把这个JOB执行模型抽象为通用的有向无环图DAG，可以将多Stage的任务串联或并行执行，调度引擎自动处理Stage的失败以及Task的失败。 数据分片的高度弹性：可以根据业务的特征，动态调整数据分片的个数，提升整体的应用执行效率。 RDD全称叫做弹性分布式数据集(Resilient Distributed Datasets)一种分布式的内存抽象，表示一个只读的记录分区的集合，它只能通过其他RDD转换而创建，为此，RDD支持丰富的转换操作(如map, join, filter, groupBy等过这种转换操作，新的RDD则包含了如何从其他RDDs衍生所必需的信息，所以说RDDs之间是有依赖关系的。基于RDDs之间的依赖，RDDs会形成一个有向无环图DAG，G描述了整个流式计算的流程，实际执行的时候，RDD是通过血缘关系(Lineage)成的，即使出现数据分区丢失，也可以通过血缘关系重建分区，总结起来，基于RDD的流式计算任务可描述为：从稳定的物理存储(如分布式文件系统)中加载记录，记入由一组确定性操作构成的DAG，然后写回稳定存储。另外RDD还可以将数据集缓存到内存中，使得在多个操作之间可以重用数据集，基于这个特点可以很方便地构建应用(图计算、机器学习等)或者交互式数据分析应用。可以说Spark最初也就是实现RDD的一个分布式系统，后面通过不断发展壮大成为现在较为完善的大数据生态系单来讲，Spark-RDD的关系类似于Hadoop-MapReduce关系。 RDD 特点RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。 分区RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。 只读如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。 由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。 RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。 依赖RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是一一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。 通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，如下图所示，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。 缓存如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。 CheckPoint虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。 给定一个RDD我们至少可以知道如下几点信息：1、分区数以及分区方式；2、由父RDDs衍生而来的相关依赖信息；3、计算每个分区的数据，计算步骤为：1）如果被缓存，则从缓存中取的分区的数据；2）如果被checkpoint，则从checkpoint处恢复数据；3）根据血缘关系计算分区的数据。 RDD 编程编程模型在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。 那么这其中的 Dirver、SparkContext、Executor、Master、Worker分别是什么？ 创建 RDD在Spark中创建RDD的创建方式大概可以分为三种：（1）、从集合中创建RDD；（2）、从外部存储创建RDD；（3）、从其他RDD创建。 由一个已经存在的Scala集合创建，集合并行化。val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))；而从集合中创建RDD，Spark主要提供了两种函数：parallelize和makeRDD。我们可以先看看这两个函数的声明： 123456789def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] 我们可以从上面看出makeRDD有两种实现，而且第一个makeRDD函数接收的参数和parallelize完全一致。其实第一种makeRDD函数实现是依赖了parallelize函数的实现，来看看Spark中是怎么实现这个makeRDD函数的： 12345def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; parallelize(seq, numSlices)&#125; 我们可以看出，这个makeRDD函数完全和parallelize函数一致。但是我们得看看第二种makeRDD函数函数实现了，它接收的参数类型是Seq[(T, Seq[String])]，Spark文档的说明是：Distribute a local Scala collection to form an RDD, with one or more location preferences (hostnames of Spark nodes) for each object. Create a new partition for each collection item.原来，这个函数还为数据提供了位置信息，来看看我们怎么使用： 12345678910111213141516171819202122scala&gt; val guigu1= sc.parallelize(List(1,2,3))guigu1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:21 scala&gt; val guigu2 = sc.makeRDD(List(1,2,3))guigu2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[11] at makeRDD at &lt;console&gt;:21 scala&gt; val seq = List((1, List("slave01")), | (2, List("slave02")))seq: List[(Int, List[String])] = List((1,List(slave01)), (2,List(slave02))) scala&gt; val guigu3 = sc.makeRDD(seq)guigu3: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at makeRDD at &lt;console&gt;:23 scala&gt; guigu3.preferredLocations(guigu3.partitions(1))res26: Seq[String] = List(slave02) scala&gt; guigu3.preferredLocations(guigu3.partitions(0))res27: Seq[String] = List(slave01) scala&gt; guigu1.preferredLocations(guigu1.partitions(0))res28: Seq[String] = List() 我们可以看到，makeRDD函数有两种实现，第一种实现其实完全和parallelize一致；而第二种实现可以为数据提供位置信息，而除此之外的实现和parallelize函数也是一致的，如下： 123456789101112def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())&#125; def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope &#123; assertNotStopped() val indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap new ParallelCollectionRDD[T](this, seq.map(_._1), seq.size, indexToPrefs)&#125; 都是返回ParallelCollectionRDD，而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。 由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等。 12scala&gt; val rdd1 = sc.textFile("hdfs://slave1:9000/txtFile")rdd1: org.apache.spark.rdd.RDD[String] = hdfs://slave1:9000/RELEASE MapPartitionsRDD[4] at textFile at &lt;console&gt;:24 RDD 编程RDD一般分为数值RDD和键值对RDD，本章不进行具体区分，先统一来看，下一章会对键值对RDD做专门说明。 TransformationRDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。常用的 Transformation： map(func): 返回通过函数func传递源的每个元素形成的新分布式数据集。 1234567891011scala&gt; var source = sc.parallelize(1 to 10)source: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at &lt;console&gt;:24scala&gt; source.collect()res7: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; val mapadd = source.map(_ * 2)mapadd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at map at &lt;console&gt;:26scala&gt; mapadd.collect()res8: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18, 20) filter(func): 返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成。 1234567891011scala&gt; var sourceFilter = sc.parallelize(Array("xiaoming","xiaojiang","xiaohe","dazhi"))sourceFilter: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24scala&gt; val filter = sourceFilter.filter(_.contains("xiao"))filter: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[11] at filter at &lt;console&gt;:26scala&gt; sourceFilter.collect()res9: Array[String] = Array(xiaoming, xiaojiang, xiaohe, dazhi)scala&gt; filter.collect()res10: Array[String] = Array(xiaoming, xiaojiang, xiaohe) flatMap(func): 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）。 1234567891011scala&gt; val sourceFlat = sc.parallelize(1 to 5)sourceFlat: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:24scala&gt; sourceFlat.collect()res11: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt; val flatMap = sourceFlat.flatMap(1 to _)flatMap: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[13] at flatMap at &lt;console&gt;:26scala&gt; flatMap.collect()res12: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5) mapPartitions(func): 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。 123456789101112131415161718192021222324252627scala&gt; val rdd = sc.parallelize(List(("kpop","female"),("zorro","male"),("mobin","male"),("lucy","female")))rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[16] at parallelize at &lt;console&gt;:24scala&gt; :paste// Entering paste mode (ctrl-D to finish)def partitionsFun(iter : Iterator[(String,String)]) : Iterator[String] = &#123; var woman = List[String]() while (iter.hasNext)&#123; val next = iter.next() next match &#123; case (_,"female") =&gt; woman = next._1 :: woman case _ =&gt; &#125; &#125; woman.iterator&#125;// Exiting paste mode, now interpreting.partitionsFun: (iter: Iterator[(String, String)])Iterator[String]scala&gt; val result = rdd.mapPartitions(partitionsFun)result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[17] at mapPartitions at &lt;console&gt;:28scala&gt; result.collect()res13: Array[String] = Array(kpop, lucy) mapPartitionsWithIndex(func): 类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U]。 123456789101112131415161718192021222324252627scala&gt; val rdd = sc.parallelize(List(("kpop","female"),("zorro","male"),("mobin","male"),("lucy","female")))rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[18] at parallelize at &lt;console&gt;:24scala&gt; :paste// Entering paste mode (ctrl-D to finish)def partitionsFun(index : Int, iter : Iterator[(String,String)]) : Iterator[String] = &#123; var woman = List[String]() while (iter.hasNext)&#123; val next = iter.next() next match &#123; case (_,"female") =&gt; woman = "["+index+"]"+next._1 :: woman case _ =&gt; &#125; &#125; woman.iterator&#125;// Exiting paste mode, now interpreting.partitionsFun: (index: Int, iter: Iterator[(String, String)])Iterator[String]scala&gt; val result = rdd.mapPartitionsWithIndex(partitionsFun)result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[19] at mapPartitionsWithIndex at &lt;console&gt;:28scala&gt; result.collect()res14: Array[String] = Array([0]kpop, [3]lucy) sample(withReplacement, fraction, seed): 以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。例子从RDD中随机且有放回的抽出50%的数据，随机种子值为3（即可能以1 2 3的其中一个起始值）。 1234567891011121314151617scala&gt; val rdd = sc.parallelize(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[20] at parallelize at &lt;console&gt;:24scala&gt; rdd.collect()res15: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; var sample1 = rdd.sample(true,0.4,2)sample1: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[21] at sample at &lt;console&gt;:26scala&gt; sample1.collect()res16: Array[Int] = Array(1, 2, 2, 7, 7, 8, 9)scala&gt; var sample2 = rdd.sample(false,0.2,3)sample2: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[22] at sample at &lt;console&gt;:26scala&gt; sample2.collect()res17: Array[Int] = Array(1, 9) union(otherDataset): 对源RDD和参数RDD求并集后返回一个新的RDD。 1234567891011scala&gt; val rdd1 = sc.parallelize(1 to 5)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[23] at parallelize at &lt;console&gt;:24scala&gt; val rdd2 = sc.parallelize(5 to 10)rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[24] at parallelize at &lt;console&gt;:24scala&gt; val rdd3 = rdd1.union(rdd2)rdd3: org.apache.spark.rdd.RDD[Int] = UnionRDD[25] at union at &lt;console&gt;:28scala&gt; rdd3.collect()res18: Array[Int] = Array(1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10) intersection(otherDataset): 对源RDD和参数RDD求交集后返回一个新的RDD。 1234567891011scala&gt; val rdd1 = sc.parallelize(1 to 7)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[26] at parallelize at &lt;console&gt;:24scala&gt; val rdd2 = sc.parallelize(5 to 10)rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[27] at parallelize at &lt;console&gt;:24scala&gt; val rdd3 = rdd1.intersection(rdd2)rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[33] at intersection at &lt;console&gt;:28scala&gt; rdd3.collect()res19: Array[Int] = Array(5, 6, 7) distinct([numTasks])): 对源RDD进行去重后返回一个新的RDD. 默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。 1234567891011121314scala&gt; val distinctRdd = sc.parallelize(List(1,2,1,5,2,9,6,1))distinctRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at &lt;console&gt;:24scala&gt; val unionRDD = distinctRdd.distinct()unionRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[37] at distinct at &lt;console&gt;:26scala&gt; unionRDD.collect()res20: Array[Int] = Array(1, 9, 5, 6, 2)scala&gt; val unionRDD = distinctRdd.distinct(2)unionRDD: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[40] at distinct at &lt;console&gt;:26scala&gt; unionRDD.collect()res21: Array[Int] = Array(6, 2, 1, 9, 5) partitionBy(partitioner): 对RDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区， 否则会生成ShuffleRDD。 1234567891011scala&gt; val rdd = sc.parallelize(Array((1,"aaa"),(2,"bbb"),(3,"ccc"),(4,"ddd")),4)rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[44] at parallelize at &lt;console&gt;:24scala&gt; rdd.partitions.sizeres24: Int = 4scala&gt; var rdd2 = rdd.partitionBy(new org.apache.spark.HashPartitioner(2))rdd2: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[45] at partitionBy at &lt;console&gt;:26scala&gt; rdd2.partitions.sizeres25: Int = 2 reduceByKey(func, [numTasks]): 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。 12345678scala&gt; val rdd = sc.parallelize(List(("female",1),("male",5),("female",5),("male",2)))rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[46] at parallelize at &lt;console&gt;:24scala&gt; val reduce = rdd.reduceByKey((x,y) =&gt; x+y)reduce: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[47] at reduceByKey at &lt;console&gt;:26scala&gt; reduce.collect()res29: Array[(String, Int)] = Array((female,6), (male,7)) groupByKey(): groupByKey也是对每个key进行操作，但只生成一个sequence。 1234567891011121314151617181920212223scala&gt; val words = Array("one", "two", "two", "three", "three", "three")words: Array[String] = Array(one, two, two, three, three, three)scala&gt; val wordPairsRDD = sc.parallelize(words).map(word =&gt; (word, 1))wordPairsRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[4] at map at &lt;console&gt;:26scala&gt; val group = wordPairsRDD.groupByKey()group: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[5] at groupByKey at &lt;console&gt;:28scala&gt; group.collect()res1: Array[(String, Iterable[Int])] = Array((two,CompactBuffer(1, 1)), (one,CompactBuffer(1)), (three,CompactBuffer(1, 1, 1)))scala&gt; group.map(t =&gt; (t._1, t._2.sum))res2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[6] at map at &lt;console&gt;:31scala&gt; res2.collect()res3: Array[(String, Int)] = Array((two,2), (one,1), (three,3))scala&gt; val map = group.map(t =&gt; (t._1, t._2.sum))map: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[7] at map at &lt;console&gt;:30scala&gt; map.collect()res4: Array[(String, Int)] = Array((two,2), (one,1), (three,3)) combineByKey[C]( createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): 对相同K，把V合并成一个集合. createCombiner: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。如果这是一个新的元素,combineByKey() 会使用一个叫作 createCombiner() 的函数来创建 那个键对应的累加器的初始值。 mergeValue: 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并。 mergeCombiners: 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。 123456789101112131415161718scala&gt; val scores = Array(("Fred", 88), ("Fred", 95), ("Fred", 91), ("Wilma", 93), ("Wilma", 95), ("Wilma", 98))scores: Array[(String, Int)] = Array((Fred,88), (Fred,95), (Fred,91), (Wilma,93), (Wilma,95), (Wilma,98))scala&gt; val input = sc.parallelize(scores)input: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[52] at parallelize at &lt;console&gt;:26scala&gt; val combine = input.combineByKey( | (v)=&gt;(v,1), | (acc:(Int,Int),v)=&gt;(acc._1+v,acc._2+1), | (acc1:(Int,Int),acc2:(Int,Int))=&gt;(acc1._1+acc2._1,acc1._2+acc2._2))combine: org.apache.spark.rdd.RDD[(String, (Int, Int))] = ShuffledRDD[53] at combineByKey at &lt;console&gt;:28scala&gt; val result = combine.map&#123; | case (key,value) =&gt; (key,value._1/value._2.toDouble)&#125;result: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[54] at map at &lt;console&gt;:30scala&gt; result.collect()res33: Array[(String, Double)] = Array((Wilma,95.33333333333333), (Fred,91.33333333333333)) aggregateByKey(zeroValue:U,[partitioner: Partitioner]) (seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U): 在kv对的RDD中，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。seqOp函数用于在每一个分区中用初始值逐步迭代value，combOp函数用于合并每个分区中的结果。 1234567891011121314151617scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:24scala&gt; val agg = rdd.aggregateByKey(0)(math.max(_,_),_+_)agg: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[13] at aggregateByKey at &lt;console&gt;:26scala&gt; agg.collect()res7: Array[(Int, Int)] = Array((3,8), (1,7), (2,3))scala&gt; agg.partitions.sizeres8: Int = 3scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),1)rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24scala&gt; val agg = rdd.aggregateByKey(0)(math.max(_,_),_+_).collect()agg: Array[(Int, Int)] = Array((1,4), (3,8), (2,3)) foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]: aggregateByKey的简化操作，seqop和combop相同。 12345678scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[91] at parallelize at &lt;console&gt;:24scala&gt; val agg = rdd.foldByKey(0)(_+_)agg: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[92] at foldByKey at &lt;console&gt;:26scala&gt; agg.collect()res61: Array[(Int, Int)] = Array((3,14), (1,9), (2,3)) sortByKey([ascending], [numTasks]): 在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD。 12345678scala&gt; val rdd = sc.parallelize(Array((3,"aa"),(6,"cc"),(2,"bb"),(1,"dd")))rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24scala&gt; rdd.sortByKey(true).collect()res9: Array[(Int, String)] = Array((1,dd), (2,bb), (3,aa), (6,cc))scala&gt; rdd.sortByKey(false).collect()res10: Array[(Int, String)] = Array((6,cc), (3,aa), (2,bb), (1,dd)) sortBy(func,[ascending], [numTasks]): 与sortByKey类似，但是更灵活,可以用func先对数据进行处理，按照处理后的数据比较结果排序。 12345678scala&gt; val rdd = sc.parallelize(List(1,2,3,4))rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[21] at parallelize at &lt;console&gt;:24scala&gt; rdd.sortBy(x =&gt; x).collect()res11: Array[Int] = Array(1, 2, 3, 4)scala&gt; rdd.sortBy(x =&gt; x%3).collect()res12: Array[Int] = Array(3, 4, 1, 2) join(otherDataset, [numTasks]): 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD。 12345678scala&gt; val rdd = sc.parallelize(Array((1,"a"),(2,"b"),(3,"c")))rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[32] at parallelize at &lt;console&gt;:24scala&gt; val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6)))rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:24scala&gt; rdd.join(rdd1).collect()res13: Array[(Int, (String, Int))] = Array((1,(a,4)), (2,(b,5)), (3,(c,6))) cogroup(otherDataset, [numTasks]): 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable\&lt;V>,Iterable\&lt;W>))类型的RDD。 1234567891011121314151617181920scala&gt; val rdd = sc.parallelize(Array((1,"a"),(2,"b"),(3,"c")))rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[37] at parallelize at &lt;console&gt;:24scala&gt; val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6)))rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[38] at parallelize at &lt;console&gt;:24scala&gt; rdd.cogroup(rdd1).collect()res14: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((1,(CompactBuffer(a),CompactBuffer(4))), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6))))scala&gt; val rdd2 = sc.parallelize(Array((4,4),(2,5),(3,6)))rdd2: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[41] at parallelize at &lt;console&gt;:24scala&gt; rdd.cogroup(rdd2).collect()res15: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((4,(CompactBuffer(),CompactBuffer(4))), (1,(CompactBuffer(a),CompactBuffer())), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6))))scala&gt; val rdd3 = sc.parallelize(Array((1,"a"),(1,"d"),(2,"b"),(3,"c")))rdd3: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[44] at parallelize at &lt;console&gt;:24scala&gt; rdd3.cogroup(rdd2).collect()res16: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((4,(CompactBuffer(),CompactBuffer(4))), (1,(CompactBuffer(d, a),CompactBuffer())), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6)))) cartesian(otherDataset): 笛卡尔积。 12345678scala&gt; val rdd1 = sc.parallelize(1 to 3)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[47] at parallelize at &lt;console&gt;:24scala&gt; val rdd2 = sc.parallelize(2 to 5)rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[48] at parallelize at &lt;console&gt;:24scala&gt; rdd1.cartesian(rdd2).collect()res17: Array[(Int, Int)] = Array((1,2), (1,3), (1,4), (1,5), (2,2), (2,3), (2,4), (2,5), (3,2), (3,3), (3,4), (3,5)) pipe(command, [envVars]): 对于每个分区，都执行一个perl或者shell脚本，返回输出的RDD。 1234567891011scala&gt; val rdd = sc.parallelize(List("hi","Hello","how","are","you"),1)rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[50] at parallelize at &lt;console&gt;:24scala&gt; rdd.pipe("/home/spark_shell/pipe.sh").collect()res18: Array[String] = Array(AA, &gt;&gt;&gt;hi, &gt;&gt;&gt;Hello, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you)scala&gt; val rdd = sc.parallelize(List("hi","Hello","how","are","you"),2)rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[52] at parallelize at &lt;console&gt;:24scala&gt; rdd.pipe("/home/spark_shell/pipe.sh").collect()res19: Array[String] = Array(AA, &gt;&gt;&gt;hi, &gt;&gt;&gt;Hello, AA, &gt;&gt;&gt;how, &gt;&gt;&gt;are, &gt;&gt;&gt;you) 123456pipe.sh:#!/bin/shecho "AA"while read LINE; do echo "&gt;&gt;&gt;"$&#123;LINE&#125;done coalesce(numPartitions): 缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。 1234567891011scala&gt; val rdd = sc.parallelize(1 to 16,4)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[54] at parallelize at &lt;console&gt;:24scala&gt; rdd.partitions.sizeres20: Int = 4scala&gt; val coalesceRDD = rdd.coalesce(3)coalesceRDD: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[55] at coalesce at &lt;console&gt;:26scala&gt; coalesceRDD.partitions.sizeres21: Int = 3 repartition(numPartitions): 根据分区数，从新通过网络随机洗牌所有数据。 1234567891011121314151617scala&gt; val rdd = sc.parallelize(1 to 16,4)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[56] at parallelize at &lt;console&gt;:24scala&gt; rdd.partitions.sizeres22: Int = 4scala&gt; val rerdd = rdd.repartition(2)rerdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[60] at repartition at &lt;console&gt;:26scala&gt; rerdd.partitions.sizeres23: Int = 2scala&gt; val rerdd = rdd.repartition(4)rerdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[64] at repartition at &lt;console&gt;:26scala&gt; rerdd.partitions.sizeres24: Int = 4 glom(): 将每一个分区形成一个数组，形成新的RDD类型时RDD[Array[T]]。 12345scala&gt; val rdd = sc.parallelize(1 to 16,4)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[65] at parallelize at &lt;console&gt;:24scala&gt; rdd.glom().collect()res25: Array[Array[Int]] = Array(Array(1, 2, 3, 4), Array(5, 6, 7, 8), Array(9, 10, 11, 12), Array(13, 14, 15, 16)) mapValues(func): 针对于(K,V)形式的类型只对V进行操作。 12345scala&gt; val rdd3 = sc.parallelize(Array((1,"a"),(1,"d"),(2,"b"),(3,"c")))rdd3: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[67] at parallelize at &lt;console&gt;:24scala&gt; rdd3.mapValues(_+"|||").collect()res26: Array[(Int, String)] = Array((1,a|||), (1,d|||), (2,b|||), (3,c|||)) subtract(other:RDD): 计算差的一种函数去除两个RDD中相同的元素，不同的RDD将保留下来 。 12345678scala&gt; val rdd = sc.parallelize(3 to 8)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[70] at parallelize at &lt;console&gt;:24scala&gt; val rdd1 = sc.parallelize(1 to 5)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[71] at parallelize at &lt;console&gt;:24scala&gt; rdd.subtract(rdd1).collect()res27: Array[Int] = Array(8, 6, 7) Action常用的 Action 如下： reduce(func): 通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的。 1234567891011scala&gt; val rdd1 = sc.makeRDD(1 to 10,2)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[85] at makeRDD at &lt;console&gt;:24scala&gt; rdd1.reduce(_+_)res50: Int = 55scala&gt; val rdd2 = sc.makeRDD(Array(("a",1),("a",3),("c",3),("d",5)))rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[86] at makeRDD at &lt;console&gt;:24scala&gt; rdd2.reduce((x,y)=&gt;(x._1 + y._1,x._2 + y._2))res51: (String, Int) = (adca,12) collect(): 在驱动程序中，以数组的形式返回数据集的所有元素。 12345scala&gt; val rdd = sc.makeRDD(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at makeRDD at &lt;console&gt;:24scala&gt; rdd.collect()res7: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) count(): 返回RDD的元素个数。 12345scala&gt; val rdd = sc.makeRDD(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[18] at makeRDD at &lt;console&gt;:24scala&gt; rdd.count()res8: Long = 10 first(): 返回RDD的第一个元素（类似于take(1)）。 12345scala&gt; val rdd = sc.makeRDD(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[19] at makeRDD at &lt;console&gt;:24scala&gt; rdd.first()res9: Int = 1 take(n): 返回一个由数据集的前n个元素组成的数组。 12345scala&gt; val rdd = sc.makeRDD(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[20] at makeRDD at &lt;console&gt;:24scala&gt; rdd.take(7)res10: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7) takeSample(withReplacement,num, [seed]): 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子。 12345scala&gt; val rdd = sc.makeRDD(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[21] at makeRDD at &lt;console&gt;:24scala&gt; rdd.takeSample(false, 2)res11: Array[Int] = Array(6, 2) takeOrdered(n): 返回前几个的排序。 12345678scala&gt; val rdd1 = sc.makeRDD(Seq(10, 5, 3, 19, 4))rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[23] at makeRDD at &lt;console&gt;:24scala&gt; rdd1.top(2)res12: Array[Int] = Array(19, 10)scala&gt; rdd1.takeOrdered(2)res13: Array[Int] = Array(3, 4) aggregate (zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U): aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。 1234567891011121314scala&gt; var rdd1 = sc.makeRDD(1 to 10,2)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[88] at makeRDD at &lt;console&gt;:24scala&gt; rdd1.aggregate(1)( | &#123;(x : Int,y : Int) =&gt; x + y&#125;, | &#123;(a : Int,b : Int) =&gt; a + b&#125; | )res56: Int = 58scala&gt; rdd1.aggregate(1)( | &#123;(x : Int,y : Int) =&gt; x * y&#125;, | &#123;(a : Int,b : Int) =&gt; a + b&#125; | )res57: Int = 30361 fold(num)(func): 折叠操作，aggregate的简化操作，seqop和combop一样。 1234567891011scala&gt; var rdd1 = sc.makeRDD(1 to 4,2)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[90] at makeRDD at &lt;console&gt;:24scala&gt; rdd1.aggregate(1)( | &#123;(x : Int,y : Int) =&gt; x + y&#125;, | &#123;(a : Int,b : Int) =&gt; a + b&#125; | )res59: Int = 13scala&gt; rdd1.fold(1)(_+_)res60: Int = 13 countByKey(): 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。 12345scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[95] at parallelize at &lt;console&gt;:24scala&gt; rdd.countByKey()res63: scala.collection.Map[Int,Long] = Map(3 -&gt; 2, 1 -&gt; 3, 2 -&gt; 1) foreach(func): 在数据集的每一个元素上，运行函数func进行更新。 123456789scala&gt; var rdd1 = sc.makeRDD(1 to 5)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[30] at makeRDD at &lt;console&gt;:24scala&gt; rdd1.collect().foreach(println)12345 saveAsTextFile(path): 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本。 saveAsSequenceFile(path): 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。 saveAsObjectFile(path): 用于将RDD中的元素序列化成对象，存储到文件中。 数值 RDD 的统计操作Spark对包含数值数据的RDD提供了一些描述性的统计操作。Spark的数值操作是通过流式算法实现的，允许以每次一个元素的方式构建出模型。这些统计数据都会在调用stats()时通过一次遍历数据计算出来，并以StatsCounter对象返回。 12345678scala&gt; var rdd1 = sc.makeRDD(1 to 100)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[42] at makeRDD at &lt;console&gt;:32scala&gt; rdd1.sum()res34: Double = 5050.0scala&gt; rdd1.max()res35: Int = 100 向RDD操作传递函数注意Spark的大部分转化操作和一部分行动操作，都需要依赖用户传递的函数来计算。在Scala中，我们可以把定义的内联函数、方法的引用或静态方法传递给Spark，就像Scala的其他函数式API一样。我们还要考虑其他一些细节，比如所传递的函数及其引用的数据需要是可序列化的(实现了Java的Serializable接口)。传递一个对象的方法或者字段时，会包含对整个对象的引用。 123456789101112131415161718class SearchFunctions(val query: String) extends java.io.Serializable&#123; def isMatch(s: String): Boolean = &#123; s.contains(query) &#125; def getMatchesFunctionReference(rdd: org.apache.spark.rdd.RDD[String]): org.apache.spark.rdd.RDD[String] = &#123; // 问题:"isMatch"表示"this.isMatch"，因此我们要传递整个"this" rdd.filter(isMatch) &#125; def getMatchesFieldReference(rdd: org.apache.spark.rdd.RDD[String]): org.apache.spark.rdd.RDD[String] = &#123; // 问题:"query"表示"this.query"，因此我们要传递整个"this" rdd.filter(x =&gt; x.contains(query)) &#125; def getMatchesNoReference(rdd: org.apache.spark.rdd.RDD[String]): org.apache.spark.rdd.RDD[String] = &#123; // 安全:只把我们需要的字段拿出来放入局部变量中 val query_ = this.query rdd.filter(x =&gt; x.contains(query_)) &#125; &#125; 如果在Scala中出现了NotSerializableException，通常问题就在于我们传递了一个不可序列化的类中的函数或字段。 在不同 RDD 类型间转换有些函数只能用于特定类型的RDD，比如mean()和variance()只能用在数值RDD上，而join()只能用在键值对RDD上。在Scala和Java中，这些函数都没有定义在标准的RDD类中，所以要访问这些附加功能，必须要确保获得了正确的专用RDD类。在Scala中，将RDD转为有特定函数的RDD(比如在RDD[Double]上进行数值操作)是由隐式转换来自动处理的。 RDD 持久化RDD 的缓存Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存个数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。如果一个有持久化数据的节点发生故障，Spark会在需要用到缓存的数据时重算丢失的数据分区。如果希望节点故障的情况不会拖累我们的执行速度，也可以把数据备份到多个节点上。 RDD 的缓存方式RDD通过persist方法或cache方法可以将前面的计算结果缓存，默认情况下persist()会把数据以序列化的形式缓存在JVM的堆空间中。但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。 通过查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在objectStorageLevel中定义的。 12345678910111213141516171819202122232425262728293031scala&gt; val rdd = sc.makeRDD(1 to 10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[19] at makeRDD at &lt;console&gt;:25scala&gt; val nocache = rdd.map(_.toString+"["+System.currentTimeMillis+"]")nocache: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[20] at map at &lt;console&gt;:27scala&gt; val cache = rdd.map(_.toString+"["+System.currentTimeMillis+"]")cache: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[21] at map at &lt;console&gt;:27scala&gt; cache.cacheres24: cache.type = MapPartitionsRDD[21] at map at &lt;console&gt;:27scala&gt; nocache.collectres25: Array[String] = Array(1[1505479375155], 2[1505479374674], 3[1505479374674], 4[1505479375153], 5[1505479375153], 6[1505479374675], 7[1505479375154], 8[1505479375154], 9[1505479374676], 10[1505479374676])scala&gt; nocache.collectres26: Array[String] = Array(1[1505479375679], 2[1505479376157], 3[1505479376157], 4[1505479375680], 5[1505479375680], 6[1505479376159], 7[1505479375680], 8[1505479375680], 9[1505479376158], 10[1505479376158])scala&gt; nocache.collectres27: Array[String] = Array(1[1505479376743], 2[1505479377218], 3[1505479377218], 4[1505479376745], 5[1505479376745], 6[1505479377219], 7[1505479376747], 8[1505479376747], 9[1505479377218], 10[1505479377218])scala&gt; cache.collectres28: Array[String] = Array(1[1505479382745], 2[1505479382253], 3[1505479382253], 4[1505479382748], 5[1505479382748], 6[1505479382257], 7[1505479382747], 8[1505479382747], 9[1505479382253], 10[1505479382253])scala&gt; cache.collectres29: Array[String] = Array(1[1505479382745], 2[1505479382253], 3[1505479382253], 4[1505479382748], 5[1505479382748], 6[1505479382257], 7[1505479382747], 8[1505479382747], 9[1505479382253], 10[1505479382253])scala&gt; cache.collectres30: Array[String] = Array(1[1505479382745], 2[1505479382253], 3[1505479382253], 4[1505479382748], 5[1505479382748], 6[1505479382257], 7[1505479382747], 8[1505479382747], 9[1505479382253], 10[1505479382253])scala&gt; cache.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY) 在存储级别的末尾加上“_2”来把持久化数据存为两份 缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。 注意：使用Tachyon可以实现堆外缓存。 RDD CheckPoint 机制Spark中对于数据的保存除了持久化操作之外，还提供了一种CheckPoint的机制，CheckPoint（本质是通过将RDD写入Disk做CheckPoint）是为了通过lineage做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做CheckPoint容错，如果之后有节点出现问题而丢失分区，从做CheckPoint的RDD开始重做Lineage，就会减少开销。CheckPoint通过将数据写入到HDFS文件系统实现了RDD的CheckPoint功能。cache和CheckPoint是有显著区别的，缓存把RDD计算出来然后放在内存中，但是RDD的依赖链（相当于数据库中的redo日志），也不能丢掉，当某个点某个executor宕了，上面cache的RDD就会丢掉，需要通过依赖链重放计算出来，不同的是，CheckPoint是把RDD保存在HDFS中，是多副本可靠存储，所以依赖链就可以丢掉了，就斩断了依赖链，是通过复制实现的高容错。如果存在以下场景，则比较适合使用CheckPoint机制： DAG中的Lineage过长，如果重算，则开销太大（如在PageRank中）。 在宽依赖上做Checkpoint获得的收益更大。 为当前RDD设置 CheckPoint。该函数将会创建一个二进制的文件，并存储到checkpoint目录中，该目录是用SparkContext.setCheckpointDir()设置的。在checkpoint的过程中，该RDD的所有依赖于父RDD中的信息将全部被移出。对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。 1234567891011121314151617181920212223242526272829303132333435363738scala&gt; val data = sc.parallelize(1 to 100 , 5)data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:12 scala&gt; sc.setCheckpointDir("hdfs://slave1:9000/checkpoint") scala&gt; data.checkpoint scala&gt; data.countscala&gt; val ch1 = sc.parallelize(1 to 2)ch1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:25scala&gt; val ch2 = ch1.map(_.toString+"["+System.currentTimeMillis+"]")ch2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[36] at map at &lt;console&gt;:27scala&gt; val ch3 = ch1.map(_.toString+"["+System.currentTimeMillis+"]")ch3: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[37] at map at &lt;console&gt;:27scala&gt; ch3.checkpointscala&gt; ch2.collectres62: Array[String] = Array(1[1505480940726], 2[1505480940243])scala&gt; ch2.collectres63: Array[String] = Array(1[1505480941957], 2[1505480941480])scala&gt; ch2.collectres64: Array[String] = Array(1[1505480942736], 2[1505480942257])scala&gt; ch3.collectres65: Array[String] = Array(1[1505480949080], 2[1505480948603])scala&gt; ch3.collectres66: Array[String] = Array(1[1505480948683], 2[1505480949161])scala&gt; ch3.collectres67: Array[String] = Array(1[1505480948683], 2[1505480949161]) CheckPoint 写流程RDD checkpoint 过程中会经过以下几个状态：[ Initialized → marked for checkpointing → checkpointing in progress → checkpointed ]，转换流程如下： data.CheckPoint这个函数调用中，设置的目录中，所有依赖的RDD都会被删除，函数必须在job运行之前调用执行，强烈建议RDD缓存在内存中（又提到一次，千万要注意哟），否则保存到文件的时候需要从头计算。初始化RDD的CheckPointData变量为ReliableRDDCheckpointData。这时候标记为Initialized状态。 在所有jobaction的时候，runJob方法中都会调用rdd.doCheckpoint,这个会向前递归调用所有的依赖的RDD，看看需不需要CheckPoint。需要需要CheckPoint，然后调用CheckPointData.get.CheckPoint()，里面标记状态为CheckpointingInProgress，里面调用具体实现类的ReliableRDDCheckpointData的doCheckpoint方法。 doCheckpoint-&gt;writeRDDToCheckpointDirectory，注意这里会把job再运行一次，如果已经cache了，就可以直接使用缓存中的RDD了，就不需要重头计算一遍了（怎么又说了一遍），这时候直接把RDD，输出到hdfs，每个分区一个文件，会先写到一个临时文件，如果全部输出完，进行rename，如果输出失败，就回滚delete。 标记状态为Checkpointed，markCheckpointed方法中清除所有的依赖，怎么清除依赖的呢，就是把RDD变量的强引用设置为null，垃圾回收了，会触发ContextCleaner里面监听清除实际BlockManager缓存中的数据。 CheckPoint 读流程如果一个RDD我们已经CheckPoint了那么是什么时候用呢，CheckPoint将RDD持久化到HDFS或本地文件夹，如果不被手动remove掉，是一直存在的，也就是说可以被下一个driverprogram使用。比如sparkstreaming挂掉了，重启后就可以使用之前CheckPoint的数据进行recover,当然在同一个driverprogram也可以使用。我们讲下在同一个driverprogram中是怎么使用CheckPoint数据的。如果一个RDD被CheckPoint了，当这个RDD上有action操作时候，或者回溯的这个RDD的时候，触发这个RDD进行计算，里面判断是否CheckPoint过，对分区和依赖的处理都是使用的RDD内部的CheckPointRDD变量。具体细节如下：如果一个RDD被CheckPoint了，那么这个RDD中对分区和依赖的处理都是使用的RDD内部的CheckPointRDD变量，具体实现是ReliableCheckpointRDD类型。这个是在CheckPoint写流程中创建的。依赖和获取分区方法中先判断是否已经CheckPoint，如果已经CheckPoint了，就斩断依赖，使用ReliableCheckpointRDD，来处理依赖和获取分区。如果没有，才往前回溯依赖。依赖就是没有依赖，因为已经斩断了依赖，获取分区数据就是读取CheckPoint到HDFS目录中不同分区保存下来的文件。 RDD 的依赖关系RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。 窄依赖：窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用。总结：窄依赖我们形象的比喻为独生子女。 宽依赖：宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition，会引起shuffle。总结：宽依赖我们形象的比喻为超生。 Lineage：RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。 1234567891011121314151617181920212223scala&gt; val text = sc.textFile("README.md")text: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:24scala&gt; val words = text.flatMap(_.split)split splitAtscala&gt; val words = text.flatMap(_.split(" "))words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at &lt;console&gt;:26scala&gt; words.map((_,1))res0: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at &lt;console&gt;:29scala&gt; res0.reduceByKeyreduceByKey reduceByKeyLocallyscala&gt; res0.reduceByKey(_+_)res1: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at &lt;console&gt;:31scala&gt; res1.dependenciesres2: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@6cfe48a4)scala&gt; res0.dependenciesres3: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.OneToOneDependency@6c9e24c4) DAG 的生成DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。 RDD 相关概念关系 输入可能以多个文件的形式存储在HDFS上，每个File都包含了很多块，称为Block。当Spark读取这些文件作为输入时，会根据具体数据格式对应的InputFormat进行解析，一般是将若干个Block合并成一个输入分片，称为InputSplit，注意InputSplit不能跨越文件。随后将为这些输入分片生成具体的Task。InputSplit与Task是一一对应的关系。随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。 每个节点可以起一个或多个Executor。 每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。 每个Task执行的结果就是生成了目标RDD的一个partiton。 注意: 这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。而Task被执行的并发度 = Executor数目 * 每个Executor核数。至于partition的数目： 对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。 在Map阶段partition数目保持不变。 在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。 RDD在计算的时候，每个分区都会起一个task，所以rdd的分区数目决定了总的的task数目。申请的计算节点（Executor）数目和每个计算节点核数，决定了你同一时刻可以并行执行的task。比如的RDD有100个分区，那么计算的时候就会生成100个task，你的资源配置为10个计算节点，每个两2个核，同一时刻可以并行的task数目为20，计算这个RDD就需要5个轮次。如果计算资源不变，你有101个task的话，就需要6个轮次，在最后一轮中，只有一个task在执行，其余核都在空转。如果资源不变，你的RDD只有2个分区，那么同一时刻只有2个task运行，其余18个核空转，造成资源浪费。这就是在spark调优中，增大RDD分区数目，增大任务并行度的做法。 键值对 RDD键值对RDD是Spark中许多操作所需要的常见数据类型。本章做特别讲解。除了在基础RDD类中定义的操作之外，Spark为包含键值对类型的RDD提供了一些专有的操作在PairRDDFunctions专门进行了定义。这些RDD被称为pairRDD。有很多种方式创建pairRDD，在输入输出章节会讲解。一般如果从一个普通的RDD转为pairRDD时，可以调用map()函数来实现，传递的函数需要返回键值对。 1val pairs = lines.map(x =&gt; (x.split(" ")(0), x)) Pair RDD 的 Transformation 操作转化操作上一章进行了练习，这一章会重点讲解。针对一个Pair RDD的转化操作： 聚合操作当数据集以键值对形式组织的时候，聚合具有相同键的元素进行一些统计是很常见的操作。之前讲解过基础RDD上的fold()、combine()、reduce()等行动操作，pairRDD上则有相应的针对键的转化操作。Spark有一组类似的操作，可以组合具有相同键的值。这些操作返回RDD，因此它们是转化操作而不是行动操作。reduceByKey()与reduce()相当类似;它们都接收一个函数，并使用该函数对值进行合并。reduceByKey()会为数据集中的每个键进行并行的归约操作，每个归约操作会将键相同的值合并起来。因为数据集中可能有大量的键，所以reduceByKey()没有被实现为向用户程序返回一个值的行动操作。实际上，它会返回一个由各键和对应键归约出来的结果值组成的新的RDD。foldByKey()则与fold()相当类似;它们都使用一个与RDD和合并函数中的数据类型相同的零值作为初始值。与fold()一样，foldByKey()操作所使用的合并函数对零值与另一个元素进行合并，结果仍为该元素。求均值操作：版本一 1input.mapValues(x =&gt; (x, 1)).reduceByKey((x, y) =&gt; (x._1 + y._1, x._2 + y._2)).map&#123; case (key, value) =&gt; (key, value._1 / value._2.toFloat) &#125; combineByKey()是最为常用的基于键进行聚合的函数。大多数基于键聚合的函数都是用它实现的。和aggregate()一样，combineByKey()可以让用户返回与输入数据的类型不同的返回值。要理解combineByKey()，要先理解它在处理数据时是如何处理每个元素的。由于combineByKey()会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。如果这是一个新的元素，combineByKey()会使用一个叫作createCombiner()的函数来创建那个键对应的累加器的初始值。需要注意的是，这一过程会在每个分区中第一次出现各个键时发生，而不是在整个RDD中第一次出现一个键时发生。如果这是一个在处理当前分区之前已经遇到的键，它会使用mergeValue()方法将该键的累加器对应的当前值与这个新的值进行合并。由于每个分区都是独立处理的，因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器，就需要使用用户提供的mergeCombiners()方法将各个分区的结果进行合并。 求均值操作：版本二 1234567val result = input.combineByKey( (v) =&gt; (v, 1), (acc: (Int, Int), v) =&gt; (acc._1 + v, acc._2 + 1), (acc1: (Int, Int), acc2: (Int, Int)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)).map&#123; case (key, value) =&gt; (key, value._1 / value._2.toFloat) &#125;result.collectAsMap().map(println(_)) 数据分组如果数据已经以预期的方式提取了键，groupByKey()就会使用RDD中的键来对数据进行分组。对于一个由类型K的键和类型V的值组成的RDD，所得到的结果RDD类型会是[K,Iterable[V]]。groupBy()可以用于未成对的数据上，也可以根据除键相同以外的条件进行分组。它可以接收一个函数，对源RDD中的每个元素使用该函数，将返回结果作为键再进行分组。多个RDD分组，可以使用cogroup函数，cogroup()的函数对多个共享同一个键的RDD进行分组。对两个键的类型均为K而值的类型分别为V和W的RDD进行cogroup()时，得到的结果RDD类型为[(K,(Iterable[V],Iterable[W]))]。如果其中的一个RDD对于另一个RDD中存在的某个键没有对应的记录，那么对应的迭代器则为空。cogroup()提供了为多个RDD进行数据分组的方法。 连接连接主要用于多个PairRDD的操作，连接方式多种多样:右外连接、左外连接、交叉连接以及内连接。普通的join操作符表示内连接2。只有在两个pairRDD中都存在的键才叫输出。当一个输入对应的某个键有多个值时，生成的pairRDD会包括来自两个输入RDD的每一组相对应的记录。leftOuterJoin()产生的pairRDD中，源RDD的每一个键都有对应的记录。每个键相应的值是由一个源RDD中的值与一个包含第二个RDD的值的Option(在Java中为Optional)对象组成的二元组。rightOuterJoin()几乎与leftOuterJoin()完全一样，只不过预期结果中的键必须出现在第二个RDD中，而二元组中的可缺失的部分则来自于源RDD而非第二个RDD。 数据排序sortByKey()函数接收一个叫作ascending的参数，表示我们是否想要让结果按升序排序(默认值为true)。 Pair RDD 的 Action 操作 Pair RDD 的数据分区Spark目前支持Hash分区和Range分区，用户也可以自定义分区，Hash分区为当前的默认分区，Spark中分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle过程属于哪个分区和Reduce的个数。注意： 只有Key-Value类型的RDD才有分区的，非Key-Value类型的RDD分区的值是None. 每个RDD的分区ID范围：0~numPartitions-1，决定这个值是属于那个分区的。 获取 RDD 的分区方式可以通过使用RDD的partitioner属性来获取RDD的分区方式。它会返回一个scala.Option对象，通过get方法获取其中的值。 1234567891011scala&gt; val pairs = sc.parallelize(List((1, 1), (2, 2), (3, 3)))pairs: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:24scala&gt; pairs.partitionerres26: Option[org.apache.spark.Partitioner] = Nonescala&gt; val partitioned = pairs.partitionBy(new org.apache.spark.HashPartitioner(2))partitioned: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[34] at partitionBy at &lt;console&gt;:26scala&gt; partitioned.partitionerres27: Option[org.apache.spark.Partitioner] = Some(org.apache.spark.HashPartitioner@2) Hash 分区方式HashPartitioner分区的原理：对于给定的key，计算其hashCode，并除于分区的个数取余，如果余数小于0，则用余数+分区的个数，最后返回的值就是这个key所属的分区ID。 12345678910111213141516171819scala&gt; val nopar = sc.parallelize(List((1,3),(1,2),(2,4),(2,3),(3,6),(3,8)),8)nopar: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24scala&gt; nopar.partitionerres20: Option[org.apache.spark.Partitioner] = Nonescala&gt;nopar.mapPartitionsWithIndex((index,iter)=&gt;&#123; Iterator(index.toString+" : "+iter.mkString("|")) &#125;).collectres0: Array[String] = Array("0 : ", 1 : (1,3), 2 : (1,2), 3 : (2,4), "4 : ", 5 : (2,3), 6 : (3,6), 7 : (3,8)) scala&gt; val hashpar = nopar.partitionBy(new org.apache.spark.HashPartitioner(7))hashpar: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[12] at partitionBy at &lt;console&gt;:26scala&gt; hashpar.countres18: Long = 6scala&gt; hashpar.partitionerres21: Option[org.apache.spark.Partitioner] = Some(org.apache.spark.HashPartitioner@7)scala&gt; hashpar.mapPartitions(iter =&gt; Iterator(iter.length)).collect()res19: Array[Int] = Array(0, 3, 1, 2, 0, 0, 0) Range 分区方式HashPartitioner分区弊端：可能导致每个分区中数据量的不均匀，极端情况下会导致某些分区拥有RDD的全部数据。RangePartitioner分区优势：尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大；但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。RangePartitioner作用：将一定范围内的数映射到某一个分区内，在实现中，分界的算法尤为重要。用到了水塘抽样算法。 自定义分区方式要实现自定义的分区器，你需要继承org.apache.spark.Partitioner类并实现下面三个方法。 numPartitions:Int:返回创建出来的分区数。 getPartition(key:Any):Int:返回给定键的分区编号(0到numPartitions-1)。 equals():Java判断相等性的标准方法。这个方法的实现非常重要，Spark需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样Spark才可以判断两个RDD的分区方式是否相同。 假设我们需要将相同后缀的数据写入相同的文件，我们通过将相同后缀的数据分区到相同的分区并保存输出来实现。 1234567891011121314151617181920212223242526package com.moqi.sparkimport org.apache.spark.&#123;Partitioner, SparkConf, SparkContext&#125;class CustomerPartitioner(numParts:Int) extends Partitioner &#123; //覆盖分区数 override def numPartitions: Int = numParts //覆盖分区号获取函数 override def getPartition(key: Any): Int = &#123; val ckey: String = key.toString ckey.substring(ckey.length-1).toInt%numParts &#125;&#125;object CustomerPartitioner &#123; def main(args: Array[String]) &#123; val conf=new SparkConf().setAppName("partitioner") val sc=new SparkContext(conf) val data=sc.parallelize(List("aa.2","bb.2","cc.3","dd.3","ee.5")) data.map((_,1)).partitionBy(new CustomerPartitioner(5)).keys.saveAsTextFile("hdfs://slave1:9000/partitioner") &#125;&#125; 1234567891011121314151617181920212223242526272829303132scala&gt; val data=sc.parallelize(List("aa.2","bb.2","cc.3","dd.3","ee.5").zipWithIndex,2)data: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[7] at parallelize at &lt;console&gt;:27scala&gt; data.collectres4: Array[(String, Int)] = Array((aa.2,0), (bb.2,1), (cc.3,2), (dd.3,3), (ee.5,4))scala&gt; data.mapPartitionsWithIndex((index,iter)=&gt;Iterator(index.toString +" : "+ iter.mkString("|"))).collectres5: Array[String] = Array(0 : (aa.2,0)|(bb.2,1), 1 : (cc.3,2)|(dd.3,3)|(ee.5,4))scala&gt; :paste// Entering paste mode (ctrl-D to finish)class CustomerPartitioner(numParts:Int) extends org.apache.spark.Partitioner&#123; //覆盖分区数 override def numPartitions: Int = numParts //覆盖分区号获取函数 override def getPartition(key: Any): Int = &#123; val ckey: String = key.toString ckey.substring(ckey.length-1).toInt%numParts &#125;&#125;// Exiting paste mode, now interpreting.defined class CustomerPartitionerscala&gt; data.partitionBy(new CustomerPartitioner(4))res7: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[9] at partitionBy at &lt;console&gt;:31scala&gt; res7.mapPartitionsWithIndex((index,iter)=&gt;Iterator(index.toString +" : "+ iter.mkString("|"))).collectres8: Array[String] = Array("0 : ", 1 : (ee.5,4), 2 : (aa.2,0)|(bb.2,1), 3 : (cc.3,2)|(dd.3,3)) 使用自定义的Partitioner是很容易的:只要把它传给partitionBy()方法即可。Spark中有许多依赖于数据混洗的方法，比如join()和groupByKey()，它们也可以接收一个可选的Partitioner对象来控制输出数据的分区方式。 分区 Shuffle 优化在分布式程序中，通信的代价是很大的，因此控制数据分布以获得最少的网络传输可以极大地提升整体性能。Spark中所有的键值对RDD都可以进行分区。系统会根据一个针对键的函数对元素进行分组。主要有哈希分区和范围分区，当然用户也可以自定义分区函数。通过分区可以有效提升程序性能。如下例子：分析这样一个应用，它在内存中保存着一张很大的用户信息表——也就是一个由(UserID,UserInfo)对组成的RDD，其中UserInfo包含一个该用户所订阅的主题的列表。该应用会周期性地将这张表与一个小文件进行组合，这个小文件中存着过去五分钟内发生的事件——其实就是一个由(UserID,LinkInfo)对组成的表，存放着过去五分钟内某网站各用户的访问情况。例如，我们可能需要对用户访问其未订阅主题的页面的情况进行统计。 1234567891011121314151617// 初始化代码，从 HDFS 的一个 Hadoop SequenceFile 读取用户信息// userData 中的元素会根据它们被读取时的来源，即 HDFS 块所在的节点来分布// Spark 此时无法获知某个特定的 User ID 对应的记录位于那个节点上val sc = new SparkContext(...)val userData = sc.sequenceFile[UserId, UserInfo]("hdfs://...").persist()// 周期性调用函数来处理过去五分钟产生的事件日志// 假设这是一个包含(UserID, LinkInfo)键值对的 SequenceFiledef processNewLogs(logFileName: String) &#123; val events = sc.sequenceFile[UserID, LinkInfo](logFileName) val joined = userData.join(events) // RDD of (UserID, (UserInfo, LinkInfo)) pairs val offTopicVisits = joined.filter &#123; // Expand the tuple into tis components case (userId, (userInfo, linkInfo)) =&gt; !userInfo.topics.contains(linkInfo.topic) &#125;.count() println("Number of visits to non-subscribed topics: " + offTopicVisits)&#125; 这段代码可以正确运行，但是不够高效。这是因为在每次调用processNewLogs()时都会用到join()操作，而我们对数据集是如何分区的却一无所知。默认情况下，连接操作会将两个数据集中的所有键的哈希值都求出来，将该哈希值相同的记录通过网络传到同一台机器上，然后在那台机器上对所有键相同的记录进行连接操作。因为userData表比每五分钟出现的访问日志表events要大得多，所以要浪费时间做很多额外工作:在每次调用时都对userData表进行哈希值计算和跨节点数据混洗，降低了程序的执行效率。优化方法： 1234val sc = new SparkContext(...)val userData = sc.sequenceFile[UserID, UserInfo]("hdfs://...") .partitionBy(new HashPartitioner(100)) // 构造 100 个分区 .persist() 我们在构建userData时调用了partitionBy()，Spark就知道了该RDD是根据键的哈希值来分区的，这样在调用join()时，Spark就会利用到这一点。具体来说，当调用userData.join(events)时，Spark只会对events进行数据混洗操作，将events中特定UserID的记录发送到userData的对应分区所在的那台机器上。这样，需要通过网络传输的数据就大大减少了，程序运行速度也可以显著提升了。 基于分区进行操作基于分区对数据进行操作可以让我们避免为每个数据元素进行重复的配置工作。诸如打开数据库连接或创建随机数生成器等操作，都是我们应当尽量避免为每个元素都配置一次的工作。Spark提供基于分区的mapPartition和foreachPartition，让你的部分代码只对RDD的每个分区运行一次，这样可以帮助降低这些操作的代价。 从分区中获益的操作能够从数据分区中获得性能提升的操作有cogroup()、groupWith()、join()、leftOuterJoin()、rightOuterJoin()、groupByKey()、reduceByKey()、combineByKey()以及lookup()等。 数据读取与保存主要方式文本文件当我们将一个文本文件读取为RDD时，输入的每一行都会成为RDD的一个元素。也可以将多个完整的文本文件一次性读取为一个pairRDD，其中键是文件名，值是文件内容。val input = sc.textFile(“./README.md”). 如果传递目录，则将目录下的所有文件读取作为RDD。文件路径支持通配符。通过wholeTextFiles()对于大量的小文件读取效率比较高，大文件效果没有那么高。Spark通过saveAsTextFile()进行文本文件的输出，该方法接收一个路径，并将RDD中的内容都输入到路径对应的文件中。Spark将传入的路径作为目录对待，会在那个目录下输出多个文件。这样，Spark就可以从多个节点上并行输出了。result.saveAsTextFile(outputFile). 123456789scala&gt; sc.textFile("./README.md")res6: org.apache.spark.rdd.RDD[String] = ./README.md MapPartitionsRDD[7] at textFile at &lt;console&gt;:25scala&gt; val readme = sc.textFile("./README.md")readme: org.apache.spark.rdd.RDD[String] = ./README.md MapPartitionsRDD[9] at textFile at &lt;console&gt;:24scala&gt; readme.collect()res7: Array[String] = Array(# Apache Spark, "", Spark is a fast and general cluster...scala&gt; readme.saveAsTextFile("hdfs://slave1:9000/test") JSON 文件如果JSON文件中每一行就是一个JSON记录，那么可以通过将JSON文件当做文本文件来读取，然后利用相关的JSON库对每一条数据进行JSON解析。 12345678910111213141516171819scala&gt; import org.json4s._ import org.json4s._scala&gt; import org.json4s.jackson.JsonMethods._ import org.json4s.jackson.JsonMethods._scala&gt; import org.json4s.jackson.Serialization import org.json4s.jackson.Serializationscala&gt; var result = sc.textFile("examples/src/main/resources/people.json")result: org.apache.spark.rdd.RDD[String] = examples/src/main/resources/people.json MapPartitionsRDD[7] at textFile at &lt;console&gt;:47scala&gt; implicit val formats = Serialization.formats(ShortTypeHints(List())) formats: org.json4s.Formats&#123;val dateFormat: org.json4s.DateFormat; val typeHints: org.json4s.TypeHints&#125; = org.json4s.Serialization$$anon$1@61f2c1dascala&gt; result.collect().foreach(x =&gt; &#123;var c = parse(x).extract[Person];println(c.name + "," + c.age)&#125;) Michael,30Andy,30Justin,19 如果JSON数据是跨行的，那么只能读入整个文件，然后对每个文件进行解析。JSON数据的输出主要是通过在输出之前将由结构化数据组成的RDD转为字符串RDD，然后使用Spark的文本文件API写出去。说白了还是以文本文件的形式存，只是文本的格式已经在程序中转换为JSON。 CSV 文件读取CSV/TSV数据和读取JSON数据相似，都需要先把文件当作普通文本文件来读取数据，然后通过将每一行进行解析实现对CSV的读取。CSV/TSV数据的输出也是需要将结构化RDD通过相关的库转换成字符串RDD，然后使用Spark的文本文件API写出去。 SequenceFile 文件SequenceFile文件是Hadoop用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。Spark有专门用来读取SequenceFile的接口。在SparkContext中，可以调用sequenceFile[keyClass, valueClass](path). 12345678910scala&gt; val data=sc.parallelize(List((2,"aa"),(3,"bb"),(4,"cc"),(5,"dd"),(6,"ee")))data: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[16] at parallelize at &lt;console&gt;:24scala&gt; data.saveAsSequenceFile("hdfs://slave1:9000/sequdata")scala&gt; val sdata = sc.sequenceFile[Int,String]("hdfs://slave1:9000/sdata/p*")sdata: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[19] at sequenceFile at &lt;console&gt;:24scala&gt; sdata.collect()res14: Array[(Int, String)] = Array((2,aa), (3,bb), (4,cc), (5,dd), (6,ee)) 可以直接调用saveAsSequenceFile(path)保存你的PairRDD，它会帮你写出数据。需要键和值能够自动转为Writable类型。 对象文件对象文件是将对象序列化后保存的文件，采用Java的序列化机制。可以通过objectFilek,v函数接收一个路径，读取对象文件，返回对应的RDD，也可以通过调用saveAsObjectFile()实现对对象文件的输出。因为是序列化所以要指定类型。 123456789101112scala&gt; val data=sc.parallelize(List((2,"aa"),(3,"bb"),(4,"cc"),(5,"dd"),(6,"ee")))data: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[20] at parallelize at &lt;console&gt;:24scala&gt; data.saveAsObjectFile("hdfs://slave1:9000/objfile")scala&gt; import org.apache.spark.rdd.RDDimport org.apache.spark.rdd.RDDscala&gt; val objrdd:RDD[(Int,String)] = sc.objectFile[(Int,String)]("hdfs://slave1:9000/objfile/p*")objrdd: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[28] at objectFile at &lt;console&gt;:25scala&gt; objrdd.collect()res20: Array[(Int, String)] = Array((2,aa), (3,bb), (4,cc), (5,dd), (6,ee)) Hadoop 文件Spark的整个生态系统与Hadoop是完全兼容的,所以对于Hadoop所支持的文件类型或者数据库类型,Spark也同样支持.另外,由于Hadoop的API有新旧两个版本,所以Spark为了能够兼容Hadoop所有的版本,也提供了两套创建操作接口.对于外部存储创建操作而言,hadoopRDD和newHadoopRDD是最为抽象的两个函数接口,主要包含以下四个参数。 输入格式(InputFormat): 制定数据输入的类型,如TextInputFormat等,新旧两个版本所引用的版本分别是org.apache.hadoop.mapred.InputFormat和org.apache.hadoop.mapreduce.InputFormat(NewInputFormat) 键类型: 指定[K,V]键值对中K的类型 值类型: 指定[K,V]键值对中V的类型 分区值: 指定由外部存储生成的RDD的partition数量的最小值,如果没有指定,系统会使用默认值defaultMinSplits 其他创建操作的API接口都是为了方便最终的Spark程序开发者而设置的,是这两个接口的高效实现版本.例如,对于textFile而言,只有path这个指定文件路径的参数,其他参数在系统内部指定了默认值。 注意: 在Hadoop中以压缩形式存储的数据,不需要指定解压方式就能够进行读取,因为Hadoop本身有一个解压器会根据压缩文件的后缀推断解压算法进行解压。 如果用Spark从Hadoop中读取某种类型的数据不知道怎么读取的时候,上网查找一个使用map-reduce的时候是怎么读取这种这种数据的,然后再将对应的读取方式改写成兼容版本的的hadoopRDD和newAPIHadoopRDD两个类就行了。 读取示例： 1234scala&gt; val data = sc.parallelize(Array((30,"hadoop"), (71,"hive"), (11,"cat")))data: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[47] at parallelize at &lt;console&gt;:35scala&gt; data.saveAsNewAPIHadoopFile("hdfs://slave1:9000/output4/",classOf[LongWritable] ,classOf[Text] ,classOf[org.apache.hadoop.mapreduce.lib.output.TextOutputFormat[LongWritable, Text]]) 对于RDD最后的归宿除了返回为集合和标量，也可以将RDD存储到外部文件系统或者数据库中，Spark系统与Hadoop是完全兼容的，所以MapReduce所支持的读写文件或者数据库类型，Spark也同样支持。另外，由于Hadoop的API有新旧两个版本，所以Spark为了能够兼容Hadoop所有的版本，也提供了两套API。将RDD保存到HDFS中在通常情况下需要关注或者设置五个参数，即文件保存的路径，key值的class类型，Value值的class类型，RDD的输出格式(OutputFormat，如TextOutputFormat/SequenceFileOutputFormat)，以及最后一个相关的参数codec(这个参数表示压缩存储的压缩形式，如DefaultCodec，Gzip，Codec等等)。 兼容旧版API saveAsObjectFile(path: String): Unit saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec]): Unit saveAsTextFile(path: String): Unit saveAsHadoopFile[F &lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unit saveAsHadoopFile[F &lt;: OutputFormat[K, V]](path: String, codec: Class[_ &lt;: CompressionCodec])(implicit fm: ClassTag[F]): Unit saveAsHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[&lt;: OutputFormat[, ]], codec: Class[ &lt;: CompressionCodec]): Unit saveAsHadoopDataset(conf: JobConf): Unit 这里列出的API，前面6个都是saveAsHadoopDataset的简易实现版本，仅仅支持将RDD存储到HDFS中，而saveAsHadoopDataset的参数类型是JobConf，所以其不仅能够将RDD存储到HDFS中，也可以将RDD存储到其他数据库中，如Hbase，MangoDB，Cassandra等。 兼容新版API saveAsNewAPIHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[_ &lt;: OutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration): Unit saveAsNewAPIHadoopFile[F &lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unit saveAsNewAPIHadoopDataset(conf: Configuration): Unit 同样的，前2个API是saveAsNewAPIHadoopDataset的简易实现，只能将RDD存到HDFS中，而saveAsNewAPIHadoopDataset比较灵活.新版的API没有codec的参数，所以要压缩存储文件到HDFS中每需要使用hadoopConfiguration参数，设置对应mapreduce.map.output.compress.codec参数和mapreduce.map.output.compress参数。注意：如果不知道怎么将RDD存储到Hadoop生态的系统中，主要上网搜索一下对应的map-reduce是怎么将数据存储进去的，然后改写成对应的saveAsHadoopDataset或saveAsNewAPIHadoopDataset就可以了。 写入示例： 12345scala&gt; val read = sc.newAPIHadoopFile[LongWritable, Text, org.apache.hadoop.mapreduce.lib.input.TextInputFormat]("hdfs://slave1:9000/output3/part*", classOf[org.apache.hadoop.mapreduce.lib.input.TextInputFormat], classOf[LongWritable], classOf[Text])read: org.apache.spark.rdd.RDD[(org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text)] = hdfs://slave1:9000/output3/part* NewHadoopRDD[48] at newAPIHadoopFile at &lt;console&gt;:35scala&gt; read.map&#123;case (k, v) =&gt; v.toString&#125;.collectres44: Array[String] = Array(30 hadoop, 71 hive, 11 cat) 文件系统Spark支持读写很多种文件系统，像本地文件系统、Amazon S3、HDFS 等。 数据库关系型数据库连接支持通过Java JDBC访问关系型数据库。需要通过Jdbc RDD进行，示例如下:MySQL 读取： 1234567891011121314151617181920def main (args: Array[String] ) &#123; val sparkConf = new SparkConf ().setMaster ("local[2]").setAppName ("JdbcApp") val sc = new SparkContext (sparkConf) val rdd = new org.apache.spark.rdd.JdbcRDD ( sc, () =&gt; &#123; Class.forName ("com.mysql.jdbc.Driver").newInstance() java.sql.DriverManager.getConnection ("jdbc:mysql://slave1:3306/rdd", "root", "hive") &#125;, "select * from rddtable where id &gt;= ? and id &lt;= ?;", 1, 10, 1, r =&gt; (r.getInt(1), r.getString(2))) println (rdd.count () ) rdd.foreach (println (_) ) sc.stop ()&#125; MySQL 写入： 1234567891011121314151617def main(args: Array[String]) &#123; val sparkConf = new SparkConf().setMaster("local[2]").setAppName("HBaseApp") val sc = new SparkContext(sparkConf) val data = sc.parallelize(List("Female", "Male","Female")) data.foreachPartition(insertData)&#125;def insertData(iterator: Iterator[String]): Unit = &#123;Class.forName ("com.mysql.jdbc.Driver").newInstance() val conn = java.sql.DriverManager.getConnection("jdbc:mysql://slave1:3306/rdd", "root", "hive") iterator.foreach(data =&gt; &#123; val ps = conn.prepareStatement("insert into rddtable(name) values (?)") ps.setString(1, data) ps.executeUpdate() &#125;)&#125; JdbcRDD 接收这样几个参数： 首先，要提供一个用于对数据库创建连接的函数。这个函数让每个节点在连接必要的配 置后创建自己读取数据的连接。 接下来，要提供一个可以读取一定范围内数据的查询，以及查询参数中lowerBound和 upperBound 的值。这些参数可以让 Spark 在不同机器上查询不同范围的数据，这样就不 会因尝试在一个节点上读取所有数据而遭遇性能瓶颈。 这个函数的最后一个参数是一个可以将输出结果从转为对操作数据有用的格式的函数。如果这个参数空缺，Spark会自动将每行结果转为一个对象数组。 HBase 数据库由于 org.apache.hadoop.hbase.mapreduce.TableInputFormat 类的实现，Spark 可以通过Hadoop输入格式访问HBase。这个输入格式会返回键值对数据，其中键的类型为org.apache.hadoop.hbase.io.ImmutableBytesWritable，而值的类型为org.apache.hadoop.hbase.client.Result.HBase 读取： 123456789101112131415161718192021222324def main(args: Array[String]) &#123; val sparkConf = new SparkConf().setMaster("local[2]").setAppName("HBaseApp") val sc = new SparkContext(sparkConf) val conf = HBaseConfiguration.create() //HBase中的表名 conf.set(TableInputFormat.INPUT_TABLE, "fruit") val hBaseRDD = sc.newAPIHadoopRDD(conf, classOf[TableInputFormat], classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable], classOf[org.apache.hadoop.hbase.client.Result]) val count = hBaseRDD.count() println("hBaseRDD RDD Count:"+ count) hBaseRDD.cache() hBaseRDD.foreach &#123; case (_, result) =&gt; val key = Bytes.toString(result.getRow) val name = Bytes.toString(result.getValue("info".getBytes, "name".getBytes)) val color = Bytes.toString(result.getValue("info".getBytes, "color".getBytes)) println("Row key:" + key + " Name:" + name + " Color:" + color) &#125; sc.stop()&#125; HBase 写入： 12345678910111213141516171819202122232425262728293031def main(args: Array[String]) &#123; val sparkConf = new SparkConf().setMaster("local[2]").setAppName("HBaseApp") val sc = new SparkContext(sparkConf) val conf = HBaseConfiguration.create() val jobConf = new JobConf(conf) jobConf.setOutputFormat(classOf[TableOutputFormat]) jobConf.set(TableOutputFormat.OUTPUT_TABLE, "fruit_spark") val fruitTable = TableName.valueOf("fruit_spark") val tableDescr = new HTableDescriptor(fruitTable) tableDescr.addFamily(new HColumnDescriptor("info".getBytes)) val admin = new HBaseAdmin(conf) if (admin.tableExists(fruitTable)) &#123; admin.disableTable(fruitTable) admin.deleteTable(fruitTable) &#125; admin.createTable(tableDescr) def convert(triple: (Int, String, Int)) = &#123; val put = new Put(Bytes.toBytes(triple._1)) put.addImmutable(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes(triple._2)) put.addImmutable(Bytes.toBytes("info"), Bytes.toBytes("price"), Bytes.toBytes(triple._3)) (new ImmutableBytesWritable, put) &#125; val initialRDD = sc.parallelize(List((1,"apple",11), (2,"banana",12), (3,"pear",13))) val localData = initialRDD.map(convert) localData.saveAsHadoopDataset(jobConf)&#125; Cassandra 和 ElasticSearch RDD 编程进阶累加器累加器用来对信息进行聚合，通常在向Spark传递函数时，比如使用map()函数或者用filter()传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，更新这些副本的值也不会影响驱动器中的对应变量。如果我们想实现所有分片处理时更新共享变量的功能，那么累加器可以实现我们想要的效果。针对一个输入的日志文件，如果我们想计算文件中所有空行的数量，我们可以编写以下程序： 1234567891011121314151617181920scala&gt; val notice = sc.textFile("./NOTICE")notice: org.apache.spark.rdd.RDD[String] = ./NOTICE MapPartitionsRDD[40] at textFile at &lt;console&gt;:32scala&gt; val blanklines = sc.accumulator(0)warning: there were two deprecation warnings; re-run with -deprecation for detailsblanklines: org.apache.spark.Accumulator[Int] = 0scala&gt; val tmp = notice.flatMap(line =&gt; &#123; | if (line == "") &#123; | blanklines += 1 | &#125; | line.split(" ") | &#125;)tmp: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[41] at flatMap at &lt;console&gt;:36scala&gt; tmp.count()res31: Long = 3213scala&gt; blanklines.valueres32: Int = 171 累加器的用法如下所示。通过在驱动器中调用SparkContext.accumulator(initialValue)方法，创建出存有初始值的累加器。返回值为org.apache.spark.Accumulator[T]对象，其中T是初始值initialValue的类型。Spark闭包里的执行器代码可以使用累加器的+=方法(在Java中是add)增加累加器的值。 驱动器程序可以调用累加器的value属性(在Java中使用value()或setValue())来访问累加器的值。注意：工作节点上的任务不能访问累加器的值。从这些任务的角度来看，累加器是一个只写变量。对于要在行动操作中使用的累加器，Spark只会把每个任务对各累加器的修改应用一次。因此，如果想要一个无论在失败还是重复计算时都绝对可靠的累加器，我们必须把它放在foreach()这样的行动操作中。转化操作中累加器可能会发生不止一次更新。 自定义累加器自定义累加器类型的功能在1.X版本中就已经提供了，但是使用起来比较麻烦，在2.0版本后，累加器的易用性有了较大的改进，而且官方还提供了一个新的抽象类：AccumulatorV2来提供更加友好的自定义类型累加器的实现方式。实现自定义类型累加器需要继承AccumulatorV2并至少覆写下例中出现的方法，下面这个累加器可以用于在程序运行过程中收集一些文本类信息，最终以Set[String]的形式返回。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.moqi.sparkimport org.apache.spark.util.AccumulatorV2import org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.collection.JavaConversions._class LogAccumulator extends org.apache.spark.util.AccumulatorV2[String, java.util.Set[String]] &#123; private val _logArray: java.util.Set[String] = new java.util.HashSet[String]() override def isZero: Boolean = &#123; _logArray.isEmpty &#125; override def reset(): Unit = &#123; _logArray.clear() &#125; override def add(v: String): Unit = &#123; _logArray.add(v) &#125; override def merge(other: org.apache.spark.util.AccumulatorV2[String, java.util.Set[String]]): Unit = &#123; other match &#123; case o: LogAccumulator =&gt; _logArray.addAll(o.value) &#125; &#125; override def value: java.util.Set[String] = &#123; java.util.Collections.unmodifiableSet(_logArray) &#125; override def copy():org.apache.spark.util.AccumulatorV2[String, java.util.Set[String]] = &#123; val newAcc = new LogAccumulator() _logArray.synchronized&#123; newAcc._logArray.addAll(_logArray) &#125; newAcc &#125;&#125;// 过滤掉带字母的object LogAccumulator &#123; def main(args: Array[String]) &#123; val conf=new SparkConf().setAppName("LogAccumulator") val sc=new SparkContext(conf) val accum = new LogAccumulator sc.register(accum, "logAccum") val sum = sc.parallelize(Array("1", "2a", "3", "4b", "5", "6", "7cd", "8", "9"), 2).filter(line =&gt; &#123; val pattern = """^-?(\d+)""" val flag = line.matches(pattern) if (!flag) &#123; accum.add(line) &#125; flag &#125;).map(_.toInt).reduce(_ + _) println("sum: " + sum) for (v &lt;- accum.value) print(v + "") println() sc.stop() &#125;&#125; 广播变量广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，甚至是机器学习算法中的一个很大的特征向量，广播变量用起来都很顺手。传统方式下，Spark会自动把闭包中所有引用到的变量发送到工作节点上。虽然这很方便，但也很低效。原因有二:首先，默认的任务发射机制是专门为小任务进行优化的；其次，事实上你可能会在多个并行操作中使用同一个变量，但是Spark会为每个任务分别发送。 12345scala&gt; val broadcastVar = sc.broadcast(Array(1, 2, 3))broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(35)scala&gt; broadcastVar.valueres33: Array[Int] = Array(1, 2, 3) 使用广播变量的过程如下： 通过对一个类型 T 的对象调用 SparkContext.broadcast 创建出一个 Broadcast[T] 对象。 任何可序列化的类型都可以这么实现。 通过 value 属性访问该对象的值(在 Java 中为 value() 方法)。 变量只会被发到各个节点一次，应作为只读值处理(修改这个值不会影响到别的节点)。 End.]]></content>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM GC 发展历程]]></title>
    <url>%2F2017%2F06%2F16%2FJVM-GC-Development-Path%2F</url>
    <content type="text"><![CDATA[本文主要介绍 JVM GC 的发展。 较早的 From - To 阶段 最早的垃圾收集 From - To 架构的 GC，把整个堆内存分成大小差不多相等的两部分，中间有一个分配的指针（Free Point），对指针设定目标值（比如 From 区域的 80%）时，触发一次 GC。GC 触发时应用进入 Stop-The-World 状态，这时垃圾回收器检查 From 区域有哪些是可以回收的那些不是，将不可以回收的拷贝到 To 区域，其他回收。一次 GC 操作完成的时候完成区域交换（From 转换为 To 区域，To 转换为 From 区域），然后指针分配内存开始从新的 From 区域开始。 这种纯粹的拷贝垃圾回收方法最大的问题在于堆内存里面永远只可以用一半的内存，所以有一半的堆是浪费的。但在当时而言还是比较领先的，比如相对于引用计数的垃圾回收方法。引用计数垃圾回收的问题在于：引用计数在计数的时候需要维持一个锁的消耗，会降低分配内存的速度；另外一个是在循环引用中，这个消耗会更大。 回收分代的思想 将堆内存分代治理建立于这样一个假设之上：代际假设 , 核心论点有两个： 大多数对象很快就会被闲置； 少部分活下来的对象会存在相当长一段时间。 因此，JVM GC 分代治理的核心基础是以下两个：（1）大多数对象都会在年轻代死亡。（2）年老代引用年轻代的对象只占很小的一部分。分代治理会发生不同区域的 GC。在年轻代发生的 GC 称为 “minor GC”，在年老代出现的 GC 称为 “major GC” 或者 “full GC”。 根据代际假设构建的堆内存首先避免了全盘扫描，这个时期的 JVM GC 发展为如上图所示结构，分为年轻代，年老代，永生代。年轻代分为 eden 区与两个 survivor 区，s0 和 s1 实现是最初的 From - To 架构，在这里我们假设 s0 为 From 区，s1 为 To 区。创建的对象首先进入 eden 区，如果发生 minor GC，eden 区中大部分对象被回收，小部分对象拷贝到 To 区，From 也拷贝到 To 区；如果在 eden 中的对象太大不能拷贝到 To 区，则会被直接移动到年老代。每次 minor GC 时 From 和 To 会交换，每交换一次区内的对象年龄会加一，当年龄到达一定值（比如15）（注：这一块在后面的 GC 实现了动态调整）的时候，这些大龄的对象也被移动到了年老代。 但是这里会发生一个问题：如果年老代的对象需要引用年轻代的对象怎么办？为了处理这些情况，年老代中有一种称为”CardTable” (卡表) 的东西，它是一个 512 字节的块。每当年老代中的对象引用年轻代中的对象时，它就会记录在此表中。当发生 minor GC 时，仅搜索该卡表以确定它是否是年轻代 GC 需要回收的对象，而不是检查旧代中的所有对象的引用。 当数据已满时，触发年老代执行 GC 。执行程序因 GC 类型而异，根据JDK 7，有5种GC类型。 Serial GC Parallel GC Parallel Old GC (Parallel Compacting GC) Concurrent Mark &amp; Sweep GC (or “CMS”) Garbage First (G1) GCSerial GC(-XX:+UseSerialGC)，即串行 GC，使用被称为 “mark-sweep-compant” 的算法。 第一步：标记年老代中幸存的对象。（标记 - mark） 第二步：从堆的最前面开始检查，只留下幸存的堆。（扫描 - sweep） 第三步：把对象从最前面开始填充，以便连续堆积对象，并将堆分为包含对象和不包含对象的两部分。（紧凑 - compant） 串行 GC 适用于小内存和少量 CPU 内核的 JVM。Parallel GC(-XX:+UseParallelGC)，即并行 GC，和串行 GC 最大的区别是用多个线程来处理 GC，因此更快，当有足够的内存和 CPU 资源时，此 GC 非常有用，它也被称为”吞吐量 GC“。 Parallel Old GC (-XX:+UseParallelOldGC)，并行旧 GC，自 JDK 5 更新以来开始支持，与并行 GC 相比，唯一的区别是老年代的 GC 算法。它经历了三个步骤：mark – summary – compaction（标记 - 摘要 - 压缩）。”摘要“步骤经历了一些更复杂的步骤。 CMS GC （-XX：+ UseConcMarkSweepGC），从下图中可以看出，CMS GC 相对于前三个复杂得多。第一步 “Initial Mark” (初始标记) 很简单，搜索最接近类加载器的对象中的幸存对象，因此暂停时间很短。在 “Concurrent Mark” (并发标记) 步骤中，跟踪并检查刚刚确认的幸存对象引用的对象。这一步的不同之处在于它在同时处理其他线程的同时继续进行。在 “Remark” (再次标记) 步骤中，将检查在并发标记步骤中新增加或停止引用的对象。最后，在 “Concurrent Sweep” (并发扫描) 步骤中进行垃圾回收，垃圾回收和其他线程同步进行。由于 CMS GC 独特的运行方式，因此 GC 的暂停时间非常短。CMS GC 也称为低延迟 GC。在应用程序的响应时间至关重要时使用。这种 GC 的主要缺点如下：（1）它比其他 GC 类型使用更多的内存和 CPU。（2）默认情况下不提供压缩步骤。因而如果由于许多内存碎片而需要执行压缩任务，那么 GC 需要的静止时间可能会比其他任何 GC 方式都要长。所以，如果在使用 CMS GC 的时候要尤其注意压缩任务执行的频率和持续时间。 G1 GC G1 是 Garbage First 的缩写，最开始的 G1 完全抛弃分代收集的思想，开始于 JDK 1.7 Update 4，可以视为上图中所有的 “E”, “O”, “S” 标志消失，只分成不同的 Region (块)。实际上现在的 G1 GC 也有了分代的逻辑。G1 的实现一直以来都在不断的变化之中。 由前文所知每种 GC 都有出现的历史背景，比如串行 GC 是在内存和 CPU 比较小的情况下出现的；并行 GC 是在吞吐量出现巨大需求的时候出现的；而 CMS GC 则是对低延迟有了更高的需求，尽量拖延 full GC 出现的时间。那么，出现 G1这种形式的垃圾收集器的原因是什么？在 JVM GC 不断发展的过程中，已经出现了自适应的堆，也就是不需要手动调节年轻代与年老代的比例，以及年轻代对象进入年老代的年龄。这些自适应堆对更灵活的堆块产生了强烈的需求：如果将堆空间划分为很多个 Region，G1 可以将某一块指定为各种不同的代（可以是年老代，Eden 或者 Survivor 区），而且各个块在空间上不需要连续的在一起，有一个 List 将它们组织在一起，这样 G1 就很容易调整各个代之间的比例。 为什么 G1 被称为 G1？ G1 会在内部维护一个优先列表，通过一个合理的模型，计算出每个 Region 的收集成本和收益期望并量化，这样每次进行 GC 时，G1 总是会选择最适合的 Region（通常垃圾比较多）进行回收，使 GC 时间满足设置的条件。 G1通过引入 Remembered Set 来避免全堆扫描（前面所说的 CardTable 是其的一种实现）。Remembered Set 用于跟踪对象引用。G1 中每个 Region 都有对应的 Remembered Set 。当 JVM 发现内部的一个引用关系需要更新（对 Reference 类型进行写操作），则立即产生一个 Write Barrier 中断这个写操作，并检查Reference 引用的对象是否处于不同的 Region 之间（用分代的思想，就是新生代和老年代之间的引用）。如果是，则通过 CardTable 通知 G1，G1 根据 CardTable 把相关引用信息记录到被引用对象所属的 Region 的Remembered Set 中，并将 Remembered Set 加入 GC Root 。这样，在 G1 进行根节点枚举时就可以扫描到该对象而不会出现遗漏。 通俗解释第二条：如果一个 Region 的 Reference 越少，JVM 倾向于认为这块 Region 里面活着的对象越少，这个 Region 块是可回收的垃圾块的百分比就越大，这样回收这个 Region 的收益就越大。所以称这种算法为 Garbage First. G1 GC 年轻代的回收 当 JVM 启动时基于启动参数，JVM 要求操作系统分配一个大的连续内存块来托管 JVM 的堆，被划分为 2048 个 Region (块)。 年轻代的块发生 “minor GC”，将活着的对象拷贝到 survivor 块（依然是 From - To 算法）。如果对象过大或者对象的年龄足够，会拷贝到年老代。 结果：图三有新增的 “Recently Copied” 两块。 G1 GC 年老代的回收 初始标记阶段：初始标记的活着的对象在年轻代的垃圾收集上。在日志中，被标记为 GC pause (young) (inital-mark). 并行标记阶段，如果找到空区域（由”X”表示），则在 Remark 阶段立即将它们移除。此外，计算确定活跃度的信息。 Remark 阶段，空区域被移除并回收，现在计算所有区域的区域活跃度。 复制清理阶段，G1 选择具有最低”活跃度“的区域（比如引用其他 Region 最少的区域），那些可以最快收集的区域。这些区域与年轻代 GC 同时收集。这在日志中表示为 [GC pause (mixed)]。G1 实际上将 Stop-The-World 的操作放在一个时间区间，这样对应用性能和稳定性较好。 复制清理阶段后，选择的区域已经被收集并压缩成图中所示的深蓝色区域和深绿色区域。 JDK 8 中 JVM 的调整JDK 8 的 JVM 去掉了永生代(PermGen)，用 Metaspace 来代替。Metaspace 使用系统的内存。 参考https://www.cubrid.org/blog/understanding-java-garbage-collection https://www.oracle.com/technetwork/tutorials/tutorials-1876574.html https://blog.idrsolutions.com/2017/05/g1gc-java-9-garbage-collector-explained-5-minutes/ https://www.sczyh30.com/posts/Java/jvm-gc-hotspot-implements/ https://software.intel.com/en-us/blogs/2014/06/18/part-1-tuning-java-garbage-collection-for-hbase https://blog.csdn.net/elinespace/article/details/78852469]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 日志相关命令整理]]></title>
    <url>%2F2017%2F06%2F03%2FLinux-Check-Log-Commands%2F</url>
    <content type="text"><![CDATA[本文主要介绍开发中一些日志的常用操作。 tailn 是显示行号；相当于 nl 命令；例子如下： tail -100f test.log 实时监控 100 行日志 tail -n 10 test.log 查询日志尾部最后 10 行的日志; tail -n +10 test.log 查询 10 行之后的所有日志; head跟 tail 是相反的，tail 是看后多少行日志；例子如下： head -n 10 test.log 查询日志文件中的头 10 行日志; head -n -10 test.log 查询日志文件除了最后 10 行的其他所有日志; cattac 是倒序查看，是 cat 单词反写；例子如下： cat -n test.log |grep “debug” 查询关键字的日志 vim进入编辑查找：vi(vim) 进入vim编辑模式： vim filename vim +n filename 进入特定行号日志 输入命令“set nu” 显示行号 输入“/关键字”,按enter键查找 查找下一个，按“n”即可 退出：按ESC键后，接着再输入:号时，vi会在屏幕的最下方等待我们输入命令 wq! 保存退出； q! 不保存退出； 切换方向 /关键字 注：正向查找，按n键把光标移动到下一个符合条件的地方 ?关键字 注：反向查找，按shift+n 键，把光标移动到下一个符合条件的 搜索关键字附近的日志 最常用的：cat -n filename |grep “关键字” 其他情况： cat filename | grep -C 5 ‘关键字’ (显示日志里匹配字串那行以及前后5行) cat filename | grep -B 5 ‘关键字’ (显示匹配字串及前5行) cat filename | grep -A 5 ‘关键字’ (显示匹配字串及后5行) 按行号查看 - 过滤出关键字附近的日志 cat -n test.log |grep “debug” 得到关键日志的行号 cat -n test.log |tail -n +92|head -n 20 选择关键字所在的中间一行. 然后查看这个关键字后 20 行的日志: tail -n +92 表示查询第 92 行之后的日志 head -n 20 则表示在前面的查询结果里再查后 20 条记录 根据日期查询日志sed -n ‘/2014-12-17 16:17:20/,/2014-12-17 16:17:36/p’ test.log 特别说明:上面的两个日期必须是日志中打印出来的日志，否则无效； 先 grep ‘2014-12-17 16:17:20’ test.log 来确定日志中是否有该 时间点 日志内容特别多，打印在屏幕上不方便查看 使用 more 和 less 命令，如： cat -n test.log |grep “debug” |more 这样就分页打印了,通过点击空格键翻页 使用 &gt;xxx.txt 将其保存到文件中，到时可以拉下这个文件分析，如：cat -n test.log |grep “debug” &gt; debug.txt 参考https://blog.csdn.net/yangkai_hudong/article/details/47783487 https://www.cnblogs.com/hunt/p/7064886.html https://blog.csdn.net/dingnning/article/details/7189862]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA - Debugger 经验总结]]></title>
    <url>%2F2017%2F06%2F02%2FIDEA-Debugger%2F</url>
    <content type="text"><![CDATA[本文主要介绍 IDEA - Debugger 的一些操作。每个都有场景和操作说明，动态 Gif 图体积较大，请耐心等待。 觉得图比较小的单击查看大图。 分析外部堆栈跟踪把报错信息复制到 Analyze -&gt; Analyze Stacktrace，快速进入程序块。开发中经常可以看到生产环境有错误日志，依照此方法快速将日志导入项目，定位问题。 场景： 操作： 返回到前一个堆栈帧IDEA 可在程序的执行流程中回退到先前的堆栈帧。要求不是最上面入口方法，选择 Drop Frame 后，等于未进入调用的方法。请注意：已经对全局状态进行的更改不会被恢复，只有本地变量会被重置。 强制从当前方法返回在当前堆栈帧中右键单击选择 Force Return 然后根据需要的返回类型输入即可。 抛出一个异常在当前堆栈帧中右键单击选择 Throw Exception 然后手动输入异常即可，比如 new NullPointerException(); 重新加载修改的类一般而言应用于在 Debugger 时发现未调用的方法有需要改动的地方，这时候修改未调用的方法，然后选择 Run -&gt; Reload Changed Classes, 快捷键 Alt + U, 然后 A. 这时候 Debugger 继续进行调用，则执行的调用方法逻辑为重新编译之后。底层逻辑是用到 JVM 的 hotSwap. 分析 Java Stream 操作IDEA Debugger 时可以可视化 Java Stream 进行的操作和对值数据的影响，需要断点停留在 Stream 上点击 Trace Current Stream Chain 按钮。 参考https://www.jetbrains.com/help/idea/analyzing-external-stacktraces.html https://www.jetbrains.com/help/idea/altering-the-program-s-execution-flow.html https://www.jetbrains.com/help/idea/analyze-java-stream-operations.html]]></content>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Start]]></title>
    <url>%2F2017%2F06%2F01%2FStart%2F</url>
    <content type="text"><![CDATA[本文简要讨论了博客起源。 博客搭建于 20170601，儿童节做出的决定。 目前定位于分享技术和个人思考，狭义来讲技术在最近一段时间是开发工具 IntelliJ IDEA 的一些最佳实践，个人思考在最近一段时间是关于一些书的读后感。 目前对 Github + Hexo + NexT 刚开始熟悉，有建议欢迎联系邮箱。 Ernest Hemingway once wrote:”The world is a fine place，and worth fighting for.”I agree with the second part.]]></content>
      <tags>
        <tag>default</tag>
      </tags>
  </entry>
</search>
